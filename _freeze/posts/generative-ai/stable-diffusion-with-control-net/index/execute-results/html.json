{
  "hash": "2f0eb502f8bde74375a144aa526fc93a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Complete Guide to Stable Diffusion with ControlNet\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-07-22\"\ncategories: [code, research, intermediate]\nformat:\n  html:\n    code-fold: false\n    math: mathjax\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# Complete Guide to Stable Diffusion with ControlNet\n![](sd-cn.png)\n\n## Introduction\n\nControlNet is a neural network architecture that allows you to control Stable Diffusion image generation with additional input conditions like edge maps, depth maps, poses, and more. It provides precise control over the composition, structure, and layout of generated images while maintaining the creative power of diffusion models.\n\n### Key Benefits\n\n- **Precise Control**: Direct influence over image structure and composition\n- **Consistency**: Maintain specific poses, edges, or layouts across generations\n- **Flexibility**: Multiple conditioning types for different use cases\n- **Quality**: Enhanced output quality with structured guidance\n\n## Installation & Setup\n\n### Environment Setup\n\n```{.bash}\n# Create conda environment\nconda create -n controlnet python=3.10\nconda activate controlnet\n\n# Install PyTorch with CUDA support\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# Install core dependencies\npip install diffusers transformers accelerate\npip install controlnet-aux\npip install opencv-python\npip install xformers  # Optional but recommended for performance\n```\n\n### Required Libraries\n\n::: {#imports .cell execution_count=1}\n``` {.python .cell-code code-summary=\"Import required libraries\"}\nimport torch\nimport numpy as np\nimport cv2\nfrom PIL import Image\nfrom diffusers import (\n    StableDiffusionControlNetPipeline,\n    ControlNetModel,\n    UniPCMultistepScheduler\n)\nfrom controlnet_aux import (\n    CannyDetector,\n    OpenposeDetector,\n    MidasDetector,\n    HEDdetector,\n    MLSDdetector,\n    LineartDetector,\n    LineartAnimeDetector\n)\nfrom transformers import pipeline\n```\n:::\n\n\n### Basic Setup Function\n\n::: {#setup-pipeline .cell execution_count=2}\n``` {.python .cell-code code-summary=\"Setup ControlNet pipeline\"}\ndef setup_controlnet_pipeline(controlnet_type=\"canny\", model_id=\"runwayml/stable-diffusion-v1-5\"):\n    \"\"\"\n    Setup ControlNet pipeline with specified type and model\n    \n    Args:\n        controlnet_type: Type of ControlNet ('canny', 'openpose', 'depth', etc.)\n        model_id: Base Stable Diffusion model to use\n    \n    Returns:\n        Configured pipeline\n    \"\"\"\n    # ControlNet model mapping\n    controlnet_models = {\n        \"canny\": \"lllyasviel/sd-controlnet-canny\",\n        \"openpose\": \"lllyasviel/sd-controlnet-openpose\",\n        \"depth\": \"lllyasviel/sd-controlnet-depth\",\n        \"hed\": \"lllyasviel/sd-controlnet-hed\",\n        \"mlsd\": \"lllyasviel/sd-controlnet-mlsd\",\n        \"normal\": \"lllyasviel/sd-controlnet-normal-map\",\n        \"scribble\": \"lllyasviel/sd-controlnet-scribble\",\n        \"seg\": \"lllyasviel/sd-controlnet-seg\"\n    }\n    \n    # Load ControlNet\n    controlnet = ControlNetModel.from_pretrained(\n        controlnet_models[controlnet_type],\n        torch_dtype=torch.float16\n    )\n    \n    # Create pipeline\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n        model_id,\n        controlnet=controlnet,\n        torch_dtype=torch.float16,\n        safety_checker=None,\n        requires_safety_checker=False\n    )\n    \n    # Optimize for GPU\n    pipe = pipe.to(\"cuda\")\n    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n    \n    # Enable memory efficient attention\n    pipe.enable_model_cpu_offload()\n    pipe.enable_xformers_memory_efficient_attention()\n    \n    return pipe\n```\n:::\n\n\n## Understanding ControlNet\n\n### Core Concept\n\nControlNet works by adding additional neural network layers to Stable Diffusion that process conditioning inputs (like edge maps or poses) and inject this information into the generation process. The original model weights remain frozen while the ControlNet layers learn to translate conditioning inputs into meaningful guidance.\n\n::: {.callout-note}\n## Architecture Overview\n\nControlNet maintains the original Stable Diffusion weights while adding trainable layers that process conditioning inputs and inject control signals at multiple resolution levels in the UNet architecture.\n:::\n\n### Architecture Overview\n\n::: {#architecture-concept .cell execution_count=3}\n``` {.python .cell-code code-summary=\"Conceptual ControlNet architecture\"}\nclass ControlNetArchitecture:\n    \"\"\"\n    Conceptual overview of ControlNet architecture\n    \"\"\"\n    def __init__(self):\n        self.encoder_layers = []  # Process conditioning input\n        self.zero_convolutions = []  # Ensure training stability\n        self.connection_layers = []  # Connect to UNet blocks\n    \n    def forward(self, x_noisy, timestep, conditioning_input):\n        # Process conditioning input through encoder\n        control_features = self.process_conditioning(conditioning_input)\n        \n        # Apply zero convolutions for stable training\n        control_features = self.apply_zero_convs(control_features)\n        \n        # Inject into UNet at multiple resolution levels\n        return self.inject_control(x_noisy, timestep, control_features)\n```\n:::\n\n\n## Basic Implementation\n\n### Canny Edge Control \n\nCanny edge detection provides structural control based on edges in the input image.\n\n::: {#canny-generation .cell execution_count=4}\n``` {.python .cell-code code-summary=\"Generate images with Canny edge control\"}\ndef generate_with_canny(pipe, image_path, prompt, negative_prompt=\"\", num_inference_steps=20):\n    \"\"\"\n    Generate image using Canny edge control\n    \"\"\"\n    # Load and preprocess image\n    original_image = Image.open(image_path)\n    original_image = original_image.resize((512, 512))\n    \n    # Create Canny detector\n    canny_detector = CannyDetector()\n    \n    # Generate Canny edge map\n    canny_image = canny_detector(original_image)\n    \n    # Generate image\n    result = pipe(\n        prompt=prompt,\n        image=canny_image,\n        negative_prompt=negative_prompt,\n        num_inference_steps=num_inference_steps,\n        guidance_scale=7.5,\n        controlnet_conditioning_scale=1.0\n    )\n    \n    return result.images[0], canny_image\n\n# Example usage\npipe = setup_controlnet_pipeline(\"canny\")\nprompt = \"a beautiful landscape painting, oil painting style, vibrant colors\"\ngenerated_image, control_image = generate_with_canny(pipe, \"input.jpg\", prompt)\n```\n:::\n\n\n### OpenPose Human Pose Control\n\nOpenPose allows control over human poses and body positions.\n\n::: {#openpose-generation .cell execution_count=5}\n``` {.python .cell-code code-summary=\"Generate images with OpenPose control\"}\ndef generate_with_openpose(pipe, image_path, prompt, negative_prompt=\"\"):\n    \"\"\"\n    Generate image using OpenPose control\n    \"\"\"\n    # Load image\n    original_image = Image.open(image_path)\n    original_image = original_image.resize((512, 512))\n    \n    # Create OpenPose detector\n    openpose_detector = OpenposeDetector.from_pretrained('lllyasviel/Annotators')\n    \n    # Generate pose keypoints\n    pose_image = openpose_detector(original_image)\n    \n    # Generate image with pose control\n    result = pipe(\n        prompt=prompt,\n        image=pose_image,\n        negative_prompt=negative_prompt,\n        num_inference_steps=20,\n        guidance_scale=7.5,\n        controlnet_conditioning_scale=1.0\n    )\n    \n    return result.images[0], pose_image\n\n# Example usage\npipe = setup_controlnet_pipeline(\"openpose\")\nprompt = \"a robot dancing, futuristic style, neon lighting\"\ngenerated_image, pose_image = generate_with_openpose(pipe, \"person_dancing.jpg\", prompt)\n```\n:::\n\n\n### Depth Map Control\n\nDepth maps provide 3D structure control for more realistic spatial relationships.\n\n::: {#depth-generation .cell execution_count=6}\n``` {.python .cell-code code-summary=\"Generate images with depth map control\"}\ndef generate_with_depth(pipe, image_path, prompt, negative_prompt=\"\"):\n    \"\"\"\n    Generate image using depth map control\n    \"\"\"\n    # Load image\n    original_image = Image.open(image_path)\n    original_image = original_image.resize((512, 512))\n    \n    # Create depth estimator\n    depth_estimator = pipeline('depth-estimation')\n    \n    # Generate depth map\n    depth = depth_estimator(original_image)['depth']\n    depth_image = Image.fromarray(np.array(depth)).convert('RGB')\n    \n    # Generate image with depth control\n    result = pipe(\n        prompt=prompt,\n        image=depth_image,\n        negative_prompt=negative_prompt,\n        num_inference_steps=20,\n        guidance_scale=7.5,\n        controlnet_conditioning_scale=1.0\n    )\n    \n    return result.images[0], depth_image\n```\n:::\n\n\n## Advanced ControlNet Types\n\n### Line Art Control\n\nPerfect for anime-style generation and clean line art conversion.\n\n::: {#lineart-setup .cell execution_count=7}\n``` {.python .cell-code code-summary=\"Setup line art control pipeline\"}\ndef setup_lineart_pipeline():\n    \"\"\"\n    Setup pipeline for line art control\n    \"\"\"\n    controlnet = ControlNetModel.from_pretrained(\n        \"lllyasviel/sd-controlnet-lineart\",\n        torch_dtype=torch.float16\n    )\n    \n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\",\n        controlnet=controlnet,\n        torch_dtype=torch.float16\n    ).to(\"cuda\")\n    \n    return pipe\n\ndef generate_with_lineart(pipe, image_path, prompt, anime_style=False):\n    \"\"\"\n    Generate using line art control\n    \"\"\"\n    original_image = Image.open(image_path).resize((512, 512))\n    \n    # Choose detector based on style\n    if anime_style:\n        detector = LineartAnimeDetector.from_pretrained('lllyasviel/Annotators')\n    else:\n        detector = LineartDetector.from_pretrained('lllyasviel/Annotators')\n    \n    lineart_image = detector(original_image)\n    \n    result = pipe(\n        prompt=prompt,\n        image=lineart_image,\n        num_inference_steps=20,\n        guidance_scale=7.5,\n        controlnet_conditioning_scale=1.0\n    )\n    \n    return result.images[0], lineart_image\n```\n:::\n\n\n### Scribble Control\n\nAllows rough sketches to guide generation.\n\n::: {#scribble-control .cell execution_count=8}\n``` {.python .cell-code code-summary=\"Scribble-based image generation\"}\ndef create_scribble_from_sketch(sketch_path):\n    \"\"\"\n    Process a rough sketch for scribble control\n    \"\"\"\n    sketch = cv2.imread(sketch_path, 0)\n    \n    # Apply threshold to create clean binary image\n    _, binary = cv2.threshold(sketch, 127, 255, cv2.THRESH_BINARY)\n    \n    # Convert to 3-channel RGB\n    scribble = cv2.cvtColor(binary, cv2.COLOR_GRAY2RGB)\n    \n    return Image.fromarray(scribble)\n\ndef generate_with_scribble(pipe, scribble_image, prompt):\n    \"\"\"\n    Generate from scribble input\n    \"\"\"\n    result = pipe(\n        prompt=prompt,\n        image=scribble_image,\n        num_inference_steps=20,\n        guidance_scale=7.5,\n        controlnet_conditioning_scale=1.0\n    )\n    \n    return result.images[0]\n```\n:::\n\n\n### Normal Map Control\n\nProvides detailed surface normal information for realistic lighting.\n\n::: {#normal-map .cell execution_count=9}\n``` {.python .cell-code code-summary=\"Generate and use normal maps for control\"}\ndef generate_normal_map(image_path):\n    \"\"\"\n    Generate normal map from image\n    \"\"\"\n    # Load depth estimator\n    depth_estimator = MidasDetector.from_pretrained('lllyasviel/Annotators')\n    \n    image = Image.open(image_path).resize((512, 512))\n    \n    # Generate depth map\n    depth_map = depth_estimator(image)\n    \n    # Convert depth to normal map (simplified)\n    depth_array = np.array(depth_map)\n    \n    # Calculate gradients\n    grad_x = cv2.Sobel(depth_array, cv2.CV_64F, 1, 0, ksize=3)\n    grad_y = cv2.Sobel(depth_array, cv2.CV_64F, 0, 1, ksize=3)\n    \n    # Create normal vectors\n    normal_x = -grad_x / 255.0\n    normal_y = -grad_y / 255.0\n    normal_z = np.ones_like(normal_x)\n    \n    # Normalize\n    length = np.sqrt(normal_x**2 + normal_y**2 + normal_z**2)\n    normal_x /= length\n    normal_y /= length\n    normal_z /= length\n    \n    # Convert to 0-255 range\n    normal_map = np.stack([\n        ((normal_x + 1) * 127.5).astype(np.uint8),\n        ((normal_y + 1) * 127.5).astype(np.uint8),\n        ((normal_z + 1) * 127.5).astype(np.uint8)\n    ], axis=-1)\n    \n    return Image.fromarray(normal_map)\n```\n:::\n\n\n## Combining Multiple ControlNets\n\n::: {.callout-tip}\n## Multi-ControlNet Benefits\n\nCombining multiple ControlNets allows for more sophisticated control by leveraging different types of conditioning simultaneously, such as pose + depth or edges + normal maps.\n:::\n\n### Multi-ControlNet Setup\n\n::: {#multi-controlnet-setup .cell execution_count=10}\n``` {.python .cell-code code-summary=\"Setup pipeline with multiple ControlNets\"}\ndef setup_multi_controlnet_pipeline(controlnet_types):\n    \"\"\"\n    Setup pipeline with multiple ControlNets\n    \"\"\"\n    from diffusers import MultiControlNetModel\n    \n    controlnet_models = {\n        \"canny\": \"lllyasviel/sd-controlnet-canny\",\n        \"openpose\": \"lllyasviel/sd-controlnet-openpose\",\n        \"depth\": \"lllyasviel/sd-controlnet-depth\"\n    }\n    \n    # Load multiple ControlNets\n    controlnets = [\n        ControlNetModel.from_pretrained(controlnet_models[ctype], torch_dtype=torch.float16)\n        for ctype in controlnet_types\n    ]\n    \n    # Create multi-ControlNet\n    multi_controlnet = MultiControlNetModel(controlnets)\n    \n    # Create pipeline\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\",\n        controlnet=multi_controlnet,\n        torch_dtype=torch.float16\n    ).to(\"cuda\")\n    \n    return pipe\n\ndef generate_with_multiple_controls(pipe, image_path, prompt):\n    \"\"\"\n    Generate using multiple control inputs\n    \"\"\"\n    original_image = Image.open(image_path).resize((512, 512))\n    \n    # Generate different control images\n    canny_detector = CannyDetector()\n    openpose_detector = OpenposeDetector.from_pretrained('lllyasviel/Annotators')\n    \n    canny_image = canny_detector(original_image)\n    pose_image = openpose_detector(original_image)\n    \n    # Generate with multiple controls\n    result = pipe(\n        prompt=prompt,\n        image=[canny_image, pose_image],\n        num_inference_steps=20,\n        guidance_scale=7.5,\n        controlnet_conditioning_scale=[1.0, 0.8]  # Different weights for each control\n    )\n    \n    return result.images[0]\n\n# Example usage\npipe = setup_multi_controlnet_pipeline([\"canny\", \"openpose\"])\nresult = generate_with_multiple_controls(pipe, \"input.jpg\", \"a cyberpunk warrior\")\n```\n:::\n\n\n## Fine-tuning Parameters\n\n### Control Strength and Guidance\n\n::: {#advanced-control .cell execution_count=11}\n``` {.python .cell-code code-summary=\"Advanced parameter control for fine-tuning\"}\ndef advanced_generation_control(pipe, control_image, prompt, **kwargs):\n    \"\"\"\n    Advanced parameter control for fine-tuning generation\n    \"\"\"\n    # Default parameters\n    params = {\n        'prompt': prompt,\n        'image': control_image,\n        'num_inference_steps': 20,\n        'guidance_scale': 7.5,\n        'controlnet_conditioning_scale': 1.0,\n        'control_guidance_start': 0.0,\n        'control_guidance_end': 1.0,\n        'eta': 0.0,\n        'generator': torch.manual_seed(42)\n    }\n    \n    # Update with custom parameters\n    params.update(kwargs)\n    \n    # Generate image\n    result = pipe(**params)\n    \n    return result.images[0]\n\n# Examples of parameter variations\nvariations = [\n    # Strong control throughout\n    {'controlnet_conditioning_scale': 1.5},\n    \n    # Weak control for more creativity\n    {'controlnet_conditioning_scale': 0.5},\n    \n    # Control only in early steps\n    {'control_guidance_end': 0.5},\n    \n    # Control only in later steps\n    {'control_guidance_start': 0.5},\n    \n    # Higher guidance for more prompt adherence\n    {'guidance_scale': 12.0},\n    \n    # More inference steps for quality\n    {'num_inference_steps': 50}\n]\n```\n:::\n\n\n### Adaptive Control Strength\n\n::: {#adaptive-control .cell execution_count=12}\n``` {.python .cell-code code-summary=\"Automatically adjust control strength based on complexity\"}\ndef adaptive_control_strength(pipe, control_image, prompt, complexity_factor=1.0):\n    \"\"\"\n    Automatically adjust control strength based on image complexity\n    \"\"\"\n    # Analyze control image complexity\n    control_array = np.array(control_image.convert('L'))\n    \n    # Calculate edge density as complexity measure\n    edges = cv2.Canny(control_array, 50, 150)\n    edge_density = np.sum(edges > 0) / edges.size\n    \n    # Adjust control strength based on complexity\n    base_strength = 1.0\n    if edge_density > 0.1:  # High detail\n        control_strength = base_strength * 0.8 * complexity_factor\n    elif edge_density < 0.05:  # Low detail\n        control_strength = base_strength * 1.2 * complexity_factor\n    else:  # Medium detail\n        control_strength = base_strength * complexity_factor\n    \n    result = pipe(\n        prompt=prompt,\n        image=control_image,\n        controlnet_conditioning_scale=control_strength,\n        num_inference_steps=20,\n        guidance_scale=7.5\n    )\n    \n    return result.images[0], control_strength\n```\n:::\n\n\n## Production Optimization\n\n### Memory Management\n\n::: {#optimized-generator .cell execution_count=13}\n``` {.python .cell-code code-summary=\"Production-ready ControlNet generator with optimization\"}\nclass OptimizedControlNetGenerator:\n    \"\"\"\n    Production-ready ControlNet generator with optimization\n    \"\"\"\n    \n    def __init__(self, controlnet_type=\"canny\", enable_cpu_offload=True):\n        self.pipe = setup_controlnet_pipeline(controlnet_type)\n        \n        if enable_cpu_offload:\n            self.pipe.enable_model_cpu_offload()\n        \n        # Enable memory efficient attention\n        self.pipe.enable_xformers_memory_efficient_attention()\n        \n        # Compile model for faster inference (PyTorch 2.0+)\n        try:\n            self.pipe.unet = torch.compile(self.pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n        except:\n            print(\"Torch compile not available, skipping optimization\")\n    \n    def generate_batch(self, control_images, prompts, batch_size=4):\n        \"\"\"\n        Generate multiple images in batches for efficiency\n        \"\"\"\n        results = []\n        \n        for i in range(0, len(prompts), batch_size):\n            batch_prompts = prompts[i:i+batch_size]\n            batch_images = control_images[i:i+batch_size]\n            \n            # Clear cache before batch\n            torch.cuda.empty_cache()\n            \n            batch_results = self.pipe(\n                prompt=batch_prompts,\n                image=batch_images,\n                num_inference_steps=20,\n                guidance_scale=7.5\n            )\n            \n            results.extend(batch_results.images)\n        \n        return results\n    \n    def generate_with_callback(self, control_image, prompt, callback=None):\n        \"\"\"\n        Generate with progress callback\n        \"\"\"\n        def progress_callback(step, timestep, latents):\n            if callback:\n                callback(step, timestep)\n        \n        result = self.pipe(\n            prompt=prompt,\n            image=control_image,\n            callback=progress_callback,\n            callback_steps=1\n        )\n        \n        return result.images[0]\n```\n:::\n\n\n### Caching and Preprocessing\n\n::: {#caching-system .cell execution_count=14}\n``` {.python .cell-code code-summary=\"Cache system for preprocessed control images\"}\nimport os\nimport hashlib\n\nclass ControlNetCache:\n    \"\"\"\n    Cache system for preprocessed control images\n    \"\"\"\n    \n    def __init__(self, cache_dir=\"./controlnet_cache\"):\n        self.cache_dir = cache_dir\n        os.makedirs(cache_dir, exist_ok=True)\n        self.detectors = {}\n    \n    def get_detector(self, detector_type):\n        \"\"\"\n        Lazy load and cache detectors\n        \"\"\"\n        if detector_type not in self.detectors:\n            detector_map = {\n                'canny': CannyDetector(),\n                'openpose': OpenposeDetector.from_pretrained('lllyasviel/Annotators'),\n                'hed': HEDdetector.from_pretrained('lllyasviel/Annotators'),\n                'mlsd': MLSDdetector.from_pretrained('lllyasviel/Annotators')\n            }\n            self.detectors[detector_type] = detector_map[detector_type]\n        \n        return self.detectors[detector_type]\n    \n    def get_control_image(self, image_path, control_type, force_refresh=False):\n        \"\"\"\n        Get control image with caching\n        \"\"\"\n        # Create cache key\n        image_hash = hashlib.md5(open(image_path, 'rb').read()).hexdigest()\n        cache_path = os.path.join(self.cache_dir, f\"{image_hash}_{control_type}.png\")\n        \n        # Check cache\n        if os.path.exists(cache_path) and not force_refresh:\n            return Image.open(cache_path)\n        \n        # Generate control image\n        original_image = Image.open(image_path).resize((512, 512))\n        detector = self.get_detector(control_type)\n        control_image = detector(original_image)\n        \n        # Save to cache\n        control_image.save(cache_path)\n        \n        return control_image\n```\n:::\n\n\n## Troubleshooting\n\n::: {.callout-warning}\n## Common Issues\n\n- **GPU Memory**: ControlNet models require significant GPU memory (8GB+ recommended)\n- **Image Format**: Ensure control images are in RGB format and proper dimensions\n- **Model Compatibility**: Match ControlNet models with compatible Stable Diffusion versions\n:::\n\n### Common Issues and Solutions\n\n::: {#diagnostics .cell execution_count=15}\n``` {.python .cell-code code-summary=\"Diagnostic function for common ControlNet issues\"}\ndef diagnose_controlnet_issues(pipe, control_image, prompt):\n    \"\"\"\n    Diagnostic function for common ControlNet issues\n    \"\"\"\n    issues = []\n    \n    # Check control image format\n    if control_image.mode != 'RGB':\n        issues.append(\"Control image should be RGB format\")\n        control_image = control_image.convert('RGB')\n    \n    # Check image size\n    if control_image.size != (512, 512):\n        issues.append(f\"Control image size {control_image.size} != (512, 512)\")\n        control_image = control_image.resize((512, 512))\n    \n    # Check GPU memory\n    if torch.cuda.is_available():\n        memory_allocated = torch.cuda.memory_allocated() / 1e9\n        memory_reserved = torch.cuda.memory_reserved() / 1e9\n        \n        if memory_reserved > 10:  # More than 10GB\n            issues.append(f\"High GPU memory usage: {memory_reserved:.1f}GB\")\n    \n    # Check prompt length\n    if len(prompt.split()) > 75:\n        issues.append(\"Very long prompt may cause issues\")\n    \n    if issues:\n        print(\"Detected issues:\")\n        for issue in issues:\n            print(f\"- {issue}\")\n    \n    return control_image\n\ndef memory_cleanup():\n    \"\"\"\n    Clean up GPU memory\n    \"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.ipc_collect()\n\n# Error handling wrapper\ndef safe_generate(pipe, control_image, prompt, max_retries=3):\n    \"\"\"\n    Generate with error handling and retries\n    \"\"\"\n    for attempt in range(max_retries):\n        try:\n            # Diagnose issues\n            control_image = diagnose_controlnet_issues(pipe, control_image, prompt)\n            \n            # Generate\n            result = pipe(\n                prompt=prompt,\n                image=control_image,\n                num_inference_steps=20,\n                guidance_scale=7.5\n            )\n            \n            return result.images[0]\n            \n        except RuntimeError as e:\n            if \"out of memory\" in str(e).lower():\n                print(f\"GPU OOM on attempt {attempt + 1}, cleaning memory...\")\n                memory_cleanup()\n                \n                if attempt == max_retries - 1:\n                    raise e\n            else:\n                raise e\n        \n        except Exception as e:\n            print(f\"Unexpected error on attempt {attempt + 1}: {e}\")\n            if attempt == max_retries - 1:\n                raise e\n    \n    return None\n```\n:::\n\n\n### Performance Benchmarking\n\n::: {#benchmarking .cell execution_count=16}\n``` {.python .cell-code code-summary=\"Benchmark ControlNet performance\"}\nimport time\nfrom contextlib import contextmanager\n\n@contextmanager\ndef timer():\n    \"\"\"\n    Simple timing context manager\n    \"\"\"\n    start = time.time()\n    yield\n    end = time.time()\n    print(f\"Execution time: {end - start:.2f} seconds\")\n\ndef benchmark_controlnet(pipe, control_image, prompt, runs=5):\n    \"\"\"\n    Benchmark ControlNet performance\n    \"\"\"\n    times = []\n    \n    # Warmup\n    _ = pipe(prompt=prompt, image=control_image, num_inference_steps=5)\n    \n    # Benchmark runs\n    for i in range(runs):\n        start_time = time.time()\n        result = pipe(\n            prompt=prompt,\n            image=control_image,\n            num_inference_steps=20,\n            guidance_scale=7.5\n        )\n        end_time = time.time()\n        times.append(end_time - start_time)\n    \n    avg_time = sum(times) / len(times)\n    print(f\"Average generation time: {avg_time:.2f} seconds\")\n    print(f\"Images per minute: {60 / avg_time:.1f}\")\n    \n    return result.images[0]\n```\n:::\n\n\n## Best Practices Summary\n\n::: {.callout-tip collapse=\"true\"}\n## Key Recommendations\n\n1. **Memory Management**: Use CPU offloading and memory efficient attention for large models\n2. **Preprocessing**: Cache control images when generating multiple variations\n3. **Parameter Tuning**: Adjust `controlnet_conditioning_scale` based on desired control strength\n4. **Quality vs Speed**: Balance `num_inference_steps` with generation time requirements\n5. **Multi-Control**: Use different conditioning scales when combining multiple ControlNets\n6. **Error Handling**: Implement robust error handling for production systems\n7. **Optimization**: Use torch.compile() and xformers for performance improvements\n:::\n\n### Parameter Reference Table\n\n| Parameter | Range | Effect |\n|-----------|-------|--------|\n| `controlnet_conditioning_scale` | 0.5-1.5 | Control strength |\n| `guidance_scale` | 5.0-15.0 | Prompt adherence |\n| `num_inference_steps` | 10-50 | Quality vs speed |\n| `control_guidance_start` | 0.0-0.5 | When control starts |\n| `control_guidance_end` | 0.5-1.0 | When control ends |\n\n: ControlNet Parameter Reference {#tbl-parameters}\n\nThis comprehensive guide provides everything needed to implement Stable Diffusion with ControlNet, from basic usage to production-ready systems. The modular structure allows for easy customization and extension based on specific requirements.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}