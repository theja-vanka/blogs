{
  "hash": "d08b0e1e23855b21e9153bc45ef17ca6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Vision Transformer (ViT) Implementation Guide\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-04-26\"\ncategories: [code, tutorial, advanced]\nformat:\n  html:\n    code-fold: false\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# Vision Transformer (ViT) Implementation Guide\n\nVision Transformers (ViT) represent a significant paradigm shift in computer vision, applying the transformer architecture initially developed for NLP to image processing tasks. This guide walks through implementing a Vision Transformer from scratch using PyTorch.\n\n## Table of Contents\n1. [Introduction to Vision Transformers](#introduction-to-vision-transformers)\n2. [Understanding the Architecture](#understanding-the-architecture)\n3. [Implementation](#implementation)\n   - [Image Patching](#image-patching)\n   - [Patch Embedding](#patch-embedding)\n   - [Position Embedding](#position-embedding)\n   - [Transformer Encoder](#transformer-encoder)\n   - [MLP Head](#mlp-head)\n4. [Training the Model](#training-the-model)\n5. [Inference and Usage](#inference-and-usage)\n6. [Optimization Techniques](#optimization-techniques)\n7. [Advanced Variants](#advanced-variants)\n\n## 1. Introduction to Vision Transformers\n\nVision Transformers (ViT) were introduced in the paper \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" by Dosovitskiy et al. in 2020. The core idea is to treat an image as a sequence of patches, similar to how words are treated in NLP, and process them using a transformer encoder.\n\nKey advantages of ViTs include:\n- Global receptive field from the start\n- Ability to capture long-range dependencies\n- Scalability to large datasets\n- No inductive bias towards local processing (unlike CNNs)\n\n\n## 2. Understanding the Architecture\n\nThe ViT architecture consists of the following components:\n\n1. **Image Patching**: Dividing the input image into fixed-size patches\n2. **Patch Embedding**: Linear projection of flattened patches\n3. **Position Embedding**: Adding positional information\n4. **Transformer Encoder**: Self-attention and feed-forward layers\n5. **MLP Head**: Final classification layer\n\n![](vit.png)\n\n## 3. Implementation\n\nLet's implement each component of the Vision Transformer step by step.\n\n### Image Patching\n\nFirst, we need to divide the input image into fixed-size patches. For a typical ViT, these are 16Ã—16 pixel patches.\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n        super().__init__()\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.num_patches = (image_size // patch_size) ** 2\n        \n        # Convert image into patches and embed them\n        # Instead of using einops, we'll use standard PyTorch operations\n        self.projection = nn.Conv2d(\n            in_channels, embed_dim, \n            kernel_size=patch_size, stride=patch_size\n        )\n        \n    def forward(self, x):\n        # x: (batch_size, channels, height, width)\n        # Convert image into patches using convolution\n        x = self.projection(x)  # (batch_size, embed_dim, grid_height, grid_width)\n        \n        # Flatten spatial dimensions and transpose to get \n        # (batch_size, num_patches, embed_dim)\n        batch_size = x.shape[0]\n        x = x.flatten(2)  # (batch_size, embed_dim, num_patches)\n        x = x.transpose(1, 2)  # (batch_size, num_patches, embed_dim)\n        \n        return x\n```\n\n### Patch Embedding\n\nAfter patching, we need to add a learnable class token and position embeddings.\n\n```python\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, num_classes, \n                 embed_dim, depth, num_heads, mlp_ratio=4, \n                 dropout_rate=0.1):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(\n            image_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embedding.num_patches\n        \n        # Class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # Position embedding for patches + class token\n        self.pos_embedding = nn.Parameter(\n            torch.zeros(1, self.num_patches + 1, embed_dim))\n        \n        self.dropout = nn.Dropout(dropout_rate)\n```\n\n\n### Position Embedding\n\nThe position embeddings are added to provide spatial information:\n\n```python\n    def forward(self, x):\n        # Get patch embeddings\n        x = self.patch_embedding(x)  # (B, num_patches, embed_dim)\n        \n        # Add class token\n        batch_size = x.shape[0]\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (B, 1, embed_dim)\n        x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches + 1, embed_dim)\n        \n        # Add position embedding\n        x = x + self.pos_embedding\n        x = self.dropout(x)\n```\n\n\n### Transformer Encoder\n\nNext, let's implement the transformer encoder blocks:\n\n```python\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout_rate=0.1):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        \n        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout_rate)\n        \n    def forward(self, x):\n        batch_size, seq_len, embed_dim = x.shape\n        \n        # Get query, key, and value projections\n        qkv = self.qkv(x)\n        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, H, N, D)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # Each is (B, H, N, D)\n        \n        # Attention\n        attn = (q @ k.transpose(-2, -1)) * self.scale  # (B, H, N, N)\n        attn = attn.softmax(dim=-1)\n        attn = self.dropout(attn)\n        \n        # Apply attention to values\n        out = (attn @ v).transpose(1, 2)  # (B, N, H, D)\n        out = out.reshape(batch_size, seq_len, embed_dim)  # (B, N, E)\n        out = self.proj(out)\n        \n        return out\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, embed_dim, num_heads, mlp_ratio=4, dropout_rate=0.1):\n        super().__init__()\n        \n        # Layer normalization\n        self.norm1 = nn.LayerNorm(embed_dim)\n        \n        # Multi-head self-attention\n        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout_rate)\n        self.dropout1 = nn.Dropout(dropout_rate)\n        \n        # Layer normalization\n        self.norm2 = nn.LayerNorm(embed_dim)\n        \n        # MLP block\n        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, mlp_hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(mlp_hidden_dim, embed_dim),\n            nn.Dropout(dropout_rate)\n        )\n        \n    def forward(self, x):\n        # Apply layer normalization and self-attention\n        attn_output = self.attn(self.norm1(x))\n        x = x + self.dropout1(attn_output)\n        \n        # Apply MLP block with residual connection\n        x = x + self.mlp(self.norm2(x))\n        return x\n```\n\nNow, let's update our main ViT class to include the transformer encoder blocks:\n\n```python\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, num_classes, \n                 embed_dim, depth, num_heads, mlp_ratio=4, \n                 dropout_rate=0.1):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(\n            image_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embedding.num_patches\n        \n        # Class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # Position embedding for patches + class token\n        self.pos_embedding = nn.Parameter(\n            torch.zeros(1, self.num_patches + 1, embed_dim))\n        \n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # Transformer encoder blocks\n        self.transformer_blocks = nn.ModuleList([\n            TransformerEncoder(embed_dim, num_heads, mlp_ratio, dropout_rate)\n            for _ in range(depth)\n        ])\n        \n        # Layer normalization\n        self.norm = nn.LayerNorm(embed_dim)\n```\n\n### MLP Head\n\nFinally, let's add the classification head and complete the forward pass:\n\n```python\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, num_classes, \n                 embed_dim, depth, num_heads, mlp_ratio=4, \n                 dropout_rate=0.1):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(\n            image_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embedding.num_patches\n        \n        # Class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # Position embedding for patches + class token\n        self.pos_embedding = nn.Parameter(\n            torch.zeros(1, self.num_patches + 1, embed_dim))\n        \n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # Transformer encoder blocks\n        self.transformer_blocks = nn.ModuleList([\n            TransformerEncoder(embed_dim, num_heads, mlp_ratio, dropout_rate)\n            for _ in range(depth)\n        ])\n        \n        # Layer normalization\n        self.norm = nn.LayerNorm(embed_dim)\n        \n        # Classification head\n        self.mlp_head = nn.Linear(embed_dim, num_classes)\n        \n        # Initialize weights\n        self._init_weights()\n        \n    def _init_weights(self):\n        # Initialize patch embedding and MLP heads\n        nn.init.normal_(self.cls_token, std=0.02)\n        nn.init.normal_(self.pos_embedding, std=0.02)\n        \n    def forward(self, x):\n        # Get patch embeddings\n        x = self.patch_embedding(x)  # (B, num_patches, embed_dim)\n        \n        # Add class token\n        batch_size = x.shape[0]\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (B, 1, embed_dim)\n        x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches + 1, embed_dim)\n        \n        # Add position embedding\n        x = x + self.pos_embedding\n        x = self.dropout(x)\n        \n        # Apply transformer blocks\n        for block in self.transformer_blocks:\n            x = block(x)\n        \n        # Apply final layer normalization\n        x = self.norm(x)\n        \n        # Take class token for classification\n        cls_token_final = x[:, 0]\n        \n        # Classification\n        return self.mlp_head(cls_token_final)\n```\n\n\n## 4. Training the Model\n\nLet's implement a training function for our Vision Transformer:\n\n```python\ndef train_vit(model, train_loader, optimizer, criterion, device, epochs=10):\n    model.train()\n    for epoch in range(epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            # Zero the gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            \n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n            \n            # Statistics\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n            \n            # Print statistics every 100 batches\n            if batch_idx % 100 == 99:\n                print(f'Epoch: {epoch+1}, Batch: {batch_idx+1}, '\n                      f'Loss: {running_loss/100:.3f}, '\n                      f'Accuracy: {100.*correct/total:.2f}%')\n                running_loss = 0.0\n                \n        # Epoch statistics\n        print(f'Epoch {epoch+1} completed. '\n              f'Accuracy: {100.*correct/total:.2f}%')\n```\n\nExample usage:\n\n```python\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Create ViT model\nmodel = VisionTransformer(\n    image_size=224,\n    patch_size=16,\n    in_channels=3,\n    num_classes=10,\n    embed_dim=768,\n    depth=12,\n    num_heads=12,\n    mlp_ratio=4,\n    dropout_rate=0.1\n).to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# Load data\ntransform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = datasets.FakeData(\n    transform=transforms.ToTensor()\n)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n\n# Train model\ntrain_vit(model, train_loader, optimizer, criterion, device, epochs=10)\n```\n\n## 5. Inference and Usage\n\nHere's how to use the model for inference:\n\n```python\ndef inference(model, image_tensor, device):\n    model.eval()\n    with torch.no_grad():\n        image_tensor = image_tensor.unsqueeze(0).to(device)  # Add batch dimension\n        output = model(image_tensor)\n        probabilities = F.softmax(output, dim=1)\n        predicted_class = torch.argmax(probabilities, dim=1)\n    return predicted_class.item(), probabilities[0]\n\n# Example usage\nimage = transform(image).to(device)\npredicted_class, probabilities = inference(model, image, device)\nprint(f\"Predicted class: {predicted_class}\")\nprint(f\"Confidence: {probabilities[predicted_class]:.4f}\")\n```\n\n## 6. Optimization Techniques\n\nTo improve the training and performance of ViT models, consider these optimization techniques:\n\n### Custom Attention Implementation\n\nThe standard attention implementation can be memory-intensive. You can use a more efficient implementation:\n\n```python\ndef efficient_attention(q, k, v, mask=None):\n    # q, k, v: [batch_size, num_heads, seq_len, head_dim]\n    \n    # Scaled dot-product attention\n    scale = q.size(-1) ** -0.5\n    attention = torch.matmul(q, k.transpose(-2, -1)) * scale  # [B, H, L, L]\n    \n    if mask is not None:\n        attention = attention.masked_fill(mask == 0, -1e9)\n    \n    attention = F.softmax(attention, dim=-1)\n    output = torch.matmul(attention, v)  # [B, H, L, D]\n    \n    return output\n```\n\n### Mixed Precision Training\n\nUse mixed precision training to reduce memory usage and increase training speed:\n\n```python\nfrom torch.cuda.amp import autocast, GradScaler\n\ndef train_with_mixed_precision(model, train_loader, optimizer, criterion, device, epochs=10):\n    scaler = GradScaler()\n    model.train()\n    \n    for epoch in range(epochs):\n        running_loss = 0.0\n        \n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Use autocast for mixed precision\n            with autocast():\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n            \n            # Scale gradients and optimize\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            running_loss += loss.item()\n            # Rest of the training loop...\n```\n\n### Regularization Techniques\n\nImplement regularization techniques such as stochastic depth to prevent overfitting:\n\n```python\nclass StochasticDepth(nn.Module):\n    def __init__(self, drop_prob=0.1):\n        super().__init__()\n        self.drop_prob = drop_prob\n        \n    def forward(self, x):\n        if not self.training or self.drop_prob == 0.:\n            return x\n        \n        keep_prob = 1 - self.drop_prob\n        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n        random_tensor.floor_()  # binarize\n        output = x.div(keep_prob) * random_tensor\n        return output\n```\n\n## 7. Advanced Variants\n\nSeveral advanced variants of Vision Transformers have been developed:\n\n### DeiT (Data-efficient Image Transformer)\n\nDeiT introduces a distillation token and a teacher-student strategy:\n\n```python\nclass DeiT(VisionTransformer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        \n        # Distillation token\n        self.dist_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n        \n        # Update position embeddings to include distillation token\n        # Original position embeddings are for [class_token, patches]\n        # New position embeddings are for [class_token, dist_token, patches]\n        num_patches = self.patch_embedding.num_patches\n        new_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 2, self.embed_dim))\n        \n        # Initialize new position embeddings with the original ones\n        # Copy class token position embedding\n        new_pos_embed.data[:, 0:1, :] = self.pos_embedding.data[:, 0:1, :]\n        # Add a new position embedding for distillation token\n        new_pos_embed.data[:, 1:2, :] = self.pos_embedding.data[:, 0:1, :]\n        # Copy patch position embeddings\n        new_pos_embed.data[:, 2:, :] = self.pos_embedding.data[:, 1:, :]\n        \n        self.pos_embedding = new_pos_embed\n        \n        # Additional classification head for distillation\n        self.head_dist = nn.Linear(self.embed_dim, kwargs.get('num_classes', 1000))\n        \n    def forward(self, x):\n        # Get patch embeddings\n        x = self.patch_embedding(x)\n        \n        # Add class and distillation tokens\n        batch_size = x.shape[0]\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n        dist_tokens = self.dist_token.expand(batch_size, -1, -1)\n        x = torch.cat((cls_tokens, dist_tokens, x), dim=1)\n        \n        # Add position embedding and apply dropout\n        x = x + self.pos_embedding\n        x = self.dropout(x)\n        \n        # Apply transformer blocks\n        for block in self.transformer_blocks:\n            x = block(x)\n        \n        # Apply final layer normalization\n        x = self.norm(x)\n        \n        # Get class and distillation tokens\n        cls_token_final = x[:, 0]\n        dist_token_final = x[:, 1]\n        \n        # Apply classification heads\n        x_cls = self.mlp_head(cls_token_final)\n        x_dist = self.head_dist(dist_token_final)\n        \n        if self.training:\n            # During training, return both outputs\n            return x_cls, x_dist\n        else:\n            # During inference, return average\n            return (x_cls + x_dist) / 2\n```\n\n### Swin Transformer\n\nSwin Transformer introduces hierarchical feature maps and shifted windows:\n\n```python\n# This is a simplified conceptual implementation of the Swin Transformer block\nclass WindowAttention(nn.Module):\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, dropout=0.):\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # (height, width)\n        self.num_heads = num_heads\n        self.scale = (dim // num_heads) ** -0.5\n        \n        # Linear projections\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n        \n        # Define relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))\n            \n        # Generate pair-wise relative position index for each token in the window\n        coords_h = torch.arange(self.window_size[0])\n        coords_w = torch.arange(self.window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        \n        # Calculate relative positions\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        \n        self.register_buffer(\"relative_position_index\", relative_position_index)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, mask=None):\n        B_, N, C = x.shape\n        \n        # Generate QKV matrices\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # 3, B_, num_heads, N, C//num_heads\n        q, k, v = qkv[0], qkv[1], qkv[2]  # each has shape [B_, num_heads, N, C//num_heads]\n        \n        # Scaled dot-product attention\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))  # B_, num_heads, N, N\n        \n        # Add relative position bias\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)]\n        relative_position_bias = relative_position_bias.view(\n            self.window_size[0] * self.window_size[1], \n            self.window_size[0] * self.window_size[1], \n            -1)  # Wh*Ww, Wh*Ww, num_heads\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # num_heads, Wh*Ww, Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0)\n        \n        # Apply mask if needed\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n        \n        # Apply softmax\n        attn = F.softmax(attn, dim=-1)\n        attn = self.dropout(attn)\n        \n        # Apply attention to values\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        \n        return x\n```\n\n## Conclusion\n\nVision Transformers represent a significant advancement in computer vision, offering a different approach from traditional CNNs. This guide has covered the essential components for implementing a Vision Transformer from scratch, including image patching, position embeddings, transformer encoders, and classification heads.\n\nBy understanding these fundamentals, you can implement your own ViT models and experiment with various modifications to improve performance for specific tasks.\n\n## References\n\n1. Dosovitskiy, A., et al. (2020). \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\" arXiv:2010.11929.\n2. Touvron, H., et al. (2021). \"Training data-efficient image transformers & distillation through attention.\" arXiv:2012.12877.\n3. Liu, Z., et al. (2021). \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.\" arXiv:2103.14030.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}