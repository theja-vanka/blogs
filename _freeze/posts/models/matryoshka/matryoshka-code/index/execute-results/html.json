{
  "hash": "6ded0c7fe8159889cc3eaf3df78d5fd3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Matryoshka Transformer: Complete Implementation Guide\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-06-28\"\ncategories: [code, tutorial, advanced]\nformat:\n  html:\n    code-fold: false\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# Matryoshka Transformer: Complete Implementation Guide\n![](matryo.png)\n\n## Introduction\n\nMatryoshka Transformers are a neural architecture that enables flexible computational budgets during inference by allowing early exits at different layers. Named after Russian nesting dolls, these models contain multiple \"nested\" representations of decreasing complexity, allowing you to trade off accuracy for speed based on your computational constraints.\n\n## Key Concepts\n\n### Core Ideas\n- **Nested Representations**: Each layer can potentially serve as a final output\n- **Early Exits**: Inference can stop at any intermediate layer\n- **Adaptive Computation**: Different inputs may require different amounts of computation\n- **Training Efficiency**: Single model training for multiple computational budgets\n\n### Architecture Overview\n```\nInput → Layer 1 → [Exit 1] → Layer 2 → [Exit 2] → ... → Layer N → [Final Exit]\n```\n\n## Implementation\n\n### 1. Basic Matryoshka Transformer Block\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import List, Optional, Tuple\n\nclass MatryoshkaTransformerBlock(nn.Module):\n    \"\"\"\n    A single transformer block with optional early exit capability\n    \"\"\"\n    def __init__(\n        self,\n        d_model: int,\n        n_heads: int,\n        d_ff: int,\n        dropout: float = 0.1,\n        has_exit: bool = False,\n        n_classes: Optional[int] = None\n    ):\n        super().__init__()\n        \n        # Standard transformer components\n        self.attention = nn.MultiheadAttention(\n            d_model, n_heads, dropout=dropout, batch_first=True\n        )\n        self.feed_forward = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_ff, d_model)\n        )\n        \n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        \n        # Early exit components\n        self.has_exit = has_exit\n        if has_exit and n_classes is not None:\n            self.exit_classifier = nn.Sequential(\n                nn.LayerNorm(d_model),\n                nn.Linear(d_model, d_model // 2),\n                nn.ReLU(),\n                nn.Dropout(dropout),\n                nn.Linear(d_model // 2, n_classes)\n            )\n    \n    def forward(\n        self, \n        x: torch.Tensor, \n        mask: Optional[torch.Tensor] = None\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        \"\"\"\n        Forward pass with optional early exit\n        \n        Returns:\n            x: Transformed input\n            exit_logits: Early exit predictions (if has_exit=True)\n        \"\"\"\n        # Self-attention\n        attn_out, _ = self.attention(x, x, x, attn_mask=mask)\n        x = self.norm1(x + self.dropout(attn_out))\n        \n        # Feed-forward\n        ff_out = self.feed_forward(x)\n        x = self.norm2(x + self.dropout(ff_out))\n        \n        # Early exit prediction\n        exit_logits = None\n        if self.has_exit:\n            # Use mean pooling for sequence classification\n            pooled = x.mean(dim=1)  # [batch_size, d_model]\n            exit_logits = self.exit_classifier(pooled)\n        \n        return x, exit_logits\n```\n\n### 2. Complete Matryoshka Transformer Model\n\n```python\nclass MatryoshkaTransformer(nn.Module):\n    \"\"\"\n    Complete Matryoshka Transformer with multiple exit points\n    \"\"\"\n    def __init__(\n        self,\n        vocab_size: int,\n        d_model: int = 512,\n        n_heads: int = 8,\n        n_layers: int = 6,\n        d_ff: int = 2048,\n        max_seq_len: int = 512,\n        n_classes: int = 2,\n        dropout: float = 0.1,\n        exit_layers: List[int] = None  # Layers with early exits\n    ):\n        super().__init__()\n        \n        self.d_model = d_model\n        self.n_layers = n_layers\n        \n        # Default exit layers (every 2 layers + final)\n        if exit_layers is None:\n            exit_layers = list(range(1, n_layers, 2)) + [n_layers - 1]\n        self.exit_layers = set(exit_layers)\n        \n        # Embeddings\n        self.token_embedding = nn.Embedding(vocab_size, d_model)\n        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n        self.dropout = nn.Dropout(dropout)\n        \n        # Transformer blocks\n        self.blocks = nn.ModuleList([\n            MatryoshkaTransformerBlock(\n                d_model=d_model,\n                n_heads=n_heads,\n                d_ff=d_ff,\n                dropout=dropout,\n                has_exit=(i in self.exit_layers),\n                n_classes=n_classes\n            )\n            for i in range(n_layers)\n        ])\n        \n        # Final classifier (always present)\n        self.final_classifier = nn.Sequential(\n            nn.LayerNorm(d_model),\n            nn.Linear(d_model, n_classes)\n        )\n        \n        # Confidence thresholds for early exits\n        self.confidence_thresholds = nn.Parameter(\n            torch.full((len(self.exit_layers),), 0.8)\n        )\n    \n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        return_all_exits: bool = False,\n        confidence_threshold: float = 0.8,\n        max_exit_layer: Optional[int] = None\n    ) -> dict:\n        \"\"\"\n        Forward pass with adaptive early exiting\n        \n        Args:\n            input_ids: Input token IDs [batch_size, seq_len]\n            attention_mask: Attention mask [batch_size, seq_len]\n            return_all_exits: Whether to return predictions from all exit points\n            confidence_threshold: Minimum confidence for early exit\n            max_exit_layer: Maximum layer to exit at (for budget constraints)\n        \n        Returns:\n            Dictionary containing predictions and exit information\n        \"\"\"\n        batch_size, seq_len = input_ids.shape\n        \n        # Embeddings\n        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n        x = self.token_embedding(input_ids) + self.position_embedding(positions)\n        x = self.dropout(x)\n        \n        # Prepare attention mask\n        if attention_mask is not None:\n            # Convert to transformer format\n            attn_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n            attn_mask = (1.0 - attn_mask) * -10000.0\n            attn_mask = attn_mask.squeeze(1).squeeze(1)\n        else:\n            attn_mask = None\n        \n        # Track exits\n        exit_predictions = []\n        exit_confidences = []\n        exit_layer = None\n        \n        # Forward through transformer blocks\n        for i, block in enumerate(self.blocks):\n            x, exit_logits = block(x, attn_mask)\n            \n            # Check for early exit\n            if exit_logits is not None:\n                exit_probs = F.softmax(exit_logits, dim=-1)\n                max_confidence = torch.max(exit_probs, dim=-1)[0]\n                \n                exit_predictions.append(exit_logits)\n                exit_confidences.append(max_confidence)\n                \n                # Early exit decision\n                if not return_all_exits:\n                    if max_exit_layer is None or i <= max_exit_layer:\n                        if torch.mean(max_confidence) >= confidence_threshold:\n                            exit_layer = i\n                            break\n        \n        # Final prediction\n        final_output = self.final_classifier(x.mean(dim=1))\n        \n        return {\n            'logits': final_output,\n            'exit_predictions': exit_predictions,\n            'exit_confidences': exit_confidences,\n            'exit_layer': exit_layer,\n            'total_layers_used': (exit_layer + 1) if exit_layer is not None else self.n_layers\n        }\n```\n\n### 3. Training Strategy\n\n```python\nclass MatryoshkaTrainer:\n    \"\"\"\n    Training strategy for Matryoshka Transformers\n    \"\"\"\n    def __init__(\n        self,\n        model: MatryoshkaTransformer,\n        exit_loss_weights: List[float] = None,\n        distillation_weight: float = 0.5\n    ):\n        self.model = model\n        self.exit_loss_weights = exit_loss_weights or [0.3, 0.3, 1.0]  # Increasing weights\n        self.distillation_weight = distillation_weight\n        \n    def compute_loss(\n        self,\n        outputs: dict,\n        labels: torch.Tensor,\n        temperature: float = 3.0\n    ) -> dict:\n        \"\"\"\n        Compute combined loss from all exit points\n        \"\"\"\n        losses = {}\n        total_loss = 0\n        \n        # Final layer loss\n        final_loss = F.cross_entropy(outputs['logits'], labels)\n        losses['final'] = final_loss\n        total_loss += final_loss\n        \n        # Early exit losses\n        if outputs['exit_predictions']:\n            for i, (exit_logits, weight) in enumerate(\n                zip(outputs['exit_predictions'], self.exit_loss_weights)\n            ):\n                # Classification loss\n                exit_loss = F.cross_entropy(exit_logits, labels)\n                losses[f'exit_{i}'] = exit_loss\n                total_loss += weight * exit_loss\n                \n                # Knowledge distillation from final layer\n                if self.distillation_weight > 0:\n                    distill_loss = F.kl_div(\n                        F.log_softmax(exit_logits / temperature, dim=-1),\n                        F.softmax(outputs['logits'] / temperature, dim=-1),\n                        reduction='batchmean'\n                    ) * (temperature ** 2)\n                    \n                    losses[f'distill_{i}'] = distill_loss\n                    total_loss += self.distillation_weight * weight * distill_loss\n        \n        losses['total'] = total_loss\n        return losses\n    \n    def train_step(\n        self,\n        batch: dict,\n        optimizer: torch.optim.Optimizer\n    ) -> dict:\n        \"\"\"\n        Single training step\n        \"\"\"\n        self.model.train()\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = self.model(\n            input_ids=batch['input_ids'],\n            attention_mask=batch['attention_mask'],\n            return_all_exits=True\n        )\n        \n        # Compute loss\n        losses = self.compute_loss(outputs, batch['labels'])\n        \n        # Backward pass\n        losses['total'].backward()\n        optimizer.step()\n        \n        return {k: v.item() for k, v in losses.items()}\n```\n\n### 4. Inference with Adaptive Computation\n\n```python\nclass AdaptiveInference:\n    \"\"\"\n    Adaptive inference with configurable exit strategies\n    \"\"\"\n    def __init__(self, model: MatryoshkaTransformer):\n        self.model = model\n    \n    def predict_with_budget(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        flop_budget: float = 1.0,  # Fraction of full model FLOPs\n        confidence_threshold: float = 0.8\n    ) -> dict:\n        \"\"\"\n        Predict with computational budget constraint\n        \"\"\"\n        max_layer = int(self.model.n_layers * flop_budget) - 1\n        \n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            confidence_threshold=confidence_threshold,\n            max_exit_layer=max_layer\n        )\n        \n        # Calculate actual computation used\n        layers_used = outputs['total_layers_used']\n        actual_budget = layers_used / self.model.n_layers\n        \n        return {\n            **outputs,\n            'computational_savings': 1.0 - actual_budget,\n            'flops_used': actual_budget\n        }\n    \n    def predict_with_latency_constraint(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        max_latency_ms: float = 100.0\n    ) -> dict:\n        \"\"\"\n        Predict with latency constraint (simplified)\n        \"\"\"\n        # This is a simplified version - in practice, you'd profile\n        # actual inference times for different exit points\n        \n        estimated_time_per_layer = 10.0  # ms\n        max_layers = int(max_latency_ms / estimated_time_per_layer)\n        \n        return self.predict_with_budget(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            flop_budget=max_layers / self.model.n_layers\n        )\n```\n\n### 5. Usage Example\n\n```python\n# Initialize model\nmodel = MatryoshkaTransformer(\n    vocab_size=30000,\n    d_model=512,\n    n_heads=8,\n    n_layers=12,\n    n_classes=2,\n    exit_layers=[2, 5, 8, 11]  # Exit points\n)\n\n# Training setup\ntrainer = MatryoshkaTrainer(model)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n# Training loop (simplified)\nfor batch in dataloader:\n    losses = trainer.train_step(batch, optimizer)\n    print(f\"Total loss: {losses['total']:.4f}\")\n\n# Inference\ninference_engine = AdaptiveInference(model)\n\n# Example: Predict with 50% computational budget\nresult = inference_engine.predict_with_budget(\n    input_ids=sample_input,\n    flop_budget=0.5,\n    confidence_threshold=0.85\n)\n\nprint(f\"Prediction: {result['logits'].argmax(-1)}\")\nprint(f\"Computational savings: {result['computational_savings']:.2%}\")\nprint(f\"Exited at layer: {result['exit_layer']}\")\n```\n\n## Advanced Features\n\n### 1. Dynamic Confidence Thresholds\n\n```python\nclass DynamicThresholdStrategy:\n    \"\"\"\n    Dynamically adjust confidence thresholds based on input characteristics\n    \"\"\"\n    def __init__(self, base_threshold: float = 0.8):\n        self.base_threshold = base_threshold\n        \n    def get_threshold(self, input_ids: torch.Tensor, layer: int) -> float:\n        \"\"\"\n        Compute dynamic threshold based on input and layer\n        \"\"\"\n        # Example: Lower threshold for longer sequences\n        seq_len = input_ids.shape[1]\n        length_factor = 1.0 - (seq_len - 50) / 500  # Adjust based on length\n        \n        # Example: Higher threshold for earlier layers\n        layer_factor = 1.0 + (0.1 * (6 - layer))  # Stricter for early exits\n        \n        return self.base_threshold * length_factor * layer_factor\n```\n\n### 2. Ensemble Early Exits\n\n```python\nclass EnsembleMatryoshka(nn.Module):\n    \"\"\"\n    Ensemble multiple exit predictions for better accuracy\n    \"\"\"\n    def __init__(self, base_model: MatryoshkaTransformer):\n        super().__init__()\n        self.base_model = base_model\n        self.ensemble_weights = nn.Parameter(torch.ones(len(base_model.exit_layers) + 1))\n        \n    def forward(self, input_ids: torch.Tensor, **kwargs) -> dict:\n        outputs = self.base_model(input_ids, return_all_exits=True, **kwargs)\n        \n        # Ensemble all available predictions\n        all_logits = outputs['exit_predictions'] + [outputs['logits']]\n        weights = F.softmax(self.ensemble_weights, dim=0)\n        \n        ensemble_logits = sum(w * logits for w, logits in zip(weights, all_logits))\n        \n        return {\n            **outputs,\n            'ensemble_logits': ensemble_logits\n        }\n```\n\n## Performance Optimization Tips\n\n1. **Layer Selection**: Choose exit layers strategically - too many exits can hurt training\n2. **Loss Weighting**: Start with lower weights for early exits, increase gradually\n3. **Confidence Calibration**: Use temperature scaling to calibrate exit confidences\n4. **Batch Processing**: Process samples with similar complexity together\n5. **Caching**: Cache intermediate representations for multiple exit strategies\n\n## Conclusion\n\nMatryoshka Transformers offer a powerful way to build efficient models that can adapt their computational cost at inference time. The key to success is careful tuning of exit strategies, loss weights, and confidence thresholds for your specific use case.\n\nThis implementation provides a solid foundation that you can extend with additional features like cascaded exits, uncertainty estimation, or task-specific adaptations.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}