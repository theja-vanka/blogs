{
  "hash": "2d0a68015a41ef539fb59b44bb68ec37",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Complete MobileNet Code Guide\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-07-19\"\ncategories: [code, research, intermediate]\nformat:\n  html:\n    code-fold: false\n    math: mathjax\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# Complete MobileNet Code Guide\n![](mobnet.png)\n\n## Introduction\n\nMobileNet is a family of efficient neural network architectures designed specifically for mobile and embedded devices. The key innovation is the use of **depthwise separable convolutions** which dramatically reduce the number of parameters and computational cost while maintaining reasonable accuracy.\n\n## Prerequisites and Setup\n\n::: {.callout-important}\n## Requirements\nBefore diving into MobileNet implementation, ensure you have the following prerequisites:\n\n**Software Requirements:**\n\n- Python 3.8+ \n- PyTorch 1.12+\n- torchvision 0.13+\n- CUDA (optional, for GPU acceleration)\n\n**Hardware Recommendations:**\n\n- 8GB+ RAM for training\n- NVIDIA GPU with 4GB+ VRAM (recommended)\n- SSD storage for faster data loading\n:::\n\n### Installation\n\n::: {#installation .cell caption='Install required packages' execution_count=1}\n``` {.python .cell-code}\n# Core dependencies\npip install torch torchvision torchaudio\npip install numpy matplotlib seaborn\npip install pillow opencv-python\n\n# Optional dependencies for advanced features\npip install tensorboard  # For training visualization\npip install ptflops      # For FLOPs calculation  \npip install onnx onnxruntime  # For model export\npip install coremltools      # For iOS deployment\npip install tensorflow-lite  # For Android deployment\n\n# Development tools\npip install jupyter notebook\npip install black isort      # Code formatting\n```\n:::\n\n\n### Quick Start Example\n\nHere's a minimal example to get you started with MobileNet:\n\n::: {#quick-start .cell caption='Quick start example with pre-trained MobileNet' execution_count=2}\n``` {.python .cell-code}\nimport torch\nimport torchvision.models as models\nfrom torchvision import transforms\nimport numpy as np\n\n# Load pre-trained model\nmodel = models.mobilenet_v2(pretrained=True)\nmodel.eval()\n\nprint(f\"‚úÖ Model loaded successfully!\")\nprint(f\"üìä Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"üíæ Model size: {sum(p.numel() * p.element_size() for p in model.parameters()) / 1024**2:.1f} MB\")\n\n# Test with random input\ndummy_input = torch.randn(1, 3, 224, 224)\nwith torch.no_grad():\n    output = model(dummy_input)\n    \nprint(f\"üéØ Output shape: {output.shape}\")\nprint(f\"üî• Top prediction: Class {torch.argmax(output).item()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n‚úÖ Model loaded successfully!\nüìä Total parameters: 3,504,872\nüíæ Model size: 13.4 MB\nüéØ Output shape: torch.Size([1, 1000])\nüî• Top prediction: Class 644\n```\n:::\n:::\n\n\n### Key Features\n\n- **Efficient**: 50x fewer parameters than AlexNet\n- **Fast**: Optimized for mobile inference\n- **Flexible**: Width and resolution multipliers for different use cases\n- **Accurate**: Competitive performance on ImageNet\n\n::: {.callout-note}\n## MobileNet Efficiency\nMobileNet achieves its efficiency through depthwise separable convolutions, which split standard convolutions into two operations: depthwise and pointwise convolutions.\n:::\n\n### Architecture Comparison Table\n\n| Architecture | Parameters (M) | FLOPs (M) | Top-1 Accuracy (%) | Model Size (MB) | Target Device |\n|--------------|----------------|-----------|-------------------|-----------------|---------------|\n| AlexNet | 61.0 | 714 | 56.5 | 233 | Desktop |\n| VGG-16 | 138.0 | 15500 | 71.5 | 528 | Desktop |\n| ResNet-50 | 25.6 | 4100 | 76.1 | 98 | Server |\n| MobileNet-V1 | 4.2 | 569 | 70.6 | 16 | Mobile |\n| MobileNet-V2 | 3.4 | 300 | 72.0 | 14 | Mobile |\n| EfficientNet-B0 | 5.3 | 390 | 77.3 | 21 | Mobile |\n\n::: {.column-margin}\n**Note:** Accuracy values are for ImageNet classification. FLOPs calculated for 224√ó224 input images.\n:::\n\n## MobileNet Architecture\n\nThe MobileNet architecture consists of:\n\n1. **Standard 3√ó3 convolution** (first layer)\n2. **13 depthwise separable convolution blocks**\n3. **Average pooling and fully connected layer**\n\n### Architecture Overview\n\n::: {#mobilenet-architecture .cell caption='Complete MobileNet architecture implementation' execution_count=3}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MobileNet(nn.Module):\n    def __init__(self, num_classes=1000, width_mult=1.0):\n        super(MobileNet, self).__init__()\n        \n        # First standard convolution\n        self.conv1 = nn.Conv2d(3, int(32 * width_mult), 3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(int(32 * width_mult))\n        \n        # Depthwise separable convolution blocks\n        self.layers = nn.ModuleList([\n            self._make_layer(int(32 * width_mult), int(64 * width_mult), 1),\n            self._make_layer(int(64 * width_mult), int(128 * width_mult), 2),\n            self._make_layer(int(128 * width_mult), int(128 * width_mult), 1),\n            self._make_layer(int(128 * width_mult), int(256 * width_mult), 2),\n            self._make_layer(int(256 * width_mult), int(256 * width_mult), 1),\n            self._make_layer(int(256 * width_mult), int(512 * width_mult), 2),\n            # 5 layers with stride 1\n            *[self._make_layer(int(512 * width_mult), int(512 * width_mult), 1) for _ in range(5)],\n            self._make_layer(int(512 * width_mult), int(1024 * width_mult), 2),\n            self._make_layer(int(1024 * width_mult), int(1024 * width_mult), 1),\n        ])\n        \n        # Global average pooling and classifier\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Linear(int(1024 * width_mult), num_classes)\n        \n    def _make_layer(self, in_channels, out_channels, stride):\n        return DepthwiseSeparableConv(in_channels, out_channels, stride)\n    \n    def forward(self, x):\n        x = F.relu6(self.bn1(self.conv1(x)))\n        \n        for layer in self.layers:\n            x = layer(x)\n            \n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        \n        return x\n```\n:::\n\n\n## Depthwise Separable Convolutions\n\nThe core innovation of MobileNet is the depthwise separable convolution, which splits a standard convolution into two operations:\n\n### Depthwise Convolution\n\nApplies a single filter per input channel (spatial filtering):\n\n::: {#depthwise-conv .cell caption='Depthwise convolution implementation' execution_count=4}\n``` {.python .cell-code}\nclass DepthwiseConv(nn.Module):\n    def __init__(self, in_channels, kernel_size=3, stride=1, padding=1):\n        super(DepthwiseConv, self).__init__()\n        self.depthwise = nn.Conv2d(\n            in_channels, in_channels, \n            kernel_size=kernel_size, \n            stride=stride, \n            padding=padding, \n            groups=in_channels,  # Key: groups = in_channels\n            bias=False\n        )\n        self.bn = nn.BatchNorm2d(in_channels)\n    \n    def forward(self, x):\n        return F.relu6(self.bn(self.depthwise(x)))\n```\n:::\n\n\n### Pointwise Convolution\n\nApplies 1√ó1 convolution to combine features (channel mixing):\n\n::: {#pointwise-conv .cell caption='Pointwise convolution implementation' execution_count=5}\n``` {.python .cell-code}\nclass PointwiseConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(PointwiseConv, self).__init__()\n        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n        self.bn = nn.BatchNorm2d(out_channels)\n    \n    def forward(self, x):\n        return F.relu6(self.bn(self.pointwise(x)))\n```\n:::\n\n\n### Complete Depthwise Separable Block\n\n::: {#depthwise-separable-block .cell caption='Complete depthwise separable convolution block' execution_count=6}\n``` {.python .cell-code}\nclass DepthwiseSeparableConv(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(DepthwiseSeparableConv, self).__init__()\n        \n        self.depthwise = DepthwiseConv(in_channels, stride=stride)\n        self.pointwise = PointwiseConv(in_channels, out_channels)\n    \n    def forward(self, x):\n        x = self.depthwise(x)\n        x = self.pointwise(x)\n        return x\n```\n:::\n\n\n### Computational Efficiency\n\n::: {.callout-important}\n## Efficiency Gains\n\n**Standard Convolution:**\n\n- Parameters: `Dk √ó Dk √ó M √ó N`\n- Computation: `Dk √ó Dk √ó M √ó N √ó Df √ó Df`\n\n**Depthwise Separable Convolution:**\n\n- Parameters: `Dk √ó Dk √ó M + M √ó N`  \n- Computation: `Dk √ó Dk √ó M √ó Df √ó Df + M √ó N √ó Df √ó Df`\n\n**Reduction Factor:**    `1/N + 1/Dk¬≤` (typically 8-9x reduction)\n:::\n\n### Efficiency Visualization\n\nLet's visualize the efficiency gains of depthwise separable convolutions:\n\n::: {#cell-fig-efficiency-comparison .cell fig-height='8' fig-width='12' execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![Computational cost comparison: Standard vs Depthwise Separable Convolutions](index_files/figure-html/fig-efficiency-comparison-output-1.png){#fig-efficiency-comparison}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nüìä **Efficiency Summary:**\n   ‚Ä¢ Total FLOPs - Standard: 19710.7M\n   ‚Ä¢ Total FLOPs - Depthwise: 2218.1M\n   ‚Ä¢ **Overall Reduction: 8.9√ó**\n```\n:::\n:::\n\n\n## Implementation from Scratch\n\nHere's a complete implementation with detailed explanations:\n\n::: {#mobilenet-complete .cell caption='Complete MobileNetV1 implementation with configurable parameters' execution_count=8}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Optional\n\nclass MobileNetV1(nn.Module):\n    \"\"\"\n    MobileNetV1 implementation with configurable width and resolution multipliers.\n    \n    Args:\n        num_classes: Number of output classes\n        width_mult: Width multiplier for channels (0.25, 0.5, 0.75, 1.0)\n        resolution_mult: Resolution multiplier for input size\n        dropout_rate: Dropout rate before classifier\n    \"\"\"\n    \n    def __init__(self, \n                 num_classes: int = 1000, \n                 width_mult: float = 1.0,\n                 dropout_rate: float = 0.2):\n        super(MobileNetV1, self).__init__()\n        \n        self.width_mult = width_mult\n        \n        # Helper function to make channels divisible by 8\n        def _make_divisible(v, divisor=8):\n            new_v = max(divisor, int(v + divisor / 2) // divisor * divisor)\n            if new_v < 0.9 * v:\n                new_v += divisor\n            return new_v\n        \n        # Define channel configurations\n        input_channel = _make_divisible(32 * width_mult)\n        \n        # First standard convolution\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, input_channel, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(input_channel),\n            nn.ReLU6(inplace=True)\n        )\n        \n        # Configuration: [output_channels, stride]\n        configs = [\n            [64, 1],\n            [128, 2], [128, 1],\n            [256, 2], [256, 1],\n            [512, 2], [512, 1], [512, 1], [512, 1], [512, 1], [512, 1],\n            [1024, 2], [1024, 1]\n        ]\n        \n        # Build depthwise separable layers\n        layers = []\n        for output_channel, stride in configs:\n            output_channel = _make_divisible(output_channel * width_mult)\n            layers.append(\n                DepthwiseSeparableConv(input_channel, output_channel, stride)\n            )\n            input_channel = output_channel\n        \n        self.features = nn.Sequential(*layers)\n        \n        # Classifier\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(dropout_rate)\n        self.classifier = nn.Linear(input_channel, num_classes)\n        \n        # Initialize weights\n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        \"\"\"Initialize weights using He initialization for ReLU networks.\"\"\"\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.dropout(x)\n        x = self.classifier(x)\n        return x\n\n# Optimized Depthwise Separable Convolution with better efficiency\nclass DepthwiseSeparableConv(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(DepthwiseSeparableConv, self).__init__()\n        \n        self.conv = nn.Sequential(\n            # Depthwise convolution\n            nn.Conv2d(in_channels, in_channels, 3, stride, 1, \n                     groups=in_channels, bias=False),\n            nn.BatchNorm2d(in_channels),\n            nn.ReLU6(inplace=True),\n            \n            # Pointwise convolution\n            nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU6(inplace=True),\n        )\n    \n    def forward(self, x):\n        return self.conv(x)\n\n# Model factory function\ndef mobilenet_v1(num_classes=1000, width_mult=1.0, pretrained=False):\n    \"\"\"\n    Create MobileNetV1 model.\n    \n    Args:\n        num_classes: Number of classes for classification\n        width_mult: Width multiplier (0.25, 0.5, 0.75, 1.0)\n        pretrained: Load pretrained weights (if available)\n    \"\"\"\n    model = MobileNetV1(num_classes=num_classes, width_mult=width_mult)\n    \n    if pretrained:\n        # In practice, you would load pretrained weights here\n        print(f\"Loading pretrained MobileNetV1 with width_mult={width_mult}\")\n    \n    return model\n```\n:::\n\n\n## Using Pre-trained MobileNet\n\n### With PyTorch (torchvision)\n\n::: {#pretrained-usage .cell caption='Using pre-trained MobileNet for inference' execution_count=9}\n``` {.python .cell-code}\nimport torch\nimport torchvision.models as models\nfrom torchvision import transforms\nfrom PIL import Image\n\n# Load pre-trained MobileNetV2\nmodel = models.mobilenet_v2(pretrained=True)\nmodel.eval()\n\n# Preprocessing pipeline\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                        std=[0.229, 0.224, 0.225]),\n])\n\n# Inference function\ndef predict_image(image_path, model, preprocess, top_k=5):\n    \"\"\"Predict top-k classes for an image.\"\"\"\n    \n    # Load and preprocess image\n    image = Image.open(image_path).convert('RGB')\n    input_tensor = preprocess(image)\n    input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n    \n    # Predict\n    with torch.no_grad():\n        output = model(input_batch)\n        probabilities = torch.nn.functional.softmax(output[0], dim=0)\n    \n    # Get top-k predictions\n    top_prob, top_indices = torch.topk(probabilities, top_k)\n    \n    return [(idx.item(), prob.item()) for idx, prob in zip(top_indices, top_prob)]\n\n# Example usage\n# predictions = predict_image('cat.jpg', model, preprocess)\n# print(predictions)\n```\n:::\n\n\n### Fine-tuning Pre-trained MobileNet\n\n::: {#fine-tuning .cell caption='Fine-tuning MobileNet for custom classification tasks' execution_count=10}\n``` {.python .cell-code}\nimport torch.optim as optim\nimport torch.nn as nn\n\ndef create_mobilenet_classifier(num_classes, pretrained=True):\n    \"\"\"Create MobileNet for custom classification task.\"\"\"\n    \n    # Load pre-trained model\n    model = models.mobilenet_v2(pretrained=pretrained)\n    \n    # Modify classifier for custom number of classes\n    model.classifier = nn.Sequential(\n        nn.Dropout(0.2),\n        nn.Linear(model.last_channel, num_classes),\n    )\n    \n    return model\n\n# Training setup for fine-tuning\ndef setup_training(model, num_classes, learning_rate=0.001):\n    \"\"\"Setup optimizer and loss function for fine-tuning.\"\"\"\n    \n    # Freeze feature extraction layers (optional)\n    for param in model.features.parameters():\n        param.requires_grad = False\n    \n    # Only train classifier\n    optimizer = optim.Adam(model.classifier.parameters(), lr=learning_rate)\n    criterion = nn.CrossEntropyLoss()\n    \n    return optimizer, criterion\n\n# Training loop\ndef train_epoch(model, dataloader, optimizer, criterion, device):\n    \"\"\"Train model for one epoch.\"\"\"\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for inputs, labels in dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    return running_loss / len(dataloader), 100 * correct / total\n```\n:::\n\n\n## Training MobileNet\n\n### Complete Training Pipeline\n\n::: {#training-pipeline .cell caption='Complete training pipeline for MobileNet' execution_count=11}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport time\n\nclass MobileNetTrainer:\n    def __init__(self, model, device='cuda'):\n        self.model = model.to(device)\n        self.device = device\n        self.history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n    \n    def train(self, train_loader, val_loader, epochs=10, lr=0.001):\n        \"\"\"Complete training pipeline.\"\"\"\n        \n        # Setup optimizer and scheduler\n        optimizer = optim.RMSprop(self.model.parameters(), lr=lr, weight_decay=1e-4)\n        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n        criterion = nn.CrossEntropyLoss()\n        \n        best_acc = 0.0\n        \n        for epoch in range(epochs):\n            print(f'Epoch {epoch+1}/{epochs}')\n            print('-' * 10)\n            \n            # Training phase\n            train_loss, train_acc = self._train_epoch(train_loader, optimizer, criterion)\n            \n            # Validation phase\n            val_loss, val_acc = self._validate_epoch(val_loader, criterion)\n            \n            # Update scheduler\n            scheduler.step()\n            \n            # Save best model\n            if val_acc > best_acc:\n                best_acc = val_acc\n                torch.save(self.model.state_dict(), 'best_mobilenet.pth')\n            \n            # Update history\n            self.history['train_loss'].append(train_loss)\n            self.history['train_acc'].append(train_acc)\n            self.history['val_loss'].append(val_loss)\n            self.history['val_acc'].append(val_acc)\n            \n            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n            print()\n    \n    def _train_epoch(self, dataloader, optimizer, criterion):\n        \"\"\"Train for one epoch.\"\"\"\n        self.model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(self.device), labels.to(self.device)\n            \n            optimizer.zero_grad()\n            outputs = self.model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n        \n        return running_loss / len(dataloader), 100 * correct / total\n    \n    def _validate_epoch(self, dataloader, criterion):\n        \"\"\"Validate for one epoch.\"\"\"\n        self.model.eval()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for inputs, labels in dataloader:\n                inputs, labels = inputs.to(self.device), labels.to(self.device)\n                \n                outputs = self.model(inputs)\n                loss = criterion(outputs, labels)\n                \n                running_loss += loss.item()\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        \n        return running_loss / len(dataloader), 100 * correct / total\n```\n:::\n\n\n### Data Loading and Augmentation\n\n::: {#data-loading .cell caption='Data loading and augmentation pipeline' execution_count=12}\n``` {.python .cell-code}\n# Data loading and augmentation\ndef get_dataloaders(data_dir, batch_size=32, num_workers=4):\n    \"\"\"Create training and validation dataloaders.\"\"\"\n    \n    # Data augmentation for training\n    train_transforms = transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    \n    # Validation transforms\n    val_transforms = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    \n    # Create datasets\n    train_dataset = datasets.ImageFolder(f'{data_dir}/train', train_transforms)\n    val_dataset = datasets.ImageFolder(f'{data_dir}/val', val_transforms)\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, \n                             shuffle=True, num_workers=num_workers)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, \n                           shuffle=False, num_workers=num_workers)\n    \n    return train_loader, val_loader, len(train_dataset.classes)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Setup\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Load data\n    # train_loader, val_loader, num_classes = get_dataloaders('path/to/data')\n    \n    # Create model\n    # model = mobilenet_v1(num_classes=num_classes, width_mult=1.0)\n    \n    # Train\n    # trainer = MobileNetTrainer(model, device)\n    # trainer.train(train_loader, val_loader, epochs=20)\n    pass\n```\n:::\n\n\n## MobileNet Variants\n\n### MobileNetV2 Implementation\n\n::: {#mobilenetv2 .cell caption='MobileNetV2 with inverted residual blocks' execution_count=13}\n``` {.python .cell-code}\nclass InvertedResidual(nn.Module):\n    \"\"\"Inverted residual block for MobileNetV2.\"\"\"\n    \n    def __init__(self, in_channels, out_channels, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        \n        hidden_dim = int(round(in_channels * expand_ratio))\n        self.use_residual = stride == 1 and in_channels == out_channels\n        \n        layers = []\n        \n        # Expansion phase\n        if expand_ratio != 1:\n            layers.extend([\n                nn.Conv2d(in_channels, hidden_dim, 1, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n            ])\n        \n        # Depthwise convolution\n        layers.extend([\n            nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, \n                     groups=hidden_dim, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True),\n            \n            # Pointwise linear projection\n            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n        ])\n        \n        self.conv = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        if self.use_residual:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\nclass MobileNetV2(nn.Module):\n    \"\"\"MobileNetV2 with inverted residuals and linear bottlenecks.\"\"\"\n    \n    def __init__(self, num_classes=1000, width_mult=1.0):\n        super(MobileNetV2, self).__init__()\n        \n        input_channel = 32\n        last_channel = 1280\n        \n        # Inverted residual settings\n        # t: expansion factor, c: output channels, n: number of blocks, s: stride\n        inverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n        \n        # Apply width multiplier\n        input_channel = int(input_channel * width_mult)\n        self.last_channel = int(last_channel * max(1.0, width_mult))\n        \n        # First convolution\n        features = [nn.Sequential(\n            nn.Conv2d(3, input_channel, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(input_channel),\n            nn.ReLU6(inplace=True)\n        )]\n        \n        # Inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = int(c * width_mult)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(InvertedResidual(input_channel, output_channel, \n                                               stride, t))\n                input_channel = output_channel\n        \n        # Last convolution\n        features.append(nn.Sequential(\n            nn.Conv2d(input_channel, self.last_channel, 1, bias=False),\n            nn.BatchNorm2d(self.last_channel),\n            nn.ReLU6(inplace=True)\n        ))\n        \n        self.features = nn.Sequential(*features)\n        \n        # Classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.last_channel, num_classes),\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n```\n:::\n\n\n### MobileNetV3 Features\n\n::: {#mobilenetv3-components .cell caption='Key components of MobileNetV3' execution_count=14}\n``` {.python .cell-code}\nclass SEBlock(nn.Module):\n    \"\"\"Squeeze-and-Excitation block.\"\"\"\n    \n    def __init__(self, in_channels, reduction=4):\n        super(SEBlock, self).__init__()\n        \n        self.se = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, in_channels // reduction, 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels // reduction, in_channels, 1),\n            nn.Hardsigmoid(inplace=True)\n        )\n    \n    def forward(self, x):\n        return x * self.se(x)\n\nclass HardSwish(nn.Module):\n    \"\"\"Hard Swish activation function.\"\"\"\n    \n    def forward(self, x):\n        return x * F.hardsigmoid(x)\n\n# MobileNetV3 would use these components along with:\n# - Neural Architecture Search (NAS) for optimal architecture\n# - Hard Swish activation instead of ReLU6\n# - Squeeze-and-Excitation blocks\n# - Optimized last layers\n```\n:::\n\n\n::: {.callout-tip}\n## MobileNetV3 Improvements\nMobileNetV3 incorporates several advanced techniques:\n\n- **Neural Architecture Search** for optimal layer configurations\n- **Squeeze-and-Excitation blocks** for attention mechanisms\n- **Hard Swish activation** for better performance\n- **Optimized head and tail** layers for efficiency\n:::\n\n## Optimization Techniques\n\n### Quantization\n\n::: {#quantization .cell caption='Post-training and dynamic quantization techniques' execution_count=15}\n``` {.python .cell-code}\nimport torch.quantization as quantization\n\ndef quantize_mobilenet(model, calibration_loader):\n    \"\"\"Apply post-training quantization to MobileNet.\"\"\"\n    \n    # Set model to evaluation mode\n    model.eval()\n    \n    # Fuse modules for better quantization\n    model_fused = torch.quantization.fuse_modules(model, [\n        ['conv', 'bn', 'relu'] for conv, bn, relu in model.named_modules()\n        if isinstance(conv, nn.Conv2d) and isinstance(bn, nn.BatchNorm2d)\n    ])\n    \n    # Set quantization config\n    model_fused.qconfig = quantization.get_default_qconfig('qnnpack')\n    \n    # Prepare model for quantization\n    model_prepared = quantization.prepare(model_fused)\n    \n    # Calibrate with representative data\n    with torch.no_grad():\n        for inputs, _ in calibration_loader:\n            model_prepared(inputs)\n    \n    # Convert to quantized model\n    model_quantized = quantization.convert(model_prepared)\n    \n    return model_quantized\n\n# Dynamic quantization (easier but less optimal)\ndef dynamic_quantize_mobilenet(model):\n    \"\"\"Apply dynamic quantization.\"\"\"\n    return quantization.quantize_dynamic(\n        model, \n        {nn.Linear, nn.Conv2d}, \n        dtype=torch.qint8\n    )\n```\n:::\n\n\n### Pruning\n\n::: {#pruning .cell caption='Magnitude-based and structured pruning techniques' execution_count=16}\n``` {.python .cell-code}\nimport torch.nn.utils.prune as prune\n\ndef prune_mobilenet(model, pruning_ratio=0.2):\n    \"\"\"Apply magnitude-based pruning to MobileNet.\"\"\"\n    \n    parameters_to_prune = []\n    \n    # Collect Conv2d and Linear layers for pruning\n    for name, module in model.named_modules():\n        if isinstance(module, (nn.Conv2d, nn.Linear)):\n            parameters_to_prune.append((module, 'weight'))\n    \n    # Apply global magnitude pruning\n    prune.global_unstructured(\n        parameters_to_prune,\n        pruning_method=prune.L1Unstructured,\n        amount=pruning_ratio,\n    )\n    \n    # Make pruning permanent\n    for module, param_name in parameters_to_prune:\n        prune.remove(module, param_name)\n    \n    return model\n\n# Structured pruning example\ndef structured_prune_mobilenet(model, pruning_ratio=0.2):\n    \"\"\"Apply structured channel pruning.\"\"\"\n    \n    for name, module in model.named_modules():\n        if isinstance(module, nn.Conv2d) and module.groups == 1:  # Skip depthwise\n            prune.ln_structured(\n                module, \n                name='weight', \n                amount=pruning_ratio, \n                n=2, \n                dim=0  # Prune output channels\n            )\n    \n    return model\n```\n:::\n\n\n## Deployment Considerations\n\n### ONNX Export\n\n::: {#onnx-export .cell caption='Exporting MobileNet to ONNX format for cross-platform deployment' execution_count=17}\n``` {.python .cell-code}\nimport torch.onnx\n\ndef export_to_onnx(model, input_shape=(1, 3, 224, 224), onnx_path=\"mobilenet.onnx\"):\n    \"\"\"Export MobileNet to ONNX format.\"\"\"\n    \n    model.eval()\n    dummy_input = torch.randn(input_shape)\n    \n    torch.onnx.export(\n        model,\n        dummy_input,\n        onnx_path,\n        export_params=True,\n        opset_version=11,\n        do_constant_folding=True,\n        input_names=['input'],\n        output_names=['output'],\n        dynamic_axes={\n            'input': {0: 'batch_size'},\n            'output': {0: 'batch_size'}\n        }\n    )\n    \n    print(f\"Model exported to {onnx_path}\")\n\n# TensorRT optimization (requires tensorrt)\ndef optimize_with_tensorrt(onnx_path):\n    \"\"\"Optimize ONNX model with TensorRT.\"\"\"\n    try:\n        import tensorrt as trt\n        \n        # Create TensorRT logger and builder\n        logger = trt.Logger(trt.Logger.WARNING)\n        builder = trt.Builder(logger)\n        \n        # Parse ONNX model\n        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n        parser = trt.OnnxParser(network, logger)\n        \n        with open(onnx_path, 'rb') as model:\n            parser.parse(model.read())\n        \n        # Build optimized engine\n        config = builder.create_builder_config()\n        config.max_workspace_size = 1 << 28  # 256MB\n        config.set_flag(trt.BuilderFlag.FP16)  # Enable FP16 precision\n        \n        engine = builder.build_engine(network, config)\n        \n        # Save engine\n        with open(\"mobilenet.trt\", \"wb\") as f:\n            f.write(engine.serialize())\n        \n        return engine\n    except ImportError:\n        print(\"TensorRT not installed. Please install TensorRT for optimization.\")\n        return None\n```\n:::\n\n\n### Mobile Deployment\n\n::: {#mobile-deployment .cell caption='Converting models for mobile deployment platforms' execution_count=18}\n``` {.python .cell-code}\n# TensorFlow Lite conversion (if using TensorFlow)\ndef convert_to_tflite(model_path, tflite_path=\"mobilenet.tflite\"):\n    \"\"\"Convert model to TensorFlow Lite format.\"\"\"\n    try:\n        import tensorflow as tf\n        \n        # Load model (assuming saved as TensorFlow model)\n        converter = tf.lite.TFLiteConverter.from_saved_model(model_path)\n        \n        # Optimization settings\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n        converter.target_spec.supported_types = [tf.float16]\n        \n        # Convert\n        tflite_model = converter.convert()\n        \n        # Save\n        with open(tflite_path, 'wb') as f:\n            f.write(tflite_model)\n        \n        print(f\"TFLite model saved to {tflite_path}\")\n    except ImportError:\n        print(\"TensorFlow not installed. Install with: pip install tensorflow\")\n\n# CoreML conversion (for iOS)\ndef convert_to_coreml(model, input_shape=(1, 3, 224, 224)):\n    \"\"\"Convert PyTorch model to CoreML format.\"\"\"\n    try:\n        import coremltools as ct\n        \n        model.eval()\n        example_input = torch.rand(input_shape)\n        \n        # Trace the model\n        traced_model = torch.jit.trace(model, example_input)\n        \n        # Convert to CoreML\n        coreml_model = ct.convert(\n            traced_model,\n            inputs=[ct.ImageType(shape=input_shape, bias=[-1, -1, -1], scale=1/127.5)]\n        )\n        \n        # Save\n        coreml_model.save(\"MobileNet.mlmodel\")\n        print(\"CoreML model saved successfully\")\n        \n    except ImportError:\n        print(\"coremltools not installed. Install with: pip install coremltools\")\n```\n:::\n\n\n### Edge Deployment with Optimization\n\n::: {#edge-deployment .cell caption='Optimized inference class for edge deployment' execution_count=19}\n``` {.python .cell-code}\n# Edge deployment with optimization\nclass OptimizedMobileNetInference:\n    \"\"\"Optimized inference class for edge deployment.\"\"\"\n    \n    def __init__(self, model_path, device='cpu'):\n        self.device = device\n        self.model = self.load_optimized_model(model_path)\n        self.preprocess = self.get_preprocessing()\n    \n    def load_optimized_model(self, model_path):\n        \"\"\"Load and optimize model for inference.\"\"\"\n        model = torch.load(model_path, map_location=self.device)\n        model.eval()\n        \n        # Apply optimizations\n        if self.device == 'cpu':\n            # Optimize for CPU inference\n            model = torch.jit.optimize_for_inference(torch.jit.script(model))\n        \n        return model\n    \n    def get_preprocessing(self):\n        \"\"\"Get optimized preprocessing pipeline.\"\"\"\n        return transforms.Compose([\n            transforms.Resize(256, interpolation=transforms.InterpolationMode.BILINEAR),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n    \n    @torch.no_grad()\n    def predict(self, image):\n        \"\"\"Fast inference on single image.\"\"\"\n        if isinstance(image, str):\n            from PIL import Image\n            image = Image.open(image).convert('RGB')\n        \n        # Preprocess\n        input_tensor = self.preprocess(image).unsqueeze(0).to(self.device)\n        \n        # Inference\n        output = self.model(input_tensor)\n        probabilities = F.softmax(output[0], dim=0)\n        \n        return probabilities.cpu().numpy()\n    \n    def batch_predict(self, images, batch_size=32):\n        \"\"\"Batch inference for multiple images.\"\"\"\n        results = []\n        \n        for i in range(0, len(images), batch_size):\n            batch = images[i:i+batch_size]\n            batch_tensor = torch.stack([\n                self.preprocess(img) for img in batch\n            ]).to(self.device)\n            \n            outputs = self.model(batch_tensor)\n            probabilities = F.softmax(outputs, dim=1)\n            results.extend(probabilities.cpu().numpy())\n        \n        return results\n```\n:::\n\n\n## Performance Analysis\n\n### Benchmarking Tools\n\n::: {#benchmarking .cell caption='Comprehensive benchmarking suite for MobileNet' execution_count=20}\n``` {.python .cell-code}\nimport time\nimport numpy as np\nfrom contextlib import contextmanager\n\nclass MobileNetBenchmark:\n    \"\"\"Comprehensive benchmarking suite for MobileNet.\"\"\"\n    \n    def __init__(self, model, device='cpu'):\n        self.model = model.to(device)\n        self.device = device\n        self.model.eval()\n    \n    @contextmanager\n    def timer(self):\n        \"\"\"Context manager for timing operations.\"\"\"\n        start = time.time()\n        yield\n        end = time.time()\n        self.last_time = end - start\n    \n    def benchmark_inference(self, input_shape=(1, 3, 224, 224), num_runs=100, warmup=10):\n        \"\"\"Benchmark inference speed.\"\"\"\n        dummy_input = torch.randn(input_shape).to(self.device)\n        \n        # Warmup\n        with torch.no_grad():\n            for _ in range(warmup):\n                _ = self.model(dummy_input)\n        \n        # Benchmark\n        times = []\n        with torch.no_grad():\n            for _ in range(num_runs):\n                with self.timer():\n                    _ = self.model(dummy_input)\n                times.append(self.last_time)\n        \n        return {\n            'mean_time': np.mean(times),\n            'std_time': np.std(times),\n            'min_time': np.min(times),\n            'max_time': np.max(times),\n            'fps': 1.0 / np.mean(times)\n        }\n    \n    def benchmark_memory(self, input_shape=(1, 3, 224, 224)):\n        \"\"\"Benchmark memory usage.\"\"\"\n        if self.device == 'cuda':\n            torch.cuda.empty_cache()\n            torch.cuda.reset_peak_memory_stats()\n            \n            dummy_input = torch.randn(input_shape).to(self.device)\n            \n            with torch.no_grad():\n                _ = self.model(dummy_input)\n            \n            memory_stats = {\n                'peak_memory_mb': torch.cuda.max_memory_allocated() / 1024**2,\n                'current_memory_mb': torch.cuda.memory_allocated() / 1024**2\n            }\n            \n            return memory_stats\n        else:\n            return {'message': 'Memory benchmarking only available for CUDA'}\n    \n    def profile_layers(self, input_shape=(1, 3, 224, 224)):\n        \"\"\"Profile individual layers.\"\"\"\n        dummy_input = torch.randn(input_shape).to(self.device)\n        \n        with torch.profiler.profile(\n            activities=[torch.profiler.ProfilerActivity.CPU, \n                       torch.profiler.ProfilerActivity.CUDA],\n            record_shapes=True,\n            profile_memory=True,\n            with_stack=True\n        ) as prof:\n            with torch.no_grad():\n                _ = self.model(dummy_input)\n        \n        return prof\n    \n    def compare_models(self, models_dict, input_shape=(1, 3, 224, 224)):\n        \"\"\"Compare multiple model variants.\"\"\"\n        results = {}\n        \n        for name, model in models_dict.items():\n            benchmark = MobileNetBenchmark(model, self.device)\n            results[name] = {\n                'inference': benchmark.benchmark_inference(input_shape),\n                'memory': benchmark.benchmark_memory(input_shape),\n                'parameters': sum(p.numel() for p in model.parameters()),\n                'model_size_mb': sum(p.numel() * p.element_size() \n                                   for p in model.parameters()) / 1024**2\n            }\n        \n        return results\n```\n:::\n\n\n### Model Comparison\n\n::: {#model-comparison .cell caption='Comparing different MobileNet variants' execution_count=21}\n\n::: {.cell-output .cell-output-stdout}\n```\nModel           Params (M)   Size (MB)  FPS      Peak Mem (MB)  \n----------------------------------------------------------------------\nMobileNet_1.0   4.23         16.14      41.1     N/A            \nMobileNet_0.75  2.59         9.86       53.5     N/A            \nMobileNet_0.5   1.33         5.08       77.5     N/A            \nMobileNet_0.25  0.47         1.79       143.6    N/A            \n```\n:::\n:::\n\n\n### Accuracy vs Efficiency Analysis\n\n::: {#cell-accuracy-efficiency .cell caption='Analyzing trade-offs between accuracy and efficiency' execution_count=22}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/accuracy-efficiency-output-1.png){#accuracy-efficiency}\n:::\n:::\n\n\n### Real-world Deployment Simulation\n\n::: {#deployment-simulation .cell caption='Simulating real-world deployment scenarios' execution_count=23}\n``` {.python .cell-code}\n# Real-world deployment simulation\nclass DeploymentSimulator:\n    \"\"\"Simulate real-world deployment scenarios.\"\"\"\n    \n    def __init__(self, model):\n        self.model = model\n    \n    def simulate_mobile_inference(self, num_images=1000, target_fps=30):\n        \"\"\"Simulate mobile device inference.\"\"\"\n        device = 'cpu'  # Mobile devices typically use CPU\n        model = self.model.to(device)\n        model.eval()\n        \n        # Simulate various image sizes\n        image_sizes = [(224, 224), (320, 320), (416, 416)]\n        results = {}\n        \n        for size in image_sizes:\n            input_tensor = torch.randn(1, 3, *size)\n            \n            # Measure inference time\n            times = []\n            with torch.no_grad():\n                for _ in range(100):  # Warmup and measurement\n                    start = time.time()\n                    _ = model(input_tensor)\n                    times.append(time.time() - start)\n            \n            avg_time = np.mean(times[10:])  # Skip first 10 for warmup\n            fps = 1.0 / avg_time\n            \n            results[f'{size[0]}x{size[1]}'] = {\n                'fps': fps,\n                'meets_target': fps >= target_fps,\n                'latency_ms': avg_time * 1000\n            }\n        \n        return results\n    \n    def battery_consumption_estimate(self, inference_time_ms, device_type='mobile'):\n        \"\"\"Estimate battery consumption per inference.\"\"\"\n        \n        # Rough estimates based on device type\n        power_consumption = {\n            'mobile': 2.0,  # Watts during inference\n            'edge': 5.0,    # Edge devices\n            'embedded': 0.5  # Low-power embedded\n        }\n        \n        power_w = power_consumption.get(device_type, 2.0)\n        energy_per_inference = (inference_time_ms / 1000) * power_w  # Joules\n        \n        # Convert to more meaningful metrics\n        battery_capacity_wh = 15  # Typical smartphone battery ~15 Wh\n        inferences_per_battery = (battery_capacity_wh * 3600) / energy_per_inference\n        \n        return {\n            'energy_per_inference_j': energy_per_inference,\n            'estimated_inferences_per_battery': int(inferences_per_battery),\n            'power_consumption_w': power_w\n        }\n```\n:::\n\n\n### Performance Metrics Dashboard\n\n::: {#cell-performance-dashboard .cell caption='Comprehensive performance analysis dashboard' execution_count=24}\n\n::: {.cell-output .cell-output-stdout}\n```\nüî¨ **Running Comprehensive MobileNet Analysis...**\n\nüñ•Ô∏è  Using device: cpu\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/performance-dashboard-output-2.png){#performance-dashboard}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nüéØ **Deployment Recommendations:**\n\nüöÄ **Fastest Inference:** MobileNet_0.25 (135.3 FPS)\n   ‚úÖ Best for: Real-time applications, video processing\n   üì± Recommended: High-end mobile devices, edge servers\n\nüíæ **Most Memory Efficient:** MobileNet_1.0 (inf MB peak)\n   ‚úÖ Best for: Memory-constrained devices\n   üì± Recommended: Budget smartphones, IoT devices\n\nüì¶ **Smallest Model:** MobileNet_0.25 (1.8 MB)\n   ‚úÖ Best for: App size constraints, OTA updates\n   üì± Recommended: Mobile apps with size limits\n\n‚ö° **Fewest Parameters:** MobileNet_0.25 (0.5M params)\n   ‚úÖ Best for: Ultra-low power devices\n   üì± Recommended: Microcontrollers, embedded systems\n\nüèÜ **Best Overall Balance:** MobileNet_0.25\n   üí° Efficiency Score: 160.509\n   ‚úÖ Best for: General-purpose mobile AI applications\n   üì± Recommended: Production deployments\n```\n:::\n:::\n\n\n## Advanced Topics\n\n### Neural Architecture Search (NAS) for MobileNet\n\n::: {#nas-mobilenet .cell caption='Neural Architecture Search for MobileNet optimization' execution_count=25}\n``` {.python .cell-code}\nclass MobileNetSearchSpace:\n    \"\"\"Define search space for MobileNet architecture optimization.\"\"\"\n    \n    def __init__(self):\n        self.width_multipliers = [0.25, 0.35, 0.5, 0.75, 1.0, 1.4]\n        self.depth_multipliers = [0.5, 0.75, 1.0, 1.25]\n        self.kernel_sizes = [3, 5, 7]\n        self.activation_functions = ['relu6', 'swish', 'hard_swish']\n    \n    def sample_architecture(self):\n        \"\"\"Sample a random architecture from search space.\"\"\"\n        import random\n        \n        return {\n            'width_mult': random.choice(self.width_multipliers),\n            'depth_mult': random.choice(self.depth_multipliers),\n            'kernel_size': random.choice(self.kernel_sizes),\n            'activation': random.choice(self.activation_functions)\n        }\n    \n    def evaluate_architecture(self, arch_config, train_loader, val_loader):\n        \"\"\"Evaluate a sampled architecture.\"\"\"\n        \n        # Create model with sampled configuration\n        model = self.create_model_from_config(arch_config)\n        \n        # Quick training (few epochs for NAS efficiency)\n        trainer = MobileNetTrainer(model)\n        trainer.train(train_loader, val_loader, epochs=5)\n        \n        # Calculate efficiency metrics\n        benchmark = MobileNetBenchmark(model)\n        perf_stats = benchmark.benchmark_inference()\n        \n        # Return multi-objective score\n        accuracy = trainer.history['val_acc'][-1]\n        latency = perf_stats['mean_time']\n        \n        # Pareto efficiency score\n        score = accuracy / (latency * 1000)  # Accuracy per ms\n        \n        return {\n            'score': score,\n            'accuracy': accuracy,\n            'latency': latency,\n            'config': arch_config\n        }\n    \n    def create_model_from_config(self, config):\n        \"\"\"Create MobileNet model from configuration.\"\"\"\n        # Simplified - in practice would build full architecture\n        return mobilenet_v1(\n            width_mult=config['width_mult'],\n            num_classes=1000\n        )\n```\n:::\n\n\n### Knowledge Distillation for MobileNet {#sec-knowledge-distillation}\n\n::: {#knowledge-distillation .cell caption='Knowledge distillation to improve MobileNet performance' execution_count=26}\n``` {.python .cell-code}\n# Knowledge Distillation for MobileNet\nclass KnowledgeDistillation:\n    \"\"\"Knowledge distillation to improve MobileNet performance.\"\"\"\n    \n    def __init__(self, teacher_model, student_model, temperature=4.0, alpha=0.3):\n        self.teacher = teacher_model\n        self.student = student_model\n        self.temperature = temperature\n        self.alpha = alpha  # Weight for distillation loss\n        \n        # Freeze teacher model\n        for param in self.teacher.parameters():\n            param.requires_grad = False\n        self.teacher.eval()\n    \n    def distillation_loss(self, student_outputs, teacher_outputs, labels):\n        \"\"\"Calculate knowledge distillation loss.\"\"\"\n        \n        # Soft targets from teacher\n        teacher_probs = F.softmax(teacher_outputs / self.temperature, dim=1)\n        student_log_probs = F.log_softmax(student_outputs / self.temperature, dim=1)\n        \n        # KL divergence loss\n        distillation_loss = F.kl_div(\n            student_log_probs, teacher_probs, reduction='batchmean'\n        ) * (self.temperature ** 2)\n        \n        # Standard cross-entropy loss\n        student_loss = F.cross_entropy(student_outputs, labels)\n        \n        # Combined loss\n        total_loss = (\n            self.alpha * distillation_loss + \n            (1 - self.alpha) * student_loss\n        )\n        \n        return total_loss\n    \n    def train_with_distillation(self, train_loader, val_loader, epochs=10):\n        \"\"\"Train student model with knowledge distillation.\"\"\"\n        \n        optimizer = optim.Adam(self.student.parameters(), lr=0.001)\n        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n        \n        for epoch in range(epochs):\n            self.student.train()\n            running_loss = 0.0\n            \n            for inputs, labels in train_loader:\n                optimizer.zero_grad()\n                \n                # Get predictions from both models\n                with torch.no_grad():\n                    teacher_outputs = self.teacher(inputs)\n                \n                student_outputs = self.student(inputs)\n                \n                # Calculate distillation loss\n                loss = self.distillation_loss(student_outputs, teacher_outputs, labels)\n                \n                loss.backward()\n                optimizer.step()\n                running_loss += loss.item()\n            \n            scheduler.step()\n            \n            # Validation\n            val_acc = self.validate(val_loader)\n            print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Val Acc: {val_acc:.2f}%')\n    \n    def validate(self, val_loader):\n        \"\"\"Validate student model.\"\"\"\n        self.student.eval()\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                outputs = self.student(inputs)\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        \n        return 100 * correct / total\n```\n:::\n\n\n## Conclusion\n\nThis comprehensive guide has covered MobileNet from fundamental concepts to production deployment. The journey through depthwise separable convolutions, implementation details, optimization techniques, and real-world deployment strategies provides a complete foundation for building efficient mobile AI applications.\n\n### Key Takeaways\n\n::: {.callout-note}\n## üéØ **Essential Insights**\n\n**Architectural Innovation:**\n- Depthwise separable convolutions reduce computation by 8-9√ó with minimal accuracy loss\n- Width multipliers provide flexible trade-offs between accuracy and efficiency\n- The architecture scales gracefully across different hardware constraints\n\n**Implementation Best Practices:**\n- Always profile on target hardware before deployment\n- Use appropriate data augmentation for robust training\n- Consider knowledge distillation for improved student model performance\n- Apply quantization and pruning strategically based on deployment requirements\n:::\n\n### Performance Summary\n\nBased on our comprehensive analysis, here are the recommended MobileNet configurations:\n\n| **Use Case** | **Configuration** | **Expected Performance** | **Deployment Target** |\n|--------------|-------------------|--------------------------|------------------------|\n| **Real-time Video** | MobileNet-V2 1.0√ó | 30+ FPS, 72% accuracy | High-end mobile devices |\n| **General Mobile AI** | MobileNet-V1 0.75√ó | 45+ FPS, 68% accuracy | Mid-range smartphones |\n| **Edge Computing** | MobileNet-V1 0.5√ó | 60+ FPS, 64% accuracy | Edge servers, IoT hubs |\n| **Embedded Systems** | MobileNet-V1 0.25√ó | 80+ FPS, 51% accuracy | Microcontrollers, sensors |\n\n### Deployment Recommendations\n\n::: {.callout-tip}\n## üöÄ **Production Deployment Checklist**\n\n**Pre-deployment:**\n\n- [ ] Benchmark on actual target hardware\n- [ ] Validate accuracy on representative test data  \n- [ ] Measure memory usage under realistic conditions\n- [ ] Test battery consumption (for mobile devices)\n- [ ] Verify model export/conversion pipeline\n\n**Optimization Pipeline:**\n\n- [ ] Apply appropriate quantization (dynamic/static)\n- [ ] Consider structured pruning for further compression\n- [ ] Export to platform-specific formats (ONNX, TFLite, CoreML)\n- [ ] Implement efficient preprocessing pipelines\n- [ ] Add monitoring and performance tracking\n\n**Platform Integration:**\n\n- [ ] Handle model loading and initialization efficiently\n- [ ] Implement proper error handling and fallbacks\n- [ ] Use background threads for inference\n- [ ] Cache models and avoid repeated loading\n- [ ] Plan for model updates and versioning\n:::\n\n### Common Pitfalls and Solutions\n\n::: {.callout-warning}\n## ‚ö†Ô∏è **Avoid These Mistakes**\n\n**Performance Issues:**\n\n- **Problem**: Model runs slower on device than benchmarks suggest\n- **Solution**: Always test with realistic input pipelines and preprocessing\n\n**Memory Problems:**\n\n- **Problem**: Out of memory errors during inference  \n- **Solution**: Monitor peak memory usage, not just model size\n\n**Accuracy Degradation:**\n\n- **Problem**: Significant accuracy drop after optimization\n- **Solution**: Use quantization-aware training and gradual pruning\n\n**Integration Challenges:**\n\n- **Problem**: Model format incompatibility with deployment platform\n- **Solution**: Test export pipeline early and validate outputs\n:::\n\n### Future Directions\n\nThe field of efficient neural networks continues to evolve rapidly:\n\n**Next-Generation Architectures:**\n\n- **EfficientNet** and **EfficientNetV2**: Better scaling strategies with compound scaling\n- **MobileViT**: Combining CNNs with Vision Transformers for mobile deployment\n- **Once-for-All Networks**: Single networks supporting multiple deployment scenarios\n\n**Advanced Optimization Techniques:**\n\n- **Neural Architecture Search (NAS)**: Automated architecture optimization\n- **Differentiable Architecture Search**: End-to-end learnable architectures  \n- **Hardware-aware NAS**: Optimizing specifically for target hardware\n\n**Deployment Innovations:**\n\n- **Edge AI Accelerators**: Custom silicon for mobile AI (Apple Neural Engine, Google Edge TPU)\n- **Federated Learning**: Training models across distributed mobile devices\n- **Model Compression**: Advanced techniques beyond pruning and quantization\n\n### Resources and Further Reading\n\n::: {.callout-note}\n## üìö **Additional Resources**\n\n**Essential Papers:**\n\n- [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)\n- [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381)\n- [Searching for MobileNetV3](https://arxiv.org/abs/1905.02244)\n\n**Implementation Resources:**\n\n- [PyTorch Mobile Documentation](https://pytorch.org/mobile/home/)\n- [TensorFlow Lite Guide](https://www.tensorflow.org/lite)\n- [ONNX Runtime Mobile](https://onnxruntime.ai/docs/tutorials/mobile/)\n\n**Community and Support:**\n\n- [PyTorch Forums - Mobile](https://discuss.pytorch.org/c/mobile/19)\n- [TensorFlow Community](https://www.tensorflow.org/community)\n- [Papers With Code - Mobile AI](https://paperswithcode.com/task/mobile-ai)\n:::\n\n### Final Thoughts {#sec-final-thoughts}\n\nMobileNet represents a paradigm shift in how we approach deep learning for resource-constrained environments. The techniques and principles covered in this guide extend beyond MobileNet itself ‚Äì they form the foundation for understanding and implementing efficient AI systems across a wide range of applications.\n\nAs mobile and edge AI continues to grow, the ability to design, implement, and deploy efficient neural networks becomes increasingly valuable. Whether you're building the next generation of mobile apps, edge computing solutions, or embedded AI systems, the concepts and code in this guide provide a solid foundation for success.\n\n::: {.callout-important}\n## üéØ **Remember**\nThe best model is not necessarily the most accurate one, but the one that best serves your users within the constraints of your deployment environment. Always optimize for the complete user experience, not just benchmark metrics.\n:::\n\n## References\n\n- Howard, A. G., et al. (2017). MobileNets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861.\n- Sandler, M., et al. (2018). MobileNetV2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4510-4520).\n- Howard, A., et al. (2019). Searching for mobilenetv3. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1314-1324).\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}