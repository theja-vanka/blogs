{
  "hash": "0214ff996ebe957cb168d408bb42396e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Complete Guide to Mamba Transformers: Implementation and Theory\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-08-23\"\ncategories: [code, tutorial, intermediate]\nformat:\n  html:\n    code-fold: false\n    math: mathjax\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# Complete Guide to Mamba Transformers: Implementation and Theory\n![](mambac.png)\n\n## Introduction to Mamba {#sec-introduction}\n\nMamba is a revolutionary architecture that addresses the quadratic complexity problem of traditional transformers through selective state space models (SSMs). Unlike transformers that use attention mechanisms, Mamba processes sequences with linear complexity while maintaining comparable or superior performance.\n\n### Key Advantages\n\n- **Linear Complexity**: $O(L)$ instead of $O(L^2)$ for sequence length $L$\n- **Selective Mechanism**: Dynamic parameter adjustment based on input\n- **Hardware Efficiency**: Better memory usage and parallelization\n- **Long Context**: Can handle much longer sequences effectively\n\n### Architecture Overview\n\n```{mermaid}\n%%| echo: false\n\ngraph LR\n    A[Input] --> B[Embedding]\n    B --> C[Mamba Blocks]\n    C --> D[Output Projection]\n    D --> E[Logits]\n```\n\n## Mathematical Foundation\n\n### State Space Models (SSMs)\n\nThe core of Mamba is based on continuous-time state space models:\n\n$$\n\\frac{dx}{dt} = Ax(t) + Bu(t)\n$$\n\n$$\ny(t) = Cx(t) + Du(t)\n$$\n\nDiscretized version:\n\n$$\nx_k = \\bar{A}x_{k-1} + \\bar{B}u_k\n$$ \n\n$$\ny_k = Cx_k + Du_k\n$$\n\nWhere:\n\n- $\\bar{A} = \\exp(\\Delta A)$ (matrix exponential)\n- $\\bar{B} = (\\Delta A)^{-1}(\\bar{A} - I)\\Delta B$\n- $\\Delta$ is the discretization step size\n\n### Selective Mechanism\n\nMamba introduces selectivity by making $B$, $C$, and $\\Delta$ input-dependent:\n\n```python\nB = Linear_B(x)    # Input-dependent B matrix\nC = Linear_C(x)    # Input-dependent C matrix  \nΔ = softplus(Linear_Δ(x))  # Input-dependent step size\n```\n\n## Core Components\n\n### Selective Scan Algorithm\n\nThe heart of Mamba is the selective scan that computes:\n\n::: {#67ffd0c4 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"false\"}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange, repeat\nimport math\n\ndef selective_scan(u, delta, A, B, C, D):\n    \"\"\"\n    Selective scan implementation\n    \n    Parameters:\n    -----------\n    u : torch.Tensor\n        Input sequence (B, L, D)\n    delta : torch.Tensor\n        Step sizes (B, L, D) \n    A : torch.Tensor\n        State matrix (D, N)\n    B : torch.Tensor\n        Input matrix (B, L, N)\n    C : torch.Tensor\n        Output matrix (B, L, N) \n    D : torch.Tensor\n        Feedthrough (D,)\n        \n    Returns:\n    --------\n    torch.Tensor\n        Output sequence (B, L, D)\n    \"\"\"\n    deltaA = torch.exp(delta.unsqueeze(-1) * A)  # (B, L, D, N)\n    deltaB = delta.unsqueeze(-1) * B.unsqueeze(2)  # (B, L, D, N)\n    \n    # Parallel scan implementation\n    x = torch.zeros(B.shape[0], A.shape[-1], device=u.device)\n    outputs = []\n    \n    for i in range(u.shape[1]):\n        x = deltaA[:, i] * x + deltaB[:, i] * u[:, i].unsqueeze(-1)\n        y = torch.einsum('bdn,bn->bd', x, C[:, i]) + D * u[:, i]\n        outputs.append(y)\n    \n    return torch.stack(outputs, dim=1)\n```\n:::\n\n\n### Mamba Block Architecture\n\n::: {#69ac4b7b .cell execution_count=2}\n``` {.python .cell-code}\nclass MambaBlock(nn.Module):\n    \"\"\"\n    Mamba block implementing selective state space model\n    \"\"\"\n    def __init__(self, d_model, d_state=16, d_conv=4, expand=2):\n        super().__init__()\n        self.d_model = d_model\n        self.d_state = d_state\n        self.d_conv = d_conv\n        self.d_inner = int(expand * d_model)\n        \n        # Input projection\n        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=False)\n        \n        # Convolution layer\n        self.conv1d = nn.Conv1d(\n            in_channels=self.d_inner,\n            out_channels=self.d_inner,\n            kernel_size=d_conv,\n            bias=True,\n            groups=self.d_inner,\n            padding=d_conv - 1,\n        )\n        \n        # SSM parameters\n        self.x_proj = nn.Linear(self.d_inner, self.d_state * 2, bias=False)\n        self.dt_proj = nn.Linear(self.d_inner, self.d_inner, bias=True)\n        \n        # Initialize A matrix (complex initialization for stability)\n        A = repeat(torch.arange(1, self.d_state + 1), 'n -> d n', d=self.d_inner)\n        self.A_log = nn.Parameter(torch.log(A))\n        \n        # Output projection\n        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)\n```\n:::\n\n\n## Complete Implementation\n\n### Full Mamba Model\n\n::: {#afa6c235 .cell execution_count=3}\n``` {.python .cell-code}\nclass Mamba(nn.Module):\n    \"\"\"\n    Complete Mamba model implementation\n    \"\"\"\n    def __init__(\n        self,\n        d_model: int,\n        n_layer: int,\n        vocab_size: int,\n        d_state: int = 16,\n        expand: int = 2,\n        dt_rank: str = \"auto\",\n        d_conv: int = 4,\n        conv_bias: bool = True,\n        bias: bool = False,\n    ):\n        super().__init__()\n        self.d_model = d_model\n        self.n_layer = n_layer\n        self.vocab_size = vocab_size\n        \n        # Token embeddings\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        \n        # Mamba layers\n        self.layers = nn.ModuleList([\n            ResidualBlock(\n                MambaBlock(\n                    d_model=d_model,\n                    d_state=d_state,\n                    expand=expand,\n                    dt_rank=dt_rank,\n                    d_conv=d_conv,\n                    conv_bias=conv_bias,\n                    bias=bias,\n                )\n            )\n            for _ in range(n_layer)\n        ])\n        \n        # Final layer norm and output projection\n        self.norm_f = RMSNorm(d_model)\n        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n        \n        # Weight tying\n        self.lm_head.weight = self.embedding.weight\n\n    def forward(self, input_ids):\n        \"\"\"\n        Forward pass\n        \n        Parameters:\n        -----------\n        input_ids : torch.Tensor\n            Input token ids (batch, seqlen)\n            \n        Returns:\n        --------\n        torch.Tensor\n            Logits (batch, seqlen, vocab_size)\n        \"\"\"\n        x = self.embedding(input_ids)\n        \n        for layer in self.layers:\n            x = layer(x)\n            \n        x = self.norm_f(x)\n        logits = self.lm_head(x)\n        \n        return logits\n```\n:::\n\n\n### Enhanced MambaBlock Implementation\n\n::: {#87e80f9e .cell execution_count=4}\n``` {.python .cell-code}\nclass MambaBlock(nn.Module):\n    def __init__(\n        self,\n        d_model,\n        d_state=16,\n        expand=2,\n        dt_rank=\"auto\",\n        d_conv=4,\n        conv_bias=True,\n        bias=False,\n    ):\n        super().__init__()\n        self.d_model = d_model\n        self.d_state = d_state\n        self.expand = expand\n        self.d_inner = int(self.expand * self.d_model)\n        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == \"auto\" else dt_rank\n        \n        # Input projections\n        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias)\n        \n        # Convolution\n        self.conv1d = nn.Conv1d(\n            in_channels=self.d_inner,\n            out_channels=self.d_inner,\n            bias=conv_bias,\n            kernel_size=d_conv,\n            groups=self.d_inner,\n            padding=d_conv - 1,\n        )\n\n        # SSM projections\n        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + self.d_state * 2, bias=False)\n        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)\n\n        # Initialize dt projection\n        dt_init_std = self.dt_rank**-0.5 * self.d_model**-0.5\n        with torch.no_grad():\n            self.dt_proj.weight.uniform_(-dt_init_std, dt_init_std)\n\n        # Initialize A matrix (S4D initialization)\n        A = repeat(\n            torch.arange(1, self.d_state + 1, dtype=torch.float32),\n            \"n -> d n\",\n            d=self.d_inner,\n        ).contiguous()\n        A_log = torch.log(A)\n        self.A_log = nn.Parameter(A_log)\n        \n        # Initialize D parameter\n        self.D = nn.Parameter(torch.ones(self.d_inner))\n        \n        # Output projection\n        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through Mamba block\n        \n        Parameters:\n        -----------\n        x : torch.Tensor\n            Input tensor (B, L, D)\n            \n        Returns:\n        --------\n        torch.Tensor\n            Output tensor (B, L, D)\n        \"\"\"\n        (B, L, D) = x.shape\n        \n        # Input projections\n        x_and_res = self.in_proj(x)  # (B, L, 2 * d_inner)\n        x, res = x_and_res.split(split_size=[self.d_inner, self.d_inner], dim=-1)\n        \n        # Convolution\n        x = rearrange(x, 'b l d -> b d l')\n        x = self.conv1d(x)[:, :, :L]  # Truncate to original length\n        x = rearrange(x, 'b d l -> b l d')\n        \n        # Activation\n        x = F.silu(x)\n        \n        # SSM\n        y = self.ssm(x)\n        \n        # Gating and output projection\n        y = y * F.silu(res)\n        output = self.out_proj(y)\n        \n        return output\n\n    def ssm(self, x):\n        \"\"\"\n        Selective State Space Model computation\n        \"\"\"\n        (B, L, D) = x.shape\n        N = self.d_state\n        \n        # Extract A matrix\n        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)\n        \n        # Compute Δ, B, C\n        x_dbl = self.x_proj(x)  # (B, L, dt_rank + 2*d_state)\n        \n        delta, B, C = torch.split(\n            x_dbl, [self.dt_rank, N, N], dim=-1\n        )  # delta: (B, L, dt_rank), B, C: (B, L, d_state)\n        \n        delta = F.softplus(self.dt_proj(delta))  # (B, L, d_inner)\n        \n        # Selective scan\n        y = self.selective_scan(x, delta, A, B, C, self.D)\n        \n        return y\n\n    def selective_scan(self, u, delta, A, B, C, D):\n        \"\"\"\n        Selective scan implementation with parallel processing\n        \"\"\"\n        (B, L, D) = u.shape\n        N = A.shape[-1]\n        \n        # Discretize A and B\n        deltaA = torch.exp(self.einsum(delta, A, 'b l d, d n -> b l d n'))\n        deltaB_u = self.einsum(delta, B, u, 'b l d, b l n, b l d -> b l d n')\n        \n        # Parallel scan (simplified version)\n        x = torch.zeros((B, D, N), device=deltaA.device, dtype=deltaA.dtype)\n        ys = []\n        \n        for i in range(L):\n            x = deltaA[:, i] * x + deltaB_u[:, i]\n            y = self.einsum(x, C[:, i], 'b d n, b n -> b d')\n            ys.append(y)\n        \n        y = torch.stack(ys, dim=1)  # (B, L, D)\n        \n        # Add skip connection\n        y = y + u * D\n        \n        return y\n    \n    @staticmethod\n    def einsum(q, k, v=None, equation=None):\n        \"\"\"Helper function for einsum operations\"\"\"\n        if v is None:\n            return torch.einsum(equation, q, k)\n        return torch.einsum(equation, q, k, v)\n```\n:::\n\n\n### Supporting Components\n\n::: {#c0e1e8ef .cell execution_count=5}\n``` {.python .cell-code}\nclass ResidualBlock(nn.Module):\n    \"\"\"Residual block with pre-normalization\"\"\"\n    def __init__(self, mixer):\n        super().__init__()\n        self.mixer = mixer\n        self.norm = RMSNorm(mixer.d_model)\n\n    def forward(self, x):\n        return self.mixer(self.norm(x)) + x\n\nclass RMSNorm(nn.Module):\n    \"\"\"Root Mean Square Layer Normalization\"\"\"\n    def __init__(self, d_model, eps=1e-5):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(d_model))\n\n    def forward(self, x):\n        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n        return output\n```\n:::\n\n\n## Training and Optimization\n\n### Training Configuration\n\n::: {#b86b21ce .cell execution_count=6}\n``` {.python .cell-code}\nclass TrainingConfig:\n    \"\"\"Configuration class for training hyperparameters\"\"\"\n    # Model architecture\n    d_model: int = 768\n    n_layer: int = 24\n    vocab_size: int = 50257\n    \n    # Training parameters\n    batch_size: int = 32\n    learning_rate: float = 1e-4\n    weight_decay: float = 0.1\n    max_seq_len: int = 2048\n    \n    # Optimization\n    warmup_steps: int = 2000\n    max_steps: int = 100000\n    eval_interval: int = 1000\n    \n    # Hardware optimization\n    mixed_precision: bool = True\n    gradient_checkpointing: bool = True\n```\n:::\n\n\n### Optimizer Setup\n\n::: {#9a9184da .cell execution_count=7}\n``` {.python .cell-code}\ndef create_optimizer(model, config):\n    \"\"\"\n    Create optimizer with proper weight decay configuration\n    \n    Parameters:\n    -----------\n    model : nn.Module\n        The model to optimize\n    config : TrainingConfig\n        Training configuration\n        \n    Returns:\n    --------\n    torch.optim.AdamW\n        Configured optimizer\n    \"\"\"\n    # Separate parameters for weight decay\n    decay = set()\n    no_decay = set()\n    \n    for mn, m in model.named_modules():\n        for pn, p in m.named_parameters():\n            fpn = f'{mn}.{pn}' if mn else pn\n            \n            if 'bias' in pn or 'norm' in pn or 'embedding' in pn:\n                no_decay.add(fpn)\n            else:\n                decay.add(fpn)\n    \n    param_dict = {pn: p for pn, p in model.named_parameters()}\n    \n    optim_groups = [\n        {\n            'params': [param_dict[pn] for pn in sorted(list(decay))], \n            'weight_decay': config.weight_decay\n        },\n        {\n            'params': [param_dict[pn] for pn in sorted(list(no_decay))], \n            'weight_decay': 0.0\n        },\n    ]\n    \n    return torch.optim.AdamW(optim_groups, lr=config.learning_rate)\n```\n:::\n\n\n### Training Loop Implementation\n\n::: {#647559f2 .cell execution_count=8}\n``` {.python .cell-code}\nclass MambaTrainer:\n    \"\"\"Comprehensive trainer for Mamba models\"\"\"\n    \n    def __init__(self, model, config, train_loader, val_loader):\n        self.model = model\n        self.config = config\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        \n        self.optimizer = create_optimizer(model, config)\n        self.scheduler = self.create_scheduler()\n        self.scaler = torch.cuda.amp.GradScaler() if config.mixed_precision else None\n        \n    def create_scheduler(self):\n        \"\"\"Create cosine annealing scheduler with warmup\"\"\"\n        def lr_lambda(step):\n            if step < self.config.warmup_steps:\n                return step / self.config.warmup_steps\n            else:\n                progress = (step - self.config.warmup_steps) / \\\n                          (self.config.max_steps - self.config.warmup_steps)\n                return 0.5 * (1 + math.cos(math.pi * progress))\n        \n        return torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n    \n    def train_step(self, batch):\n        \"\"\"Single training step with mixed precision\"\"\"\n        self.model.train()\n        \n        input_ids = batch['input_ids']\n        targets = input_ids[:, 1:].contiguous()\n        input_ids = input_ids[:, :-1].contiguous()\n        \n        with torch.cuda.amp.autocast(enabled=self.config.mixed_precision):\n            logits = self.model(input_ids)\n            loss = F.cross_entropy(\n                logits.view(-1, logits.size(-1)), \n                targets.view(-1),\n                ignore_index=-1\n            )\n        \n        # Backward pass with gradient scaling\n        if self.scaler:\n            self.scaler.scale(loss).backward()\n            self.scaler.unscale_(self.optimizer)\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n        else:\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n            self.optimizer.step()\n        \n        self.optimizer.zero_grad()\n        self.scheduler.step()\n        \n        return loss.item()\n```\n:::\n\n\n## Practical Applications\n\n### Text Generation\n\n::: {#edc61fcc .cell execution_count=9}\n``` {.python .cell-code}\ndef generate_text(model, tokenizer, prompt, max_length=100, temperature=0.8):\n    \"\"\"\n    Generate text using Mamba model\n    \n    Parameters:\n    -----------\n    model : Mamba\n        Trained Mamba model\n    tokenizer : Tokenizer\n        Text tokenizer\n    prompt : str\n        Input prompt\n    max_length : int\n        Maximum generation length\n    temperature : float\n        Sampling temperature\n        \n    Returns:\n    --------\n    str\n        Generated text\n    \"\"\"\n    model.eval()\n    \n    # Tokenize prompt\n    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n    \n    with torch.no_grad():\n        for _ in range(max_length):\n            # Forward pass\n            logits = model(input_ids)\n            \n            # Sample next token\n            next_token_logits = logits[:, -1, :] / temperature\n            probs = F.softmax(next_token_logits, dim=-1)\n            next_token = torch.multinomial(probs, num_samples=1)\n            \n            # Append to sequence\n            input_ids = torch.cat([input_ids, next_token], dim=1)\n            \n            # Check for end token\n            if next_token.item() == tokenizer.eos_token_id:\n                break\n    \n    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n\n# Usage example\n# prompt = \"The future of artificial intelligence is\"\n# generated = generate_text(model, tokenizer, prompt)\n# print(generated)\n```\n:::\n\n\n### Document Classification\n\n::: {#2a60ff73 .cell execution_count=10}\n``` {.python .cell-code}\nclass MambaClassifier(nn.Module):\n    \"\"\"Mamba-based document classifier\"\"\"\n    \n    def __init__(self, mamba_model, num_classes):\n        super().__init__()\n        self.mamba = mamba_model\n        self.classifier = nn.Linear(mamba_model.d_model, num_classes)\n        \n    def forward(self, input_ids, attention_mask=None):\n        \"\"\"\n        Forward pass for classification\n        \n        Parameters:\n        -----------\n        input_ids : torch.Tensor\n            Input token ids\n        attention_mask : torch.Tensor, optional\n            Attention mask for padding tokens\n            \n        Returns:\n        --------\n        torch.Tensor\n            Classification logits\n        \"\"\"\n        # Get Mamba features\n        hidden_states = self.mamba.embedding(input_ids)\n        \n        for layer in self.mamba.layers:\n            hidden_states = layer(hidden_states)\n        \n        hidden_states = self.mamba.norm_f(hidden_states)\n        \n        # Global average pooling\n        if attention_mask is not None:\n            mask = attention_mask.unsqueeze(-1).expand_as(hidden_states).float()\n            pooled = (hidden_states * mask).sum(1) / mask.sum(1)\n        else:\n            pooled = hidden_states.mean(1)\n        \n        # Classification\n        logits = self.classifier(pooled)\n        return logits\n```\n:::\n\n\n## Performance Optimization\n\n### Memory Optimization\n\n::: {#7fd35770 .cell execution_count=11}\n``` {.python .cell-code}\nclass OptimizedMamba(Mamba):\n    \"\"\"Memory-optimized Mamba with gradient checkpointing\"\"\"\n    \n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.gradient_checkpointing = True\n        \n    def forward(self, input_ids):\n        \"\"\"Forward pass with optional gradient checkpointing\"\"\"\n        x = self.embedding(input_ids)\n        \n        # Use checkpointing for memory efficiency\n        for layer in self.layers:\n            if self.gradient_checkpointing and self.training:\n                x = torch.utils.checkpoint.checkpoint(layer, x)\n            else:\n                x = layer(x)\n                \n        x = self.norm_f(x)\n        logits = self.lm_head(x)\n        \n        return logits\n\ndef profile_memory(model, input_size):\n    \"\"\"\n    Profile memory usage of the model\n    \n    Parameters:\n    -----------\n    model : nn.Module\n        Model to profile\n    input_size : tuple\n        Input tensor size\n        \n    Returns:\n    --------\n    float\n        Peak memory usage in GB\n    \"\"\"\n    dummy_input = torch.randint(0, model.vocab_size, input_size)\n    \n    torch.cuda.reset_peak_memory_stats()\n    \n    with torch.cuda.amp.autocast():\n        output = model(dummy_input)\n        loss = output.sum()\n        loss.backward()\n    \n    peak_memory = torch.cuda.max_memory_allocated() / 1024**3  # GB\n    print(f\"Peak memory usage: {peak_memory:.2f} GB\")\n    \n    return peak_memory\n```\n:::\n\n\n## Performance Comparison\n\n### Complexity Analysis\n\n| Metric | Transformer | Mamba |\n|--------|-------------|--------|\n| Time Complexity | $O(L^2d)$ | $O(Ld)$ |\n| Memory Complexity | $O(L^2)$ | $O(L)$ |\n| Parallelization | High (attention) | Medium (selective scan) |\n| Long Context Scaling | Quadratic | Linear |\n\n: Computational complexity comparison between Transformer and Mamba architectures\n\n### Benchmarking Implementation\n\n::: {#6d1aae94 .cell execution_count=12}\n``` {.python .cell-code}\ndef benchmark_models():\n    \"\"\"\n    Compare Mamba vs Transformer performance across sequence lengths\n    \n    Returns:\n    --------\n    dict\n        Benchmark results containing memory and time measurements\n    \"\"\"\n    sequence_lengths = [512, 1024, 2048, 4096, 8192]\n    results = {\n        'mamba': {'memory': [], 'time': []},\n        'transformer': {'memory': [], 'time': []}\n    }\n    \n    for seq_len in sequence_lengths:\n        # Benchmark Mamba\n        mamba_model = Mamba(d_model=768, n_layer=12, vocab_size=50257)\n        mamba_memory, mamba_time = benchmark_single_model(mamba_model, seq_len)\n        \n        # Benchmark would require transformer implementation\n        # transformer_model = GPT2Model.from_pretrained('gpt2')\n        # transformer_memory, transformer_time = benchmark_single_model(transformer_model, seq_len)\n        \n        results['mamba']['memory'].append(mamba_memory)\n        results['mamba']['time'].append(mamba_time)\n        # results['transformer']['memory'].append(transformer_memory)\n        # results['transformer']['time'].append(transformer_time)\n    \n    return results\n\ndef benchmark_single_model(model, seq_len):\n    \"\"\"\n    Benchmark a single model for memory and time\n    \n    Parameters:\n    -----------\n    model : nn.Module\n        Model to benchmark\n    seq_len : int\n        Sequence length to test\n        \n    Returns:\n    --------\n    tuple\n        (memory_usage_gb, time_seconds)\n    \"\"\"\n    import time\n    \n    batch_size = 8\n    vocab_size = getattr(model, 'vocab_size', 50257)\n    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n    \n    # Memory benchmark\n    torch.cuda.reset_peak_memory_stats()\n    \n    start_time = time.time()\n    with torch.cuda.amp.autocast():\n        output = model(input_ids)\n        loss = output.logits.mean() if hasattr(output, 'logits') else output.mean()\n        loss.backward()\n    \n    end_time = time.time()\n    \n    memory_used = torch.cuda.max_memory_allocated() / 1024**3  # GB\n    time_taken = end_time - start_time\n    \n    return memory_used, time_taken\n```\n:::\n\n\n## Advanced Extensions\n\n### Multi-Modal Mamba\n\n::: {#fbc6f79e .cell execution_count=13}\n``` {.python .cell-code}\nclass MultiModalMamba(nn.Module):\n    \"\"\"Multi-modal Mamba for text and vision processing\"\"\"\n    \n    def __init__(self, text_vocab_size, d_model, n_layer):\n        super().__init__()\n        \n        # Text processing\n        self.text_embedding = nn.Embedding(text_vocab_size, d_model)\n        \n        # Vision processing\n        self.vision_encoder = nn.Linear(768, d_model)  # From vision transformer\n        \n        # Shared Mamba layers\n        self.mamba_layers = nn.ModuleList([\n            MambaBlock(d_model) for _ in range(n_layer)\n        ])\n        \n        # Modality fusion\n        self.fusion_layer = nn.Linear(d_model * 2, d_model)\n        \n    def forward(self, text_ids, vision_features):\n        \"\"\"\n        Process multi-modal inputs\n        \n        Parameters:\n        -----------\n        text_ids : torch.Tensor\n            Text token ids\n        vision_features : torch.Tensor\n            Vision features from encoder\n            \n        Returns:\n        --------\n        torch.Tensor\n            Fused multi-modal representations\n        \"\"\"\n        # Process text\n        text_embeds = self.text_embedding(text_ids)\n        \n        # Process vision\n        vision_embeds = self.vision_encoder(vision_features)\n        \n        # Combine modalities\n        combined = torch.cat([text_embeds, vision_embeds], dim=-1)\n        fused = self.fusion_layer(combined)\n        \n        # Process through Mamba\n        for layer in self.mamba_layers:\n            fused = layer(fused)\n            \n        return fused\n```\n:::\n\n\n### Sparse Mamba Implementation\n\n::: {#82aadec4 .cell execution_count=14}\n``` {.python .cell-code}\nclass SparseMamba(MambaBlock):\n    \"\"\"Sparse version of Mamba with reduced connectivity\"\"\"\n    \n    def __init__(self, *args, sparsity_ratio=0.1, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.sparsity_ratio = sparsity_ratio\n        self.register_buffer('sparsity_mask', torch.ones(self.d_inner, self.d_state))\n        \n        # Initialize sparse connectivity\n        self._initialize_sparse_mask()\n    \n    def _initialize_sparse_mask(self):\n        \"\"\"Initialize sparse connectivity pattern\"\"\"\n        # Random sparsity pattern\n        num_connections = int(self.d_inner * self.d_state * (1 - self.sparsity_ratio))\n        flat_mask = torch.zeros(self.d_inner * self.d_state)\n        indices = torch.randperm(self.d_inner * self.d_state)[:num_connections]\n        flat_mask[indices] = 1\n        self.sparsity_mask = flat_mask.view(self.d_inner, self.d_state)\n    \n    def ssm(self, x):\n        \"\"\"SSM computation with sparse connections\"\"\"\n        (B, L, D) = x.shape\n        N = self.d_state\n        \n        # Apply sparsity mask to A matrix\n        A = -torch.exp(self.A_log.float())\n        A = A * self.sparsity_mask  # Apply sparsity\n        \n        # Rest of the SSM computation remains the same\n        x_dbl = self.x_proj(x)\n        delta, B, C = torch.split(x_dbl, [self.dt_rank, N, N], dim=-1)\n        delta = F.softplus(self.dt_proj(delta))\n        \n        y = self.selective_scan(x, delta, A, B, C, self.D)\n        return y\n```\n:::\n\n\n### Mixture of Experts (MoE) Mamba\n\n::: {#863b6fca .cell execution_count=15}\n``` {.python .cell-code}\nclass MambaExpert(nn.Module):\n    \"\"\"Individual expert in MoE Mamba\"\"\"\n    \n    def __init__(self, d_model, expert_id):\n        super().__init__()\n        self.expert_id = expert_id\n        self.mamba_block = MambaBlock(d_model)\n        \n    def forward(self, x):\n        return self.mamba_block(x)\n\nclass MambaMoE(nn.Module):\n    \"\"\"Mamba with Mixture of Experts\"\"\"\n    \n    def __init__(self, d_model, num_experts=8, top_k=2):\n        super().__init__()\n        self.num_experts = num_experts\n        self.top_k = top_k\n        \n        # Router network\n        self.router = nn.Linear(d_model, num_experts)\n        \n        # Expert networks\n        self.experts = nn.ModuleList([\n            MambaExpert(d_model, i) for i in range(num_experts)\n        ])\n        \n        # Load balancing\n        self.load_balancing_loss_coeff = 0.01\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass through MoE Mamba\n        \n        Parameters:\n        -----------\n        x : torch.Tensor\n            Input tensor (batch_size, seq_len, d_model)\n            \n        Returns:\n        --------\n        torch.Tensor\n            Output tensor (batch_size, seq_len, d_model)\n        \"\"\"\n        batch_size, seq_len, d_model = x.shape\n        \n        # Flatten for routing\n        x_flat = x.view(-1, d_model)  # (batch_size * seq_len, d_model)\n        \n        # Route tokens to experts\n        router_logits = self.router(x_flat)  # (batch_size * seq_len, num_experts)\n        routing_weights = F.softmax(router_logits, dim=-1)\n        \n        # Select top-k experts\n        top_k_weights, top_k_indices = torch.topk(routing_weights, self.top_k, dim=-1)\n        top_k_weights = F.softmax(top_k_weights, dim=-1)\n        \n        # Initialize output\n        output = torch.zeros_like(x_flat)\n        \n        # Process tokens through selected experts\n        for i in range(self.top_k):\n            expert_indices = top_k_indices[:, i]\n            expert_weights = top_k_weights[:, i].unsqueeze(-1)\n            \n            # Group tokens by expert\n            for expert_id in range(self.num_experts):\n                mask = expert_indices == expert_id\n                if mask.any():\n                    expert_input = x_flat[mask]\n                    expert_output = self.experts[expert_id](\n                        expert_input.view(-1, 1, d_model)\n                    ).view(-1, d_model)\n                    \n                    output[mask] += expert_weights[mask] * expert_output\n        \n        # Load balancing loss\n        if self.training:\n            load_balancing_loss = self._compute_load_balancing_loss(routing_weights)\n            # This would be added to the main loss during training\n        \n        return output.view(batch_size, seq_len, d_model)\n    \n    def _compute_load_balancing_loss(self, routing_weights):\n        \"\"\"Compute load balancing loss for even expert utilization\"\"\"\n        # Fraction of tokens routed to each expert\n        expert_usage = routing_weights.sum(dim=0) / routing_weights.shape[0]\n        \n        # Ideal usage (uniform distribution)\n        ideal_usage = 1.0 / self.num_experts\n        \n        # L2 penalty for deviation from uniform usage\n        load_balancing_loss = torch.sum((expert_usage - ideal_usage) ** 2)\n        \n        return self.load_balancing_loss_coeff * load_balancing_loss\n```\n:::\n\n\n### Bidirectional Mamba\n\n::: {#9110a9ef .cell execution_count=16}\n``` {.python .cell-code}\nclass BidirectionalMamba(nn.Module):\n    \"\"\"Bidirectional Mamba for enhanced context modeling\"\"\"\n    \n    def __init__(self, d_model, d_state=16, expand=2):\n        super().__init__()\n        \n        # Forward and backward Mamba blocks\n        self.forward_mamba = MambaBlock(d_model, d_state, expand)\n        self.backward_mamba = MambaBlock(d_model, d_state, expand)\n        \n        # Fusion layer\n        self.fusion = nn.Linear(d_model * 2, d_model)\n        \n    def forward(self, x):\n        \"\"\"\n        Bidirectional processing of input sequence\n        \n        Parameters:\n        -----------\n        x : torch.Tensor\n            Input tensor (batch_size, seq_len, d_model)\n            \n        Returns:\n        --------\n        torch.Tensor\n            Bidirectionally processed output\n        \"\"\"\n        # Forward direction\n        forward_output = self.forward_mamba(x)\n        \n        # Backward direction (reverse sequence)\n        backward_input = torch.flip(x, dims=[1])\n        backward_output = self.backward_mamba(backward_input)\n        backward_output = torch.flip(backward_output, dims=[1])\n        \n        # Combine forward and backward\n        combined = torch.cat([forward_output, backward_output], dim=-1)\n        output = self.fusion(combined)\n        \n        return output\n```\n:::\n\n\n## Model Analysis and Interpretability\n\n### Visualization Tools\n\n::: {#50d146ac .cell execution_count=17}\n``` {.python .cell-code}\nclass MambaVisualizer:\n    \"\"\"Visualization tools for Mamba model analysis\"\"\"\n    \n    def __init__(self, model):\n        self.model = model\n        self.activations = {}\n        self.hooks = []\n        \n    def register_hooks(self):\n        \"\"\"Register hooks to capture intermediate activations\"\"\"\n        def hook_fn(name):\n            def hook(module, input, output):\n                self.activations[name] = output.detach()\n            return hook\n        \n        for name, module in self.model.named_modules():\n            if isinstance(module, MambaBlock):\n                self.hooks.append(\n                    module.register_forward_hook(hook_fn(name))\n                )\n    \n    def get_state_importance(self, input_text, layer_idx=-1):\n        \"\"\"\n        Compute importance scores similar to attention weights\n        \n        Parameters:\n        -----------\n        input_text : str\n            Input text to analyze\n        layer_idx : int\n            Layer index to analyze\n            \n        Returns:\n        --------\n        torch.Tensor\n            Importance scores for each position\n        \"\"\"\n        self.register_hooks()\n        \n        # Forward pass\n        tokens = self.tokenizer.encode(input_text, return_tensors='pt')\n        with torch.no_grad():\n            output = self.model(tokens)\n        \n        # Get activations from specified layer\n        layer_name = f'layers.{layer_idx}'\n        if layer_name in self.activations:\n            activations = self.activations[layer_name]\n            \n            # Compute importance as gradient of output w.r.t. hidden states\n            importance = torch.autograd.grad(\n                output.sum(), activations, retain_graph=True\n            )[0]\n            \n            # Normalize importance scores\n            importance = F.softmax(importance.abs().sum(-1), dim=-1)\n            \n        self.remove_hooks()\n        return importance\n    \n    def remove_hooks(self):\n        \"\"\"Remove all registered hooks\"\"\"\n        for hook in self.hooks:\n            hook.remove()\n        self.hooks = []\n\ndef analyze_state_space(model, input_sequence):\n    \"\"\"\n    Analyze the state space dynamics of Mamba\n    \n    Parameters:\n    -----------\n    model : Mamba\n        Trained Mamba model\n    input_sequence : torch.Tensor\n        Input sequence to analyze\n        \n    Returns:\n    --------\n    dict\n        Dictionary containing state analysis results\n    \"\"\"\n    # Extract state trajectories\n    states = []\n    \n    def state_hook(module, input, output):\n        # Capture state evolution during selective scan\n        if hasattr(module, 'ssm'):\n            # This would require modifying the SSM to return intermediate states\n            states.append(module.current_state.detach())\n    \n    # Register hooks\n    hooks = []\n    for module in model.modules():\n        if isinstance(module, MambaBlock):\n            hooks.append(module.register_forward_hook(state_hook))\n    \n    # Forward pass\n    with torch.no_grad():\n        output = model(input_sequence)\n    \n    # Remove hooks\n    for hook in hooks:\n        hook.remove()\n    \n    # Analyze state dynamics\n    if states:\n        state_tensor = torch.stack(states, dim=0)  # (layers, batch, seq_len, state_dim)\n        \n        # Compute state change magnitudes\n        state_changes = torch.norm(state_tensor[1:] - state_tensor[:-1], dim=-1)\n        \n        # Identify critical transition points\n        mean_change = state_changes.mean()\n        std_change = state_changes.std()\n        critical_points = torch.where(state_changes > mean_change + 2 * std_change)\n        \n        return {\n            'states': state_tensor,\n            'state_changes': state_changes,\n            'critical_points': critical_points\n        }\n    \n    return {'states': None, 'state_changes': None, 'critical_points': None}\n```\n:::\n\n\n## Production Deployment\n\n### Model Serving with FastAPI\n\n::: {#fdf6d87e .cell execution_count=18}\n``` {.python .cell-code}\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport uvicorn\nimport asyncio\nfrom typing import List, Optional\nimport time\n\napp = FastAPI(title=\"Mamba Model API\")\n\nclass GenerationRequest(BaseModel):\n    \"\"\"Request model for text generation\"\"\"\n    prompt: str\n    max_length: int = 100\n    temperature: float = 0.8\n    top_p: float = 0.95\n    num_return_sequences: int = 1\n\nclass GenerationResponse(BaseModel):\n    \"\"\"Response model for text generation\"\"\"\n    generated_texts: List[str]\n    generation_time: float\n\nclass MambaServer:\n    \"\"\"Production server for Mamba model inference\"\"\"\n    \n    def __init__(self, model_path: str, device: str = \"cuda\"):\n        self.model = self.load_model(model_path, device)\n        self.tokenizer = self.load_tokenizer(model_path)\n        self.device = device\n        \n    def load_model(self, model_path: str, device: str):\n        \"\"\"Load optimized Mamba model for inference\"\"\"\n        model = Mamba.from_pretrained(model_path)\n        model = model.half().to(device)\n        model.eval()\n        \n        # Compile for faster inference\n        model = torch.compile(model, mode=\"max-autotune\")\n        \n        return model\n    \n    def load_tokenizer(self, model_path: str):\n        \"\"\"Load tokenizer\"\"\"\n        # Assuming using HuggingFace tokenizer\n        from transformers import AutoTokenizer\n        return AutoTokenizer.from_pretrained(model_path)\n    \n    async def generate(self, request: GenerationRequest) -> GenerationResponse:\n        \"\"\"Generate text asynchronously\"\"\"\n        start_time = time.time()\n        \n        try:\n            # Tokenize input\n            input_ids = self.tokenizer.encode(\n                request.prompt, \n                return_tensors='pt'\n            ).to(self.device)\n            \n            # Generate\n            with torch.no_grad():\n                generated_sequences = []\n                \n                for _ in range(request.num_return_sequences):\n                    generated_ids = await self.generate_sequence(\n                        input_ids, \n                        request.max_length,\n                        request.temperature,\n                        request.top_p\n                    )\n                    \n                    generated_text = self.tokenizer.decode(\n                        generated_ids[0], \n                        skip_special_tokens=True\n                    )\n                    generated_sequences.append(generated_text)\n            \n            generation_time = time.time() - start_time\n            \n            return GenerationResponse(\n                generated_texts=generated_sequences,\n                generation_time=generation_time\n            )\n            \n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n    \n    async def generate_sequence(self, input_ids, max_length, temperature, top_p):\n        \"\"\"Generate a single sequence with top-p sampling\"\"\"\n        current_ids = input_ids.clone()\n        \n        for _ in range(max_length):\n            # Run inference in thread pool to avoid blocking\n            logits = await asyncio.get_event_loop().run_in_executor(\n                None, lambda: self.model(current_ids)\n            )\n            \n            # Sample next token\n            next_token_logits = logits[:, -1, :] / temperature\n            \n            # Top-p sampling\n            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n            \n            # Remove tokens with cumulative probability above threshold\n            sorted_indices_to_remove = cumulative_probs > top_p\n            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n            sorted_indices_to_remove[..., 0] = 0\n            \n            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n            next_token_logits[indices_to_remove] = -float('Inf')\n            \n            # Sample\n            probs = F.softmax(next_token_logits, dim=-1)\n            next_token = torch.multinomial(probs, num_samples=1)\n            \n            # Append token\n            current_ids = torch.cat([current_ids, next_token], dim=1)\n            \n            # Check for end token\n            if next_token.item() == self.tokenizer.eos_token_id:\n                break\n        \n        return current_ids\n\n# Initialize server\n# mamba_server = MambaServer(\"path/to/mamba/model\")\n\n@app.post(\"/generate\", response_model=GenerationResponse)\nasync def generate_text(request: GenerationRequest):\n    \"\"\"API endpoint for text generation\"\"\"\n    return await mamba_server.generate(request)\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return {\"status\": \"healthy\"}\n\n# if __name__ == \"__main__\":\n#     uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n```\n:::\n\n\n### Distributed Training Setup\n\n::: {#aa9765b8 .cell execution_count=19}\n``` {.python .cell-code}\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nimport os\n\nclass DistributedMambaTrainer:\n    \"\"\"Distributed trainer for large-scale Mamba training\"\"\"\n    \n    def __init__(self, model, config, train_dataset, val_dataset):\n        self.config = config\n        self.train_dataset = train_dataset\n        self.val_dataset = val_dataset\n        \n        # Initialize distributed training\n        self.setup_distributed()\n        \n        # Setup model\n        self.model = self.setup_model(model)\n        \n        # Setup data loaders\n        self.train_loader, self.val_loader = self.setup_data_loaders()\n        \n        # Setup optimizer and scheduler\n        self.optimizer = create_optimizer(self.model, config)\n        self.scheduler = self.create_scheduler()\n        \n    def setup_distributed(self):\n        \"\"\"Initialize distributed training environment\"\"\"\n        dist.init_process_group(backend='nccl')\n        \n        self.local_rank = int(os.environ['LOCAL_RANK'])\n        self.global_rank = int(os.environ['RANK'])\n        self.world_size = int(os.environ['WORLD_SIZE'])\n        \n        torch.cuda.set_device(self.local_rank)\n        \n    def setup_model(self, model):\n        \"\"\"Setup model for distributed training\"\"\"\n        model = model.to(self.local_rank)\n        \n        # Wrap with DDP\n        model = DDP(\n            model, \n            device_ids=[self.local_rank],\n            find_unused_parameters=False\n        )\n        \n        return model\n    \n    def setup_data_loaders(self):\n        \"\"\"Setup distributed data loaders\"\"\"\n        train_sampler = DistributedSampler(\n            self.train_dataset,\n            num_replicas=self.world_size,\n            rank=self.global_rank,\n            shuffle=True\n        )\n        \n        val_sampler = DistributedSampler(\n            self.val_dataset,\n            num_replicas=self.world_size,\n            rank=self.global_rank,\n            shuffle=False\n        )\n        \n        from torch.utils.data import DataLoader\n        \n        train_loader = DataLoader(\n            self.train_dataset,\n            batch_size=self.config.batch_size,\n            sampler=train_sampler,\n            num_workers=4,\n            pin_memory=True\n        )\n        \n        val_loader = DataLoader(\n            self.val_dataset,\n            batch_size=self.config.batch_size,\n            sampler=val_sampler,\n            num_workers=4,\n            pin_memory=True\n        )\n        \n        return train_loader, val_loader\n    \n    def train(self):\n        \"\"\"Main distributed training loop\"\"\"\n        for epoch in range(self.config.num_epochs):\n            self.train_loader.sampler.set_epoch(epoch)\n            \n            # Training\n            self.model.train()\n            train_loss = self.train_epoch()\n            \n            # Validation\n            if self.global_rank == 0:  # Only on main process\n                val_loss = self.validate()\n                print(f\"Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n                \n                # Save checkpoint\n                self.save_checkpoint(epoch, train_loss, val_loss)\n    \n    def train_epoch(self):\n        \"\"\"Train for one epoch with distributed synchronization\"\"\"\n        total_loss = 0\n        num_batches = 0\n        \n        for batch in self.train_loader:\n            input_ids = batch['input_ids'].to(self.local_rank)\n            targets = input_ids[:, 1:].contiguous()\n            input_ids = input_ids[:, :-1].contiguous()\n            \n            # Forward pass\n            with torch.cuda.amp.autocast():\n                logits = self.model(input_ids)\n                loss = F.cross_entropy(\n                    logits.view(-1, logits.size(-1)),\n                    targets.view(-1)\n                )\n            \n            # Backward pass\n            self.optimizer.zero_grad()\n            loss.backward()\n            \n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n            \n            self.optimizer.step()\n            self.scheduler.step()\n            \n            total_loss += loss.item()\n            num_batches += 1\n        \n        # Average loss across all processes\n        avg_loss = total_loss / num_batches\n        loss_tensor = torch.tensor(avg_loss).to(self.local_rank)\n        dist.all_reduce(loss_tensor, op=dist.ReduceOp.AVG)\n        \n        return loss_tensor.item()\n    \n    def save_checkpoint(self, epoch, train_loss, val_loss):\n        \"\"\"Save training checkpoint\"\"\"\n        if self.global_rank == 0:\n            checkpoint = {\n                'epoch': epoch,\n                'model_state_dict': self.model.module.state_dict(),\n                'optimizer_state_dict': self.optimizer.state_dict(),\n                'scheduler_state_dict': self.scheduler.state_dict(),\n                'train_loss': train_loss,\n                'val_loss': val_loss,\n                'config': self.config\n            }\n            \n            torch.save(checkpoint, f'checkpoint_epoch_{epoch}.pt')\n```\n:::\n\n\n## Experimental Features\n\n### Adaptive Computation Time (ACT)\n\n::: {#26d8a7ee .cell execution_count=20}\n``` {.python .cell-code}\nclass ACTMamba(nn.Module):\n    \"\"\"Mamba with Adaptive Computation Time\"\"\"\n    \n    def __init__(self, d_model, max_computation_steps=10, threshold=0.99):\n        super().__init__()\n        self.max_computation_steps = max_computation_steps\n        self.threshold = threshold\n        \n        # Mamba layer\n        self.mamba = MambaBlock(d_model)\n        \n        # Halting probability predictor\n        self.halting_predictor = nn.Linear(d_model, 1)\n        \n    def forward(self, x):\n        \"\"\"\n        Forward pass with adaptive computation time\n        \n        Parameters:\n        -----------\n        x : torch.Tensor\n            Input tensor (batch_size, seq_len, d_model)\n            \n        Returns:\n        --------\n        tuple\n            (output, ponder_cost) where ponder_cost is regularization term\n        \"\"\"\n        batch_size, seq_len, d_model = x.shape\n        \n        # Initialize states\n        state = x\n        halting_probs = torch.zeros(batch_size, seq_len, 1, device=x.device)\n        remainders = torch.ones(batch_size, seq_len, 1, device=x.device)\n        n_updates = torch.zeros(batch_size, seq_len, 1, device=x.device)\n        \n        output = torch.zeros_like(x)\n        \n        for step in range(self.max_computation_steps):\n            # Predict halting probability\n            p = torch.sigmoid(self.halting_predictor(state))\n            \n            # Update halting probabilities\n            still_running = (halting_probs < self.threshold).float()\n            new_halted = (halting_probs + p * still_running >= self.threshold).float()\n            still_running = still_running - new_halted\n            \n            # Update remainder for newly halted\n            halting_probs = halting_probs + p * still_running\n            remainders = remainders - p * still_running\n            \n            # Weight for this step\n            step_weight = p * still_running + new_halted * remainders\n            \n            # Apply Mamba transformation\n            transformed_state = self.mamba(state)\n            \n            # Update output\n            output = output + step_weight * transformed_state\n            \n            # Update state for next iteration\n            state = transformed_state\n            \n            # Update computation counter\n            n_updates = n_updates + still_running + new_halted\n            \n            # Check if all sequences have halted\n            if (halting_probs >= self.threshold).all():\n                break\n        \n        # Ponder cost (regularization term)\n        ponder_cost = n_updates.mean()\n        \n        return output, ponder_cost\n```\n:::\n\n\n### Hierarchical Processing\n\n::: {#5e5f6f45 .cell execution_count=21}\n``` {.python .cell-code}\nclass HierarchicalMamba(nn.Module):\n    \"\"\"Hierarchical Mamba for multi-scale processing\"\"\"\n    \n    def __init__(self, d_model, n_layer, hierarchy_levels=3):\n        super().__init__()\n        \n        self.hierarchy_levels = hierarchy_levels\n        \n        # Different Mamba blocks for different hierarchical levels\n        self.local_mamba = nn.ModuleList([\n            MambaBlock(d_model, d_state=16) \n            for _ in range(n_layer // hierarchy_levels)\n        ])\n        \n        self.global_mamba = nn.ModuleList([\n            MambaBlock(d_model, d_state=32) \n            for _ in range(n_layer // hierarchy_levels)\n        ])\n        \n        self.cross_hierarchy = nn.ModuleList([\n            nn.MultiheadAttention(d_model, num_heads=8) \n            for _ in range(hierarchy_levels)\n        ])\n    \n    def forward(self, x):\n        \"\"\"\n        Hierarchical processing of input\n        \n        Parameters:\n        -----------\n        x : torch.Tensor\n            Input tensor (batch_size, seq_len, d_model)\n            \n        Returns:\n        --------\n        torch.Tensor\n            Hierarchically processed output\n        \"\"\"\n        local_features = x\n        \n        # Process at local level\n        for layer in self.local_mamba:\n            local_features = layer(local_features)\n        \n        # Global processing (with downsampling)\n        global_features = local_features[:, ::4, :]  # Sample every 4th token\n        \n        for layer in self.global_mamba:\n            global_features = layer(global_features)\n        \n        # Cross-hierarchy attention\n        enhanced_local, _ = self.cross_hierarchy[0](\n            local_features, global_features, global_features\n        )\n        \n        return enhanced_local + local_features\n```\n:::\n\n\n## Conclusion and Future Directions\n\nThis comprehensive guide has covered the implementation and practical applications of Mamba transformers, from fundamental concepts to advanced optimization techniques. The key contributions of Mamba include:\n\n### Key Advantages\n\n1. **Linear Complexity**: Mamba achieves $O(L)$ computational complexity compared to $O(L^2)$ for traditional transformers, enabling efficient processing of long sequences.\n\n2. **Selective Mechanism**: The input-dependent parameterization allows the model to dynamically focus on relevant information, improving modeling capabilities.\n\n3. **Hardware Efficiency**: Better memory utilization and parallelization characteristics make Mamba suitable for resource-constrained environments.\n\n4. **Scalability**: The linear scaling properties enable processing of much longer contexts than traditional attention-based models.\n\n### Implementation Considerations\n\n- **State Space Modeling**: The core selective scan algorithm requires careful implementation for numerical stability\n- **Memory Optimization**: Gradient checkpointing and mixed-precision training are essential for large-scale deployment\n- **Custom Kernels**: Production deployments benefit significantly from optimized CUDA implementations\n\n### Future Research Directions\n\n1. **Theoretical Analysis**: Deeper understanding of the selective mechanism's theoretical properties\n2. **Architecture Improvements**: Exploring hybrid architectures combining Mamba with other sequence modeling approaches\n3. **Multi-modal Applications**: Extending Mamba to vision, audio, and other modalities\n4. **Hardware Optimization**: Developing specialized hardware accelerators for selective scan operations\n\n### Practical Applications\n\nMamba shows particular promise for:\n\n- **Long Document Processing**: Technical documents, legal texts, and scientific papers\n- **Time Series Analysis**: Financial data, sensor measurements, and sequential predictions  \n- **Code Generation**: Software development with large codebases and long contexts\n- **Conversational AI**: Multi-turn dialogues with extended conversation history\n\nThe Mamba architecture represents a significant advancement in sequence modeling, offering a compelling alternative to attention-based transformers with superior scalability and efficiency characteristics. As the field continues to evolve, Mamba's linear complexity and selective processing capabilities position it as a foundation for next-generation language models and sequential AI systems.\n\n## References\n\n```bibtex\n@article{gu2023mamba,\n  title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},\n  author={Gu, Albert and Dao, Tri},\n  journal={arXiv preprint arXiv:2312.00752},\n  year={2023}\n}\n\n@article{gu2021efficiently,\n  title={Efficiently modeling long sequences with structured state spaces},\n  author={Gu, Albert and Goel, Karan and R{\\'e}, Christopher},\n  journal={arXiv preprint arXiv:2111.00396},\n  year={2021}\n}\n```\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}