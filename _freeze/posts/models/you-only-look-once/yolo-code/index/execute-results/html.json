{
  "hash": "273580370669d0ac4f269711f2719ffd",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Complete YOLO Object Detection Code Guide\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-07-12\"\ncategories: [code, tutorial, beginner]\nformat:\n  html:\n    code-fold: false\n    math: mathjax\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# Complete YOLO Object Detection Code Guide\n![](yolo-code.png)\n\n## 1. Introduction to YOLO\n\nYOLO (You Only Look Once) is a state-of-the-art real-time object detection algorithm that revolutionized computer vision by treating object detection as a single regression problem. Unlike traditional methods that apply classifiers to different parts of an image, YOLO looks at the entire image once and predicts bounding boxes and class probabilities directly.\n\n### Key Advantages:\n- **Speed**: Real-time detection (30+ FPS)\n- **Global Context**: Sees entire image during training and testing\n- **Unified Architecture**: Single neural network for end-to-end training\n- **Versatility**: Works well across different object types\n\n### YOLO Evolution:\n- **YOLOv1** (2016): Original paper, 45 FPS\n- **YOLOv2/YOLO9000** (2016): Better accuracy, 40+ FPS\n- **YOLOv3** (2018): Multi-scale detection, Darknet-53\n- **YOLOv4** (2020): Improved accuracy and speed\n- **YOLOv5** (2020): PyTorch implementation, user-friendly\n- **YOLOv8** (2023): Latest Ultralytics version, best performance\n\n## 2. YOLO Architecture Overview\n\n### Core Concept\nYOLO divides an image into an S×S grid. Each grid cell predicts:\n- **B bounding boxes** (x, y, width, height, confidence)\n- **C class probabilities**\n\n### Network Architecture (YOLOv8)\n```\nInput Image (640×640×3)\n        ↓\nBackbone (CSPDarknet53)\n        ↓\nNeck (PANet)\n        ↓\nHead (Detection layers)\n        ↓\nOutput (Predictions)\n```\n\n### Loss Function Components:\n1. **Localization Loss**: Bounding box coordinate errors\n2. **Confidence Loss**: Object presence confidence\n3. **Classification Loss**: Class prediction errors\n\n## 3. Setting Up the Environment\n\n### Prerequisites\n```bash\n# Python 3.8+\npython --version\n\n# Create virtual environment\npython -m venv yolo_env\nsource yolo_env/bin/activate  # Linux/Mac\n# or\nyolo_env\\Scripts\\activate     # Windows\n```\n\n### Install Dependencies\n```bash\n# Install PyTorch (check pytorch.org for your system)\npip install torch torchvision torchaudio\n\n# Install Ultralytics YOLOv8\npip install ultralytics\n\n# Additional dependencies\npip install opencv-python pillow matplotlib numpy pandas\npip install jupyter notebook  # For interactive development\n```\n\n### Verify Installation\n```python\nimport torch\nimport ultralytics\nfrom ultralytics import YOLO\nimport cv2\nimport numpy as np\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Ultralytics version: {ultralytics.__version__}\")\n```\n\n## 4. YOLOv8 Implementation\n\n### Basic Object Detection\n```python\nfrom ultralytics import YOLO\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Load pre-trained model\nmodel = YOLO('yolov8n.pt')  # nano version for speed\n# model = YOLO('yolov8s.pt')  # small\n# model = YOLO('yolov8m.pt')  # medium\n# model = YOLO('yolov8l.pt')  # large\n# model = YOLO('yolov8x.pt')  # extra large\n\n# Single image inference\ndef detect_objects(image_path):\n    \"\"\"\n    Detect objects in a single image\n    \"\"\"\n    results = model(image_path)\n    \n    # Process results\n    for result in results:\n        # Get bounding boxes, confidence scores, and class IDs\n        boxes = result.boxes.xyxy.cpu().numpy()  # x1, y1, x2, y2\n        confidences = result.boxes.conf.cpu().numpy()\n        class_ids = result.boxes.cls.cpu().numpy()\n        \n        # Load image\n        img = cv2.imread(image_path)\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # Draw bounding boxes\n        for i, (box, conf, cls_id) in enumerate(zip(boxes, confidences, class_ids)):\n            x1, y1, x2, y2 = map(int, box)\n            class_name = model.names[int(cls_id)]\n            \n            # Draw rectangle and label\n            cv2.rectangle(img_rgb, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            cv2.putText(img_rgb, f'{class_name}: {conf:.2f}', \n                       (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n        \n        return img_rgb, results\n\n# Example usage\nimage_path = 'path/to/your/image.jpg'\ndetected_img, results = detect_objects(image_path)\n\n# Display results\nplt.figure(figsize=(12, 8))\nplt.imshow(detected_img)\nplt.axis('off')\nplt.title('YOLO Object Detection Results')\nplt.show()\n```\n\n### Video Processing\n```python\ndef process_video(video_path, output_path=None):\n    \"\"\"\n    Process video for object detection\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    \n    # Get video properties\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    \n    # Setup video writer if output path provided\n    if output_path:\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    \n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        # Run YOLO detection\n        results = model(frame)\n        \n        # Draw results on frame\n        annotated_frame = results[0].plot()\n        \n        # Save or display frame\n        if output_path:\n            out.write(annotated_frame)\n        else:\n            cv2.imshow('YOLO Detection', annotated_frame)\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n    \n    cap.release()\n    if output_path:\n        out.release()\n    cv2.destroyAllWindows()\n\n# Example usage\nprocess_video('input_video.mp4', 'output_video.mp4')\n```\n\n### Real-time Webcam Detection\n```python\ndef real_time_detection():\n    \"\"\"\n    Real-time object detection from webcam\n    \"\"\"\n    cap = cv2.VideoCapture(0)  # Use 0 for default camera\n    \n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        # Run YOLO detection\n        results = model(frame)\n        \n        # Draw results\n        annotated_frame = results[0].plot()\n        \n        # Display frame\n        cv2.imshow('Real-time YOLO Detection', annotated_frame)\n        \n        # Exit on 'q' key press\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n    \n    cap.release()\n    cv2.destroyAllWindows()\n\n# Start real-time detection\nreal_time_detection()\n```\n\n## 5. Custom Dataset Training {#custom-training}\n\n### Dataset Preparation\n```python\nimport os\nimport shutil\nfrom pathlib import Path\n\ndef create_dataset_structure(base_path):\n    \"\"\"\n    Create YOLO dataset structure\n    \"\"\"\n    paths = [\n        'train/images',\n        'train/labels',\n        'val/images',\n        'val/labels',\n        'test/images',\n        'test/labels'\n    ]\n    \n    for path in paths:\n        Path(base_path / path).mkdir(parents=True, exist_ok=True)\n    \n    print(f\"Dataset structure created at {base_path}\")\n\n# Create dataset structure\ndataset_path = Path('custom_dataset')\ncreate_dataset_structure(dataset_path)\n```\n\n### Data Configuration File\n```yaml\n# data.yaml\ntrain: ../custom_dataset/train/images\nval: ../custom_dataset/val/images\ntest: ../custom_dataset/test/images\n\nnc: 3  # number of classes\nnames: ['person', 'car', 'bicycle']  # class names\n```\n\n### Training Script\n```python\nfrom ultralytics import YOLO\nimport torch\n\ndef train_custom_model():\n    \"\"\"\n    Train YOLO model on custom dataset\n    \"\"\"\n    # Load a pre-trained model\n    model = YOLO('yolov8n.pt')\n    \n    # Train the model\n    results = model.train(\n        data='data.yaml',           # dataset config file\n        epochs=100,                 # number of training epochs\n        imgsz=640,                  # image size\n        batch_size=16,              # batch size\n        device='cuda' if torch.cuda.is_available() else 'cpu',\n        workers=4,                  # number of data loader workers\n        project='runs/train',       # project directory\n        name='custom_model',        # experiment name\n        save=True,                  # save model checkpoints\n        save_period=10,             # save checkpoint every N epochs\n        cache=True,                 # cache images for faster training\n        augment=True,               # use data augmentation\n        lr0=0.01,                   # initial learning rate\n        weight_decay=0.0005,        # weight decay\n        warmup_epochs=3,            # warmup epochs\n        patience=50,                # early stopping patience\n        verbose=True                # verbose output\n    )\n    \n    return results\n\n# Start training\nif __name__ == \"__main__\":\n    results = train_custom_model()\n    print(\"Training completed!\")\n```\n\n### Data Augmentation\n```python\n# Custom augmentation configuration\naugmentation_config = {\n    'hsv_h': 0.015,      # HSV-Hue augmentation\n    'hsv_s': 0.7,        # HSV-Saturation augmentation\n    'hsv_v': 0.4,        # HSV-Value augmentation\n    'degrees': 10.0,     # rotation degrees\n    'translate': 0.1,    # translation\n    'scale': 0.5,        # scale\n    'shear': 2.0,        # shear degrees\n    'perspective': 0.0,  # perspective\n    'flipud': 0.0,       # flip up-down probability\n    'fliplr': 0.5,       # flip left-right probability\n    'mosaic': 1.0,       # mosaic probability\n    'mixup': 0.1,        # mixup probability\n    'copy_paste': 0.1    # copy-paste probability\n}\n```\n\n## 6. Advanced Features\n\n### Model Validation and Metrics\n```python\ndef validate_model(model_path, data_config):\n    \"\"\"\n    Validate trained model and get metrics\n    \"\"\"\n    model = YOLO(model_path)\n    \n    # Validate the model\n    results = model.val(\n        data=data_config,\n        imgsz=640,\n        batch_size=16,\n        device='cuda' if torch.cuda.is_available() else 'cpu',\n        plots=True,\n        save_json=True\n    )\n    \n    # Print metrics\n    print(f\"mAP50: {results.box.map50:.4f}\")\n    print(f\"mAP50-95: {results.box.map:.4f}\")\n    print(f\"Precision: {results.box.mp:.4f}\")\n    print(f\"Recall: {results.box.mr:.4f}\")\n    \n    return results\n\n# Validate model\nvalidation_results = validate_model('runs/train/custom_model/weights/best.pt', 'data.yaml')\n```\n\n### Model Export and Optimization\n```python\ndef export_model(model_path, export_format='onnx'):\n    \"\"\"\n    Export model to different formats\n    \"\"\"\n    model = YOLO(model_path)\n    \n    # Export options\n    export_formats = {\n        'onnx': model.export(format='onnx'),\n        'torchscript': model.export(format='torchscript'),\n        'tflite': model.export(format='tflite'),\n        'tensorrt': model.export(format='engine'),  # TensorRT\n        'openvino': model.export(format='openvino'),\n        'coreml': model.export(format='coreml')\n    }\n    \n    return export_formats[export_format]\n\n# Export to ONNX\nonnx_model = export_model('runs/train/custom_model/weights/best.pt', 'onnx')\n```\n\n### Hyperparameter Tuning\n```python\ndef hyperparameter_tuning():\n    \"\"\"\n    Automated hyperparameter tuning\n    \"\"\"\n    model = YOLO('yolov8n.pt')\n    \n    # Tune hyperparameters\n    model.tune(\n        data='data.yaml',\n        epochs=30,\n        iterations=300,\n        optimizer='AdamW',\n        plots=True,\n        save=True\n    )\n\n# Run hyperparameter tuning\nhyperparameter_tuning()\n```\n\n## 7. Performance Optimization\n\n### Multi-GPU Training\n```python\ndef multi_gpu_training():\n    \"\"\"\n    Training with multiple GPUs\n    \"\"\"\n    if torch.cuda.device_count() > 1:\n        model = YOLO('yolov8n.pt')\n        \n        # Multi-GPU training\n        results = model.train(\n            data='data.yaml',\n            epochs=100,\n            imgsz=640,\n            batch_size=32,  # Increase batch size for multi-GPU\n            device='0,1,2,3',  # Specify GPU IDs\n            workers=8\n        )\n    else:\n        print(\"Multiple GPUs not available\")\n\nmulti_gpu_training()\n```\n\n### Inference Optimization\n```python\nimport time\nimport numpy as np\n\ndef benchmark_model(model_path, test_images):\n    \"\"\"\n    Benchmark model performance\n    \"\"\"\n    model = YOLO(model_path)\n    \n    # Warm up\n    for _ in range(10):\n        model('path/to/test/image.jpg')\n    \n    # Benchmark\n    times = []\n    for image_path in test_images:\n        start_time = time.time()\n        results = model(image_path)\n        end_time = time.time()\n        times.append(end_time - start_time)\n    \n    avg_time = np.mean(times)\n    fps = 1 / avg_time\n    \n    print(f\"Average inference time: {avg_time:.4f} seconds\")\n    print(f\"FPS: {fps:.2f}\")\n    \n    return avg_time, fps\n\n# Benchmark your model\ntest_images = ['test1.jpg', 'test2.jpg', 'test3.jpg']\navg_time, fps = benchmark_model('yolov8n.pt', test_images)\n```\n\n### Memory Optimization\n```python\ndef memory_efficient_inference(model_path, image_path):\n    \"\"\"\n    Memory efficient inference for large images\n    \"\"\"\n    model = YOLO(model_path)\n    \n    # Process image in tiles for large images\n    def process_large_image(image_path, tile_size=640, overlap=0.1):\n        img = cv2.imread(image_path)\n        h, w = img.shape[:2]\n        \n        if h <= tile_size and w <= tile_size:\n            # Small image, process normally\n            return model(img)\n        \n        # Split into tiles\n        results = []\n        step = int(tile_size * (1 - overlap))\n        \n        for y in range(0, h, step):\n            for x in range(0, w, step):\n                # Extract tile\n                tile = img[y:y+tile_size, x:x+tile_size]\n                \n                # Process tile\n                tile_results = model(tile)\n                \n                # Adjust coordinates\n                for result in tile_results:\n                    if result.boxes is not None:\n                        result.boxes.xyxy[:, [0, 2]] += x\n                        result.boxes.xyxy[:, [1, 3]] += y\n                \n                results.extend(tile_results)\n        \n        return results\n    \n    return process_large_image(image_path)\n\n# Process large image\nlarge_image_results = memory_efficient_inference('yolov8n.pt', 'large_image.jpg')\n```\n\n## 8. Real-world Applications\n\n### Security Camera System\n```python\nclass SecuritySystem:\n    def __init__(self, model_path, camera_sources):\n        self.model = YOLO(model_path)\n        self.cameras = camera_sources\n        self.alerts = []\n    \n    def monitor_cameras(self):\n        \"\"\"\n        Monitor multiple camera feeds\n        \"\"\"\n        for camera_id, source in self.cameras.items():\n            cap = cv2.VideoCapture(source)\n            \n            while True:\n                ret, frame = cap.read()\n                if not ret:\n                    break\n                \n                # Detect objects\n                results = self.model(frame)\n                \n                # Check for specific objects (e.g., person)\n                for result in results:\n                    if result.boxes is not None:\n                        classes = result.boxes.cls.cpu().numpy()\n                        if 0 in classes:  # Person detected\n                            self.trigger_alert(camera_id, frame)\n                \n                # Display frame\n                annotated_frame = results[0].plot()\n                cv2.imshow(f'Camera {camera_id}', annotated_frame)\n                \n                if cv2.waitKey(1) & 0xFF == ord('q'):\n                    break\n            \n            cap.release()\n        \n        cv2.destroyAllWindows()\n    \n    def trigger_alert(self, camera_id, frame):\n        \"\"\"\n        Trigger security alert\n        \"\"\"\n        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n        alert = {\n            'camera_id': camera_id,\n            'timestamp': timestamp,\n            'frame': frame\n        }\n        self.alerts.append(alert)\n        print(f\"ALERT: Person detected on Camera {camera_id} at {timestamp}\")\n\n# Setup security system\ncameras = {\n    'cam1': 0,  # Webcam\n    'cam2': 'rtsp://camera2/stream',  # IP camera\n}\nsecurity = SecuritySystem('yolov8n.pt', cameras)\n```\n\n### Traffic Monitoring\n```python\nclass TrafficMonitor:\n    def __init__(self, model_path):\n        self.model = YOLO(model_path)\n        self.vehicle_count = 0\n        self.speed_violations = []\n    \n    def analyze_traffic(self, video_path):\n        \"\"\"\n        Analyze traffic from video feed\n        \"\"\"\n        cap = cv2.VideoCapture(video_path)\n        \n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            \n            # Detect vehicles\n            results = self.model(frame)\n            \n            # Count vehicles\n            vehicle_classes = [2, 3, 5, 7]  # car, motorcycle, bus, truck\n            current_vehicles = 0\n            \n            for result in results:\n                if result.boxes is not None:\n                    classes = result.boxes.cls.cpu().numpy()\n                    current_vehicles += sum(1 for cls in classes if cls in vehicle_classes)\n            \n            self.vehicle_count = max(self.vehicle_count, current_vehicles)\n            \n            # Display results\n            annotated_frame = results[0].plot()\n            cv2.putText(annotated_frame, f'Vehicles: {current_vehicles}', \n                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n            \n            cv2.imshow('Traffic Monitor', annotated_frame)\n            \n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n        \n        cap.release()\n        cv2.destroyAllWindows()\n        \n        print(f\"Maximum vehicles detected: {self.vehicle_count}\")\n\n# Monitor traffic\ntraffic_monitor = TrafficMonitor('yolov8n.pt')\ntraffic_monitor.analyze_traffic('traffic_video.mp4')\n```\n\n### Quality Control System\n```python\nclass QualityControl:\n    def __init__(self, model_path):\n        self.model = YOLO(model_path)\n        self.defect_log = []\n    \n    def inspect_products(self, image_paths):\n        \"\"\"\n        Inspect products for defects\n        \"\"\"\n        for image_path in image_paths:\n            results = self.model(image_path)\n            \n            # Analyze results for defects\n            defects_found = []\n            for result in results:\n                if result.boxes is not None:\n                    classes = result.boxes.cls.cpu().numpy()\n                    confidences = result.boxes.conf.cpu().numpy()\n                    \n                    for cls, conf in zip(classes, confidences):\n                        if conf > 0.5:  # Confidence threshold\n                            defect_type = self.model.names[int(cls)]\n                            defects_found.append(defect_type)\n            \n            # Log results\n            inspection_result = {\n                'image': image_path,\n                'defects': defects_found,\n                'status': 'FAIL' if defects_found else 'PASS',\n                'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\")\n            }\n            \n            self.defect_log.append(inspection_result)\n            print(f\"Inspected {image_path}: {inspection_result['status']}\")\n        \n        return self.defect_log\n\n# Quality control inspection\nqc = QualityControl('custom_defect_model.pt')\nproduct_images = ['product1.jpg', 'product2.jpg', 'product3.jpg']\ninspection_results = qc.inspect_products(product_images)\n```\n\n## Best Practices and Tips\n\n### Performance Tips\n1. **Choose the right model size**: Use YOLOv8n for speed, YOLOv8x for accuracy\n2. **Optimize image size**: Use 640x640 for balance, smaller for speed\n3. **Use appropriate batch size**: Maximize GPU utilization\n4. **Enable model compilation**: Use TorchScript or TensorRT for production\n5. **Implement model caching**: Load models once and reuse\n\n### Training Tips\n1. **Data quality over quantity**: Focus on high-quality, diverse training data\n2. **Proper data augmentation**: Use appropriate augmentations for your domain\n3. **Monitor training metrics**: Watch for overfitting and adjust accordingly\n4. **Use transfer learning**: Start with pre-trained weights\n5. **Regular validation**: Validate on held-out data during training\n\n### Deployment Tips\n1. **Model versioning**: Keep track of model versions and performance\n2. **A/B testing**: Test different models in production\n3. **Monitoring**: Track inference time and accuracy in production\n4. **Fallback mechanisms**: Have backup models for critical applications\n5. **Documentation**: Document model performance and limitations\n\nThis comprehensive guide covers the essential aspects of working with YOLO for object detection. Start with the basic implementations and gradually explore advanced features as your needs grow. Remember to always validate your models thoroughly before deploying them in production environments.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}