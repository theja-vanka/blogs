{
  "hash": "c33eb44280a1b9ad7c06a7f04fcb2219",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"CLIP Code Guide: Complete Implementation and Usage\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-05-29\"\ncategories: [code, tutorial, beginner]\nformat:\n  html:\n    code-fold: false\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# CLIP Code Guide: Complete Implementation and Usage\n\n![](clip.png)\n\n\n## Introduction to CLIP\n\nCLIP (Contrastive Language-Image Pre-training) is a neural network architecture developed by OpenAI that learns visual concepts from natural language supervision. It can understand images in the context of natural language descriptions, enabling zero-shot classification and multimodal understanding.\n\n### Key Features:\n- Zero-shot image classification\n- Text-image similarity computation\n- Multimodal embeddings\n- Transfer learning capabilities\n\n## Architecture Overview\n\nCLIP consists of two main components:\n\n1. **Text Encoder**: Processes text descriptions (typically a Transformer)\n2. **Image Encoder**: Processes images (typically a Vision Transformer or ResNet)\n\nThe model learns to maximize the cosine similarity between corresponding text-image pairs while minimizing it for non-corresponding pairs.\n\n## Setting Up the Environment\n\n### Installing Dependencies\n\n```bash\n# Basic installation\npip install torch torchvision transformers\npip install clip-by-openai  # Official OpenAI CLIP\npip install open-clip-torch  # OpenCLIP (more models)\n\n# For development and training\npip install wandb datasets accelerate\npip install matplotlib pillow requests\n```\n\n### Alternative Installation\n\n```bash\n# Install from source\ngit clone https://github.com/openai/CLIP.git\ncd CLIP\npip install -e .\n```\n\n## Basic CLIP Usage\n\n### 1. Loading Pre-trained CLIP Model\n\n```python\nimport clip\nimport torch\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\n# Load model and preprocessing\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\n# Available models: ViT-B/32, ViT-B/16, ViT-L/14, RN50, RN101, RN50x4, etc.\nprint(f\"Available models: {clip.available_models()}\")\n```\n\n### 2. Image Classification (Zero-shot)\n\n```python\ndef zero_shot_classification(image_path, text_options):\n    # Load and preprocess image\n    image = Image.open(image_path)\n    image_input = preprocess(image).unsqueeze(0).to(device)\n    \n    # Tokenize text options\n    text_inputs = clip.tokenize(text_options).to(device)\n    \n    # Get predictions\n    with torch.no_grad():\n        image_features = model.encode_image(image_input)\n        text_features = model.encode_text(text_inputs)\n        \n        # Calculate similarities\n        similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n        \n    # Get results\n    values, indices = similarities[0].topk(len(text_options))\n    \n    results = []\n    for value, index in zip(values, indices):\n        results.append({\n            'label': text_options[index],\n            'confidence': value.item()\n        })\n    \n    return results\n\n# Example usage\ntext_options = [\"a dog\", \"a cat\", \"a car\", \"a bird\", \"a house\"]\nresults = zero_shot_classification(\"path/to/image.jpg\", text_options)\n\nfor result in results:\n    print(f\"{result['label']}: {result['confidence']:.2%}\")\n```\n\n### 3. Text-Image Similarity\n\n```python\ndef compute_similarity(image_path, text_description):\n    # Load image\n    image = Image.open(image_path)\n    image_input = preprocess(image).unsqueeze(0).to(device)\n    \n    # Tokenize text\n    text_input = clip.tokenize([text_description]).to(device)\n    \n    # Get features\n    with torch.no_grad():\n        image_features = model.encode_image(image_input)\n        text_features = model.encode_text(text_input)\n        \n        # Normalize features\n        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n        \n        # Compute similarity\n        similarity = (image_features @ text_features.T).item()\n    \n    return similarity\n\n# Example usage\nsimilarity = compute_similarity(\"dog.jpg\", \"a golden retriever sitting in grass\")\nprint(f\"Similarity: {similarity:.4f}\")\n```\n\n## Custom CLIP Implementation\n\n### Basic CLIP Architecture\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import GPT2Model, GPT2Tokenizer\nimport timm\n\nclass CLIPModel(nn.Module):\n    def __init__(self, \n                 image_encoder_name='resnet50',\n                 text_encoder_name='gpt2',\n                 embed_dim=512,\n                 image_resolution=224,\n                 vocab_size=49408):\n        super().__init__()\n        \n        self.embed_dim = embed_dim\n        self.image_resolution = image_resolution\n        \n        # Image encoder\n        self.visual = timm.create_model(image_encoder_name, pretrained=True, num_classes=0)\n        visual_dim = self.visual.num_features\n        \n        # Text encoder\n        self.text_encoder = GPT2Model.from_pretrained(text_encoder_name)\n        text_dim = self.text_encoder.config.n_embd\n        \n        # Projection layers\n        self.visual_projection = nn.Linear(visual_dim, embed_dim, bias=False)\n        self.text_projection = nn.Linear(text_dim, embed_dim, bias=False)\n        \n        # Learnable temperature parameter\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n        \n        self.initialize_parameters()\n    \n    def initialize_parameters(self):\n        # Initialize projection layers\n        nn.init.normal_(self.visual_projection.weight, std=0.02)\n        nn.init.normal_(self.text_projection.weight, std=0.02)\n    \n    def encode_image(self, image):\n        # Extract visual features\n        visual_features = self.visual(image)\n        # Project to common embedding space\n        image_features = self.visual_projection(visual_features)\n        # Normalize\n        image_features = F.normalize(image_features, dim=-1)\n        return image_features\n    \n    def encode_text(self, text):\n        # Get text features from last token\n        text_outputs = self.text_encoder(text)\n        # Use last token's representation\n        text_features = text_outputs.last_hidden_state[:, -1, :]\n        # Project to common embedding space\n        text_features = self.text_projection(text_features)\n        # Normalize\n        text_features = F.normalize(text_features, dim=-1)\n        return text_features\n    \n    def forward(self, image, text):\n        image_features = self.encode_image(image)\n        text_features = self.encode_text(text)\n        \n        # Compute logits\n        logit_scale = self.logit_scale.exp()\n        logits_per_image = logit_scale * image_features @ text_features.t()\n        logits_per_text = logits_per_image.t()\n        \n        return logits_per_image, logits_per_text\n```\n\n### CLIP Loss Function\n\n```python\ndef clip_loss(logits_per_image, logits_per_text):\n    \"\"\"\n    Contrastive loss for CLIP training\n    \"\"\"\n    batch_size = logits_per_image.shape[0]\n    labels = torch.arange(batch_size, device=logits_per_image.device)\n    \n    # Cross-entropy loss for both directions\n    loss_i = F.cross_entropy(logits_per_image, labels)\n    loss_t = F.cross_entropy(logits_per_text, labels)\n    \n    # Average the losses\n    loss = (loss_i + loss_t) / 2\n    return loss\n```\n\n## Training CLIP from Scratch\n\n### Dataset Preparation\n\n```python\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport json\n\nclass ImageTextDataset(Dataset):\n    def __init__(self, data_path, image_dir, transform=None, tokenizer=None, max_length=77):\n        with open(data_path, 'r') as f:\n            self.data = json.load(f)\n        \n        self.image_dir = image_dir\n        self.transform = transform\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        # Load image\n        image_path = os.path.join(self.image_dir, item['image'])\n        image = Image.open(image_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        # Tokenize text\n        text = item['caption']\n        if self.tokenizer:\n            text_tokens = self.tokenizer(\n                text,\n                max_length=self.max_length,\n                padding='max_length',\n                truncation=True,\n                return_tensors='pt'\n            )['input_ids'].squeeze(0)\n        else:\n            # Simple tokenization for demonstration\n            text_tokens = torch.zeros(self.max_length, dtype=torch.long)\n        \n        return image, text_tokens, text\n```\n\n### Training Loop\n\n```python\ndef train_clip(model, dataloader, optimizer, scheduler, device, num_epochs):\n    model.train()\n    \n    for epoch in range(num_epochs):\n        total_loss = 0\n        num_batches = 0\n        \n        for batch_idx, (images, text_tokens, _) in enumerate(dataloader):\n            images = images.to(device)\n            text_tokens = text_tokens.to(device)\n            \n            # Forward pass\n            logits_per_image, logits_per_text = model(images, text_tokens)\n            \n            # Compute loss\n            loss = clip_loss(logits_per_image, logits_per_text)\n            \n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            num_batches += 1\n            \n            # Logging\n            if batch_idx % 100 == 0:\n                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n        \n        # Update learning rate\n        scheduler.step()\n        \n        avg_loss = total_loss / num_batches\n        print(f'Epoch {epoch} completed. Average Loss: {avg_loss:.4f}')\n\n# Training setup\nmodel = CLIPModel().to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n\n# Start training\ntrain_clip(model, dataloader, optimizer, scheduler, device, num_epochs=100)\n```\n\n## Fine-tuning CLIP\n\n### Domain-Specific Fine-tuning\n\n```python\ndef fine_tune_clip(pretrained_model, dataloader, num_epochs=10, lr=1e-5):\n    # Freeze most layers, only fine-tune projection layers\n    for param in pretrained_model.visual.parameters():\n        param.requires_grad = False\n    \n    for param in pretrained_model.text_encoder.parameters():\n        param.requires_grad = False\n    \n    # Only train projection layers\n    optimizer = torch.optim.Adam([\n        {'params': pretrained_model.visual_projection.parameters()},\n        {'params': pretrained_model.text_projection.parameters()},\n        {'params': [pretrained_model.logit_scale]}\n    ], lr=lr)\n    \n    pretrained_model.train()\n    \n    for epoch in range(num_epochs):\n        for batch_idx, (images, text_tokens, _) in enumerate(dataloader):\n            images = images.to(device)\n            text_tokens = text_tokens.to(device)\n            \n            logits_per_image, logits_per_text = pretrained_model(images, text_tokens)\n            loss = clip_loss(logits_per_image, logits_per_text)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            if batch_idx % 50 == 0:\n                print(f'Fine-tune Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n```\n\n## Advanced Applications\n\n### 1. Image Search with CLIP\n\n```python\nclass CLIPImageSearch:\n    def __init__(self, model, preprocess):\n        self.model = model\n        self.preprocess = preprocess\n        self.image_features = None\n        self.image_paths = None\n    \n    def index_images(self, image_paths):\n        \"\"\"Pre-compute features for all images\"\"\"\n        self.image_paths = image_paths\n        features = []\n        \n        for img_path in image_paths:\n            image = Image.open(img_path)\n            image_input = self.preprocess(image).unsqueeze(0).to(device)\n            \n            with torch.no_grad():\n                image_feature = self.model.encode_image(image_input)\n                features.append(image_feature)\n        \n        self.image_features = torch.cat(features, dim=0)\n        self.image_features = self.image_features / self.image_features.norm(dim=-1, keepdim=True)\n    \n    def search(self, query_text, top_k=5):\n        \"\"\"Search for images matching the text query\"\"\"\n        text_input = clip.tokenize([query_text]).to(device)\n        \n        with torch.no_grad():\n            text_features = self.model.encode_text(text_input)\n            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n            \n            # Compute similarities\n            similarities = (text_features @ self.image_features.T).squeeze(0)\n            \n            # Get top-k results\n            top_similarities, top_indices = similarities.topk(top_k)\n        \n        results = []\n        for sim, idx in zip(top_similarities, top_indices):\n            results.append({\n                'path': self.image_paths[idx],\n                'similarity': sim.item()\n            })\n        \n        return results\n\n# Usage example\nsearch_engine = CLIPImageSearch(model, preprocess)\nsearch_engine.index_images(list_of_image_paths)\nresults = search_engine.search(\"a red sports car\", top_k=10)\n```\n\n### 2. Content-Based Image Clustering\n\n```python\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\ndef cluster_images_by_content(image_paths, n_clusters=5):\n    # Extract features for all images\n    features = []\n    \n    for img_path in image_paths:\n        image = Image.open(img_path)\n        image_input = preprocess(image).unsqueeze(0).to(device)\n        \n        with torch.no_grad():\n            feature = model.encode_image(image_input)\n            features.append(feature.cpu().numpy())\n    \n    # Convert to numpy array\n    features = np.vstack(features)\n    \n    # Perform clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    cluster_labels = kmeans.fit_predict(features)\n    \n    # Organize results\n    clusters = {}\n    for i, label in enumerate(cluster_labels):\n        if label not in clusters:\n            clusters[label] = []\n        clusters[label].append(image_paths[i])\n    \n    return clusters\n```\n\n### 3. Visual Question Answering\n\n```python\ndef visual_qa(image_path, question, answer_choices):\n    \"\"\"Simple VQA using CLIP\"\"\"\n    image = Image.open(image_path)\n    image_input = preprocess(image).unsqueeze(0).to(device)\n    \n    # Create prompts combining question with each answer\n    prompts = [f\"Question: {question} Answer: {choice}\" for choice in answer_choices]\n    text_inputs = clip.tokenize(prompts).to(device)\n    \n    with torch.no_grad():\n        image_features = model.encode_image(image_input)\n        text_features = model.encode_text(text_inputs)\n        \n        # Compute similarities\n        similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n    \n    # Return the most likely answer\n    best_idx = similarities.argmax().item()\n    return answer_choices[best_idx], similarities[0][best_idx].item()\n\n# Example usage\nanswer, confidence = visual_qa(\n    \"image.jpg\",\n    \"What color is the car?\",\n    [\"red\", \"blue\", \"green\", \"yellow\", \"black\"]\n)\nprint(f\"Answer: {answer}, Confidence: {confidence:.2%}\")\n```\n\n## Performance Optimization\n\n### 1. Batch Processing\n\n```python\ndef batch_encode_images(image_paths, batch_size=32):\n    \"\"\"Process images in batches for better efficiency\"\"\"\n    all_features = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_paths = image_paths[i:i+batch_size]\n        batch_images = []\n        \n        for path in batch_paths:\n            image = Image.open(path)\n            image_input = preprocess(image)\n            batch_images.append(image_input)\n        \n        batch_tensor = torch.stack(batch_images).to(device)\n        \n        with torch.no_grad():\n            batch_features = model.encode_image(batch_tensor)\n            all_features.append(batch_features.cpu())\n    \n    return torch.cat(all_features, dim=0)\n```\n\n### 2. Mixed Precision Training\n\n```python\nfrom torch.cuda.amp import autocast, GradScaler\n\ndef train_with_mixed_precision(model, dataloader, optimizer, num_epochs):\n    scaler = GradScaler()\n    model.train()\n    \n    for epoch in range(num_epochs):\n        for images, text_tokens, _ in dataloader:\n            images = images.to(device)\n            text_tokens = text_tokens.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Forward pass with autocast\n            with autocast():\n                logits_per_image, logits_per_text = model(images, text_tokens)\n                loss = clip_loss(logits_per_image, logits_per_text)\n            \n            # Backward pass with scaling\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n```\n\n### 3. Model Quantization\n\n```python\nimport torch.quantization as quantization\n\ndef quantize_clip_model(model):\n    \"\"\"Quantize CLIP model for inference\"\"\"\n    model.eval()\n    \n    # Specify quantization configuration\n    model.qconfig = quantization.get_default_qconfig('fbgemm')\n    \n    # Prepare model for quantization\n    model_prepared = quantization.prepare(model, inplace=False)\n    \n    # Calibrate with sample data (you need to provide calibration data)\n    # ... calibration code here ...\n    \n    # Convert to quantized model\n    model_quantized = quantization.convert(model_prepared, inplace=False)\n    \n    return model_quantized\n```\n\n## Common Issues and Solutions\n\n### 1. Memory Management\n\n```python\n# Clear GPU cache\ntorch.cuda.empty_cache()\n\n# Use gradient checkpointing for large models\ndef enable_gradient_checkpointing(model):\n    if hasattr(model.visual, 'set_grad_checkpointing'):\n        model.visual.set_grad_checkpointing(True)\n    if hasattr(model.text_encoder, 'gradient_checkpointing_enable'):\n        model.text_encoder.gradient_checkpointing_enable()\n```\n\n### 2. Handling Different Image Sizes\n\n```python\nfrom torchvision import transforms\n\ndef create_adaptive_transform(target_size=224):\n    return transforms.Compose([\n        transforms.Resize(target_size, interpolation=transforms.InterpolationMode.BICUBIC),\n        transforms.CenterCrop(target_size),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        )\n    ])\n```\n\n### 3. Text Preprocessing\n\n```python\nimport re\n\ndef preprocess_text(text, max_length=77):\n    \"\"\"Clean and preprocess text for CLIP\"\"\"\n    # Remove special characters and extra whitespace\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = ' '.join(text.split())\n    \n    # Truncate if too long\n    words = text.split()\n    if len(words) > max_length - 2:  # Account for special tokens\n        text = ' '.join(words[:max_length-2])\n    \n    return text\n```\n\n### 4. Model Evaluation Utilities\n\n```python\ndef evaluate_zero_shot_accuracy(model, preprocess, test_loader, class_names):\n    \"\"\"Evaluate zero-shot classification accuracy\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    \n    # Encode class names\n    text_inputs = clip.tokenize([f\"a photo of a {name}\" for name in class_names]).to(device)\n    \n    with torch.no_grad():\n        text_features = model.encode_text(text_inputs)\n        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n        \n        for images, labels in test_loader:\n            images = images.to(device)\n            \n            # Encode images\n            image_features = model.encode_image(images)\n            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n            \n            # Compute similarities\n            similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n            predictions = similarities.argmax(dim=-1)\n            \n            correct += (predictions == labels.to(device)).sum().item()\n            total += labels.size(0)\n    \n    accuracy = correct / total\n    return accuracy\n\n# Usage\naccuracy = evaluate_zero_shot_accuracy(model, preprocess, test_loader, class_names)\nprint(f\"Zero-shot accuracy: {accuracy:.2%}\")\n```\n\n## Conclusion\n\nThis guide covers the essential aspects of working with CLIP, from basic usage to advanced implementations. Key takeaways:\n\n1. **Start Simple**: Use pre-trained models for most applications\n2. **Understand the Architecture**: CLIP's power comes from joint text-image training\n3. **Optimize for Your Use Case**: Fine-tune or customize based on your specific needs\n4. **Monitor Performance**: Use proper evaluation metrics and optimization techniques\n5. **Handle Edge Cases**: Implement robust preprocessing and error handling\n\nFor production deployments, consider:\n\n- Model quantization for faster inference\n- Batch processing for efficiency\n- Proper error handling and fallbacks\n- Monitoring and logging for performance tracking\n\nThe field of multimodal AI is rapidly evolving, so stay updated with the latest research and implementations to leverage CLIP's full potential in your applications.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}