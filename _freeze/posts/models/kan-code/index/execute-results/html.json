{
  "hash": "b09e49338eba5bd7733c72235d4e230b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Kolmogorov-Arnold Networks: Complete Implementation Guide\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-07-02\"\ncategories: [code, tutorial, advanced]\nformat:\n  html:\n    code-fold: false\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# Kolmogorov-Arnold Networks: Complete Implementation Guide\n![](kan.png)\n\n## Introduction\n\nKolmogorov-Arnold Networks (KANs) represent a paradigm shift in neural network architecture, drawing inspiration from the mathematical foundations laid by Andrey Kolmogorov and Vladimir Arnold in the 1950s. Unlike traditional Multi-Layer Perceptrons (MLPs) that place learnable parameters on nodes, KANs position learnable activation functions on edges, fundamentally changing how neural networks process and learn from data.\n\n## Architecture Overview\n\n### Traditional MLPs vs KANs\n\n**Multi-Layer Perceptrons (MLPs):**\n- Learnable parameters: weights and biases on nodes\n- Fixed activation functions (ReLU, sigmoid, etc.)\n- Linear transformations followed by pointwise nonlinearities\n\n**Kolmogorov-Arnold Networks (KANs):**\n- Learnable parameters: activation functions on edges\n- No traditional weight matrices\n- Each edge has its own learnable univariate function\n\n## Mathematical Formulation\n\n### Layer-wise Computation\n\nFor a KAN with L layers, the computation at layer l can be expressed as:\n\n```python\n# Pseudocode for KAN layer computation\ndef kan_layer_forward(x, phi_functions):\n    \"\"\"\n    x: input tensor of shape (batch_size, input_dim)\n    phi_functions: learnable univariate functions for each edge\n    \"\"\"\n    output = torch.zeros(batch_size, output_dim)\n    \n    for i in range(input_dim):\n        for j in range(output_dim):\n            # Apply learnable activation function φ_{i,j} to input x_i\n            output[:, j] += phi_functions[i][j](x[:, i])\n    \n    return output\n```\n\n### Learnable Activation Functions\n\nThe core innovation of KANs lies in the learnable activation functions. These are typically implemented using:\n\n1. **B-splines**: Piecewise polynomial functions that provide smooth, differentiable approximations\n2. **Residual connections**: Allow the network to learn both the spline component and a base function\n3. **Grid-based parameterization**: Enables efficient computation and gradient flow\n\n## Implementation Details\n\n### B-spline Based Activation Functions\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass BSplineActivation(nn.Module):\n    def __init__(self, grid_size=5, spline_order=3, grid_range=(-1, 1)):\n        super().__init__()\n        self.grid_size = grid_size\n        self.spline_order = spline_order\n        self.grid_range = grid_range\n        \n        # Create uniform grid\n        self.register_buffer('grid', torch.linspace(\n            grid_range[0], grid_range[1], grid_size + 1\n        ))\n        \n        # Extend grid for B-spline computation\n        h = (grid_range[1] - grid_range[0]) / grid_size\n        extended_grid = torch.cat([\n            torch.arange(grid_range[0] - spline_order * h, grid_range[0], h),\n            self.grid,\n            torch.arange(grid_range[1] + h, grid_range[1] + (spline_order + 1) * h, h)\n        ])\n        self.register_buffer('extended_grid', extended_grid)\n        \n        # Learnable coefficients for B-spline\n        self.coefficients = nn.Parameter(\n            torch.randn(grid_size + spline_order)\n        )\n        \n        # Scale parameter for the activation\n        self.scale = nn.Parameter(torch.ones(1))\n        \n    def forward(self, x):\n        # Compute B-spline basis functions\n        batch_size = x.shape[0]\n        x_expanded = x.unsqueeze(-1)  # (batch_size, 1)\n        \n        # Compute B-spline values\n        spline_values = self.compute_bspline(x_expanded)\n        \n        # Linear combination with learnable coefficients\n        output = torch.sum(spline_values * self.coefficients, dim=-1)\n        \n        return self.scale * output\n    \n    def compute_bspline(self, x):\n        \"\"\"Compute B-spline basis functions using Cox-de Boor recursion\"\"\"\n        grid = self.extended_grid\n        order = self.spline_order\n        \n        # Initialize basis functions\n        basis = torch.zeros(x.shape[0], len(grid) - 1, device=x.device)\n        \n        # Find intervals\n        for i in range(len(grid) - 1):\n            mask = (x.squeeze(-1) >= grid[i]) & (x.squeeze(-1) < grid[i + 1])\n            basis[mask, i] = 1.0\n        \n        # Cox-de Boor recursion\n        for k in range(1, order + 1):\n            new_basis = torch.zeros_like(basis)\n            for i in range(len(grid) - k - 1):\n                if grid[i + k] != grid[i]:\n                    alpha1 = (x.squeeze(-1) - grid[i]) / (grid[i + k] - grid[i])\n                    new_basis[:, i] += alpha1 * basis[:, i]\n                \n                if grid[i + k + 1] != grid[i + 1]:\n                    alpha2 = (grid[i + k + 1] - x.squeeze(-1)) / (grid[i + k + 1] - grid[i + 1])\n                    new_basis[:, i] += alpha2 * basis[:, i + 1]\n            \n            basis = new_basis\n        \n        return basis[:, :len(self.coefficients)]\n\nclass KANLayer(nn.Module):\n    def __init__(self, input_dim, output_dim, grid_size=5, spline_order=3):\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        \n        # Create learnable activation functions for each edge\n        self.activations = nn.ModuleList([\n            nn.ModuleList([\n                BSplineActivation(grid_size, spline_order) \n                for _ in range(output_dim)\n            ]) for _ in range(input_dim)\n        ])\n        \n        # Base linear transformation (residual connection)\n        self.base_weight = nn.Parameter(torch.randn(input_dim, output_dim) * 0.1)\n        \n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.zeros(batch_size, self.output_dim, device=x.device)\n        \n        # Apply learnable activations\n        for i in range(self.input_dim):\n            for j in range(self.output_dim):\n                activated = self.activations[i][j](x[:, i])\n                output[:, j] += activated\n        \n        # Add base linear transformation\n        base_output = torch.matmul(x, self.base_weight)\n        \n        return output + base_output\n\nclass KolmogorovArnoldNetwork(nn.Module):\n    def __init__(self, layer_dims, grid_size=5, spline_order=3):\n        super().__init__()\n        self.layers = nn.ModuleList()\n        \n        for i in range(len(layer_dims) - 1):\n            layer = KANLayer(\n                layer_dims[i], \n                layer_dims[i + 1], \n                grid_size, \n                spline_order\n            )\n            self.layers.append(layer)\n    \n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n    \n    def regularization_loss(self, regularization_factor=1e-4):\n        \"\"\"Compute regularization loss to encourage sparsity\"\"\"\n        reg_loss = 0\n        for layer in self.layers:\n            for i in range(layer.input_dim):\n                for j in range(layer.output_dim):\n                    # L1 regularization on activation function coefficients\n                    reg_loss += torch.sum(torch.abs(layer.activations[i][j].coefficients))\n        \n        return regularization_factor * reg_loss\n```\n\n### Training Loop Implementation\n\n```python\ndef train_kan(model, train_loader, val_loader, epochs=100, lr=1e-3):\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=10\n    )\n    \n    criterion = nn.MSELoss()\n    \n    train_losses = []\n    val_losses = []\n    \n    for epoch in range(epochs):\n        # Training phase\n        model.train()\n        train_loss = 0\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            \n            output = model(data)\n            loss = criterion(output, target)\n            \n            # Add regularization\n            reg_loss = model.regularization_loss()\n            total_loss = loss + reg_loss\n            \n            total_loss.backward()\n            \n            # Gradient clipping for stability\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n            train_loss += total_loss.item()\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for data, target in val_loader:\n                output = model(data)\n                val_loss += criterion(output, target).item()\n        \n        avg_train_loss = train_loss / len(train_loader)\n        avg_val_loss = val_loss / len(val_loader)\n        \n        train_losses.append(avg_train_loss)\n        val_losses.append(avg_val_loss)\n        \n        scheduler.step(avg_val_loss)\n        \n        if epoch % 10 == 0:\n            print(f'Epoch {epoch}: Train Loss = {avg_train_loss:.6f}, '\n                  f'Val Loss = {avg_val_loss:.6f}')\n    \n    return train_losses, val_losses\n```\n\n## Advanced Features and Optimizations\n\n### 1. Pruning and Sparsification\n\n```python\ndef prune_kan(model, threshold=1e-2):\n    \"\"\"Remove edges with small activation function magnitudes\"\"\"\n    with torch.no_grad():\n        for layer in model.layers:\n            for i in range(layer.input_dim):\n                for j in range(layer.output_dim):\n                    activation = layer.activations[i][j]\n                    \n                    # Compute magnitude of activation function\n                    magnitude = torch.norm(activation.coefficients)\n                    \n                    if magnitude < threshold:\n                        # Zero out the activation function\n                        activation.coefficients.fill_(0)\n                        activation.scale.fill_(0)\n```\n\n### 2. Symbolic Regression Capabilities\n\n```python\ndef symbolic_extraction(model, input_names, output_names):\n    \"\"\"Extract symbolic expressions from trained KAN\"\"\"\n    expressions = []\n    \n    for layer_idx, layer in enumerate(model.layers):\n        layer_expressions = []\n        \n        for j in range(layer.output_dim):\n            terms = []\n            \n            for i in range(layer.input_dim):\n                activation = layer.activations[i][j]\n                \n                # Check if activation is significant\n                if torch.norm(activation.coefficients) > 1e-3:\n                    # Fit simple function to activation\n                    func_type = fit_symbolic_function(activation)\n                    terms.append(f\"{func_type}({input_names[i]})\")\n            \n            if terms:\n                expression = \" + \".join(terms)\n                layer_expressions.append(expression)\n        \n        expressions.append(layer_expressions)\n    \n    return expressions\n\ndef fit_symbolic_function(activation):\n    \"\"\"Fit symbolic function to learned activation\"\"\"\n    # Sample the activation function\n    x_test = torch.linspace(-1, 1, 100)\n    y_test = activation(x_test).detach()\n    \n    # Try fitting common functions\n    functions = {\n        'linear': lambda x, a, b: a * x + b,\n        'quadratic': lambda x, a, b, c: a * x**2 + b * x + c,\n        'sin': lambda x, a, b, c: a * torch.sin(b * x + c),\n        'exp': lambda x, a, b: a * torch.exp(b * x),\n        'tanh': lambda x, a, b: a * torch.tanh(b * x)\n    }\n    \n    best_fit = 'linear'  # Default\n    min_error = float('inf')\n    \n    for func_name, func in functions.items():\n        try:\n            # Simplified fitting (in practice, use scipy.optimize)\n            if func_name == 'linear':\n                # Simple linear regression\n                A = torch.stack([x_test, torch.ones_like(x_test)], dim=1)\n                params = torch.linalg.lstsq(A, y_test).solution\n                pred = func(x_test, params[0], params[1])\n            else:\n                # Use first-order approximation\n                pred = y_test  # Placeholder\n            \n            error = torch.mean((y_test - pred)**2)\n            \n            if error < min_error:\n                min_error = error\n                best_fit = func_name\n        \n        except:\n            continue\n    \n    return best_fit\n```\n\n### 3. Grid Adaptation\n\n```python\ndef adaptive_grid_refinement(model, train_loader, refinement_factor=2):\n    \"\"\"Adapt grid points based on function complexity\"\"\"\n    model.eval()\n    \n    with torch.no_grad():\n        # Collect statistics on activation function usage\n        activation_stats = {}\n        \n        for batch_idx, (data, target) in enumerate(train_loader):\n            if batch_idx > 10:  # Sample a few batches\n                break\n                \n            for layer_idx, layer in enumerate(model.layers):\n                if layer_idx not in activation_stats:\n                    activation_stats[layer_idx] = {}\n                \n                for i in range(layer.input_dim):\n                    for j in range(layer.output_dim):\n                        key = (i, j)\n                        if key not in activation_stats[layer_idx]:\n                            activation_stats[layer_idx][key] = []\n                        \n                        # Record input values for this activation\n                        if layer_idx == 0:\n                            input_vals = data[:, i]\n                        else:\n                            # Would need to track intermediate activations\n                            input_vals = data[:, i]  # Simplified\n                        \n                        activation_stats[layer_idx][key].extend(\n                            input_vals.cpu().numpy().tolist()\n                        )\n        \n        # Refine grids based on usage patterns\n        for layer_idx, layer in enumerate(model.layers):\n            for i in range(layer.input_dim):\n                for j in range(layer.output_dim):\n                    activation = layer.activations[i][j]\n                    key = (i, j)\n                    \n                    if key in activation_stats[layer_idx]:\n                        input_range = activation_stats[layer_idx][key]\n                        \n                        # Compute density and refine grid\n                        hist, bins = torch.histogram(\n                            torch.tensor(input_range), bins=activation.grid_size\n                        )\n                        \n                        # Areas with high density get more grid points\n                        high_density_regions = hist > hist.mean()\n                        \n                        if high_density_regions.any():\n                            # Refine grid (simplified implementation)\n                            new_grid_size = activation.grid_size * refinement_factor\n                            # Would need to properly interpolate coefficients\n```\n\n## Practical Applications and Use Cases\n\n### 1. Function Approximation\n\n```python\n# Example: Approximating a complex mathematical function\ndef test_function_approximation():\n    # Generate synthetic data\n    def target_function(x):\n        return torch.sin(x[:, 0]) * torch.cos(x[:, 1]) + 0.5 * x[:, 0]**2\n    \n    # Create dataset\n    n_samples = 1000\n    x = torch.randn(n_samples, 2)\n    y = target_function(x).unsqueeze(1)\n    \n    # Split data\n    train_size = int(0.8 * n_samples)\n    train_x, test_x = x[:train_size], x[train_size:]\n    train_y, test_y = y[:train_size], y[train_size:]\n    \n    # Create KAN model\n    model = KolmogorovArnoldNetwork([2, 5, 1], grid_size=10)\n    \n    # Train model\n    train_dataset = torch.utils.data.TensorDataset(train_x, train_y)\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n    \n    val_dataset = torch.utils.data.TensorDataset(test_x, test_y)\n    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n    \n    train_losses, val_losses = train_kan(model, train_loader, val_loader)\n    \n    # Evaluate\n    model.eval()\n    with torch.no_grad():\n        predictions = model(test_x)\n        mse = torch.mean((predictions - test_y)**2)\n        print(f\"Test MSE: {mse.item():.6f}\")\n    \n    return model, train_losses, val_losses\n```\n\n### 2. Scientific Computing\n\n```python\n# Example: Solving differential equations\ndef solve_pde_with_kan():\n    \"\"\"Use KAN to solve partial differential equations\"\"\"\n    \n    class PDESolver(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.kan = KolmogorovArnoldNetwork([2, 10, 10, 1])\n        \n        def forward(self, x, t):\n            inputs = torch.stack([x, t], dim=1)\n            return self.kan(inputs)\n        \n        def physics_loss(self, x, t):\n            \"\"\"Compute physics-informed loss for PDE\"\"\"\n            x.requires_grad_(True)\n            t.requires_grad_(True)\n            \n            u = self.forward(x, t)\n            \n            # Compute derivatives\n            u_t = torch.autograd.grad(\n                u, t, torch.ones_like(u), create_graph=True\n            )[0]\n            \n            u_x = torch.autograd.grad(\n                u, x, torch.ones_like(u), create_graph=True\n            )[0]\n            \n            u_xx = torch.autograd.grad(\n                u_x, x, torch.ones_like(u_x), create_graph=True\n            )[0]\n            \n            # PDE residual: u_t - u_xx = 0 (heat equation)\n            pde_residual = u_t - u_xx\n            \n            return torch.mean(pde_residual**2)\n    \n    # Training would involve minimizing physics loss\n    # along with boundary and initial conditions\n    return PDESolver()\n```\n\n## Performance Analysis and Comparisons\n\n### Computational Complexity\n\n**Memory Complexity:**\n- MLPs: O(Σ(n_i × n_{i+1})) where n_i is the number of neurons in layer i\n- KANs: O(Σ(n_i × n_{i+1} × G)) where G is the grid size for B-splines\n\n**Time Complexity:**\n- Forward pass: O(Σ(n_i × n_{i+1} × G × k)) where k is the spline order\n- Backward pass: Similar, with additional complexity for B-spline derivative computation\n\n### Advantages of KANs\n\n1. **Interpretability**: Learnable activation functions can be visualized and analyzed\n2. **Expressiveness**: Can represent complex functions with fewer parameters in some cases\n3. **Scientific Computing**: Natural fit for problems requiring symbolic regression\n4. **Adaptive Capacity**: Can learn specialized activation functions for different parts of the input space\n\n### Limitations\n\n1. **Computational Overhead**: B-spline computation is more expensive than simple activations\n2. **Memory Usage**: Requires more memory due to grid-based parameterization\n3. **Training Stability**: Can be more sensitive to hyperparameter choices\n4. **Limited Scale**: Current implementations don't scale to very large networks easily\n\n## Best Practices and Hyperparameter Tuning\n\n### Grid Size Selection\n\n```python\ndef tune_grid_size(data_complexity, input_dim):\n    \"\"\"Heuristic for selecting appropriate grid size\"\"\"\n    base_grid_size = max(5, min(20, int(math.log(data_complexity) * 2)))\n    \n    # Adjust based on input dimensionality\n    if input_dim > 10:\n        base_grid_size = max(3, base_grid_size - 2)\n    elif input_dim < 3:\n        base_grid_size = min(25, base_grid_size + 3)\n    \n    return base_grid_size\n```\n\n### Regularization Strategies\n\n```python\ndef advanced_regularization(model, l1_factor=1e-4, smoothness_factor=1e-3):\n    \"\"\"Comprehensive regularization for KANs\"\"\"\n    reg_loss = 0\n    \n    for layer in model.layers:\n        for i in range(layer.input_dim):\n            for j in range(layer.output_dim):\n                activation = layer.activations[i][j]\n                \n                # L1 regularization for sparsity\n                l1_loss = torch.sum(torch.abs(activation.coefficients))\n                \n                # Smoothness regularization\n                if len(activation.coefficients) > 1:\n                    smoothness_loss = torch.sum(\n                        (activation.coefficients[1:] - activation.coefficients[:-1])**2\n                    )\n                else:\n                    smoothness_loss = 0\n                \n                reg_loss += l1_factor * l1_loss + smoothness_factor * smoothness_loss\n    \n    return reg_loss\n```\n\n## Future Directions and Research Opportunities\n\n### 1. Scalability Improvements\n- Efficient GPU implementations of B-spline computations\n- Sparse KAN architectures for high-dimensional problems\n- Distributed training strategies\n\n### 2. Theoretical Developments\n- Approximation theory for KAN architectures\n- Convergence guarantees and optimization landscapes\n- Connections to other function approximation methods\n\n### 3. Application Domains\n- Scientific machine learning and physics-informed neural networks\n- Automated theorem proving and symbolic computation\n- Interpretable AI for critical applications\n\n## Conclusion\n\nKolmogorov-Arnold Networks represent a fundamental rethinking of neural network architecture, moving from node-based to edge-based learnable parameters. While they face challenges in terms of computational efficiency and scalability, their unique properties make them particularly well-suited for scientific computing, interpretable AI, and function approximation tasks.\n\nThe mathematical elegance of KANs, rooted in classical approximation theory, combined with their practical capabilities for symbolic regression and interpretable modeling, positions them as an important tool in the modern machine learning toolkit. As implementation techniques improve and computational bottlenecks are addressed, we can expect to see broader adoption of KAN-based approaches across various domains.\n\nThe code implementations provided here offer a foundation for experimenting with KANs, but ongoing research continues to refine these architectures and explore their full potential. Whether KANs will revolutionize neural network design remains to be seen, but they certainly offer a compelling alternative perspective on how neural networks can learn and represent complex functions.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}