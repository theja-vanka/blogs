{
  "hash": "1470639277a56ce6435c2e12c474129e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Python Multiprocessing and Multithreading: A Comprehensive Guide\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-07-06\"\ncategories: [code, tutorial, beginner]\nformat:\n  html:\n    code-fold: false\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# Python Multiprocessing and Multithreading: A Comprehensive Guide\n![](multi-star.png)\n\n## Introduction\n\nPython provides two primary approaches for concurrent execution: **multithreading** and **multiprocessing**. Understanding when and how to use each is crucial for writing efficient Python applications.\n\n- **Multithreading**: Multiple threads within a single process sharing memory space\n- **Multiprocessing**: Multiple separate processes, each with its own memory space\n\n## Understanding Concurrency vs Parallelism\n\n### Concurrency\nConcurrency is about dealing with multiple tasks at once, but not necessarily executing them simultaneously. Tasks may be interleaved or switched between rapidly.\n\n### Parallelism\nParallelism is about executing multiple tasks simultaneously, typically on multiple CPU cores.\n\n```python\n# Concurrent execution (may not be parallel)\nimport threading\nimport time\n\ndef task(name):\n    for i in range(3):\n        print(f\"Task {name}: {i}\")\n        time.sleep(0.1)\n\n# Create threads\nt1 = threading.Thread(target=task, args=(\"A\",))\nt2 = threading.Thread(target=task, args=(\"B\",))\n\n# Start threads\nt1.start()\nt2.start()\n\n# Wait for completion\nt1.join()\nt2.join()\n```\n\n## The Global Interpreter Lock (GIL)\n\nThe GIL is a mutex that protects access to Python objects, preventing multiple threads from executing Python bytecode simultaneously. This has important implications:\n\n### GIL Impact\n- **CPU-bound tasks**: Multithreading provides little benefit due to GIL\n- **I/O-bound tasks**: Multithreading can be effective as GIL is released during I/O operations\n- **Multiprocessing**: Bypasses GIL limitations by using separate processes\n\n### When GIL is Released\n- File I/O operations\n- Network I/O operations\n- Image processing (PIL/Pillow)\n- NumPy operations\n- Time.sleep() calls\n\n## Multithreading with threading Module\n\n### Basic Thread Creation\n\n```python\nimport threading\nimport time\n\n# Method 1: Using Thread class directly\ndef worker_function(name, delay):\n    for i in range(5):\n        print(f\"Worker {name}: {i}\")\n        time.sleep(delay)\n\n# Create and start threads\nthread1 = threading.Thread(target=worker_function, args=(\"A\", 0.5))\nthread2 = threading.Thread(target=worker_function, args=(\"B\", 0.3))\n\nthread1.start()\nthread2.start()\n\nthread1.join()\nthread2.join()\n```\n\n### Thread Subclassing\n\n```python\nimport threading\nimport time\n\nclass WorkerThread(threading.Thread):\n    def __init__(self, name, delay):\n        super().__init__()\n        self.name = name\n        self.delay = delay\n    \n    def run(self):\n        for i in range(5):\n            print(f\"Worker {self.name}: {i}\")\n            time.sleep(self.delay)\n\n# Create and start threads\nworker1 = WorkerThread(\"A\", 0.5)\nworker2 = WorkerThread(\"B\", 0.3)\n\nworker1.start()\nworker2.start()\n\nworker1.join()\nworker2.join()\n```\n\n### Thread Pool Executor\n\n```python\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport time\n\ndef task(name, duration):\n    print(f\"Starting task {name}\")\n    time.sleep(duration)\n    return f\"Task {name} completed\"\n\n# Using ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=3) as executor:\n    # Submit tasks\n    futures = [\n        executor.submit(task, \"A\", 2),\n        executor.submit(task, \"B\", 1),\n        executor.submit(task, \"C\", 3)\n    ]\n    \n    # Collect results as they complete\n    for future in as_completed(futures):\n        result = future.result()\n        print(result)\n```\n\n### Thread-Safe Operations\n\n```python\nimport threading\nimport time\n\nclass ThreadSafeCounter:\n    def __init__(self):\n        self.value = 0\n        self.lock = threading.Lock()\n    \n    def increment(self):\n        with self.lock:\n            temp = self.value\n            time.sleep(0.001)  # Simulate processing\n            self.value = temp + 1\n    \n    def get_value(self):\n        with self.lock:\n            return self.value\n\n# Demonstrate thread safety\ncounter = ThreadSafeCounter()\n\ndef worker():\n    for _ in range(100):\n        counter.increment()\n\nthreads = []\nfor _ in range(10):\n    t = threading.Thread(target=worker)\n    threads.append(t)\n    t.start()\n\nfor t in threads:\n    t.join()\n\nprint(f\"Final counter value: {counter.get_value()}\")\n```\n\n## Multiprocessing with multiprocessing Module\n\n### Basic Process Creation\n\n```python\nimport multiprocessing\nimport time\nimport os\n\ndef worker_function(name, delay):\n    process_id = os.getpid()\n    for i in range(5):\n        print(f\"Worker {name} (PID: {process_id}): {i}\")\n        time.sleep(delay)\n\nif __name__ == \"__main__\":\n    # Create and start processes\n    process1 = multiprocessing.Process(target=worker_function, args=(\"A\", 0.5))\n    process2 = multiprocessing.Process(target=worker_function, args=(\"B\", 0.3))\n    \n    process1.start()\n    process2.start()\n    \n    process1.join()\n    process2.join()\n```\n\n### Process Pool\n\n```python\nimport multiprocessing\nimport time\n\ndef compute_square(n):\n    \"\"\"CPU-intensive task\"\"\"\n    return n * n\n\ndef compute_with_delay(n):\n    \"\"\"Simulate processing time\"\"\"\n    time.sleep(0.1)\n    return n * n\n\nif __name__ == \"__main__\":\n    numbers = list(range(1, 11))\n    \n    # Sequential execution\n    start_time = time.time()\n    sequential_results = [compute_with_delay(n) for n in numbers]\n    sequential_time = time.time() - start_time\n    \n    # Parallel execution\n    start_time = time.time()\n    with multiprocessing.Pool(processes=4) as pool:\n        parallel_results = pool.map(compute_with_delay, numbers)\n    parallel_time = time.time() - start_time\n    \n    print(f\"Sequential time: {sequential_time:.2f} seconds\")\n    print(f\"Parallel time: {parallel_time:.2f} seconds\")\n    print(f\"Speedup: {sequential_time/parallel_time:.2f}x\")\n```\n\n### Process Pool Executor\n\n```python\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nimport time\n\ndef cpu_intensive_task(n):\n    \"\"\"Simulate CPU-intensive computation\"\"\"\n    total = 0\n    for i in range(n * 1000000):\n        total += i\n    return total\n\nif __name__ == \"__main__\":\n    tasks = [100, 200, 300, 400, 500]\n    \n    with ProcessPoolExecutor(max_workers=4) as executor:\n        # Submit all tasks\n        futures = [executor.submit(cpu_intensive_task, task) for task in tasks]\n        \n        # Collect results\n        for i, future in enumerate(as_completed(futures)):\n            result = future.result()\n            print(f\"Task {i+1} completed with result: {result}\")\n```\n\n## Communication Between Processes/Threads\n\n### Queues\n\n```python\nimport multiprocessing\nimport threading\nimport time\n\n# Process Queue\ndef producer(queue, items):\n    for item in items:\n        queue.put(item)\n        print(f\"Produced: {item}\")\n        time.sleep(0.1)\n    queue.put(None)  # Sentinel value\n\ndef consumer(queue):\n    while True:\n        item = queue.get()\n        if item is None:\n            break\n        print(f\"Consumed: {item}\")\n        time.sleep(0.2)\n\nif __name__ == \"__main__\":\n    # Process communication\n    process_queue = multiprocessing.Queue()\n    items = ['item1', 'item2', 'item3', 'item4']\n    \n    producer_process = multiprocessing.Process(target=producer, args=(process_queue, items))\n    consumer_process = multiprocessing.Process(target=consumer, args=(process_queue,))\n    \n    producer_process.start()\n    consumer_process.start()\n    \n    producer_process.join()\n    consumer_process.join()\n```\n\n### Pipes\n\n```python\nimport multiprocessing\nimport time\n\ndef sender(conn, messages):\n    for msg in messages:\n        conn.send(msg)\n        print(f\"Sent: {msg}\")\n        time.sleep(0.1)\n    conn.close()\n\ndef receiver(conn):\n    while True:\n        try:\n            msg = conn.recv()\n            print(f\"Received: {msg}\")\n        except EOFError:\n            break\n\nif __name__ == \"__main__\":\n    parent_conn, child_conn = multiprocessing.Pipe()\n    messages = ['Hello', 'World', 'From', 'Process']\n    \n    sender_process = multiprocessing.Process(target=sender, args=(child_conn, messages))\n    receiver_process = multiprocessing.Process(target=receiver, args=(parent_conn,))\n    \n    sender_process.start()\n    receiver_process.start()\n    \n    sender_process.join()\n    receiver_process.join()\n```\n\n### Shared Memory\n\n```python\nimport multiprocessing\nimport time\n\ndef worker(shared_list, shared_value, lock, worker_id):\n    for i in range(5):\n        with lock:\n            shared_value.value += 1\n            shared_list[worker_id] = shared_value.value\n            print(f\"Worker {worker_id}: Updated shared_value to {shared_value.value}\")\n        time.sleep(0.1)\n\nif __name__ == \"__main__\":\n    # Create shared objects\n    shared_list = multiprocessing.Array('i', [0] * 3)  # Array of integers\n    shared_value = multiprocessing.Value('i', 0)       # Single integer\n    lock = multiprocessing.Lock()\n    \n    processes = []\n    for i in range(3):\n        p = multiprocessing.Process(target=worker, args=(shared_list, shared_value, lock, i))\n        processes.append(p)\n        p.start()\n    \n    for p in processes:\n        p.join()\n    \n    print(f\"Final shared_list: {list(shared_list)}\")\n    print(f\"Final shared_value: {shared_value.value}\")\n```\n\n## Synchronization Primitives\n\n### Locks\n\n```python\nimport threading\nimport time\n\n# Thread Lock\nshared_resource = 0\nlock = threading.Lock()\n\ndef increment_with_lock():\n    global shared_resource\n    for _ in range(100000):\n        with lock:\n            shared_resource += 1\n\ndef increment_without_lock():\n    global shared_resource\n    for _ in range(100000):\n        shared_resource += 1\n\n# Demonstrate race condition\nshared_resource = 0\nthreads = []\nfor _ in range(5):\n    t = threading.Thread(target=increment_without_lock)\n    threads.append(t)\n    t.start()\n\nfor t in threads:\n    t.join()\n\nprint(f\"Without lock: {shared_resource}\")\n\n# With lock\nshared_resource = 0\nthreads = []\nfor _ in range(5):\n    t = threading.Thread(target=increment_with_lock)\n    threads.append(t)\n    t.start()\n\nfor t in threads:\n    t.join()\n\nprint(f\"With lock: {shared_resource}\")\n```\n\n### Semaphores\n\n```python\nimport threading\nimport time\n\n# Semaphore to limit concurrent access\nsemaphore = threading.Semaphore(2)  # Allow 2 concurrent accesses\n\ndef access_resource(worker_id):\n    with semaphore:\n        print(f\"Worker {worker_id} accessing resource\")\n        time.sleep(2)  # Simulate work\n        print(f\"Worker {worker_id} finished\")\n\nthreads = []\nfor i in range(5):\n    t = threading.Thread(target=access_resource, args=(i,))\n    threads.append(t)\n    t.start()\n\nfor t in threads:\n    t.join()\n```\n\n### Condition Variables\n\n```python\nimport threading\nimport time\nimport random\n\n# Producer-Consumer with Condition\ncondition = threading.Condition()\nbuffer = []\nMAX_SIZE = 5\n\ndef producer():\n    for i in range(10):\n        with condition:\n            while len(buffer) >= MAX_SIZE:\n                print(\"Buffer full, producer waiting...\")\n                condition.wait()\n            \n            item = f\"item_{i}\"\n            buffer.append(item)\n            print(f\"Produced: {item}\")\n            condition.notify_all()\n        \n        time.sleep(random.uniform(0.1, 0.5))\n\ndef consumer(consumer_id):\n    for _ in range(5):\n        with condition:\n            while not buffer:\n                print(f\"Consumer {consumer_id} waiting...\")\n                condition.wait()\n            \n            item = buffer.pop(0)\n            print(f\"Consumer {consumer_id} consumed: {item}\")\n            condition.notify_all()\n        \n        time.sleep(random.uniform(0.1, 0.5))\n\n# Start producer and consumers\nproducer_thread = threading.Thread(target=producer)\nconsumer_threads = [threading.Thread(target=consumer, args=(i,)) for i in range(2)]\n\nproducer_thread.start()\nfor t in consumer_threads:\n    t.start()\n\nproducer_thread.join()\nfor t in consumer_threads:\n    t.join()\n```\n\n## Performance Comparison\n\n### I/O-Bound Tasks\n\n```python\nimport time\nimport requests\nimport threading\nimport multiprocessing\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n\ndef fetch_url(url):\n    \"\"\"Simulate I/O-bound task\"\"\"\n    try:\n        response = requests.get(url, timeout=5)\n        return f\"Status: {response.status_code}\"\n    except:\n        return \"Error\"\n\ndef time_execution(func, *args, **kwargs):\n    start = time.time()\n    result = func(*args, **kwargs)\n    end = time.time()\n    return result, end - start\n\n# Sequential execution\ndef sequential_fetch(urls):\n    return [fetch_url(url) for url in urls]\n\n# Threaded execution\ndef threaded_fetch(urls):\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        return list(executor.map(fetch_url, urls))\n\n# Process execution\ndef process_fetch(urls):\n    with ProcessPoolExecutor(max_workers=10) as executor:\n        return list(executor.map(fetch_url, urls))\n\nif __name__ == \"__main__\":\n    urls = ['https://httpbin.org/delay/1'] * 10\n    \n    # Compare performance\n    _, seq_time = time_execution(sequential_fetch, urls)\n    _, thread_time = time_execution(threaded_fetch, urls)\n    _, process_time = time_execution(process_fetch, urls)\n    \n    print(f\"Sequential: {seq_time:.2f}s\")\n    print(f\"Threading: {thread_time:.2f}s\")\n    print(f\"Multiprocessing: {process_time:.2f}s\")\n```\n\n### CPU-Bound Tasks\n\n```python\nimport time\nimport multiprocessing\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n\ndef cpu_bound_task(n):\n    \"\"\"CPU-intensive computation\"\"\"\n    total = 0\n    for i in range(n):\n        total += i ** 2\n    return total\n\ndef compare_performance():\n    numbers = [1000000] * 8\n    \n    # Sequential\n    start = time.time()\n    sequential_results = [cpu_bound_task(n) for n in numbers]\n    sequential_time = time.time() - start\n    \n    # Threading\n    start = time.time()\n    with ThreadPoolExecutor(max_workers=8) as executor:\n        thread_results = list(executor.map(cpu_bound_task, numbers))\n    thread_time = time.time() - start\n    \n    # Multiprocessing\n    start = time.time()\n    with ProcessPoolExecutor(max_workers=8) as executor:\n        process_results = list(executor.map(cpu_bound_task, numbers))\n    process_time = time.time() - start\n    \n    print(f\"CPU-bound task comparison:\")\n    print(f\"Sequential: {sequential_time:.2f}s\")\n    print(f\"Threading: {thread_time:.2f}s\")\n    print(f\"Multiprocessing: {process_time:.2f}s\")\n    print(f\"Process speedup: {sequential_time/process_time:.2f}x\")\n\nif __name__ == \"__main__\":\n    compare_performance()\n```\n\n## Best Practices\n\n### 1. Choose the Right Approach\n\n```python\n# For I/O-bound tasks: Use threading\nimport threading\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef io_bound_work():\n    # File operations, network requests, database queries\n    pass\n\n# For CPU-bound tasks: Use multiprocessing\nimport multiprocessing\nfrom concurrent.futures import ProcessPoolExecutor\n\ndef cpu_bound_work():\n    # Mathematical computations, image processing, data analysis\n    pass\n```\n\n### 2. Resource Management\n\n```python\nimport multiprocessing\nimport threading\nfrom contextlib import contextmanager\n\n@contextmanager\ndef managed_thread_pool(max_workers):\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        yield executor\n\n@contextmanager\ndef managed_process_pool(max_workers):\n    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n        yield executor\n\n# Usage\nwith managed_thread_pool(4) as executor:\n    futures = [executor.submit(some_function, arg) for arg in args]\n    results = [future.result() for future in futures]\n```\n\n### 3. Error Handling\n\n```python\nimport threading\nimport multiprocessing\nimport logging\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\ndef safe_worker(task_id):\n    try:\n        # Your work here\n        result = f\"Task {task_id} completed\"\n        return result\n    except Exception as e:\n        logging.error(f\"Task {task_id} failed: {e}\")\n        return None\n\ndef execute_with_error_handling():\n    with ThreadPoolExecutor(max_workers=4) as executor:\n        futures = [executor.submit(safe_worker, i) for i in range(10)]\n        \n        for future in as_completed(futures):\n            try:\n                result = future.result()\n                if result:\n                    print(result)\n            except Exception as e:\n                logging.error(f\"Future failed: {e}\")\n```\n\n### 4. Graceful Shutdown\n\n```python\nimport threading\nimport time\nimport signal\nimport sys\n\nclass GracefulWorker:\n    def __init__(self):\n        self.shutdown_event = threading.Event()\n        self.threads = []\n    \n    def worker(self, worker_id):\n        while not self.shutdown_event.is_set():\n            print(f\"Worker {worker_id} working...\")\n            time.sleep(1)\n        print(f\"Worker {worker_id} shutting down\")\n    \n    def start_workers(self, num_workers):\n        for i in range(num_workers):\n            t = threading.Thread(target=self.worker, args=(i,))\n            t.start()\n            self.threads.append(t)\n    \n    def shutdown(self):\n        print(\"Initiating graceful shutdown...\")\n        self.shutdown_event.set()\n        for t in self.threads:\n            t.join()\n        print(\"All workers shut down\")\n\n# Usage\nworker_manager = GracefulWorker()\n\ndef signal_handler(signum, frame):\n    worker_manager.shutdown()\n    sys.exit(0)\n\nsignal.signal(signal.SIGINT, signal_handler)\nworker_manager.start_workers(3)\n\n# Keep main thread alive\ntry:\n    while True:\n        time.sleep(1)\nexcept KeyboardInterrupt:\n    worker_manager.shutdown()\n```\n\n## Advanced Topics\n\n### 1. Custom Thread Pool\n\n```python\nimport threading\nimport queue\nimport time\n\nclass SimpleThreadPool:\n    def __init__(self, num_workers):\n        self.task_queue = queue.Queue()\n        self.workers = []\n        self.shutdown = False\n        \n        for _ in range(num_workers):\n            worker = threading.Thread(target=self._worker)\n            worker.start()\n            self.workers.append(worker)\n    \n    def _worker(self):\n        while not self.shutdown:\n            try:\n                task, args, kwargs = self.task_queue.get(timeout=1)\n                if task is None:\n                    break\n                task(*args, **kwargs)\n                self.task_queue.task_done()\n            except queue.Empty:\n                continue\n    \n    def submit(self, task, *args, **kwargs):\n        self.task_queue.put((task, args, kwargs))\n    \n    def close(self):\n        self.shutdown = True\n        for _ in self.workers:\n            self.task_queue.put((None, (), {}))\n        for worker in self.workers:\n            worker.join()\n\n# Usage\ndef sample_task(name, delay):\n    print(f\"Task {name} starting\")\n    time.sleep(delay)\n    print(f\"Task {name} completed\")\n\npool = SimpleThreadPool(3)\nfor i in range(5):\n    pool.submit(sample_task, f\"Task-{i}\", 1)\n\ntime.sleep(6)\npool.close()\n```\n\n### 2. Async-style with Threading\n\n```python\nimport threading\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass AsyncResult:\n    def __init__(self, future):\n        self.future = future\n    \n    def get(self, timeout=None):\n        return self.future.result(timeout=timeout)\n    \n    def is_ready(self):\n        return self.future.done()\n\nclass AsyncExecutor:\n    def __init__(self, max_workers=4):\n        self.executor = ThreadPoolExecutor(max_workers=max_workers)\n    \n    def submit(self, func, *args, **kwargs):\n        future = self.executor.submit(func, *args, **kwargs)\n        return AsyncResult(future)\n    \n    def map(self, func, iterable):\n        return [self.submit(func, item) for item in iterable]\n    \n    def shutdown(self):\n        self.executor.shutdown(wait=True)\n\n# Usage\ndef long_running_task(n):\n    time.sleep(n)\n    return n * n\n\nasync_executor = AsyncExecutor(max_workers=3)\n\n# Submit tasks\nresults = []\nfor i in range(1, 4):\n    result = async_executor.submit(long_running_task, i)\n    results.append(result)\n\n# Wait for results\nfor i, result in enumerate(results):\n    print(f\"Task {i+1} result: {result.get()}\")\n\nasync_executor.shutdown()\n```\n\n### 3. Process Pool with Initialization\n\n```python\nimport multiprocessing\nimport time\n\n# Global variable for each process\nprocess_data = None\n\ndef init_process(shared_data):\n    global process_data\n    process_data = shared_data\n    print(f\"Process {multiprocessing.current_process().name} initialized\")\n\ndef worker_with_init(item):\n    global process_data\n    # Use the initialized data\n    result = item * process_data\n    return result\n\nif __name__ == \"__main__\":\n    shared_value = 10\n    \n    with multiprocessing.Pool(\n        processes=4,\n        initializer=init_process,\n        initargs=(shared_value,)\n    ) as pool:\n        items = [1, 2, 3, 4, 5]\n        results = pool.map(worker_with_init, items)\n        print(f\"Results: {results}\")\n```\n\n## Real-World Examples\n\n### 1. Web Scraper\n\n```python\nimport requests\nimport threading\nimport time\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom urllib.parse import urljoin, urlparse\nimport queue\n\nclass WebScraper:\n    def __init__(self, max_workers=10):\n        self.max_workers = max_workers\n        self.session = requests.Session()\n        self.results = []\n        self.lock = threading.Lock()\n    \n    def fetch_url(self, url):\n        try:\n            response = self.session.get(url, timeout=10)\n            response.raise_for_status()\n            return {\n                'url': url,\n                'status': response.status_code,\n                'content_length': len(response.content),\n                'title': self._extract_title(response.text)\n            }\n        except Exception as e:\n            return {\n                'url': url,\n                'error': str(e)\n            }\n    \n    def _extract_title(self, html):\n        # Simple title extraction\n        try:\n            start = html.find('<title>') + 7\n            end = html.find('</title>', start)\n            return html[start:end].strip()\n        except:\n            return \"No title\"\n    \n    def scrape_urls(self, urls):\n        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            future_to_url = {executor.submit(self.fetch_url, url): url for url in urls}\n            \n            for future in as_completed(future_to_url):\n                result = future.result()\n                with self.lock:\n                    self.results.append(result)\n        \n        return self.results\n\n# Usage\nif __name__ == \"__main__\":\n    urls = [\n        'https://httpbin.org/delay/1',\n        'https://httpbin.org/delay/2',\n        'https://httpbin.org/status/200',\n        'https://httpbin.org/status/404'\n    ]\n    \n    scraper = WebScraper(max_workers=4)\n    results = scraper.scrape_urls(urls)\n    \n    for result in results:\n        print(result)\n```\n\n### 2. File Processing Pipeline\n\n```python\nimport os\nimport threading\nimport multiprocessing\nfrom concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\nimport json\nimport time\n\nclass FileProcessor:\n    def __init__(self, input_dir, output_dir, max_workers=4):\n        self.input_dir = input_dir\n        self.output_dir = output_dir\n        self.max_workers = max_workers\n        self.processed_files = []\n        self.lock = threading.Lock()\n    \n    def process_file(self, filepath):\n        \"\"\"Process a single file\"\"\"\n        try:\n            with open(filepath, 'r') as f:\n                data = json.load(f)\n            \n            # Simulate processing\n            processed_data = {\n                'original_file': filepath,\n                'processed_at': time.time(),\n                'record_count': len(data) if isinstance(data, list) else 1,\n                'processing_time': 0.1\n            }\n            \n            time.sleep(0.1)  # Simulate processing time\n            \n            # Write processed file\n            output_filename = f\"processed_{os.path.basename(filepath)}\"\n            output_path = os.path.join(self.output_dir, output_filename)\n            \n            with open(output_path, 'w') as f:\n                json.dump(processed_data, f, indent=2)\n            \n            return {\n                'input': filepath,\n                'output': output_path,\n                'status': 'success'\n            }\n        \n        except Exception as e:\n            return {\n                'input': filepath,\n                'error': str(e),\n                'status': 'failed'\n            }\n    \n    def process_directory(self):\n        \"\"\"Process all JSON files in the input directory\"\"\"\n        json_files = []\n        for root, dirs, files in os.walk(self.input_dir):\n            for file in files:\n                if file.endswith('.json'):\n                    json_files.append(os.path.join(root, file))\n        \n        print(f\"Found {len(json_files)} JSON files to process\")\n        \n        # Process files in parallel\n        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:\n            results = list(executor.map(self.process_file, json_files))\n        \n        return results\n\n# Usage example\nif __name__ == \"__main__\":\n    # Create sample data\n    os.makedirs('input_data', exist_ok=True)\n    os.makedirs('output_data', exist_ok=True)\n    \n    # Create sample JSON files\n    for i in range(5):\n        sample_data = [{'id': j, 'value': j * 10} for j in range(100)]\n        with open(f'input_data/sample_{i}.json', 'w') as f:\n            json.dump(sample_data, f)\n    \n    # Process files\n    processor = FileProcessor('input_data', 'output_data', max_workers=4)\n    results = processor.process_directory()\n    \n    # Print results\n    for result in results:\n        print(result)\n```\n\n### 3. Real-time Data Processing\n\n```python\nimport threading\nimport queue\nimport time\nimport random\nimport json\nfrom datetime import datetime\n\nclass DataProcessor:\n    def __init__(self, num_workers=3):\n        self.input_queue = queue.Queue()\n        self.output_queue = queue.Queue()\n        self.num_workers = num_workers\n        self.workers = []\n        self.running = False\n        self.processed_count = 0\n        self.lock = threading.Lock()\n    \n    def worker(self, worker_id):\n        \"\"\"Process data items from the queue\"\"\"\n        while self.running:\n            try:\n                data = self.input_queue.get(timeout=1)\n                if data is None:\n                    break\n                \n                # Simulate processing\n                processed_data = self.process_data(data, worker_id)\n                self.output_queue.put(processed_data)\n                \n                with self.lock:\n                    self.processed_count += 1\n                \n                self.input_queue.task_done()\n                \n            except queue.Empty:\n                continue\n    \n    def process_data(self, data, worker_id):\n        \"\"\"Process individual data item\"\"\"\n        # Simulate processing time\n        time.sleep(random.uniform(0.1, 0.5))\n        \n        return {\n            'worker_id': worker_id,\n            'original_data': data,\n            'processed_at': datetime.now().isoformat(),\n            'result': data['value'] * 2 if 'value' in data else 'processed'\n        }\n    \n    def start(self):\n        \"\"\"Start the worker threads\"\"\"\n        self.running = True\n        for i in range(self.num_workers):\n            worker = threading.Thread(target=self.worker, args=(i,))\n            worker.start()\n            self.workers.append(worker)\n    \n    def stop(self):\n        \"\"\"Stop all worker threads\"\"\"\n        self.running = False\n        \n        # Add sentinel values to wake up workers\n        for _ in range(self.num_workers):\n            self.input_queue.put(None)\n        \n        # Wait for workers to finish\n        for worker in self.workers:\n            worker.join()\n    \n    def add_data(self, data):\n        \"\"\"Add data to the processing queue\"\"\"\n        self.input_queue.put(data)\n    \n    def get_result(self, timeout=None):\n        \"\"\"Get processed result\"\"\"\n        try:\n            return self.output_queue.get(timeout=timeout)\n        except queue.Empty:\n            return None\n    \n    def get_stats(self):\n        \"\"\"Get processing statistics\"\"\"\n        return {\n            'input_queue_size': self.input_queue.qsize(),\n            'output_queue_size': self.output_queue.qsize(),\n            'processed_count': self.processed_count,\n            'active_workers': len([w for w in self.workers if w.is_alive()])\n        }\n\n# Usage example\nif __name__ == \"__main__\":\n    processor = DataProcessor(num_workers=3)\n    processor.start()\n    \n    # Simulate data streaming\n    def data_generator():\n        for i in range(20):\n            yield {'id': i, 'value': random.randint(1, 100)}\n            time.sleep(0.1)\n    \n    # Add data to processor\n    for data in data_generator():\n        processor.add_data(data)\n        print(f\"Added data: {data}\")\n    \n    # Collect results\n    results = []\n    start_time = time.time()\n    while len(results) < 20 and time.time() - start_time < 30:\n        result = processor.get_result(timeout=1)\n        if result:\n            results.append(result)\n            print(f\"Got result: {result}\")\n    \n    # Print statistics\n    print(f\"Final stats: {processor.get_stats()}\")\n    \n\n## Troubleshooting Common Issues\n\n### 1. Race Conditions\n\n```python\nimport threading\nimport time\n\n# Problem: Race condition\nshared_counter = 0\n\ndef unsafe_increment():\n    global shared_counter\n    for _ in range(100000):\n        shared_counter += 1  # This is not atomic!\n\n# Solution: Use locks\nsafe_counter = 0\ncounter_lock = threading.Lock()\n\ndef safe_increment():\n    global safe_counter\n    for _ in range(100000):\n        with counter_lock:\n            safe_counter += 1\n\n# Alternative: Use atomic operations\nfrom threading import Lock\nimport threading\n\nclass AtomicCounter:\n    def __init__(self):\n        self._value = 0\n        self._lock = Lock()\n    \n    def increment(self):\n        with self._lock:\n            self._value += 1\n    \n    @property\n    def value(self):\n        with self._lock:\n            return self._value\n\n# Usage\natomic_counter = AtomicCounter()\n\ndef worker():\n    for _ in range(100000):\n        atomic_counter.increment()\n\nthreads = [threading.Thread(target=worker) for _ in range(5)]\nfor t in threads:\n    t.start()\nfor t in threads:\n    t.join()\n\nprint(f\"Atomic counter final value: {atomic_counter.value}\")\n```\n\n### 2. Deadlocks\n\n```python\nimport threading\nimport time\n\n# Problem: Deadlock scenario\nlock1 = threading.Lock()\nlock2 = threading.Lock()\n\ndef task1():\n    with lock1:\n        print(\"Task 1 acquired lock1\")\n        time.sleep(0.1)\n        with lock2:\n            print(\"Task 1 acquired lock2\")\n\ndef task2():\n    with lock2:\n        print(\"Task 2 acquired lock2\")\n        time.sleep(0.1)\n        with lock1:\n            print(\"Task 2 acquired lock1\")\n\n# Solution: Always acquire locks in the same order\ndef safe_task1():\n    with lock1:\n        print(\"Safe Task 1 acquired lock1\")\n        time.sleep(0.1)\n        with lock2:\n            print(\"Safe Task 1 acquired lock2\")\n\ndef safe_task2():\n    with lock1:  # Same order as safe_task1\n        print(\"Safe Task 2 acquired lock1\")\n        time.sleep(0.1)\n        with lock2:\n            print(\"Safe Task 2 acquired lock2\")\n\n# Alternative: Use timeout\nimport threading\n\ndef task_with_timeout():\n    if lock1.acquire(timeout=1):\n        try:\n            print(\"Acquired lock1\")\n            if lock2.acquire(timeout=1):\n                try:\n                    print(\"Acquired lock2\")\n                    # Do work\n                finally:\n                    lock2.release()\n            else:\n                print(\"Could not acquire lock2\")\n        finally:\n            lock1.release()\n    else:\n        print(\"Could not acquire lock1\")\n```\n\n### 3. Memory Leaks in Multiprocessing\n\n```python\nimport multiprocessing\nimport psutil\nimport os\n\n# Problem: Not properly cleaning up processes\ndef memory_leak_example():\n    processes = []\n    for i in range(10):\n        p = multiprocessing.Process(target=lambda: time.sleep(10))\n        p.start()\n        processes.append(p)\n    # Forgetting to join processes can lead to zombie processes\n\n# Solution: Proper cleanup\ndef proper_process_management():\n    processes = []\n    try:\n        for i in range(10):\n            p = multiprocessing.Process(target=lambda: time.sleep(1))\n            p.start()\n            processes.append(p)\n        \n        # Wait for all processes to complete\n        for p in processes:\n            p.join()\n    \n    except KeyboardInterrupt:\n        print(\"Interrupting processes...\")\n        for p in processes:\n            p.terminate()\n        for p in processes:\n            p.join()\n\n# Context manager approach\nfrom contextlib import contextmanager\n\n@contextmanager\ndef managed_processes(target_func, num_processes):\n    processes = []\n    try:\n        for i in range(num_processes):\n            p = multiprocessing.Process(target=target_func)\n            p.start()\n            processes.append(p)\n        yield processes\n    finally:\n        for p in processes:\n            if p.is_alive():\n                p.terminate()\n        for p in processes:\n            p.join()\n\n# Usage\ndef worker_task():\n    time.sleep(1)\n    print(f\"Worker {os.getpid()} finished\")\n\nif __name__ == \"__main__\":\n    with managed_processes(worker_task, 4) as processes:\n        print(f\"Started {len(processes)} processes\")\n        # Processes will be properly cleaned up\n```\n\n### 4. Pickle Errors in Multiprocessing\n\n```python\nimport multiprocessing\nimport pickle\n\n# Problem: Cannot pickle certain objects\nclass UnpicklableClass:\n    def __init__(self):\n        self.lambda_func = lambda x: x * 2  # Cannot pickle lambda\n        self.file_handle = open('temp.txt', 'w')  # Cannot pickle file handles\n\n# Solution: Use picklable alternatives\nclass PicklableClass:\n    def __init__(self):\n        self.multiplier = 2\n    \n    def multiply(self, x):\n        return x * self.multiplier\n\ndef process_with_method(obj, value):\n    return obj.multiply(value)\n\n# Alternative: Use dill for advanced pickling\ntry:\n    import dill\n    \n    def advanced_pickle_function():\n        func = lambda x: x * 2\n        return dill.dumps(func)\n    \nexcept ImportError:\n    print(\"dill not available\")\n\n# Using multiprocessing with proper pickling\ndef safe_multiprocessing_example():\n    if __name__ == \"__main__\":\n        obj = PicklableClass()\n        values = [1, 2, 3, 4, 5]\n        \n        with multiprocessing.Pool(processes=4) as pool:\n            results = pool.starmap(process_with_method, [(obj, v) for v in values])\n        \n        print(f\"Results: {results}\")\n```\n\n### 5. Exception Handling in Concurrent Code\n\n```python\nimport threading\nimport multiprocessing\nimport logging\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\n\ndef risky_task(task_id):\n    import random\n    if random.random() < 0.3:  # 30% chance of failure\n        raise ValueError(f\"Task {task_id} failed\")\n    return f\"Task {task_id} completed\"\n\n# Thread exception handling\ndef handle_thread_exceptions():\n    results = []\n    errors = []\n    \n    with ThreadPoolExecutor(max_workers=4) as executor:\n        futures = [executor.submit(risky_task, i) for i in range(10)]\n        \n        for future in as_completed(futures):\n            try:\n                result = future.result()\n                results.append(result)\n            except Exception as e:\n                errors.append(str(e))\n                logging.error(f\"Task failed: {e}\")\n    \n    print(f\"Completed: {len(results)}, Failed: {len(errors)}\")\n    return results, errors\n\n# Process exception handling\ndef handle_process_exceptions():\n    results = []\n    errors = []\n    \n    with ProcessPoolExecutor(max_workers=4) as executor:\n        futures = [executor.submit(risky_task, i) for i in range(10)]\n        \n        for future in as_completed(futures):\n            try:\n                result = future.result()\n                results.append(result)\n            except Exception as e:\n                errors.append(str(e))\n                logging.error(f\"Process task failed: {e}\")\n    \n    print(f\"Completed: {len(results)}, Failed: {len(errors)}\")\n    return results, errors\n\n# Custom exception handler\nclass ExceptionHandler:\n    def __init__(self):\n        self.exceptions = []\n        self.lock = threading.Lock()\n    \n    def handle_exception(self, exception):\n        with self.lock:\n            self.exceptions.append(exception)\n            logging.error(f\"Exception caught: {exception}\")\n\ndef task_with_exception_handler(task_id, exception_handler):\n    try:\n        return risky_task(task_id)\n    except Exception as e:\n        exception_handler.handle_exception(e)\n        return None\n\n# Usage\nif __name__ == \"__main__\":\n    print(\"Thread exception handling:\")\n    handle_thread_exceptions()\n    \n    print(\"\\nProcess exception handling:\")\n    handle_process_exceptions()\n```\n\n### 6. Performance Monitoring\n\n```python\nimport time\nimport threading\nimport multiprocessing\nimport psutil\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n\nclass PerformanceMonitor:\n    def __init__(self):\n        self.start_time = None\n        self.end_time = None\n        self.cpu_percent = []\n        self.memory_percent = []\n        self.monitoring = False\n        self.monitor_thread = None\n    \n    def start_monitoring(self):\n        self.start_time = time.time()\n        self.monitoring = True\n        self.monitor_thread = threading.Thread(target=self._monitor)\n        self.monitor_thread.start()\n    \n    def stop_monitoring(self):\n        self.end_time = time.time()\n        self.monitoring = False\n        if self.monitor_thread:\n            self.monitor_thread.join()\n    \n    def _monitor(self):\n        while self.monitoring:\n            self.cpu_percent.append(psutil.cpu_percent())\n            self.memory_percent.append(psutil.virtual_memory().percent)\n            time.sleep(0.1)\n    \n    def get_stats(self):\n        duration = self.end_time - self.start_time if self.end_time else 0\n        return {\n            'duration': duration,\n            'avg_cpu': sum(self.cpu_percent) / len(self.cpu_percent) if self.cpu_percent else 0,\n            'max_cpu': max(self.cpu_percent) if self.cpu_percent else 0,\n            'avg_memory': sum(self.memory_percent) / len(self.memory_percent) if self.memory_percent else 0,\n            'max_memory': max(self.memory_percent) if self.memory_percent else 0\n        }\n\ndef cpu_intensive_task(n):\n    total = 0\n    for i in range(n * 100000):\n        total += i\n    return total\n\ndef benchmark_approaches():\n    tasks = [1000] * 8\n    \n    # Sequential\n    monitor = PerformanceMonitor()\n    monitor.start_monitoring()\n    sequential_results = [cpu_intensive_task(n) for n in tasks]\n    monitor.stop_monitoring()\n    sequential_stats = monitor.get_stats()\n    \n    # Threading\n    monitor = PerformanceMonitor()\n    monitor.start_monitoring()\n    with ThreadPoolExecutor(max_workers=4) as executor:\n        thread_results = list(executor.map(cpu_intensive_task, tasks))\n    monitor.stop_monitoring()\n    thread_stats = monitor.get_stats()\n    \n    # Multiprocessing\n    monitor = PerformanceMonitor()\n    monitor.start_monitoring()\n    with ProcessPoolExecutor(max_workers=4) as executor:\n        process_results = list(executor.map(cpu_intensive_task, tasks))\n    monitor.stop_monitoring()\n    process_stats = monitor.get_stats()\n    \n    print(\"Performance Comparison:\")\n    print(f\"Sequential - Duration: {sequential_stats['duration']:.2f}s, \"\n          f\"Avg CPU: {sequential_stats['avg_cpu']:.1f}%, \"\n          f\"Max CPU: {sequential_stats['max_cpu']:.1f}%\")\n    \n    print(f\"Threading - Duration: {thread_stats['duration']:.2f}s, \"\n          f\"Avg CPU: {thread_stats['avg_cpu']:.1f}%, \"\n          f\"Max CPU: {thread_stats['max_cpu']:.1f}%\")\n    \n    print(f\"Multiprocessing - Duration: {process_stats['duration']:.2f}s, \"\n          f\"Avg CPU: {process_stats['avg_cpu']:.1f}%, \"\n          f\"Max CPU: {process_stats['max_cpu']:.1f}%\")\n\nif __name__ == \"__main__\":\n    benchmark_approaches()\n```\n\n## Key Takeaways\n\n### When to Use Threading\n- I/O-bound operations (file reading, network requests, database queries)\n- Tasks that spend time waiting for external resources\n- When you need shared memory access\n- Lighter weight than processes\n\n### When to Use Multiprocessing\n- CPU-intensive computations\n- Tasks that can be parallelized independently\n- When you need to bypass the GIL\n- When process isolation is important for stability\n\n### General Best Practices\n- Always use context managers (`with` statements) for resource management\n- Handle exceptions properly in concurrent code\n- Use appropriate synchronization primitives to avoid race conditions\n- Monitor performance to ensure concurrency is actually helping\n- Consider using `concurrent.futures` for simpler concurrent programming\n- Be mindful of the overhead of creating threads/processes\n- Test concurrent code thoroughly as bugs can be hard to reproduce\n\n### Common Pitfalls to Avoid\n- Race conditions due to shared state\n- Deadlocks from improper lock ordering\n- Memory leaks from not properly cleaning up processes\n- Pickle errors when passing objects between processes\n- Not handling exceptions in concurrent tasks\n- Creating too many threads/processes (use pools instead)\n\nThis guide provides a solid foundation for understanding and implementing concurrent programming in Python. Remember that the choice between threading and multiprocessing depends on your specific use case, and sometimes a hybrid approach or alternative solutions like asyncio might be more appropriate.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}