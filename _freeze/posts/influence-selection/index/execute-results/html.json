{
  "hash": "36e26427012259da65c4d1a697b71c1b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Active Learning Influence Selection: A Comprehensive Guide\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-04-19\"\ncategories: [code, research]\nformat:\n  html:\n    code-fold: false\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# Active Learning Influence Selection: A Comprehensive Guide\n\n![](learn.png)\n\n## Introduction\n\nActive learning is a machine learning paradigm where the algorithm can interactively query an oracle (typically a human annotator) to label new data points. The key idea is to select the most informative samples to be labeled, reducing the overall labeling effort while maintaining or improving model performance. This guide focuses on influence selection methods used in active learning strategies.\n\n## Table of Contents\n\n1. [Fundamentals of Active Learning](#fundamentals-of-active-learning)\n2. [Influence Selection Strategies](#influence-selection-strategies)\n3. [Uncertainty-Based Methods](#uncertainty-based-methods)\n4. [Diversity-Based Methods](#diversity-based-methods)\n5. [Expected Model Change](#expected-model-change)\n6. [Expected Error Reduction](#expected-error-reduction)\n7. [Influence Functions](#influence-functions)\n8. [Query-by-Committee](#query-by-committee)\n9. [Implementation Considerations](#implementation-considerations)\n10. [Evaluation Metrics](#evaluation-metrics)\n11. [Practical Examples](#practical-examples)\n12. [Advanced Topics](#advanced-topics)\n\n## Fundamentals of Active Learning\n\n### The Active Learning Loop\n\nThe typical active learning process follows these steps:\n\n1. Start with a small labeled dataset and a large unlabeled pool\n2. Train an initial model on the labeled data\n3. Apply an influence selection strategy to choose informative samples from the unlabeled pool\n4. Get annotations for the selected samples\n5. Add the newly labeled samples to the training set\n6. Retrain the model and repeat steps 3-6 until a stopping condition is met\n\n### Pool-Based vs. Stream-Based Learning\n\n- **Pool-based**: The learner has access to a pool of unlabeled data and selects the most informative samples\n- **Stream-based**: Samples arrive sequentially, and the learner must decide on-the-fly whether to request labels\n\n## Influence Selection Strategies\n\nInfluence selection is about identifying which unlabeled samples would be most beneficial to label next. Here are the main strategies:\n\n## Uncertainty-Based Methods\n\nThese methods select samples that the model is most uncertain about.\n\n### Least Confidence\n\n::: {#50dc8418 .cell execution_count=1}\n``` {.python .cell-code}\ndef least_confidence(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    \n    # Get the confidence values for the most probable class\n    confidences = np.max(probabilities, axis=1)\n    \n    # Select the k samples with the lowest confidence\n    least_confident_indices = np.argsort(confidences)[:k]\n    \n    return unlabeled_pool[least_confident_indices]\n```\n:::\n\n\n### Margin Sampling\n\nSelects samples with the smallest margin between the two most likely classes:\n\n::: {#a5c803f3 .cell execution_count=2}\n``` {.python .cell-code}\ndef margin_sampling(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    \n    # Sort the probabilities in descending order\n    sorted_probs = np.sort(probabilities, axis=1)[:, ::-1]\n    \n    # Calculate the margin between the first and second most probable classes\n    margins = sorted_probs[:, 0] - sorted_probs[:, 1]\n    \n    # Select the k samples with the smallest margins\n    smallest_margin_indices = np.argsort(margins)[:k]\n    \n    return unlabeled_pool[smallest_margin_indices]\n```\n:::\n\n\n### Entropy-Based Sampling\n\nSelects samples with the highest predictive entropy:\n\n::: {#479a8d1c .cell execution_count=3}\n``` {.python .cell-code}\ndef entropy_sampling(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    \n    # Calculate entropy for each sample\n    entropies = -np.sum(probabilities * np.log(probabilities + 1e-10), axis=1)\n    \n    # Select the k samples with the highest entropy\n    highest_entropy_indices = np.argsort(entropies)[::-1][:k]\n    \n    return unlabeled_pool[highest_entropy_indices]\n```\n:::\n\n\n### Bayesian Active Learning by Disagreement (BALD)\n\nFor Bayesian models, BALD selects samples that maximize the mutual information between predictions and model parameters:\n\n::: {#cdf26342 .cell execution_count=4}\n``` {.python .cell-code}\ndef bald_sampling(bayesian_model, unlabeled_pool, k, n_samples=100):\n    # Get multiple predictions by sampling from the model's posterior\n    probs_samples = []\n    for _ in range(n_samples):\n        probs = bayesian_model.predict_proba(unlabeled_pool)\n        probs_samples.append(probs)\n    \n    # Stack into a 3D array: (samples, data points, classes)\n    probs_samples = np.stack(probs_samples)\n    \n    # Calculate the average probability across all samples\n    mean_probs = np.mean(probs_samples, axis=0)\n    \n    # Calculate the entropy of the average prediction\n    entropy_mean = -np.sum(mean_probs * np.log(mean_probs + 1e-10), axis=1)\n    \n    # Calculate the average entropy across all samples\n    entropy_samples = -np.sum(probs_samples * np.log(probs_samples + 1e-10), axis=2)\n    mean_entropy = np.mean(entropy_samples, axis=0)\n    \n    # Mutual information = entropy of the mean - mean of entropies\n    bald_scores = entropy_mean - mean_entropy\n    \n    # Select the k samples with the highest BALD scores\n    highest_bald_indices = np.argsort(bald_scores)[::-1][:k]\n    \n    return unlabeled_pool[highest_bald_indices]\n```\n:::\n\n\n## Diversity-Based Methods\n\nThese methods aim to select a diverse set of examples to ensure broad coverage of the input space.\n\n### Clustering-Based Sampling\n\n::: {#778c3d38 .cell execution_count=5}\n``` {.python .cell-code}\ndef clustering_based_sampling(unlabeled_pool, k, n_clusters=None):\n    if n_clusters is None:\n        n_clusters = k\n    \n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(unlabeled_pool)\n    \n    # Get the cluster centers and distances to each point\n    centers = kmeans.cluster_centers_\n    distances = kmeans.transform(unlabeled_pool)  # Distance to each cluster center\n    \n    # Select one sample from each cluster (closest to the center)\n    selected_indices = []\n    for i in range(n_clusters):\n        # Get the samples in this cluster\n        cluster_samples = np.where(kmeans.labels_ == i)[0]\n        \n        # Find the sample closest to the center\n        closest_sample = cluster_samples[np.argmin(distances[cluster_samples, i])]\n        selected_indices.append(closest_sample)\n    \n    # If we need more samples than clusters, fill with the most uncertain samples\n    if k > n_clusters:\n        # Implementation depends on uncertainty measure\n        pass\n    \n    return unlabeled_pool[selected_indices[:k]]\n```\n:::\n\n\n### Core-Set Approach\n\nThe core-set approach aims to select a subset of data that best represents the whole dataset:\n\n::: {#04c9f398 .cell execution_count=6}\n``` {.python .cell-code}\ndef core_set_sampling(labeled_pool, unlabeled_pool, k):\n    # Combine labeled and unlabeled data for distance calculations\n    all_data = np.vstack((labeled_pool, unlabeled_pool))\n    \n    # Compute pairwise distances\n    distances = pairwise_distances(all_data)\n    \n    # Split distances into labeled-unlabeled and unlabeled-unlabeled\n    n_labeled = labeled_pool.shape[0]\n    dist_labeled_unlabeled = distances[:n_labeled, n_labeled:]\n    \n    # For each unlabeled sample, find the minimum distance to any labeled sample\n    min_distances = np.min(dist_labeled_unlabeled, axis=0)\n    \n    # Select the k samples with the largest minimum distances\n    farthest_indices = np.argsort(min_distances)[::-1][:k]\n    \n    return unlabeled_pool[farthest_indices]\n```\n:::\n\n\n## Expected Model Change\n\nThe Expected Model Change (EMC) method selects samples that would cause the greatest change in the model if they were labeled:\n\n::: {#cfd11f56 .cell execution_count=7}\n``` {.python .cell-code}\ndef expected_model_change(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    n_classes = probabilities.shape[1]\n    \n    # Calculate expected gradient length for each sample\n    expected_changes = []\n    for i, x in enumerate(unlabeled_pool):\n        # Calculate expected gradient length across all possible labels\n        change = 0\n        for c in range(n_classes):\n            # For each possible class, calculate the gradient if this was the true label\n            x_expanded = x.reshape(1, -1)\n            # Here we would compute the gradient of the model with respect to the sample\n            # For simplicity, we use a placeholder\n            gradient = compute_gradient(model, x_expanded, c)\n            norm_gradient = np.linalg.norm(gradient)\n            \n            # Weight by the probability of this class\n            change += probabilities[i, c] * norm_gradient\n        \n        expected_changes.append(change)\n    \n    # Select the k samples with the highest expected change\n    highest_change_indices = np.argsort(expected_changes)[::-1][:k]\n    \n    return unlabeled_pool[highest_change_indices]\n```\n:::\n\n\n*Note: The `compute_gradient` function would need to be implemented based on the specific model being used.*\n\n## Expected Error Reduction\n\nThe Expected Error Reduction method selects samples that, when labeled, would minimally reduce the model's expected error:\n\n::: {#666fad88 .cell execution_count=8}\n``` {.python .cell-code}\ndef expected_error_reduction(model, unlabeled_pool, unlabeled_pool_remaining, k):\n    # Predict probabilities for all remaining unlabeled data\n    current_probs = model.predict_proba(unlabeled_pool_remaining)\n    current_entropy = -np.sum(current_probs * np.log(current_probs + 1e-10), axis=1)\n    \n    expected_error_reductions = []\n    \n    # For each sample in the unlabeled pool we're considering\n    for i, x in enumerate(unlabeled_pool):\n        # Predict probabilities for this sample\n        probs = model.predict_proba(x.reshape(1, -1))[0]\n        \n        # Calculate the expected error reduction for each possible label\n        error_reduction = 0\n        for c in range(len(probs)):\n            # Create a hypothetical new model with this labeled sample\n            # For simplicity, we use a placeholder function\n            hypothetical_model = train_with_additional_sample(model, x, c)\n            \n            # Get new probabilities with this model\n            new_probs = hypothetical_model.predict_proba(unlabeled_pool_remaining)\n            new_entropy = -np.sum(new_probs * np.log(new_probs + 1e-10), axis=1)\n            \n            # Expected entropy reduction\n            reduction = np.sum(current_entropy - new_entropy)\n            \n            # Weight by the probability of this class\n            error_reduction += probs[c] * reduction\n        \n        expected_error_reductions.append(error_reduction)\n    \n    # Select the k samples with the highest expected error reduction\n    highest_reduction_indices = np.argsort(expected_error_reductions)[::-1][:k]\n    \n    return unlabeled_pool[highest_reduction_indices]\n```\n:::\n\n\n*Note: The `train_with_additional_sample` function would need to be implemented based on the specific model being used.*\n\n## Influence Functions\n\nInfluence functions approximate the effect of adding or removing a training example without retraining the model:\n\n::: {#cb9d98fd .cell execution_count=9}\n``` {.python .cell-code}\ndef influence_function_sampling(model, unlabeled_pool, labeled_pool, k, labels):\n    influences = []\n    \n    # For each unlabeled sample\n    for x_u in unlabeled_pool:\n        # Calculate the influence of adding this sample to the training set\n        influence = calculate_influence(model, x_u, labeled_pool, labels)\n        influences.append(influence)\n    \n    # Select the k samples with the highest influence\n    highest_influence_indices = np.argsort(influences)[::-1][:k]\n    \n    return unlabeled_pool[highest_influence_indices]\n```\n:::\n\n\n*Note: The `calculate_influence` function would need to be implemented based on the specific model and influence metric being used.*\n\n## Query-by-Committee\n\nQuery-by-Committee (QBC) methods train multiple models (a committee) and select samples where they disagree:\n\n::: {#0a36a327 .cell execution_count=10}\n``` {.python .cell-code}\ndef query_by_committee(committee_models, unlabeled_pool, k):\n    # Get predictions from all committee members\n    all_predictions = []\n    for model in committee_models:\n        preds = model.predict(unlabeled_pool)\n        all_predictions.append(preds)\n    \n    # Stack predictions into a 2D array (committee members, data points)\n    all_predictions = np.stack(all_predictions)\n    \n    # Calculate disagreement (e.g., using vote entropy)\n    disagreements = []\n    for i in range(unlabeled_pool.shape[0]):\n        # Count votes for each class\n        votes = np.bincount(all_predictions[:, i])\n        # Normalize to get probabilities\n        vote_probs = votes / len(committee_models)\n        # Calculate entropy\n        entropy = -np.sum(vote_probs * np.log2(vote_probs + 1e-10))\n        disagreements.append(entropy)\n    \n    # Select the k samples with the highest disagreement\n    highest_disagreement_indices = np.argsort(disagreements)[::-1][:k]\n    \n    return unlabeled_pool[highest_disagreement_indices]\n```\n:::\n\n\n## Implementation Considerations\n\n### Batch Mode Active Learning\n\nIn practice, it's often more efficient to select multiple samples at once. However, simply selecting the top-k samples may lead to redundancy. Consider using:\n\n1. **Greedy Selection with Diversity**: Select one sample at a time, then update the diversity metrics to avoid selecting similar samples.\n\n::: {#d728e378 .cell execution_count=11}\n``` {.python .cell-code}\ndef batch_selection_with_diversity(model, unlabeled_pool, k, lambda_diversity=0.5):\n    selected_indices = []\n    remaining_indices = list(range(len(unlabeled_pool)))\n    \n    # Calculate uncertainty scores for all samples\n    probabilities = model.predict_proba(unlabeled_pool)\n    entropies = -np.sum(probabilities * np.log(probabilities + 1e-10), axis=1)\n    \n    # Calculate distance matrix for diversity\n    distance_matrix = pairwise_distances(unlabeled_pool)\n    \n    for _ in range(k):\n        if not remaining_indices:\n            break\n        \n        scores = np.zeros(len(remaining_indices))\n        \n        # Calculate uncertainty scores\n        uncertainty_scores = entropies[remaining_indices]\n        \n        # Calculate diversity scores (if we have already selected some samples)\n        if selected_indices:\n            # For each remaining sample, calculate the minimum distance to any selected sample\n            diversity_scores = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n        else:\n            diversity_scores = np.zeros(len(remaining_indices))\n        \n        # Normalize scores\n        uncertainty_scores = (uncertainty_scores - np.min(uncertainty_scores)) / (np.max(uncertainty_scores) - np.min(uncertainty_scores) + 1e-10)\n        if selected_indices:\n            diversity_scores = (diversity_scores - np.min(diversity_scores)) / (np.max(diversity_scores) - np.min(diversity_scores) + 1e-10)\n        \n        # Combine scores\n        scores = (1 - lambda_diversity) * uncertainty_scores + lambda_diversity * diversity_scores\n        \n        # Select the sample with the highest score\n        best_idx = np.argmax(scores)\n        selected_idx = remaining_indices[best_idx]\n        \n        # Add to selected and remove from remaining\n        selected_indices.append(selected_idx)\n        remaining_indices.remove(selected_idx)\n    \n    return unlabeled_pool[selected_indices]\n```\n:::\n\n\n2. **Submodular Function Maximization**: Use a submodular function to ensure diversity in the selected batch.\n\n### Handling Imbalanced Data\n\nActive learning can inadvertently reinforce class imbalance. Consider:\n\n1. **Stratified Sampling**: Ensure representation from all classes.\n2. **Hybrid Approaches**: Combine uncertainty-based and density-based methods.\n3. **Diversity Constraints**: Explicitly enforce diversity in feature space.\n\n### Computational Efficiency\n\nSome methods (like expected error reduction) can be computationally expensive. Consider:\n\n1. **Subsample the Unlabeled Pool**: Only consider a random subset for selection.\n2. **Pre-compute Embeddings**: Use a fixed feature extractor to pre-compute embeddings.\n3. **Approximate Methods**: Use approximations for expensive operations.\n\n## Evaluation Metrics\n\n### Learning Curves\n\nPlot model performance vs. number of labeled samples:\n\n::: {#35ae210c .cell execution_count=12}\n``` {.python .cell-code}\ndef plot_learning_curve(model_factory, X_train, y_train, X_test, y_test, \n                        active_learning_strategy, initial_size=10, \n                        batch_size=10, n_iterations=20):\n    # Initialize with a small labeled set\n    labeled_indices = np.random.choice(len(X_train), initial_size, replace=False)\n    unlabeled_indices = np.setdiff1d(np.arange(len(X_train)), labeled_indices)\n    \n    performance = []\n    \n    for i in range(n_iterations):\n        # Create a fresh model\n        model = model_factory()\n        \n        # Train on the currently labeled data\n        model.fit(X_train[labeled_indices], y_train[labeled_indices])\n        \n        # Evaluate on the test set\n        score = model.score(X_test, y_test)\n        performance.append((len(labeled_indices), score))\n        \n        # Select the next batch of samples\n        if len(unlabeled_indices) > 0:\n            # Use the specified active learning strategy\n            selected_indices = active_learning_strategy(\n                model, X_train[unlabeled_indices], batch_size\n            )\n            \n            # Map back to original indices\n            selected_original_indices = unlabeled_indices[selected_indices]\n            \n            # Update labeled and unlabeled indices\n            labeled_indices = np.append(labeled_indices, selected_original_indices)\n            unlabeled_indices = np.setdiff1d(unlabeled_indices, selected_original_indices)\n    \n    # Plot the learning curve\n    counts, scores = zip(*performance)\n    plt.figure(figsize=(10, 6))\n    plt.plot(counts, scores, 'o-')\n    plt.xlabel('Number of labeled samples')\n    plt.ylabel('Model accuracy')\n    plt.title('Active Learning Performance')\n    plt.grid(True)\n    \n    return performance\n```\n:::\n\n\n### Comparison with Random Sampling\n\nAlways compare your active learning strategy with random sampling as a baseline.\n\n### Annotation Efficiency\n\nCalculate how many annotations you saved compared to using the entire dataset.\n\n## Practical Examples\n\n### Image Classification with Uncertainty Sampling\n\n::: {#93ba94bb .cell execution_count=13}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n# Load data\nmnist = fetch_openml('mnist_784', version=1, cache=True)\nX, y = mnist['data'], mnist['target']\n\n# Split into train and test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Initially, only a small portion is labeled\ninitial_size = 100\nlabeled_indices = np.random.choice(len(X_train), initial_size, replace=False)\nunlabeled_indices = np.setdiff1d(np.arange(len(X_train)), labeled_indices)\n\n# Tracking performance\nactive_learning_performance = []\nrandom_sampling_performance = []\n\n# Active learning loop\nfor i in range(10):  # 10 iterations\n    # Train a model on the currently labeled data\n    model = RandomForestClassifier(n_estimators=50, random_state=42)\n    model.fit(X_train.iloc[labeled_indices], y_train.iloc[labeled_indices])\n    \n    # Evaluate on the test set\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    active_learning_performance.append((len(labeled_indices), accuracy))\n    \n    print(f\"Iteration {i+1}: {len(labeled_indices)} labeled samples, \"\n          f\"accuracy: {accuracy:.4f}\")\n    \n    # Select 100 new samples using entropy sampling\n    if len(unlabeled_indices) > 0:\n        # Predict probabilities for each unlabeled sample\n        probs = model.predict_proba(X_train.iloc[unlabeled_indices])\n        \n        # Calculate entropy\n        entropies = -np.sum(probs * np.log(probs + 1e-10), axis=1)\n        \n        # Select samples with the highest entropy\n        top_indices = np.argsort(entropies)[::-1][:100]\n        \n        # Update labeled and unlabeled indices\n        selected_indices = unlabeled_indices[top_indices]\n        labeled_indices = np.append(labeled_indices, selected_indices)\n        unlabeled_indices = np.setdiff1d(unlabeled_indices, selected_indices)\n\n# Plot learning curve\ncounts, scores = zip(*active_learning_performance)\nplt.figure(figsize=(10, 6))\nplt.plot(counts, scores, 'o-', label='Active Learning')\nplt.xlabel('Number of labeled samples')\nplt.ylabel('Model accuracy')\nplt.title('Active Learning Performance')\nplt.grid(True)\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIteration 1: 100 labeled samples, accuracy: 0.6755\nIteration 2: 200 labeled samples, accuracy: 0.7300\nIteration 3: 300 labeled samples, accuracy: 0.7716\nIteration 4: 400 labeled samples, accuracy: 0.8097\nIteration 5: 500 labeled samples, accuracy: 0.8359\nIteration 6: 600 labeled samples, accuracy: 0.8366\nIteration 7: 700 labeled samples, accuracy: 0.8494\nIteration 8: 800 labeled samples, accuracy: 0.8509\nIteration 9: 900 labeled samples, accuracy: 0.8678\nIteration 10: 1000 labeled samples, accuracy: 0.8758\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-2.png){width=829 height=523}\n:::\n:::\n\n\n### Text Classification with Query-by-Committee\n\n::: {#63cb6e87 .cell execution_count=14}\n``` {.python .cell-code}\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.datasets import fetch_20newsgroups\n\n# Load data\ncategories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\ntwenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\ntwenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n\n# Feature extraction\nvectorizer = TfidfVectorizer(stop_words='english')\nX_train = vectorizer.fit_transform(twenty_train.data)\nX_test = vectorizer.transform(twenty_test.data)\ny_train = twenty_train.target\ny_test = twenty_test.target\n\n# Initially, only a small portion is labeled\ninitial_size = 20\nlabeled_indices = np.random.choice(len(X_train.toarray()), initial_size, replace=False)\nunlabeled_indices = np.setdiff1d(np.arange(len(X_train.toarray())), labeled_indices)\n\n# Create a committee of models\nmodels = [\n    ('nb', MultinomialNB()),\n    ('svm', SVC(kernel='linear', probability=True)),\n    ('svm2', SVC(kernel='rbf', probability=True))\n]\n\n# Active learning loop\nfor i in range(10):  # 10 iterations\n    # Train each model on the currently labeled data\n    committee_models = []\n    for name, model in models:\n        model.fit(X_train[labeled_indices], y_train[labeled_indices])\n        committee_models.append(model)\n    \n    # Evaluate using the VotingClassifier\n    voting_clf = VotingClassifier(estimators=models, voting='soft')\n    voting_clf.fit(X_train[labeled_indices], y_train[labeled_indices])\n    \n    accuracy = voting_clf.score(X_test, y_test)\n    print(f\"Iteration {i+1}: {len(labeled_indices)} labeled samples, \"\n          f\"accuracy: {accuracy:.4f}\")\n    \n    # Select 10 new samples using Query-by-Committee\n    if len(unlabeled_indices) > 0:\n        # Get predictions from all committee members\n        all_predictions = []\n        for model in committee_models:\n            preds = model.predict(X_train[unlabeled_indices])\n            all_predictions.append(preds)\n        \n        # Calculate vote entropy\n        vote_entropies = []\n        all_predictions = np.array(all_predictions)\n        for i in range(len(unlabeled_indices)):\n            # Count votes for each class\n            votes = np.bincount(all_predictions[:, i], minlength=len(categories))\n            # Normalize to get probabilities\n            vote_probs = votes / len(committee_models)\n            # Calculate entropy\n            entropy = -np.sum(vote_probs * np.log2(vote_probs + 1e-10))\n            vote_entropies.append(entropy)\n        \n        # Select samples with the highest vote entropy\n        top_indices = np.argsort(vote_entropies)[::-1][:10]\n        \n        # Update labeled and unlabeled indices\n        selected_indices = unlabeled_indices[top_indices]\n        labeled_indices = np.append(labeled_indices, selected_indices)\n        unlabeled_indices = np.setdiff1d(unlabeled_indices, selected_indices)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIteration 1: 20 labeled samples, accuracy: 0.2124\nIteration 2: 30 labeled samples, accuracy: 0.2310\nIteration 3: 40 labeled samples, accuracy: 0.3668\nIteration 4: 50 labeled samples, accuracy: 0.2983\nIteration 5: 60 labeled samples, accuracy: 0.4627\nIteration 6: 70 labeled samples, accuracy: 0.6312\nIteration 7: 80 labeled samples, accuracy: 0.6864\nIteration 8: 90 labeled samples, accuracy: 0.6125\nIteration 9: 100 labeled samples, accuracy: 0.7597\nIteration 10: 110 labeled samples, accuracy: 0.7870\n```\n:::\n:::\n\n\n## Advanced Topics\n\n### Transfer Learning with Active Learning\n\nCombining transfer learning with active learning can be powerful:\n\n1. Use pre-trained models as feature extractors.\n2. Apply active learning on the feature space.\n3. Fine-tune the model on the selected samples.\n\n### Active Learning with Deep Learning\n\nSpecial considerations for deep learning models:\n\n1. **Uncertainty Estimation**: Use dropout or ensemble methods for better uncertainty estimation.\n2. **Batch Normalization**: Be careful with batch normalization layers when retraining.\n3. **Data Augmentation**: Apply data augmentation to increase the effective size of the labeled pool.\n\n::: {#5cd6829c .cell execution_count=15}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Subset\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\n# Define a simple CNN\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x, dropout=True):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        if dropout:\n            x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        if dropout:\n            x = self.dropout2(x)\n        x = self.fc2(x)\n        return x\n\n# MC Dropout for uncertainty estimation\ndef mc_dropout_uncertainty(model, data_loader, n_samples=10):\n    model.eval()\n    all_probs = []\n    \n    with torch.no_grad():\n        for _ in range(n_samples):\n            batch_probs = []\n            for data, _ in data_loader:\n                output = model(data, dropout=True)\n                probs = F.softmax(output, dim=1)\n                batch_probs.append(probs)\n            \n            # Concatenate batch probabilities\n            all_probs.append(torch.cat(batch_probs))\n    \n    # Stack along a new dimension\n    all_probs = torch.stack(all_probs)\n    \n    # Calculate the mean probabilities\n    mean_probs = torch.mean(all_probs, dim=0)\n    \n    # Calculate entropy of the mean prediction\n    entropy = -torch.sum(mean_probs * torch.log(mean_probs + 1e-10), dim=1)\n    \n    return entropy.numpy()\n```\n:::\n\n\n### Semi-Supervised Active Learning\n\nLeverage both labeled and unlabeled data during training:\n\n1. **Self-Training**: Use model predictions on unlabeled data as pseudo-labels.\n2. **Co-Training**: Train multiple models and use their predictions to teach each other.\n3. **Consistency Regularization**: Enforce consistent predictions across different perturbations.\n\n::: {#59de7fcf .cell execution_count=16}\n``` {.python .cell-code}\ndef semi_supervised_active_learning(labeled_X, labeled_y, unlabeled_X, model, confidence_threshold=0.95):\n    # Train model on labeled data\n    model.fit(labeled_X, labeled_y)\n    \n    # Predict on unlabeled data\n    probabilities = model.predict_proba(unlabeled_X)\n    max_probs = np.max(probabilities, axis=1)\n    \n    # Get high confidence predictions\n    confident_indices = np.where(max_probs >= confidence_threshold)[0]\n    \n    # Get pseudo-labels for confident predictions\n    pseudo_labels = model.predict(unlabeled_X[confident_indices])\n    \n    # Train on combined dataset\n    combined_X = np.vstack([labeled_X, unlabeled_X[confident_indices]])\n    combined_y = np.concatenate([labeled_y, pseudo_labels])\n    \n    model.fit(combined_X, combined_y)\n    \n    return model, confident_indices\n```\n:::\n\n\n### Active Learning for Domain Adaptation\n\nWhen labeled data from the target domain is scarce, active learning can help select the most informative samples:\n\n1. **Domain Discrepancy Measures**: Select samples that minimize domain discrepancy.\n2. **Adversarial Selection**: Select samples that the domain discriminator is most uncertain about.\n3. **Feature Space Alignment**: Select samples that help align feature spaces between domains.\n\n### Human-in-the-Loop Considerations\n\n1. **Annotation Interface Design**: Make the annotation process intuitive and efficient.\n2. **Cognitive Load Management**: Group similar samples to reduce cognitive switching.\n3. **Explanations**: Provide model explanations to help annotators understand the current model's decisions.\n4. **Quality Control**: Incorporate mechanisms to detect and correct annotation errors.\n\n## Conclusion\n\nActive learning provides a powerful framework for efficiently building machine learning models with limited labeled data. By selecting the most informative samples for annotation, active learning can significantly reduce the labeling effort while maintaining high model performance.\n\nThe key to successful active learning is choosing the right influence selection strategy for your specific problem and data characteristics. Consider the following when designing your active learning pipeline:\n\n1. **Data Characteristics**: Dense vs. sparse data, balanced vs. imbalanced classes, feature distribution.\n2. **Model Type**: Linear models, tree-based models, deep learning models.\n3. **Computational Resources**: Available memory and processing power.\n4. **Annotation Budget**: Number of samples that can be labeled.\n5. **Task Complexity**: Classification vs. regression, number of classes, difficulty of the task.\n\nBy carefully considering these factors and implementing the appropriate influence selection methods, you can build high-performance models with minimal annotation effort.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}