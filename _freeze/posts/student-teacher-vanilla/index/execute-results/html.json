{
  "hash": "8b833e347fc6797f34cc2f7dcc3b141e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Student-Teacher Network Training Guide in PyTorch\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-05-28\"\ncategories: [code, advanced]\nformat:\n  html:\n    code-fold: false\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# Student-Teacher Network Training Guide in PyTorch\n![](stutea.jpg)\n\n## Overview\n\nStudent-teacher networks, also known as knowledge distillation, involve training a smaller \"student\" model to mimic the behavior of a larger, pre-trained \"teacher\" model. This technique helps compress large models while maintaining performance.\n\n## Key Concepts\n\n### Knowledge Distillation Loss\nThe student learns from both:\n\n1. **Hard targets**: Original ground truth labels\n2. **Soft targets**: Teacher's probability distributions (softened with temperature)\n\n### Temperature Scaling\nHigher temperature values create softer probability distributions, making it easier for the student to learn from the teacher's uncertainty.\n\n## Complete Implementation\n\n### Import Libraries\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nimport numpy as np\n```\n\n### Set device\n```python\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n```\n\n### Define Teacher Model\n```python\nclass TeacherNetwork(nn.Module):\n    \"\"\"Large teacher network (e.g., ResNet-50 equivalent)\"\"\"\n    def __init__(self, num_classes=10):\n        super(TeacherNetwork, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            \n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, 3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            \n            nn.Conv2d(128, 256, 3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, 3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(256, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n```\n### Define Student Model\n```python\nclass StudentNetwork(nn.Module):\n    \"\"\"Smaller student network\"\"\"\n    def __init__(self, num_classes=10):\n        super(StudentNetwork, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            \n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            \n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(128, 64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64, num_classes)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n```\n### Define Distillation Loss\n```python\nclass DistillationLoss(nn.Module):\n    \"\"\"\n    Knowledge Distillation Loss combining:\n    1. Cross-entropy loss with true labels\n    2. KL divergence loss with teacher predictions\n    \"\"\"\n    def __init__(self, alpha=0.7, temperature=4.0):\n        super(DistillationLoss, self).__init__()\n        self.alpha = alpha  # Weight for distillation loss\n        self.temperature = temperature\n        self.ce_loss = nn.CrossEntropyLoss()\n        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n    \n    def forward(self, student_logits, teacher_logits, labels):\n        # Cross-entropy loss with true labels\n        ce_loss = self.ce_loss(student_logits, labels)\n        \n        # Soft targets from teacher\n        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)\n        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=1)\n        \n        # KL divergence loss\n        kl_loss = self.kl_loss(student_log_probs, teacher_probs) * (self.temperature ** 2)\n        \n        # Combined loss\n        total_loss = (1 - self.alpha) * ce_loss + self.alpha * kl_loss\n        \n        return total_loss, ce_loss, kl_loss\n```\n### Load and Preprocess Data\n```python\ndef load_data(batch_size=128):\n    \"\"\"Load CIFAR-10 dataset\"\"\"\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n    ])\n    \n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n    ])\n    \n    train_dataset = torchvision.datasets.CIFAR10(\n        root='./data', train=True, download=True, transform=transform_train\n    )\n    test_dataset = torchvision.datasets.CIFAR10(\n        root='./data', train=False, download=True, transform=transform_test\n    )\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    \n    return train_loader, test_loader\n```\n\n### Train Teacher Model\n```python\ndef train_teacher(model, train_loader, test_loader, epochs=50, lr=0.001):\n    \"\"\"Train the teacher network\"\"\"\n    print(\"Training Teacher Network...\")\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n    \n    model.train()\n    best_acc = 0.0\n    \n    for epoch in range(epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        progress_bar = tqdm(train_loader, desc=f'Teacher Epoch {epoch+1}/{epochs}')\n        for batch_idx, (inputs, targets) in enumerate(progress_bar):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n            \n            progress_bar.set_postfix({\n                'Loss': f'{running_loss/(batch_idx+1):.4f}',\n                'Acc': f'{100.*correct/total:.2f}%'\n            })\n        \n        # Evaluate\n        test_acc = evaluate_model(model, test_loader)\n        print(f'Teacher Epoch {epoch+1}: Train Acc: {100.*correct/total:.2f}%, Test Acc: {test_acc:.2f}%')\n        \n        if test_acc > best_acc:\n            best_acc = test_acc\n            torch.save(model.state_dict(), 'teacher_best.pth')\n        \n        scheduler.step()\n    \n    print(f'Teacher training completed. Best accuracy: {best_acc:.2f}%')\n    return model\n```\n\n### Train Student Model with Distillation\n```python\ndef train_student(student, teacher, train_loader, test_loader, epochs=100, lr=0.001):\n    \"\"\"Train the student network using knowledge distillation\"\"\"\n    print(\"Training Student Network with Knowledge Distillation...\")\n    \n    distillation_loss = DistillationLoss(alpha=0.7, temperature=4.0)\n    optimizer = optim.Adam(student.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n    \n    teacher.eval()  # Teacher in evaluation mode\n    student.train()\n    best_acc = 0.0\n    \n    for epoch in range(epochs):\n        running_loss = 0.0\n        running_ce_loss = 0.0\n        running_kl_loss = 0.0\n        correct = 0\n        total = 0\n        \n        progress_bar = tqdm(train_loader, desc=f'Student Epoch {epoch+1}/{epochs}')\n        for batch_idx, (inputs, targets) in enumerate(progress_bar):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Get predictions from both networks\n            with torch.no_grad():\n                teacher_logits = teacher(inputs)\n            \n            student_logits = student(inputs)\n            \n            # Calculate distillation loss\n            total_loss, ce_loss, kl_loss = distillation_loss(\n                student_logits, teacher_logits, targets\n            )\n            \n            total_loss.backward()\n            optimizer.step()\n            \n            # Statistics\n            running_loss += total_loss.item()\n            running_ce_loss += ce_loss.item()\n            running_kl_loss += kl_loss.item()\n            \n            _, predicted = student_logits.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n            \n            progress_bar.set_postfix({\n                'Loss': f'{running_loss/(batch_idx+1):.4f}',\n                'CE': f'{running_ce_loss/(batch_idx+1):.4f}',\n                'KL': f'{running_kl_loss/(batch_idx+1):.4f}',\n                'Acc': f'{100.*correct/total:.2f}%'\n            })\n        \n        # Evaluate\n        test_acc = evaluate_model(student, test_loader)\n        print(f'Student Epoch {epoch+1}: Train Acc: {100.*correct/total:.2f}%, Test Acc: {test_acc:.2f}%')\n        \n        if test_acc > best_acc:\n            best_acc = test_acc\n            torch.save(student.state_dict(), 'student_best.pth')\n        \n        scheduler.step()\n    \n    print(f'Student training completed. Best accuracy: {best_acc:.2f}%')\n    return student\n```\n### Train Student Model Baseline\n```python\ndef train_student_baseline(student, train_loader, test_loader, epochs=100, lr=0.001):\n    \"\"\"Train student without distillation (baseline comparison)\"\"\"\n    print(\"Training Student Network (Baseline - No Distillation)...\")\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(student.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n    \n    student.train()\n    best_acc = 0.0\n    \n    for epoch in range(epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        progress_bar = tqdm(train_loader, desc=f'Baseline Epoch {epoch+1}/{epochs}')\n        for batch_idx, (inputs, targets) in enumerate(progress_bar):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            outputs = student(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n            \n            progress_bar.set_postfix({\n                'Loss': f'{running_loss/(batch_idx+1):.4f}',\n                'Acc': f'{100.*correct/total:.2f}%'\n            })\n        \n        # Evaluate\n        test_acc = evaluate_model(student, test_loader)\n        print(f'Baseline Epoch {epoch+1}: Train Acc: {100.*correct/total:.2f}%, Test Acc: {test_acc:.2f}%')\n        \n        if test_acc > best_acc:\n            best_acc = test_acc\n            torch.save(student.state_dict(), 'student_baseline_best.pth')\n        \n        scheduler.step()\n    \n    print(f'Baseline training completed. Best accuracy: {best_acc:.2f}%')\n    return student\n```\n\n### Evaluate Model\n```python\ndef evaluate_model(model, test_loader):\n    \"\"\"Evaluate model accuracy\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    \n    accuracy = 100. * correct / total\n    model.train()\n    return accuracy\n```\n\n### Count Parameters\n```python\ndef count_parameters(model):\n    \"\"\"Count trainable parameters\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n```\n\n### Main Execution\n```python\n# Load data\ntrain_loader, test_loader = load_data(batch_size=128)\n\n# Initialize networks\nteacher = TeacherNetwork(num_classes=10).to(device)\nstudent_distilled = StudentNetwork(num_classes=10).to(device)\nstudent_baseline = StudentNetwork(num_classes=10).to(device)\n\nprint(f\"Teacher parameters: {count_parameters(teacher):,}\")\nprint(f\"Student parameters: {count_parameters(student_distilled):,}\")\nprint(f\"Compression ratio: {count_parameters(teacher) / count_parameters(student_distilled):.1f}x\")\n\n# Train teacher (or load pre-trained)\ntry:\n    teacher.load_state_dict(torch.load('teacher_best.pth'))\n    print(\"Loaded pre-trained teacher model\")\nexcept FileNotFoundError:\n    print(\"Training teacher from scratch...\")\n    teacher = train_teacher(teacher, train_loader, test_loader, epochs=50)\n\nteacher_acc = evaluate_model(teacher, test_loader)\nprint(f\"Teacher accuracy: {teacher_acc:.2f}%\")\n\n# Train student with knowledge distillation\nstudent_distilled = train_student(\n    student_distilled, teacher, train_loader, test_loader, epochs=100\n)\n\n# Train student baseline (without distillation)\nstudent_baseline = train_student_baseline(\n    student_baseline, train_loader, test_loader, epochs=100\n)\n\n# Final evaluation\ndistilled_acc = evaluate_model(student_distilled, test_loader)\nbaseline_acc = evaluate_model(student_baseline, test_loader)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"FINAL RESULTS\")\nprint(\"=\"*50)\nprint(f\"Teacher accuracy:           {teacher_acc:.2f}%\")\nprint(f\"Student (distilled):        {distilled_acc:.2f}%\")\nprint(f\"Student (baseline):         {baseline_acc:.2f}%\")\nprint(f\"Distillation improvement:   {distilled_acc - baseline_acc:.2f}%\")\nprint(f\"Parameters reduction:       {count_parameters(teacher) / count_parameters(student_distilled):.1f}x\")\n```\n\n## Advanced Techniques\n\n### 1. Feature-Level Distillation\n\n```python\nclass FeatureDistillationLoss(nn.Module):\n    \"\"\"Distillation using intermediate feature maps\"\"\"\n    def __init__(self, alpha=0.5, beta=0.3, temperature=4.0):\n        super().__init__()\n        self.alpha = alpha      # Weight for prediction distillation\n        self.beta = beta        # Weight for feature distillation\n        self.temperature = temperature\n        self.ce_loss = nn.CrossEntropyLoss()\n        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n        self.mse_loss = nn.MSELoss()\n    \n    def forward(self, student_logits, teacher_logits, student_features, teacher_features, labels):\n        # Standard distillation loss\n        ce_loss = self.ce_loss(student_logits, labels)\n        \n        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)\n        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=1)\n        kl_loss = self.kl_loss(student_log_probs, teacher_probs) * (self.temperature ** 2)\n        \n        # Feature distillation loss\n        feature_loss = self.mse_loss(student_features, teacher_features)\n        \n        total_loss = (1 - self.alpha - self.beta) * ce_loss + self.alpha * kl_loss + self.beta * feature_loss\n        \n        return total_loss, ce_loss, kl_loss, feature_loss\n```\n\n### 2. Attention Transfer\n\n```python\nclass AttentionTransferLoss(nn.Module):\n    \"\"\"Transfer attention maps from teacher to student\"\"\"\n    def __init__(self, p=2):\n        super().__init__()\n        self.p = p\n    \n    def attention_map(self, feature_map):\n        # Compute attention as the L2 norm across channels\n        return torch.norm(feature_map, p=self.p, dim=1, keepdim=True)\n    \n    def forward(self, student_features, teacher_features):\n        student_attention = self.attention_map(student_features)\n        teacher_attention = self.attention_map(teacher_features)\n        \n        # Normalize attention maps\n        student_attention = F.normalize(student_attention.view(student_attention.size(0), -1))\n        teacher_attention = F.normalize(teacher_attention.view(teacher_attention.size(0), -1))\n        \n        return F.mse_loss(student_attention, teacher_attention)\n```\n\n### 3. Progressive Knowledge Distillation\n\n```python\nclass ProgressiveDistillation:\n    \"\"\"Gradually increase distillation weight during training\"\"\"\n    def __init__(self, initial_alpha=0.1, final_alpha=0.9, warmup_epochs=20):\n        self.initial_alpha = initial_alpha\n        self.final_alpha = final_alpha\n        self.warmup_epochs = warmup_epochs\n    \n    def get_alpha(self, epoch):\n        if epoch < self.warmup_epochs:\n            alpha = self.initial_alpha + (self.final_alpha - self.initial_alpha) * (epoch / self.warmup_epochs)\n        else:\n            alpha = self.final_alpha\n        return alpha\n```\n\n## Hyperparameter Guidelines\n\n### Temperature (T)\n- **Low (1-2)**: Hard targets, less knowledge transfer\n- **Medium (3-5)**: Balanced knowledge transfer (recommended)\n- **High (6-10)**: Very soft targets, may lose important information\n\n### Alpha (Î±)\n- **0.1-0.3**: Focus on ground truth labels\n- **0.5-0.7**: Balanced approach (recommended)\n- **0.8-0.9**: Heavy focus on teacher knowledge\n\n### Learning Rate\n- Start with same LR as baseline training\n- Consider lower LR for student to avoid overfitting to teacher\n- Use learning rate scheduling\n\n## Best Practices\n\n1. **Teacher Quality**: Ensure teacher model is well-trained and robust\n2. **Architecture Matching**: Student should have similar structure but smaller capacity\n3. **Temperature Tuning**: Experiment with different temperature values\n4. **Regularization**: Use dropout and weight decay to prevent overfitting\n5. **Evaluation**: Compare against baseline student training\n6. **Multi-Teacher**: Consider ensemble of teachers for better knowledge transfer\n\n## Common Issues and Solutions\n\n### Problem: Student performs worse than baseline\n**Solutions:**\n\n- Reduce temperature value\n- Decrease alpha (give more weight to ground truth)\n- Check teacher model quality\n- Ensure proper normalization\n\n### Problem: Slow convergence\n**Solutions:**\n\n- Increase learning rate\n- Use progressive distillation\n- Warm up the distillation loss\n- Check gradient flow\n\n### Problem: Overfitting to teacher\n**Solutions:**\n\n- Add regularization\n- Reduce alpha value\n- Use data augmentation\n- Early stopping based on validation loss\n\nThis comprehensive guide provides both theoretical understanding and practical implementation of student-teacher networks in PyTorch, with advanced techniques and troubleshooting tips for successful knowledge distillation.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}