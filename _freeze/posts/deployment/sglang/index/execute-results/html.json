{
  "hash": "caa865088c8eac69308e0b8b647a86d3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"SGLang: Comprehensive Guide to Structured Generation Language\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-08-25\"\ncategories: [code, tutorial, intermediate]\nformat:\n  html:\n    code-fold: false\n    math: mathjax\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# SGLang: Comprehensive Guide to Structured Generation Language\n![](lang.png)\n\n::: {.callout-note}\n## About This Guide\n\nThis comprehensive guide covers SGLang (Structured Generation Language), a revolutionary framework that transforms how developers interact with large language models (LLMs) and vision-language models. SGLang achieves unprecedented performance improvements while maintaining programming simplicity and flexibility.\n:::\n\n## Introduction {#sec-introduction}\n\nSGLang (Structured Generation Language) is a revolutionary framework that transforms how developers interact with large language models (LLMs) and vision-language models. By co-designing both the frontend programming interface and the backend runtime system, SGLang achieves unprecedented performance improvements while maintaining programming simplicity and flexibility.\n\n## What is SGLang? {#sec-what-is-sglang}\n\nSGLang is a fast serving framework for large language models and vision language models that makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language. SGLang consists of a frontend language and a runtime, where the frontend simplifies programming with primitives for generation and parallelism control, and the runtime accelerates execution with novel optimizations.\n\n### Key Benefits\n\n::: {.panel-tabset}\n\n#### Performance\nUp to **5x throughput improvements** over traditional serving methods through advanced optimization techniques.\n\n#### Controllability\nFine-grained control over generation processes with structured primitives and constraint handling.\n\n#### Expressiveness\nRich primitives for complex LLM programming patterns including parallel execution and multi-step reasoning.\n\n#### Efficiency\nAdvanced caching and optimization techniques including RadixAttention for automatic KV cache reuse.\n\n#### Multimodal Support\nNative support for both language and vision-language models with unified processing pipeline.\n\n:::\n\n## Key Features {#sec-key-features}\n\n### Frontend Language Features\n\n```{mermaid}\n%%| echo: false\n\ngraph TD\n    A[Frontend Language] --> B[Embedded DSL]\n    A --> C[Generation Primitives]\n    A --> D[Parallelism Control]\n    A --> E[Structured Outputs]\n    A --> F[Template System]\n    \n    B --> B1[Python Integration]\n    C --> C1[\"gen()\" function]\n    C --> C2[\"select()\" function]\n    D --> D1[\"fork()\" for Parallel]\n    E --> E1[JSON/XML Support]\n    F --> F1[Dynamic Prompts]\n```\n\n- **Embedded DSL**: Domain-specific language embedded in Python\n- **Generation Primitives**: Built-in functions for text generation and control\n- **Parallelism Control**: Native support for parallel generation calls\n- **Structured Outputs**: Easy handling of JSON, XML, and custom formats\n- **Template System**: Powerful templating for dynamic prompt construction\n\n### Backend Runtime Features\n\n```{mermaid}\n%%| echo: false\ngraph TD\n    A[Backend Runtime] --> B[RadixAttention]\n    A --> C[Zero-overhead Scheduler]\n    A --> D[Continuous Batching]\n    A --> E[Speculative Decoding]\n    A --> F[Multi-modal Processing]\n    A --> G[Quantization Support]\n    A --> H[Parallel Execution]\n    \n    B --> B1[KV Cache Reuse]\n    D --> D1[Dynamic Batching]\n    G --> G1[FP4/FP8/INT4/AWQ/GPTQ]\n    H --> H1[Tensor/Pipeline/Expert/Data]\n```\n\n## Architecture Overview {#sec-architecture}\n\nSGLang's architecture consists of two main components:\n\n::: {.callout-tip collapse=\"true\"}\n## Architecture Details\n\n### 1. Frontend Language\nThe frontend provides a Python-embedded DSL that simplifies LLM programming with:\n\n- Intuitive syntax for generation tasks\n- Built-in primitives for common patterns\n- Automatic optimization of generation calls\n- Type safety and error handling\n\n### 2. Backend Runtime\nThe backend proposes RadixAttention, a technique for automatic and efficient KV cache reuse across multiple LLM generation calls. The runtime includes:\n\n- High-performance serving engine\n- Advanced memory management\n- Automatic optimization passes\n- Multi-GPU/multi-node support\n:::\n\n## Installation and Setup {#sec-installation}\n\n### Prerequisites\n\n::: {.callout-important}\n## System Requirements\n- Python 3.8 or higher\n- CUDA 11.8+ (for GPU acceleration)\n- PyTorch 2.0+\n:::\n\n### Basic Installation\n\n```{.bash filename=\"install.sh\"}\n# Install from PyPI\npip install sglang\n\n# Or install from source\ngit clone https://github.com/sgl-project/sglang.git\ncd sglang\npip install -e .\n```\n\n### GPU Support\n\n```{.bash}\n# For CUDA support\npip install sglang[cuda]\n\n# For ROCm/AMD GPU support\npip install sglang[rocm]\n```\n\n### Docker Installation\n\n```{.bash}\n# Pull official Docker image\ndocker pull lmsysorg/sglang:latest\n\n# Run with GPU support\ndocker run --gpus all -p 30000:30000 lmsysorg/sglang:latest\n```\n\n## Core Concepts {#sec-core-concepts}\n\n### 1. Generation Functions\n\nThe core abstraction in SGLang is the generation function, which encapsulates prompts and generation logic:\n\n```{.python filename=\"basic_generation.py\"}\nimport sglang as sgl\n\n@sgl.function\ndef simple_chat(s, user_message):\n    s += sgl.user(user_message)\n    s += sgl.assistant(sgl.gen(\"response\", max_tokens=100))\n```\n\n### 2. State Management\n\nSGLang uses a state object `s` to track conversation history and manage generation context:\n\n```{.python filename=\"state_management.py\"}\n@sgl.function\ndef multi_turn_chat(s, messages):\n    for msg in messages:\n        s += sgl.user(msg)\n        s += sgl.assistant(sgl.gen(\"response\", stop=\"\\n\"))\n```\n\n### 3. Control Primitives\n\n::: {.panel-tabset}\n\n#### gen()\nGenerate text with specified constraints and parameters.\n\n#### select()\nChoose from predefined options or multiple choice answers.\n\n#### fork()\nCreate parallel execution branches for concurrent processing.\n\n#### image()\nProcess image inputs for vision-language model tasks.\n\n:::\n\n## Frontend Language Features {#sec-frontend}\n\n### Generation Primitives\n\n#### Basic Text Generation\n\n```{.python filename=\"story_generator.py\"}\n@sgl.function\ndef story_writer(s, theme):\n    s += f\"Write a story about {theme}:\\n\"\n    s += sgl.gen(\"story\", max_tokens=500, temperature=0.7)\n```\n\n#### Structured Generation\n\n```{.python filename=\"json_generator.py\"}\n@sgl.function\ndef json_generator(s, query):\n    s += f\"Generate JSON for: {query}\\n\"\n    s += sgl.gen(\"json\", max_tokens=200, regex=r'\\{.*\\}')\n```\n\n#### Conditional Generation\n\n```{.python filename=\"conditional_response.py\"}\n@sgl.function\ndef conditional_response(s, question, context):\n    s += f\"Context: {context}\\n\"\n    s += f\"Question: {question}\\n\"\n    \n    # First, determine if answerable\n    s += \"Is this answerable? \"\n    s += sgl.gen(\"answerable\", choices=[\"Yes\", \"No\"])\n    \n    if s[\"answerable\"] == \"Yes\":\n        s += \"\\nAnswer: \"\n        s += sgl.gen(\"answer\", max_tokens=100)\n    else:\n        s += \"\\nI don't have enough information to answer this question.\"\n```\n\n### Parallel Execution\n\n```{.python filename=\"parallel_processing.py\"}\n@sgl.function\ndef parallel_summarization(s, documents):\n    # Fork execution for parallel processing\n    s += sgl.fork([\n        lambda: summarize_doc(doc) for doc in documents\n    ])\n    \n    # Combine results\n    summaries = [s[f\"summary_{i}\"] for i in range(len(documents))]\n    return summaries\n```\n\n### Template System\n\n```{.python filename=\"email_template.py\"}\n@sgl.function\ndef email_generator(s, recipient, subject, tone=\"professional\"):\n    s += sgl.system(f\"Write emails in a {tone} tone.\")\n    s += f\"To: {recipient}\\n\"\n    s += f\"Subject: {subject}\\n\\n\"\n    s += sgl.gen(\"body\", max_tokens=300)\n```\n\n## Backend Runtime {#sec-backend}\n\n### RadixAttention\n\n::: {.callout-note}\n## RadixAttention Innovation\n\nRadixAttention structures and automates the reuse of Key-Value (KV) caches during runtime by storing them in a radix tree data structure.\n:::\n\nThis enables:\n\n- **Prefix Sharing**: Common prompt prefixes are cached and reused\n- **Memory Efficiency**: Reduced memory usage through intelligent caching  \n- **Speed Improvements**: Faster generation through cache hits\n\n```{mermaid}\n%%| echo: false\ngraph TD\n    A[Input Prompts] --> B[Radix Tree]\n    B --> C[Shared Prefixes]\n    B --> D[Unique Suffixes]\n    C --> E[KV Cache Reuse]\n    D --> F[New Computation]\n    E --> G[Performance Boost]\n    F --> G\n```\n\n### Continuous Batching\n\nThe runtime implements continuous batching to:\n\n- Process multiple requests simultaneously\n- Dynamically adjust batch sizes\n- Optimize GPU utilization\n\n### Speculative Decoding\n\nAcceleration technique that:\n\n- Predicts multiple tokens ahead\n- Verifies predictions in parallel\n- Falls back to standard decoding when needed\n\n## Basic Usage Examples {#sec-basic-examples}\n\n### 1. Simple Text Generation\n\n```{.python filename=\"poem_generator.py\"}\nimport sglang as sgl\n\n# Set backend\nsgl.set_default_backend(sgl.RuntimeEndpoint(\"http://localhost:30000\"))\n\n@sgl.function\ndef generate_poem(s, topic):\n    s += f\"Write a haiku about {topic}:\\n\"\n    s += sgl.gen(\"poem\", max_tokens=50)\n\n# Execute\nresult = generate_poem(\"spring\")\nprint(result[\"poem\"])\n```\n\n### 2. Multi-step Reasoning\n\n```{.python filename=\"math_solver.py\"}\n@sgl.function\ndef math_solver(s, problem):\n    s += f\"Problem: {problem}\\n\"\n    s += \"Let me solve this step by step.\\n\"\n    s += \"Step 1: \"\n    s += sgl.gen(\"step1\", max_tokens=50, stop=\"\\n\")\n    s += \"\\nStep 2: \"\n    s += sgl.gen(\"step2\", max_tokens=50, stop=\"\\n\")\n    s += \"\\nTherefore, the answer is: \"\n    s += sgl.gen(\"answer\", max_tokens=20)\n\nresult = math_solver(\"What is 15% of 240?\")\n```\n\n### 3. JSON Structured Output\n\n```{.python filename=\"info_extractor.py\"}\n@sgl.function\ndef extract_info(s, text):\n    s += f\"Extract key information from this text:\\n{text}\\n\"\n    s += \"Output as JSON:\\n\"\n    s += sgl.gen(\n        \"info\", \n        max_tokens=200, \n        regex=r'\\{[^}]*\"name\"[^}]*\"age\"[^}]*\"location\"[^}]*\\}'\n    )\n\nresult = extract_info(\"John Smith is 30 years old and lives in New York.\")\n```\n\n### 4. Role-playing Conversation\n\n```{.python filename=\"roleplay.py\"}\n@sgl.function\ndef roleplay_chat(s, character, user_input):\n    s += sgl.system(f\"You are {character}. Stay in character.\")\n    s += sgl.user(user_input)\n    s += sgl.assistant(sgl.gen(\"response\", max_tokens=150))\n\nresult = roleplay_chat(\"a wise old wizard\", \"How do I learn magic?\")\n```\n\n## Advanced Programming Patterns {#sec-advanced-patterns}\n\n### 1. Chain of Thought Reasoning\n\n```{.python filename=\"cot_reasoning.py\"}\n@sgl.function\ndef cot_reasoning(s, question):\n    s += f\"Question: {question}\\n\"\n    s += \"Let me think through this step by step:\\n\"\n    \n    for i in range(3):\n        s += f\"Step {i+1}: \"\n        s += sgl.gen(f\"step_{i+1}\", max_tokens=100, stop=\"\\n\")\n        s += \"\\n\"\n    \n    s += \"Final Answer: \"\n    s += sgl.gen(\"answer\", max_tokens=50)\n```\n\n### 2. Self-Correction Loop\n\n```{.python filename=\"self_correction.py\"}\n@sgl.function\ndef self_correct(s, task, max_iterations=3):\n    s += f\"Task: {task}\\n\"\n    \n    for i in range(max_iterations):\n        s += f\"Attempt {i+1}: \"\n        s += sgl.gen(f\"attempt_{i+1}\", max_tokens=200)\n        \n        s += \"\\nIs this correct? \"\n        s += sgl.gen(\"correct\", choices=[\"Yes\", \"No\"])\n        \n        if s[\"correct\"] == \"Yes\":\n            break\n        else:\n            s += \"\\nLet me try again.\\n\"\n```\n\n### 3. Tree Search Generation\n\n```{.python filename=\"tree_search.py\"}\n@sgl.function\ndef tree_search_story(s, prompt, branches=3, depth=2):\n    s += prompt\n    \n    def explore_branch(state, current_depth):\n        if current_depth >= depth:\n            return\n        \n        candidates = []\n        for i in range(branches):\n            state += sgl.gen(f\"branch_{current_depth}_{i}\", max_tokens=50)\n            candidates.append(state[f\"branch_{current_depth}_{i}\"])\n        \n        # Select best candidate (simplified selection)\n        best_idx = 0  # In practice, use a scoring function\n        state += candidates[best_idx]\n        explore_branch(state, current_depth + 1)\n    \n    explore_branch(s, 0)\n```\n\n### 4. Parallel Agent Collaboration\n\n```{.python filename=\"multi_agent.py\"}\n@sgl.function\ndef multi_agent_discussion(s, topic, agents):\n    s += f\"Topic: {topic}\\n\"\n    s += \"Discussion:\\n\"\n    \n    # Initialize agents\n    agent_states = {}\n    for agent in agents:\n        agent_states[agent] = sgl.fork(lambda: agent_response(agent, topic))\n    \n    # Simulate rounds of discussion\n    for round in range(3):\n        s += f\"\\nRound {round + 1}:\\n\"\n        for agent in agents:\n            s += f\"{agent}: \"\n            s += sgl.gen(f\"{agent}_round_{round}\", max_tokens=100)\n            s += \"\\n\"\n```\n\n## Performance Optimization {#sec-performance}\n\n### 1. Batch Processing\n\n::: {.callout-tip}\n## Optimization Strategy\n\nProcess multiple inputs in a single batch for maximum throughput efficiency.\n:::\n\n```{.python filename=\"batch_processing.py\"}\n# Process multiple inputs in a single batch\n@sgl.function\ndef batch_classification(s, texts):\n    results = []\n    for text in texts:\n        s += f\"Classify: {text}\\nCategory: \"\n        s += sgl.gen(\"category\", choices=[\"positive\", \"negative\", \"neutral\"])\n        results.append(s[\"category\"])\n    return results\n\n# Execute with batching enabled\nsgl.set_default_backend(\n    sgl.RuntimeEndpoint(\"http://localhost:30000\", batch_size=32)\n)\n```\n\n### 2. Caching Strategies\n\n```{.python filename=\"caching.py\"}\n# Enable aggressive caching for repeated patterns\n@sgl.function\ndef cached_qa(s, question, context):\n    # Use consistent formatting for better cache hits\n    s += f\"Context: {context}\\n\"\n    s += f\"Question: {question}\\n\"\n    s += \"Answer: \"\n    s += sgl.gen(\"answer\", max_tokens=100, temperature=0.0)  # Deterministic for caching\n```\n\n### 3. Memory Management\n\n```{.python filename=\"memory_management.py\"}\n# Optimize memory usage for long conversations\n@sgl.function\ndef efficient_chat(s, messages, max_context_length=2000):\n    # Truncate context to stay within limits\n    total_length = sum(len(msg) for msg in messages)\n    if total_length > max_context_length:\n        messages = messages[-(max_context_length // 100):]\n    \n    for msg in messages:\n        s += sgl.user(msg)\n        s += sgl.assistant(sgl.gen(\"response\", max_tokens=150))\n```\n\n## Vision-Language Model Support {#sec-vision-language}\n\n### 1. Image Understanding\n\n```{.python filename=\"image_description.py\"}\n@sgl.function\ndef describe_image(s, image_path, detail_level=\"medium\"):\n    s += sgl.image(image_path)\n    s += f\"Describe this image in {detail_level} detail:\\n\"\n    s += sgl.gen(\"description\", max_tokens=300)\n\n# Usage\nresult = describe_image(\"/path/to/image.jpg\", \"high\")\n```\n\n### 2. Visual Question Answering\n\n```{.python filename=\"visual_qa.py\"}\n@sgl.function\ndef visual_qa(s, image_path, question):\n    s += sgl.image(image_path)\n    s += f\"Question: {question}\\n\"\n    s += \"Answer: \"\n    s += sgl.gen(\"answer\", max_tokens=150)\n\nresult = visual_qa(\"/path/to/chart.png\", \"What is the highest value in this chart?\")\n```\n\n### 3. Multi-modal Reasoning\n\n```{.python filename=\"multimodal_analysis.py\"}\n@sgl.function\ndef multimodal_analysis(s, image_path, context):\n    s += f\"Context: {context}\\n\"\n    s += sgl.image(image_path)\n    s += \"Based on the context and image, analyze:\\n\"\n    s += \"1. Visual elements: \"\n    s += sgl.gen(\"visual\", max_tokens=100, stop=\"\\n\")\n    s += \"\\n2. Relationship to context: \"\n    s += sgl.gen(\"relationship\", max_tokens=100, stop=\"\\n\")\n    s += \"\\n3. Conclusion: \"\n    s += sgl.gen(\"conclusion\", max_tokens=100)\n```\n\n## Deployment and Serving {#sec-deployment}\n\n### 1. Starting a Server\n\n```{.bash filename=\"start_server.sh\"}\n# Basic server startup\npython -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000\n\n# With specific configurations\npython -m sglang.launch_server \\\n    --model-path meta-llama/Llama-2-7b-chat-hf \\\n    --port 30000 \\\n    --host 0.0.0.0 \\\n    --tp-size 2 \\\n    --mem-fraction-static 0.8\n```\n\n### 2. Client Configuration\n\n```{.python filename=\"client_setup.py\"}\nimport sglang as sgl\n\n# Connect to local server\nbackend = sgl.RuntimeEndpoint(\"http://localhost:30000\")\nsgl.set_default_backend(backend)\n\n# Connect to remote server with authentication\nbackend = sgl.RuntimeEndpoint(\n    \"https://api.example.com\",\n    headers={\"Authorization\": \"Bearer your-token\"}\n)\n```\n\n### 3. Load Balancing\n\n```{.python filename=\"load_balancing.py\"}\n# Multiple endpoints for load distribution\nendpoints = [\n    \"http://server1:30000\",\n    \"http://server2:30000\", \n    \"http://server3:30000\"\n]\n\nbackend = sgl.LoadBalancedEndpoint(endpoints)\nsgl.set_default_backend(backend)\n```\n\n### 4. Production Deployment\n\n```{.yaml filename=\"docker-compose.yml\"}\n# Docker Compose example\nversion: '3.8'\nservices:\n  sglang-server:\n    image: lmsysorg/sglang:latest\n    ports:\n      - \"30000:30000\"\n    environment:\n      - MODEL_PATH=meta-llama/Llama-2-7b-chat-hf\n      - PORT=30000\n      - TP_SIZE=2\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 2\n              capabilities: [gpu]\n```\n\n## Best Practices {#sec-best-practices}\n\n### 1. Prompt Engineering\n\n```{.python filename=\"prompt_engineering.py\"}\n# Use clear, structured prompts\n@sgl.function\ndef good_prompt(s, task, examples):\n    s += \"Task: \" + task + \"\\n\\n\"\n    \n    # Provide examples\n    for i, example in enumerate(examples):\n        s += f\"Example {i+1}:\\n\"\n        s += f\"Input: {example['input']}\\n\"\n        s += f\"Output: {example['output']}\\n\\n\"\n    \n    s += \"Now, complete this task:\\n\"\n    s += \"Input: \" + sgl.gen(\"input\") + \"\\n\"\n    s += \"Output: \" + sgl.gen(\"output\", max_tokens=200)\n```\n\n### 2. Error Handling\n\n```{.python filename=\"error_handling.py\"}\n@sgl.function\ndef robust_generation(s, prompt):\n    try:\n        s += prompt\n        s += sgl.gen(\"response\", max_tokens=100, timeout=30)\n        \n        # Validate output\n        if len(s[\"response\"].strip()) == 0:\n            s += \"Please provide a more detailed response: \"\n            s += sgl.gen(\"retry\", max_tokens=150)\n            \n    except sgl.GenerationError as e:\n        s += f\"Generation failed: {e}. Using fallback.\"\n        s += \"I apologize, but I cannot process this request.\"\n```\n\n### 3. Testing Strategies\n\n```{.python filename=\"testing.py\"}\nimport unittest\nimport sglang as sgl\n\nclass TestSGLangFunctions(unittest.TestCase):\n    def setUp(self):\n        # Use mock backend for testing\n        self.backend = sgl.MockBackend()\n        sgl.set_default_backend(self.backend)\n    \n    def test_simple_generation(self):\n        @sgl.function\n        def test_func(s):\n            s += \"Hello\"\n            s += sgl.gen(\"response\", max_tokens=10)\n        \n        result = test_func()\n        self.assertIn(\"response\", result)\n    \n    def test_structured_output(self):\n        @sgl.function\n        def json_test(s):\n            s += \"Generate JSON: \"\n            s += sgl.gen(\"json\", regex=r'\\{.*\\}')\n        \n        result = json_test()\n        self.assertTrue(result[\"json\"].startswith(\"{\"))\n```\n\n### 4. Monitoring and Logging\n\n```{.python filename=\"monitoring.py\"}\nimport logging\nimport time\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@sgl.function\ndef monitored_generation(s, prompt):\n    start_time = time.time()\n    \n    try:\n        s += prompt\n        s += sgl.gen(\"response\", max_tokens=100)\n        \n        duration = time.time() - start_time\n        logger.info(f\"Generation completed in {duration:.2f}s\")\n        \n    except Exception as e:\n        logger.error(f\"Generation failed: {e}\")\n        raise\n```\n\n## Comparison with Other Frameworks {#sec-comparisons}\n\n::: {.panel-tabset}\n\n### SGLang vs. LMQL\n\n| Feature | SGLang | LMQL |\n|---------|--------|------|\n| Performance | High (RadixAttention) | Medium |\n| Python Integration | Native embedding | External DSL |\n| Caching | Automatic | Manual |\n| Parallelism | Built-in | Limited |\n\n### SGLang vs. Guidance\n\n| Feature | SGLang | Guidance |\n|---------|--------|----------|\n| Runtime Optimization | Yes | Limited |\n| Structured Output | Advanced | Basic |\n| Vision Support | Yes | No |\n| Deployment | Production-ready | Research-focused |\n\n### SGLang vs. LangChain\n\n| Feature | SGLang | LangChain |\n|---------|--------|-----------|\n| Level | Low-level control | High-level abstractions |\n| Performance | Optimized runtime | Variable |\n| Flexibility | High | Medium |\n| Learning Curve | Moderate | Low |\n\n:::\n\n## Troubleshooting {#sec-troubleshooting}\n\n### Common Issues\n\n#### 1. Connection Problems\n\n```{.python filename=\"debug_connection.py\"}\n# Debug connection issues\ntry:\n    backend = sgl.RuntimeEndpoint(\"http://localhost:30000\")\n    backend.health_check()\n    print(\"Server is healthy\")\nexcept ConnectionError:\n    print(\"Cannot connect to server. Check if it's running.\")\n```\n\n#### 2. Memory Issues\n\n```{.bash filename=\"memory_debug.sh\"}\n# Monitor memory usage\nnvidia-smi\n\n# Adjust memory settings\npython -m sglang.launch_server \\\n    --model-path your-model \\\n    --mem-fraction-static 0.6  # Reduce if getting OOM\n```\n\n#### 3. Generation Timeouts\n\n```{.python filename=\"timeout_handling.py\"}\n@sgl.function\ndef timeout_handling(s, prompt):\n    try:\n        s += prompt\n        s += sgl.gen(\"response\", max_tokens=100, timeout=30)\n    except sgl.TimeoutError:\n        s += \"Request timed out. Please try again.\"\n```\n\n#### 4. Performance Issues\n\n```{.python filename=\"performance_debug.py\"}\n# Enable performance profiling\nsgl.set_debug_mode(True)\n\n@sgl.function\ndef profiled_function(s, input):\n    with sgl.profile(\"generation\"):\n        s += input\n        s += sgl.gen(\"output\", max_tokens=100)\n```\n\n### Debugging Tips\n\n::: {.callout-warning}\n## Debugging Checklist\n\n1. **Enable Verbose Logging**\n   ```python\n   import logging\n   logging.getLogger(\"sglang\").setLevel(logging.DEBUG)\n   ```\n\n2. **Check Server Logs**\n   ```bash\n   # Server logs show detailed execution info\n   tail -f sglang_server.log\n   ```\n\n3. **Use Mock Backend for Testing**\n   ```python\n   # Test logic without actual model calls\n   sgl.set_default_backend(sgl.MockBackend())\n   ```\n:::\n\n## Contributing {#sec-contributing}\n\n### Development Setup\n\n```{.bash filename=\"dev_setup.sh\"}\n# Clone repository\ngit clone https://github.com/sgl-project/sglang.git\ncd sglang\n\n# Create development environment\nconda create -n sglang-dev python=3.9\nconda activate sglang-dev\n\n# Install in development mode\npip install -e .\npip install -r requirements-dev.txt\n```\n\n### Running Tests\n\n```{.bash filename=\"run_tests.sh\"}\n# Run all tests\npython -m pytest tests/\n\n# Run specific test category\npython -m pytest tests/test_frontend.py\n\n# Run with coverage\npython -m pytest --cov=sglang tests/\n```\n\n### Code Style\n\n```{.bash filename=\"code_style.sh\"}\n# Format code\nblack sglang/\nisort sglang/\n\n# Check style\nflake8 sglang/\nmypy sglang/\n```\n\n### Submitting PRs\n\n::: {.callout-tip}\n## Pull Request Guidelines\n\n1. Fork the repository\n2. Create a feature branch\n3. Add tests for new functionality\n4. Update documentation\n5. Submit pull request with clear description\n:::\n\n## Resources {#sec-resources}\n\n### Official Documentation\n\n- [SGLang Documentation](https://docs.sglang.ai/)\n- [GitHub Repository](https://github.com/sgl-project/sglang)\n- [Paper: SGLang: Efficient Execution of Structured Language Model Programs](https://arxiv.org/abs/2312.07104)\n\n### Community\n\n- [GitHub Discussions](https://github.com/sgl-project/sglang/discussions)\n- [Discord Server](https://discord.gg/sglang)\n- [Twitter Updates](https://twitter.com/sglang_ai)\n\n### Examples and Tutorials\n\n- [Official Examples](https://github.com/sgl-project/sglang/tree/main/examples)\n- [Tutorial Notebooks](https://github.com/sgl-project/sglang/tree/main/notebooks)\n- [Cookbook Recipes](https://github.com/sgl-project/sglang/tree/main/cookbook)\n\n### Related Projects\n\n- [UC Berkeley Sky Computing Lab](https://sky.cs.berkeley.edu/project/sglang/)\n- [LMSYS Organization](https://lmsys.org/)\n\n---\n\n::: {.callout-note}\n## Final Note\n\nThis guide covers the essential aspects of SGLang, from basic concepts to advanced usage patterns. As SGLang continues to evolve rapidly, always refer to the official documentation for the most current information and updates.\n\nFor questions and support, please visit the [GitHub Discussions](https://github.com/sgl-project/sglang/discussions) or check the official documentation.\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}