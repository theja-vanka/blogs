{
  "hash": "c13a569280bd0d9074abcb2e8a674b07",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"LitServe with MobileNetV2 - Complete Code Guide\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-05-27\"\ncategories: [code, mlops, intermediate]\nformat:\n  html:\n    code-fold: false\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n\n# LitServe with MobileNetV2 - Complete Code Guide\n\n![](litlog.jpg)\n\nThis guide demonstrates how to deploy a MobileNetV2 image classification model using LitServe for efficient, scalable inference.\n\n## Installation\n\n```bash\n# Install required packages\npip install litserve torch torchvision pillow requests\n```\n\n## Basic Implementation\n\n### Simple MobileNetV2 API\n\n```python\n# mobilenet_api.py\nimport io\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom PIL import Image\nimport litserve as ls\n\n\nclass MobileNetV2API(ls.LitAPI):\n    def setup(self, device):\n        \"\"\"Initialize the model and preprocessing pipeline\"\"\"\n        # Load pre-trained MobileNetV2\n        self.model = models.mobilenet_v2(pretrained=True)\n        self.model.eval()\n        self.model.to(device)\n        \n        # Define image preprocessing\n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n        \n        # ImageNet class labels (first 10 for brevity)\n        self.class_labels = [\n            \"tench\", \"goldfish\", \"great white shark\", \"tiger shark\",\n            \"hammerhead\", \"electric ray\", \"stingray\", \"cock\", \n            \"hen\", \"ostrich\"\n            # ... add all 1000 ImageNet classes\n        ]\n\n    def decode_request(self, request):\n        \"\"\"Parse incoming request and prepare image\"\"\"\n        if isinstance(request, dict) and \"image\" in request:\n            # Handle base64 encoded image\n            import base64\n            image_data = base64.b64decode(request[\"image\"])\n            image = Image.open(io.BytesIO(image_data)).convert('RGB')\n        else:\n            # Handle direct image upload\n            image = Image.open(io.BytesIO(request)).convert('RGB')\n        \n        return image\n\n    def predict(self, image):\n        \"\"\"Run inference on the preprocessed image\"\"\"\n        # Preprocess image\n        input_tensor = self.transform(image).unsqueeze(0)\n        input_tensor = input_tensor.to(next(self.model.parameters()).device)\n        \n        # Run inference\n        with torch.no_grad():\n            outputs = self.model(input_tensor)\n            probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n        \n        return probabilities\n\n    def encode_response(self, probabilities):\n        \"\"\"Format the response\"\"\"\n        # Get top 5 predictions\n        top5_prob, top5_indices = torch.topk(probabilities, 5)\n        \n        results = []\n        for i in range(5):\n            idx = top5_indices[i].item()\n            prob = top5_prob[i].item()\n            label = self.class_labels[idx] if idx < len(self.class_labels) else f\"class_{idx}\"\n            results.append({\n                \"class\": label,\n                \"confidence\": round(prob, 4),\n                \"class_id\": idx\n            })\n        \n        return {\n            \"predictions\": results,\n            \"model\": \"mobilenet_v2\"\n        }\n\n\nif __name__ == \"__main__\":\n    # Create and run the API\n    api = MobileNetV2API()\n    server = ls.LitServer(api, accelerator=\"auto\", max_batch_size=4)\n    server.run(port=8000)\n```\n\n### Client Code for Testing\n\n```python\n# client.py\nimport requests\nimport base64\nfrom PIL import Image\nimport io\n\n\ndef encode_image(image_path):\n    \"\"\"Encode image to base64\"\"\"\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode('utf-8')\n\n\ndef test_image_classification(image_path, server_url=\"http://localhost:8000/predict\"):\n    \"\"\"Test the MobileNetV2 API\"\"\"\n    # Method 1: Send as base64 in JSON\n    encoded_image = encode_image(image_path)\n    response = requests.post(\n        server_url,\n        json={\"image\": encoded_image}\n    )\n    \n    if response.status_code == 200:\n        result = response.json()\n        print(\"Top 5 Predictions:\")\n        for pred in result[\"predictions\"]:\n            print(f\"  {pred['class']}: {pred['confidence']:.4f}\")\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n\n\ndef test_direct_upload(image_path, server_url=\"http://localhost:8000/predict\"):\n    \"\"\"Test with direct image upload\"\"\"\n    with open(image_path, 'rb') as f:\n        files = {'file': f}\n        response = requests.post(server_url, files=files)\n    \n    if response.status_code == 200:\n        result = response.json()\n        print(\"Predictions:\", result)\n\n\nif __name__ == \"__main__\":\n    # Test with a sample image\n    test_image_classification(\"sample_image.jpg\")\n```\n\n## Advanced Features\n\n### Batch Processing with Custom Batching\n\n```python\n# advanced_mobilenet_api.py\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom PIL import Image\nimport litserve as ls\nimport json\nfrom typing import List, Any\n\n\nclass BatchedMobileNetV2API(ls.LitAPI):\n    def setup(self, device):\n        \"\"\"Initialize model with batch processing capabilities\"\"\"\n        self.model = models.mobilenet_v2(pretrained=True)\n        self.model.eval()\n        self.model.to(device)\n        self.device = device\n        \n        # Optimized transform for batch processing\n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n        \n        # Load class labels from file or define them\n        self.load_imagenet_labels()\n\n    def load_imagenet_labels(self):\n        \"\"\"Load ImageNet class labels\"\"\"\n        # You can download from: https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n        try:\n            with open('imagenet_classes.txt', 'r') as f:\n                self.class_labels = [line.strip() for line in f.readlines()]\n        except FileNotFoundError:\n            # Fallback to first few classes\n            self.class_labels = [f\"class_{i}\" for i in range(1000)]\n\n    def decode_request(self, request):\n        \"\"\"Handle both single images and batch requests\"\"\"\n        if isinstance(request, dict):\n            if \"images\" in request:  # Batch request\n                images = []\n                for img_data in request[\"images\"]:\n                    if isinstance(img_data, str):  # base64\n                        import base64\n                        image_data = base64.b64decode(img_data)\n                        image = Image.open(io.BytesIO(image_data)).convert('RGB')\n                    else:  # direct bytes\n                        image = Image.open(io.BytesIO(img_data)).convert('RGB')\n                    images.append(image)\n                return images\n            elif \"image\" in request:  # Single request\n                import base64\n                image_data = base64.b64decode(request[\"image\"])\n                image = Image.open(io.BytesIO(image_data)).convert('RGB')\n                return [image]  # Wrap in list for consistent handling\n        \n        # Direct upload\n        image = Image.open(io.BytesIO(request)).convert('RGB')\n        return [image]\n\n    def batch(self, inputs: List[Any]) -> List[Any]:\n        \"\"\"Custom batching logic\"\"\"\n        # Flatten all images from all requests\n        all_images = []\n        batch_sizes = []\n        \n        for inp in inputs:\n            if isinstance(inp, list):\n                all_images.extend(inp)\n                batch_sizes.append(len(inp))\n            else:\n                all_images.append(inp)\n                batch_sizes.append(1)\n        \n        return all_images, batch_sizes\n\n    def predict(self, batch_data):\n        \"\"\"Process batch of images efficiently\"\"\"\n        images, batch_sizes = batch_data\n        \n        # Preprocess all images\n        batch_tensor = torch.stack([\n            self.transform(img) for img in images\n        ]).to(self.device)\n        \n        # Run batch inference\n        with torch.no_grad():\n            outputs = self.model(batch_tensor)\n            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n        \n        return probabilities, batch_sizes\n\n    def unbatch(self, output):\n        \"\"\"Split batch results back to individual responses\"\"\"\n        probabilities, batch_sizes = output\n        results = []\n        start_idx = 0\n        \n        for batch_size in batch_sizes:\n            batch_probs = probabilities[start_idx:start_idx + batch_size]\n            results.append(batch_probs)\n            start_idx += batch_size\n        \n        return results\n\n    def encode_response(self, probabilities):\n        \"\"\"Format response for batch or single predictions\"\"\"\n        if len(probabilities.shape) == 1:  # Single prediction\n            probabilities = probabilities.unsqueeze(0)\n        \n        all_results = []\n        for prob_vector in probabilities:\n            top5_prob, top5_indices = torch.topk(prob_vector, 5)\n            \n            predictions = []\n            for i in range(5):\n                idx = top5_indices[i].item()\n                prob = top5_prob[i].item()\n                predictions.append({\n                    \"class\": self.class_labels[idx],\n                    \"confidence\": round(prob, 4),\n                    \"class_id\": idx\n                })\n            \n            all_results.append(predictions)\n        \n        return {\n            \"predictions\": all_results[0] if len(all_results) == 1 else all_results,\n            \"model\": \"mobilenet_v2\",\n            \"batch_size\": len(all_results)\n        }\n\n\nif __name__ == \"__main__\":\n    api = BatchedMobileNetV2API()\n    server = ls.LitServer(\n        api,\n        accelerator=\"auto\",\n        max_batch_size=8,\n        batch_timeout=0.1,  # 100ms timeout for batching\n    )\n    server.run(port=8000, num_workers=2)\n```\n\n### Adding Model Quantization for Better Performance\n\n```python\n# quantized_mobilenet_api.py\nimport torch\nimport torch.quantization\nfrom torchvision import models\nimport litserve as ls\n\n\nclass QuantizedMobileNetV2API(ls.LitAPI):\n    def setup(self, device):\n        \"\"\"Setup quantized MobileNetV2 for faster inference\"\"\"\n        # Load pre-trained model\n        self.model = models.mobilenet_v2(pretrained=True)\n        self.model.eval()\n        \n        # Apply dynamic quantization for CPU inference\n        if device == \"cpu\":\n            self.model = torch.quantization.quantize_dynamic(\n                self.model,\n                {torch.nn.Linear, torch.nn.Conv2d},\n                dtype=torch.qint8\n            )\n            print(\"Applied dynamic quantization for CPU\")\n        else:\n            self.model.to(device)\n            print(f\"Using device: {device}\")\n        \n        # Rest of setup code...\n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n\n    # ... rest of the methods remain the same\n```\n\n## Performance Optimization\n\n### Configuration for Production\n\n```python\n# production_config.py\nimport litserve as ls\nfrom advanced_mobilenet_api import BatchedMobileNetV2API\n\ndef create_production_server():\n    \"\"\"Create optimized server for production\"\"\"\n    api = BatchedMobileNetV2API()\n    \n    server = ls.LitServer(\n        api,\n        accelerator=\"auto\",  # Auto-detect GPU/CPU\n        max_batch_size=16,   # Larger batches for throughput\n        batch_timeout=0.05,  # 50ms batching timeout\n        workers_per_device=2,  # Multiple workers per GPU\n        timeout=30,          # Request timeout\n    )\n    \n    return server\n\nif __name__ == \"__main__\":\n    server = create_production_server()\n    server.run(\n        port=8000,\n        host=\"0.0.0.0\",  # Accept external connections\n        num_workers=4     # Total number of workers\n    )\n```\n\n### Monitoring and Logging\n\n```python\n# monitored_api.py\nimport time\nimport logging\nfrom collections import defaultdict\nimport litserve as ls\n\n\nclass MonitoredMobileNetV2API(BatchedMobileNetV2API):\n    def setup(self, device):\n        \"\"\"Setup with monitoring\"\"\"\n        super().setup(device)\n        \n        # Setup logging\n        logging.basicConfig(level=logging.INFO)\n        self.logger = logging.getLogger(__name__)\n        \n        # Metrics tracking\n        self.metrics = defaultdict(list)\n        self.request_count = 0\n\n    def predict(self, batch_data):\n        \"\"\"Predict with timing and metrics\"\"\"\n        start_time = time.time()\n        \n        result = super().predict(batch_data)\n        \n        # Log timing\n        inference_time = time.time() - start_time\n        batch_size = len(batch_data[0])\n        \n        self.metrics['inference_times'].append(inference_time)\n        self.metrics['batch_sizes'].append(batch_size)\n        self.request_count += 1\n        \n        self.logger.info(\n            f\"Processed batch of {batch_size} images in {inference_time:.3f}s \"\n            f\"(Total requests: {self.request_count})\"\n        )\n        \n        return result\n\n    def encode_response(self, probabilities):\n        \"\"\"Add metrics to response\"\"\"\n        response = super().encode_response(probabilities)\n        \n        # Add performance metrics every 100 requests\n        if self.request_count % 100 == 0:\n            avg_time = sum(self.metrics['inference_times'][-100:]) / min(100, len(self.metrics['inference_times']))\n            response['metrics'] = {\n                'avg_inference_time': round(avg_time, 4),\n                'total_requests': self.request_count\n            }\n        \n        return response\n```\n\n## Deployment\n\n### Docker Deployment\n\n```dockerfile\n# Dockerfile\nFROM python:3.9-slim\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    wget \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Download ImageNet labels\nRUN wget -O imagenet_classes.txt https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n\nEXPOSE 8000\n\nCMD [\"python\", \"production_config.py\"]\n```\n\n```yaml\n# docker-compose.yml\nversion: '3.8'\n\nservices:\n  mobilenet-api:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - CUDA_VISIBLE_DEVICES=0  # Set GPU if available\n    volumes:\n      - ./models:/app/models  # Optional: for custom models\n    restart: unless-stopped\n    \n  # Optional: Add nginx for load balancing\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n    depends_on:\n      - mobilenet-api\n```\n\n### Kubernetes Deployment\n\n```yaml\n# k8s-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mobilenet-api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mobilenet-api\n  template:\n    metadata:\n      labels:\n        app: mobilenet-api\n    spec:\n      containers:\n      - name: mobilenet-api\n        image: your-registry/mobilenet-api:latest\n        ports:\n        - containerPort: 8000\n        resources:\n          requests:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"2Gi\"\n            cpu: \"1000m\"\n        env:\n        - name: WORKERS\n          value: \"2\"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mobilenet-service\nspec:\n  selector:\n    app: mobilenet-api\n  ports:\n  - port: 80\n    targetPort: 8000\n  type: LoadBalancer\n```\n\n## Testing\n\n### Comprehensive Test Suite\n\n```python\n# test_api.py\nimport pytest\nimport requests\nimport base64\nimport numpy as np\nfrom PIL import Image\nimport io\n\n\nclass TestMobileNetV2API:\n    def setup_class(self):\n        \"\"\"Setup test configuration\"\"\"\n        self.base_url = \"http://localhost:8000\"\n        self.test_image = self.create_test_image()\n\n    def create_test_image(self):\n        \"\"\"Create a test image\"\"\"\n        # Create a simple test image\n        img = Image.new('RGB', (224, 224), color='red')\n        img_buffer = io.BytesIO()\n        img.save(img_buffer, format='JPEG')\n        img_buffer.seek(0)\n        return img_buffer.getvalue()\n\n    def test_single_prediction(self):\n        \"\"\"Test single image prediction\"\"\"\n        encoded_image = base64.b64encode(self.test_image).decode('utf-8')\n        \n        response = requests.post(\n            f\"{self.base_url}/predict\",\n            json={\"image\": encoded_image}\n        )\n        \n        assert response.status_code == 200\n        result = response.json()\n        assert \"predictions\" in result\n        assert len(result[\"predictions\"]) == 5\n        assert all(\"class\" in pred and \"confidence\" in pred for pred in result[\"predictions\"])\n\n    def test_batch_prediction(self):\n        \"\"\"Test batch prediction\"\"\"\n        encoded_image = base64.b64encode(self.test_image).decode('utf-8')\n        \n        response = requests.post(\n            f\"{self.base_url}/predict\",\n            json={\"images\": [encoded_image, encoded_image]}\n        )\n        \n        assert response.status_code == 200\n        result = response.json()\n        assert \"batch_size\" in result\n        assert result[\"batch_size\"] == 2\n\n    def test_performance(self):\n        \"\"\"Test API performance\"\"\"\n        import time\n        \n        encoded_image = base64.b64encode(self.test_image).decode('utf-8')\n        \n        # Warmup\n        requests.post(f\"{self.base_url}/predict\", json={\"image\": encoded_image})\n        \n        # Time multiple requests\n        times = []\n        for _ in range(10):\n            start = time.time()\n            response = requests.post(f\"{self.base_url}/predict\", json={\"image\": encoded_image})\n            times.append(time.time() - start)\n            assert response.status_code == 200\n        \n        avg_time = sum(times) / len(times)\n        print(f\"Average response time: {avg_time:.3f}s\")\n        assert avg_time < 1.0  # Should respond within 1 second\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])\n```\n\n### Load Testing\n\n```python\n# load_test.py\nimport asyncio\nimport aiohttp\nimport base64\nimport time\nfrom PIL import Image\nimport io\n\n\nasync def send_request(session, url, data):\n    \"\"\"Send a single request\"\"\"\n    try:\n        async with session.post(url, json=data) as response:\n            result = await response.json()\n            return response.status, time.time()\n    except Exception as e:\n        return 500, time.time()\n\n\nasync def load_test(num_requests=100, concurrent=10):\n    \"\"\"Run load test\"\"\"\n    # Create test image\n    img = Image.new('RGB', (224, 224), color='blue')\n    img_buffer = io.BytesIO()\n    img.save(img_buffer, format='JPEG')\n    encoded_image = base64.b64encode(img_buffer.getvalue()).decode('utf-8')\n    \n    url = \"http://localhost:8000/predict\"\n    data = {\"image\": encoded_image}\n    \n    start_time = time.time()\n    \n    async with aiohttp.ClientSession() as session:\n        # Create semaphore to limit concurrent requests\n        semaphore = asyncio.Semaphore(concurrent)\n        \n        async def bounded_request():\n            async with semaphore:\n                return await send_request(session, url, data)\n        \n        # Send all requests\n        tasks = [bounded_request() for _ in range(num_requests)]\n        results = await asyncio.gather(*tasks)\n    \n    total_time = time.time() - start_time\n    \n    # Analyze results\n    successful = sum(1 for status, _ in results if status == 200)\n    failed = num_requests - successful\n    \n    print(f\"Load Test Results:\")\n    print(f\"  Total Requests: {num_requests}\")\n    print(f\"  Successful: {successful}\")\n    print(f\"  Failed: {failed}\")\n    print(f\"  Total Time: {total_time:.2f}s\")\n    print(f\"  Requests/sec: {num_requests/total_time:.2f}\")\n    print(f\"  Concurrent: {concurrent}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(load_test(num_requests=200, concurrent=20))\n```\n\n## Requirements File\n\n```txt\n# requirements.txt\nlitserve>=0.2.0\ntorch>=1.9.0\ntorchvision>=0.10.0\nPillow>=8.0.0\nrequests>=2.25.0\nnumpy>=1.21.0\naiohttp>=3.8.0  # For async testing\npytest>=6.0.0  # For testing\n```\n\n## Best Practices\n\n1. **Model Optimization**: Use quantization and TorchScript for production\n2. **Batch Processing**: Configure appropriate batch sizes based on your hardware\n3. **Error Handling**: Implement comprehensive error handling for robustness\n4. **Monitoring**: Add logging and metrics collection for production monitoring\n5. **Security**: Implement authentication and input validation for production APIs\n6. **Caching**: Consider caching frequently requested predictions\n7. **Scaling**: Use container orchestration for high-availability deployments\n\nThis guide provides a complete foundation for deploying MobileNetV2 with LitServe, from basic implementation to production-ready deployment with monitoring and testing.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}