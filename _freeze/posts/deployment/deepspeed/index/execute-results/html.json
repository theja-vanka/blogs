{
  "hash": "99217980f018df91551ec33b98bd7e2b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"DeepSpeed with PyTorch: Complete Code Guide\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-06-21\"\ncategories: [code, tutorial, intermediate]\nformat:\n  html:\n    code-fold: false\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# DeepSpeed with PyTorch: Complete Code Guide\n![](DeepSpeed.png)\n\n## Introduction\n\nDeepSpeed is a deep learning optimization library that makes distributed training easy, efficient, and effective. It provides system innovations like ZeRO (Zero Redundancy Optimizer) to enable training massive models with trillions of parameters.\n\nKey benefits:\n- **Memory Efficiency**: ZeRO reduces memory consumption by partitioning optimizer states, gradients, and model parameters\n- **Speed**: Achieves high training throughput through optimized kernels and communication\n- **Scale**: Enables training of models with billions/trillions of parameters\n- **Ease of Use**: Simple integration with existing PyTorch code\n\n## Installation\n\n```bash\n# Install DeepSpeed\npip install deepspeed\n\n# Or install from source for latest features\ngit clone https://github.com/microsoft/DeepSpeed.git\ncd DeepSpeed\npip install .\n\n# Verify installation\nds_report\n```\n\n## Basic Setup\n\n### Simple Model Training\n\n```python\nimport torch\nimport torch.nn as nn\nimport deepspeed\nfrom torch.utils.data import DataLoader, Dataset\n\n# Define a simple model\nclass SimpleModel(nn.Module):\n    def __init__(self, input_size=1000, hidden_size=2000, output_size=1000):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, output_size)\n        )\n    \n    def forward(self, x):\n        return self.layers(x)\n\n# Dummy dataset\nclass DummyDataset(Dataset):\n    def __init__(self, size=1000):\n        self.size = size\n        \n    def __len__(self):\n        return self.size\n    \n    def __getitem__(self, idx):\n        return torch.randn(1000), torch.randn(1000)\n\n# Initialize model and data\nmodel = SimpleModel()\ndataset = DummyDataset()\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Initialize DeepSpeed\nmodel_engine, optimizer, _, _ = deepspeed.initialize(\n    model=model,\n    model_parameters=model.parameters(),\n    config_params={\n        \"train_batch_size\": 32,\n        \"optimizer\": {\n            \"type\": \"Adam\",\n            \"params\": {\"lr\": 0.001}\n        },\n        \"fp16\": {\"enabled\": True}\n    }\n)\n\n# Training loop\nfor epoch in range(10):\n    for batch_idx, (data, target) in enumerate(dataloader):\n        # Forward pass\n        outputs = model_engine(data)\n        loss = nn.MSELoss()(outputs, target)\n        \n        # Backward pass\n        model_engine.backward(loss)\n        model_engine.step()\n        \n        if batch_idx % 10 == 0:\n            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n```\n\n### Configuration File Approach\n\n```python\nimport deepspeed\nimport argparse\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--local_rank', type=int, default=-1,\n                       help='local rank passed from distributed launcher')\n    parser.add_argument('--deepspeed_config', type=str, default='ds_config.json',\n                       help='deepspeed config file')\n    args = parser.parse_args()\n    \n    # Initialize distributed training\n    deepspeed.init_distributed()\n    \n    model = SimpleModel()\n    \n    # Initialize with config file\n    model_engine, optimizer, trainloader, _ = deepspeed.initialize(\n        args=args,\n        model=model,\n        model_parameters=model.parameters(),\n        training_data=dataset\n    )\n    \n    # Training loop\n    for step, batch in enumerate(trainloader):\n        loss = model_engine(batch)\n        model_engine.backward(loss)\n        model_engine.step()\n\nif __name__ == '__main__':\n    main()\n```\n\n## Configuration Files\n\n### Basic Configuration (`ds_config.json`)\n\n```json\n{\n  \"train_batch_size\": 64,\n  \"train_micro_batch_size_per_gpu\": 16,\n  \"gradient_accumulation_steps\": 1,\n  \"optimizer\": {\n    \"type\": \"Adam\",\n    \"params\": {\n      \"lr\": 3e-5,\n      \"betas\": [0.8, 0.999],\n      \"eps\": 1e-8,\n      \"weight_decay\": 3e-7\n    }\n  },\n  \"scheduler\": {\n    \"type\": \"WarmupLR\",\n    \"params\": {\n      \"warmup_min_lr\": 0,\n      \"warmup_max_lr\": 3e-5,\n      \"warmup_num_steps\": 1000\n    }\n  },\n  \"fp16\": {\n    \"enabled\": true,\n    \"auto_cast\": false,\n    \"loss_scale\": 0,\n    \"initial_scale_power\": 16,\n    \"loss_scale_window\": 1000,\n    \"hysteresis\": 2,\n    \"min_loss_scale\": 1\n  },\n  \"zero_optimization\": {\n    \"stage\": 2,\n    \"allgather_partitions\": true,\n    \"allgather_bucket_size\": 2e8,\n    \"overlap_comm\": true,\n    \"reduce_scatter\": true,\n    \"reduce_bucket_size\": 2e8,\n    \"contiguous_gradients\": true\n  },\n  \"gradient_clipping\": 1.0,\n  \"wall_clock_breakdown\": false\n}\n```\n\n### Advanced Configuration with ZeRO Stage 3\n\n```json\n{\n  \"train_batch_size\": 64,\n  \"train_micro_batch_size_per_gpu\": 4,\n  \"gradient_accumulation_steps\": 4,\n  \"optimizer\": {\n    \"type\": \"AdamW\",\n    \"params\": {\n      \"lr\": 3e-4,\n      \"betas\": [0.9, 0.95],\n      \"eps\": 1e-8,\n      \"weight_decay\": 0.1\n    }\n  },\n  \"fp16\": {\n    \"enabled\": true,\n    \"auto_cast\": false,\n    \"loss_scale\": 0,\n    \"initial_scale_power\": 16,\n    \"loss_scale_window\": 1000,\n    \"hysteresis\": 2,\n    \"min_loss_scale\": 1\n  },\n  \"zero_optimization\": {\n    \"stage\": 3,\n    \"offload_optimizer\": {\n      \"device\": \"cpu\",\n      \"pin_memory\": true\n    },\n    \"offload_param\": {\n      \"device\": \"cpu\",\n      \"pin_memory\": true\n    },\n    \"overlap_comm\": true,\n    \"contiguous_gradients\": true,\n    \"sub_group_size\": 1e9,\n    \"reduce_bucket_size\": \"auto\",\n    \"stage3_prefetch_bucket_size\": \"auto\",\n    \"stage3_param_persistence_threshold\": \"auto\",\n    \"stage3_max_live_parameters\": 1e9,\n    \"stage3_max_reuse_distance\": 1e9,\n    \"stage3_gather_16bit_weights_on_model_save\": true\n  },\n  \"activation_checkpointing\": {\n    \"partition_activations\": false,\n    \"cpu_checkpointing\": true,\n    \"contiguous_memory_optimization\": false,\n    \"number_checkpoints\": null,\n    \"synchronize_checkpoint_boundary\": false,\n    \"profile\": false\n  }\n}\n```\n\n## ZeRO Optimizer States\n\n### ZeRO Stage 1: Optimizer State Partitioning\n\n```python\n# Configuration for ZeRO Stage 1\nzero_stage1_config = {\n    \"train_batch_size\": 64,\n    \"optimizer\": {\n        \"type\": \"Adam\",\n        \"params\": {\"lr\": 0.001}\n    },\n    \"zero_optimization\": {\n        \"stage\": 1,\n        \"reduce_bucket_size\": 5e8\n    },\n    \"fp16\": {\"enabled\": True}\n}\n\nmodel_engine, optimizer, _, _ = deepspeed.initialize(\n    model=model,\n    model_parameters=model.parameters(),\n    config_params=zero_stage1_config\n)\n```\n\n### ZeRO Stage 2: Gradient + Optimizer State Partitioning\n\n```python\n# Configuration for ZeRO Stage 2\nzero_stage2_config = {\n    \"train_batch_size\": 64,\n    \"optimizer\": {\n        \"type\": \"Adam\",\n        \"params\": {\"lr\": 0.001}\n    },\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"allgather_partitions\": True,\n        \"allgather_bucket_size\": 2e8,\n        \"overlap_comm\": True,\n        \"reduce_scatter\": True,\n        \"reduce_bucket_size\": 2e8,\n        \"contiguous_gradients\": True\n    },\n    \"fp16\": {\"enabled\": True}\n}\n```\n\n### ZeRO Stage 3: Full Parameter Partitioning\n\n```python\n# Configuration for ZeRO Stage 3\nzero_stage3_config = {\n    \"train_batch_size\": 32,\n    \"train_micro_batch_size_per_gpu\": 8,\n    \"optimizer\": {\n        \"type\": \"Adam\",\n        \"params\": {\"lr\": 0.001}\n    },\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"overlap_comm\": True,\n        \"contiguous_gradients\": True,\n        \"sub_group_size\": 1e9,\n        \"reduce_bucket_size\": \"auto\",\n        \"stage3_prefetch_bucket_size\": \"auto\",\n        \"stage3_param_persistence_threshold\": \"auto\",\n        \"stage3_max_live_parameters\": 1e9,\n        \"stage3_max_reuse_distance\": 1e9\n    },\n    \"fp16\": {\"enabled\": True}\n}\n\n# Special handling for ZeRO Stage 3\nwith deepspeed.zero.Init(config_dict_or_path=zero_stage3_config):\n    model = SimpleModel()\n\nmodel_engine, optimizer, _, _ = deepspeed.initialize(\n    model=model,\n    config_params=zero_stage3_config\n)\n```\n\n## Model Parallelism\n\n### Pipeline Parallelism\n\n```python\nimport deepspeed\nfrom deepspeed.pipe import PipelineModule\n\nclass PipelineModel(nn.Module):\n    def __init__(self, layers_per_stage=2):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            nn.Linear(1000, 1000) for _ in range(8)\n        ])\n        \n    def forward(self, x):\n        for layer in self.layers:\n            x = torch.relu(layer(x))\n        return x\n\n# Convert to pipeline model\ndef partition_layers():\n    layers = []\n    for i in range(8):\n        layers.append(nn.Sequential(\n            nn.Linear(1000, 1000),\n            nn.ReLU()\n        ))\n    return layers\n\n# Create pipeline\nmodel = PipelineModule(\n    layers=partition_layers(),\n    num_stages=4,  # Number of pipeline stages\n    partition_method='type:Linear'\n)\n\n# Pipeline-specific config\npipeline_config = {\n    \"train_batch_size\": 64,\n    \"train_micro_batch_size_per_gpu\": 16,\n    \"gradient_accumulation_steps\": 1,\n    \"optimizer\": {\n        \"type\": \"Adam\",\n        \"params\": {\"lr\": 0.001}\n    },\n    \"pipeline\": {\n        \"stages\": \"auto\",\n        \"partition\": \"balanced\"\n    },\n    \"fp16\": {\"enabled\": True}\n}\n\nengine, _, _, _ = deepspeed.initialize(\n    model=model,\n    config_params=pipeline_config\n)\n```\n\n### Tensor Parallelism (with Megatron)\n\n```python\n# Example using DeepSpeed with Megatron-style tensor parallelism\nimport deepspeed\nfrom deepspeed.moe import MoE\n\nclass TensorParallelLinear(nn.Module):\n    def __init__(self, input_size, output_size, world_size):\n        super().__init__()\n        self.world_size = world_size\n        self.rank = torch.distributed.get_rank()\n        \n        # Split output dimension across ranks\n        self.output_size_per_partition = output_size // world_size\n        self.weight = nn.Parameter(\n            torch.randn(input_size, self.output_size_per_partition)\n        )\n        \n    def forward(self, x):\n        output = torch.matmul(x, self.weight)\n        # All-gather outputs from all partitions\n        gathered = [torch.zeros_like(output) for _ in range(self.world_size)]\n        torch.distributed.all_gather(gathered, output)\n        return torch.cat(gathered, dim=-1)\n```\n\n## Mixed Precision Training\n\n### FP16 Configuration\n\n```python\nfp16_config = {\n    \"train_batch_size\": 64,\n    \"optimizer\": {\n        \"type\": \"Adam\",\n        \"params\": {\"lr\": 0.001}\n    },\n    \"fp16\": {\n        \"enabled\": True,\n        \"auto_cast\": False,\n        \"loss_scale\": 0,\n        \"initial_scale_power\": 16,\n        \"loss_scale_window\": 1000,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    }\n}\n\n# Custom loss scaling\ndef train_with_custom_scaling(model_engine, dataloader):\n    for batch in dataloader:\n        outputs = model_engine(batch)\n        loss = compute_loss(outputs, batch)\n        \n        # DeepSpeed handles scaling automatically\n        model_engine.backward(loss)\n        model_engine.step()\n```\n\n### BF16 Configuration\n\n```python\nbf16_config = {\n    \"train_batch_size\": 64,\n    \"optimizer\": {\n        \"type\": \"Adam\",\n        \"params\": {\"lr\": 0.001}\n    },\n    \"bf16\": {\n        \"enabled\": True\n    }\n}\n```\n\n## Advanced Features\n\n### Activation Checkpointing\n\n```python\nactivation_checkpointing_config = {\n    \"train_batch_size\": 64,\n    \"optimizer\": {\n        \"type\": \"Adam\",\n        \"params\": {\"lr\": 0.001}\n    },\n    \"activation_checkpointing\": {\n        \"partition_activations\": False,\n        \"cpu_checkpointing\": True,\n        \"contiguous_memory_optimization\": False,\n        \"number_checkpoints\": None,\n        \"synchronize_checkpoint_boundary\": False,\n        \"profile\": False\n    },\n    \"fp16\": {\"enabled\": True}\n}\n```\n\n### CPU Offloading\n\n```python\ncpu_offload_config = {\n    \"train_batch_size\": 32,\n    \"optimizer\": {\n        \"type\": \"Adam\",\n        \"params\": {\"lr\": 0.001}\n    },\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": True\n        },\n        \"offload_param\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": True\n        }\n    },\n    \"fp16\": {\"enabled\": True}\n}\n```\n\n### Mixture of Experts (MoE)\n\n```python\nfrom deepspeed.moe import MoE\n\nclass MoEModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.embedding = nn.Embedding(1000, 512)\n        \n        # MoE layer\n        self.moe_layer = MoE(\n            hidden_size=512,\n            expert=nn.Sequential(\n                nn.Linear(512, 2048),\n                nn.ReLU(),\n                nn.Linear(2048, 512)\n            ),\n            num_experts=8,\n            k=2  # Top-k routing\n        )\n        \n        self.output = nn.Linear(512, 1000)\n    \n    def forward(self, x):\n        x = self.embedding(x)\n        x, _, _ = self.moe_layer(x)\n        return self.output(x)\n\nmoe_config = {\n    \"train_batch_size\": 64,\n    \"optimizer\": {\n        \"type\": \"Adam\",\n        \"params\": {\"lr\": 0.001}\n    },\n    \"fp16\": {\"enabled\": True}\n}\n```\n\n### Model Saving and Loading\n\n```python\n# Saving model\ndef save_model(model_engine, checkpoint_dir):\n    model_engine.save_checkpoint(checkpoint_dir)\n\n# Loading model\ndef load_model(model_engine, checkpoint_dir):\n    _, client_states = model_engine.load_checkpoint(checkpoint_dir)\n    return client_states\n\n# Usage\ncheckpoint_dir = \"./checkpoints\"\nsave_model(model_engine, checkpoint_dir)\n\n# Later, load the model\nclient_states = load_model(model_engine, checkpoint_dir)\n```\n\n## Troubleshooting\n\n### Common Issues and Solutions\n\n```python\n# 1. Memory issues\n# Solution: Reduce batch size or enable CPU offloading\n\n# 2. Slow training\n# Check communication overlap settings\noverlap_config = {\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"overlap_comm\": True,\n        \"contiguous_gradients\": True\n    }\n}\n\n# 3. Gradient explosion\n# Enable gradient clipping\ngradient_clip_config = {\n    \"gradient_clipping\": 1.0\n}\n\n# 4. Loss scaling issues with FP16\n# Use automatic loss scaling\nauto_loss_scale_config = {\n    \"fp16\": {\n        \"enabled\": True,\n        \"loss_scale\": 0,  # 0 means automatic\n        \"initial_scale_power\": 16\n    }\n}\n```\n\n### Debugging Tools\n\n```python\n# Enable profiling\nprofiling_config = {\n    \"wall_clock_breakdown\": True,\n    \"memory_breakdown\": True\n}\n\n# Memory monitoring\ndef monitor_memory():\n    if torch.cuda.is_available():\n        print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")\n        print(f\"GPU Memory Cached: {torch.cuda.memory_reserved() / 1e9:.2f}GB\")\n\n# Communication profiling\ndef profile_communication():\n    torch.distributed.barrier()  # Synchronize all processes\n    start_time = time.time()\n    # Your training step here\n    torch.distributed.barrier()\n    end_time = time.time()\n    print(f\"Step time: {end_time - start_time:.4f}s\")\n```\n\n## Best Practices\n\n### 1. Batch Size Tuning\n\n```python\n# Find optimal batch size\ndef find_optimal_batch_size(model, start_batch_size=16):\n    batch_size = start_batch_size\n    while True:\n        try:\n            config = {\n                \"train_micro_batch_size_per_gpu\": batch_size,\n                \"gradient_accumulation_steps\": 64 // batch_size,\n                \"optimizer\": {\"type\": \"Adam\", \"params\": {\"lr\": 0.001}},\n                \"fp16\": {\"enabled\": True}\n            }\n            \n            model_engine, _, _, _ = deepspeed.initialize(\n                model=model, config_params=config\n            )\n            \n            # Test with dummy data\n            dummy_input = torch.randn(batch_size, 1000).cuda()\n            output = model_engine(dummy_input)\n            loss = output.sum()\n            model_engine.backward(loss)\n            model_engine.step()\n            \n            print(f\"Batch size {batch_size} works!\")\n            batch_size *= 2\n            \n        except RuntimeError as e:\n            if \"out of memory\" in str(e):\n                print(f\"Max batch size: {batch_size // 2}\")\n                break\n            else:\n                raise e\n```\n\n### 2. Learning Rate Scaling\n\n```python\n# Scale learning rate with batch size\ndef scale_learning_rate(base_lr, base_batch_size, actual_batch_size):\n    return base_lr * (actual_batch_size / base_batch_size)\n\n# Example\nbase_config = {\n    \"train_batch_size\": 1024,\n    \"optimizer\": {\n        \"type\": \"Adam\",\n        \"params\": {\n            \"lr\": scale_learning_rate(3e-4, 64, 1024)\n        }\n    }\n}\n```\n\n### 3. Efficient Data Loading\n\n```python\nclass EfficientDataLoader:\n    def __init__(self, dataset, batch_size, num_workers=4):\n        self.dataloader = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=True,\n            num_workers=num_workers,\n            pin_memory=True,\n            persistent_workers=True\n        )\n    \n    def __iter__(self):\n        for batch in self.dataloader:\n            # Move to GPU asynchronously\n            batch = [x.cuda(non_blocking=True) for x in batch]\n            yield batch\n```\n\n### 4. Model Architecture Tips\n\n```python\n# Use activation checkpointing for large models\nclass CheckpointedModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            nn.Linear(1000, 1000) for _ in range(100)\n        ])\n    \n    def forward(self, x):\n        # Checkpoint every 10 layers\n        for i in range(0, len(self.layers), 10):\n            def create_forward(start_idx):\n                def forward_chunk(x):\n                    for j in range(start_idx, min(start_idx + 10, len(self.layers))):\n                        x = torch.relu(self.layers[j](x))\n                    return x\n                return forward_chunk\n            \n            x = torch.utils.checkpoint.checkpoint(create_forward(i), x)\n        return x\n```\n\n### 5. Multi-Node Training Script\n\n```python\n# launch_script.py\nimport subprocess\nimport sys\n\ndef launch_distributed_training():\n    cmd = [\n        \"deepspeed\",\n        \"--num_gpus=8\",\n        \"--num_nodes=4\",\n        \"--master_addr=your_master_node\",\n        \"--master_port=29500\",\n        \"train.py\",\n        \"--deepspeed_config=ds_config.json\"\n    ]\n    \n    subprocess.run(cmd)\n\nif __name__ == \"__main__\":\n    launch_distributed_training()\n```\n\nThis guide covers the essential aspects of using DeepSpeed with PyTorch. Remember to experiment with different configurations based on your specific model architecture and hardware setup. Start with simpler configurations (ZeRO Stage 1-2) and gradually move to more advanced features (ZeRO Stage 3, CPU offloading) as needed.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}