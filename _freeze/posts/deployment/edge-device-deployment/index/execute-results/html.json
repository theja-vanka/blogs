{
  "hash": "1f9dcc3b4d0836416e811d9778a4b2da",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"PyTorch Model Deployment on Edge Devices - Complete Code Guide\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-06-21\"\ncategories: [code, tutorial, intermediate]\nformat:\n  html:\n    code-fold: false\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# PyTorch Model Deployment on Edge Devices - Complete Code Guide\n![](edge.png)\n\n## Prerequisites\n\n```bash\n# Install required packages\npip install torch torchvision\npip install torch-model-archiver\npip install onnx onnxruntime\npip install tensorflow  # for TensorFlow Lite conversion\n```\n\n## 1. Model Optimization\n\n### Quantization\n\n```python\nimport torch\nimport torch.quantization as quantization\nfrom torch.quantization import get_default_qconfig\nimport torchvision.models as models\n\n# Load your trained model\nmodel = models.resnet18(pretrained=True)\nmodel.eval()\n\n# Post-training quantization (easiest method)\ndef post_training_quantization(model, sample_data):\n    \"\"\"\n    Apply post-training quantization to reduce model size\n    \"\"\"\n    # Set model to evaluation mode\n    model.eval()\n    \n    # Fuse conv, bn and relu\n    model_fused = torch.quantization.fuse_modules(model, [['conv1', 'bn1', 'relu']])\n    \n    # Specify quantization configuration\n    model_fused.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n    \n    # Prepare the model for quantization\n    model_prepared = torch.quantization.prepare(model_fused)\n    \n    # Calibrate with sample data\n    with torch.no_grad():\n        for data in sample_data:\n            model_prepared(data)\n    \n    # Convert to quantized model\n    quantized_model = torch.quantization.convert(model_prepared)\n    \n    return quantized_model\n\n# Example usage\nsample_data = [torch.randn(1, 3, 224, 224) for _ in range(100)]\nquantized_model = post_training_quantization(model, sample_data)\n```\n\n### Pruning\n\n```python\nimport torch.nn.utils.prune as prune\n\ndef prune_model(model, pruning_amount=0.3):\n    \"\"\"\n    Apply magnitude-based pruning to reduce model complexity\n    \"\"\"\n    parameters_to_prune = []\n    \n    # Collect all conv and linear layers\n    for name, module in model.named_modules():\n        if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):\n            parameters_to_prune.append((module, 'weight'))\n    \n    # Apply global magnitude pruning\n    prune.global_unstructured(\n        parameters_to_prune,\n        pruning_method=prune.L1Unstructured,\n        amount=pruning_amount,\n    )\n    \n    # Remove pruning reparameterization to make pruning permanent\n    for module, param in parameters_to_prune:\n        prune.remove(module, param)\n    \n    return model\n\n# Apply pruning\npruned_model = prune_model(model.copy(), pruning_amount=0.3)\n```\n\n## 2. Model Conversion\n\n### Convert to TorchScript\n\n```python\ndef convert_to_torchscript(model, sample_input, save_path):\n    \"\"\"\n    Convert PyTorch model to TorchScript for deployment\n    \"\"\"\n    model.eval()\n    \n    # Method 1: Tracing (recommended for models without control flow)\n    try:\n        traced_model = torch.jit.trace(model, sample_input)\n        traced_model.save(save_path)\n        print(f\"Model traced and saved to {save_path}\")\n        return traced_model\n    except Exception as e:\n        print(f\"Tracing failed: {e}\")\n        \n        # Method 2: Scripting (for models with control flow)\n        try:\n            scripted_model = torch.jit.script(model)\n            scripted_model.save(save_path)\n            print(f\"Model scripted and saved to {save_path}\")\n            return scripted_model\n        except Exception as e:\n            print(f\"Scripting also failed: {e}\")\n            return None\n\n# Example usage\nsample_input = torch.randn(1, 3, 224, 224)\ntorchscript_model = convert_to_torchscript(model, sample_input, \"model.pt\")\n```\n\n### Convert to ONNX\n\n```python\nimport onnx\nimport onnxruntime as ort\n\ndef convert_to_onnx(model, sample_input, onnx_path):\n    \"\"\"\n    Convert PyTorch model to ONNX format\n    \"\"\"\n    model.eval()\n    \n    torch.onnx.export(\n        model,                      # model being run\n        sample_input,               # model input\n        onnx_path,                 # where to save the model\n        export_params=True,         # store the trained parameter weights\n        opset_version=11,          # ONNX version to export to\n        do_constant_folding=True,   # optimize constant folding\n        input_names=['input'],      # model's input names\n        output_names=['output'],    # model's output names\n        dynamic_axes={\n            'input': {0: 'batch_size'},\n            'output': {0: 'batch_size'}\n        }\n    )\n    \n    # Verify the ONNX model\n    onnx_model = onnx.load(onnx_path)\n    onnx.checker.check_model(onnx_model)\n    print(f\"ONNX model saved and verified at {onnx_path}\")\n\n# Convert to ONNX\nconvert_to_onnx(model, sample_input, \"model.onnx\")\n\n# Test ONNX Runtime inference\ndef test_onnx_inference(onnx_path, sample_input):\n    \"\"\"Test ONNX model inference\"\"\"\n    ort_session = ort.InferenceSession(onnx_path)\n    \n    # Convert input to numpy\n    input_np = sample_input.numpy()\n    \n    # Run inference\n    outputs = ort_session.run(None, {'input': input_np})\n    return outputs[0]\n\n# Test the converted model\nonnx_output = test_onnx_inference(\"model.onnx\", sample_input)\n```\n\n### Convert to TensorFlow Lite\n\n```python\nimport tensorflow as tf\n\ndef pytorch_to_tflite(onnx_path, tflite_path):\n    \"\"\"\n    Convert ONNX model to TensorFlow Lite\n    \"\"\"\n    # Convert ONNX to TensorFlow\n    from onnx_tf.backend import prepare\n    import onnx\n    \n    onnx_model = onnx.load(onnx_path)\n    tf_rep = prepare(onnx_model)\n    tf_rep.export_graph(\"temp_tf_model\")\n    \n    # Convert to TensorFlow Lite\n    converter = tf.lite.TFLiteConverter.from_saved_model(\"temp_tf_model\")\n    \n    # Apply optimizations\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    \n    # Convert model\n    tflite_model = converter.convert()\n    \n    # Save the model\n    with open(tflite_path, 'wb') as f:\n        f.write(tflite_model)\n    \n    print(f\"TensorFlow Lite model saved to {tflite_path}\")\n\n# Convert to TensorFlow Lite\npytorch_to_tflite(\"model.onnx\", \"model.tflite\")\n```\n\n## 3. Mobile Deployment\n\n### Android Deployment\n\n```java\n// Android Java code for PyTorch Mobile\npublic class ModelInference {\n    private Module model;\n    \n    public ModelInference(String modelPath) {\n        model = LiteModuleLoader.load(modelPath);\n    }\n    \n    public float[] predict(Bitmap bitmap) {\n        // Preprocess image\n        Tensor inputTensor = TensorImageUtils.bitmapToFloat32Tensor(\n            bitmap,\n            TensorImageUtils.TORCHVISION_NORM_MEAN_RGB,\n            TensorImageUtils.TORCHVISION_NORM_STD_RGB\n        );\n        \n        // Run inference\n        Tensor outputTensor = model.forward(IValue.from(inputTensor)).toTensor();\n        \n        // Get results\n        return outputTensor.getDataAsFloatArray();\n    }\n}\n```\n\n### iOS Deployment (Swift)\n\n```swift\n// iOS Swift code for PyTorch Mobile\nimport LibTorch\n\nclass ModelInference {\n    private var model: TorchModule\n    \n    init(modelPath: String) {\n        model = TorchModule(fileAtPath: modelPath)!\n    }\n    \n    func predict(image: UIImage) -> [Float] {\n        // Preprocess image\n        guard let pixelBuffer = image.pixelBuffer() else { return [] }\n        guard let inputTensor = TorchTensor.fromPixelBuffer(pixelBuffer) else { return [] }\n        \n        // Run inference\n        guard let outputTensor = model.predict(inputs: [inputTensor]) else { return [] }\n        \n        // Get results\n        return outputTensor[0].floatArray\n    }\n}\n```\n\n### Python Mobile Preprocessing\n\n```python\ndef create_mobile_model(model, sample_input):\n    \"\"\"\n    Create optimized model for mobile deployment\n    \"\"\"\n    model.eval()\n    \n    # Convert to TorchScript\n    traced_model = torch.jit.trace(model, sample_input)\n    \n    # Optimize for mobile\n    optimized_model = optimize_for_mobile(traced_model)\n    \n    # Save mobile-optimized model\n    optimized_model._save_for_lite_interpreter(\"mobile_model.ptl\")\n    \n    return optimized_model\n\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\n# Create mobile model\nmobile_model = create_mobile_model(model, sample_input)\n```\n\n## 4. Raspberry Pi Deployment\n\n```python\n# Raspberry Pi deployment script\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport time\nimport psutil\nimport threading\n\nclass RaspberryPiInference:\n    def __init__(self, model_path, device='cpu'):\n        self.device = torch.device(device)\n        self.model = torch.jit.load(model_path, map_location=self.device)\n        self.model.eval()\n        \n        # Define preprocessing transforms\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                               std=[0.229, 0.224, 0.225])\n        ])\n        \n        # Performance monitoring\n        self.inference_times = []\n        \n    def preprocess_image(self, image_path):\n        \"\"\"Preprocess image for inference\"\"\"\n        image = Image.open(image_path).convert('RGB')\n        input_tensor = self.transform(image).unsqueeze(0)\n        return input_tensor.to(self.device)\n    \n    def inference(self, image_path):\n        \"\"\"Run inference on image\"\"\"\n        start_time = time.time()\n        \n        # Preprocess\n        input_tensor = self.preprocess_image(image_path)\n        \n        # Inference\n        with torch.no_grad():\n            outputs = self.model(input_tensor)\n            predictions = torch.nn.functional.softmax(outputs[0], dim=0)\n        \n        inference_time = time.time() - start_time\n        self.inference_times.append(inference_time)\n        \n        return predictions.cpu().numpy(), inference_time\n    \n    def get_system_stats(self):\n        \"\"\"Get system performance statistics\"\"\"\n        return {\n            'cpu_percent': psutil.cpu_percent(),\n            'memory_percent': psutil.virtual_memory().percent,\n            'temperature': self.get_cpu_temperature()\n        }\n    \n    def get_cpu_temperature(self):\n        \"\"\"Get CPU temperature (Raspberry Pi specific)\"\"\"\n        try:\n            with open('/sys/class/thermal/thermal_zone0/temp', 'r') as f:\n                temp = float(f.read()) / 1000.0\n            return temp\n        except:\n            return None\n\n# Usage example\nif __name__ == \"__main__\":\n    # Initialize inference engine\n    inference_engine = RaspberryPiInference(\"model.pt\")\n    \n    # Run inference\n    predictions, inference_time = inference_engine.inference(\"test_image.jpg\")\n    \n    print(f\"Inference time: {inference_time:.3f} seconds\")\n    print(f\"Top prediction: {predictions.max():.3f}\")\n    print(f\"System stats: {inference_engine.get_system_stats()}\")\n```\n\n## 5. NVIDIA Jetson Deployment\n\n```python\n# NVIDIA Jetson optimized deployment\nimport torch\nimport tensorrt as trt\nimport pycuda.driver as cuda\nimport pycuda.autoinit\nimport numpy as np\n\nclass JetsonTensorRTInference:\n    def __init__(self, onnx_model_path, trt_engine_path=None):\n        self.onnx_path = onnx_model_path\n        self.engine_path = trt_engine_path or onnx_model_path.replace('.onnx', '.trt')\n        \n        # Build or load TensorRT engine\n        if not os.path.exists(self.engine_path):\n            self.build_engine()\n        \n        self.engine = self.load_engine()\n        self.context = self.engine.create_execution_context()\n        \n        # Allocate GPU memory\n        self.allocate_buffers()\n    \n    def build_engine(self):\n        \"\"\"Build TensorRT engine from ONNX model\"\"\"\n        logger = trt.Logger(trt.Logger.WARNING)\n        builder = trt.Builder(logger)\n        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n        parser = trt.OnnxParser(network, logger)\n        \n        # Parse ONNX model\n        with open(self.onnx_path, 'rb') as model:\n            if not parser.parse(model.read()):\n                for error in range(parser.num_errors):\n                    print(parser.get_error(error))\n                return None\n        \n        # Build engine\n        config = builder.create_builder_config()\n        config.max_workspace_size = 1 << 28  # 256MB\n        config.set_flag(trt.BuilderFlag.FP16)  # Enable FP16 precision\n        \n        engine = builder.build_engine(network, config)\n        \n        # Save engine\n        with open(self.engine_path, 'wb') as f:\n            f.write(engine.serialize())\n        \n        return engine\n    \n    def load_engine(self):\n        \"\"\"Load TensorRT engine\"\"\"\n        runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))\n        with open(self.engine_path, 'rb') as f:\n            return runtime.deserialize_cuda_engine(f.read())\n    \n    def allocate_buffers(self):\n        \"\"\"Allocate GPU memory buffers\"\"\"\n        self.bindings = []\n        self.inputs = []\n        self.outputs = []\n        \n        for binding in self.engine:\n            shape = self.engine.get_binding_shape(binding)\n            size = trt.volume(shape) * self.engine.max_batch_size\n            dtype = trt.nptype(self.engine.get_binding_dtype(binding))\n            \n            # Allocate host and device buffers\n            host_mem = cuda.pagelocked_empty(size, dtype)\n            device_mem = cuda.mem_alloc(host_mem.nbytes)\n            \n            self.bindings.append(int(device_mem))\n            \n            if self.engine.binding_is_input(binding):\n                self.inputs.append({'host': host_mem, 'device': device_mem})\n            else:\n                self.outputs.append({'host': host_mem, 'device': device_mem})\n    \n    def inference(self, input_data):\n        \"\"\"Run TensorRT inference\"\"\"\n        # Copy input data to GPU\n        np.copyto(self.inputs[0]['host'], input_data.ravel())\n        cuda.memcpy_htod(self.inputs[0]['device'], self.inputs[0]['host'])\n        \n        # Run inference\n        self.context.execute_v2(bindings=self.bindings)\n        \n        # Copy output data from GPU\n        cuda.memcpy_dtoh(self.outputs[0]['host'], self.outputs[0]['device'])\n        \n        return self.outputs[0]['host']\n\n# Usage for Jetson\njetson_inference = JetsonTensorRTInference(\"model.onnx\")\n```\n\n## 6. Performance Optimization\n\n### Benchmarking Script\n\n```python\nimport time\nimport numpy as np\nimport torch\nimport psutil\nfrom contextlib import contextmanager\n\n@contextmanager\ndef timer():\n    \"\"\"Context manager for timing code execution\"\"\"\n    start = time.perf_counter()\n    yield\n    end = time.perf_counter()\n    print(f\"Execution time: {end - start:.4f} seconds\")\n\nclass ModelBenchmark:\n    def __init__(self, model, input_shape, device='cpu'):\n        self.model = model.to(device)\n        self.device = device\n        self.input_shape = input_shape\n        \n    def benchmark_inference(self, num_runs=100, warmup_runs=10):\n        \"\"\"Benchmark model inference performance\"\"\"\n        # Generate random input\n        dummy_input = torch.randn(self.input_shape).to(self.device)\n        \n        # Warmup runs\n        self.model.eval()\n        with torch.no_grad():\n            for _ in range(warmup_runs):\n                _ = self.model(dummy_input)\n        \n        # Benchmark runs\n        inference_times = []\n        memory_usage = []\n        \n        for i in range(num_runs):\n            # Monitor memory before inference\n            if self.device == 'cuda':\n                torch.cuda.empty_cache()\n                memory_before = torch.cuda.memory_allocated()\n            else:\n                memory_before = psutil.Process().memory_info().rss\n            \n            # Time inference\n            start_time = time.perf_counter()\n            with torch.no_grad():\n                output = self.model(dummy_input)\n            \n            if self.device == 'cuda':\n                torch.cuda.synchronize()\n            \n            end_time = time.perf_counter()\n            \n            # Monitor memory after inference\n            if self.device == 'cuda':\n                memory_after = torch.cuda.memory_allocated()\n            else:\n                memory_after = psutil.Process().memory_info().rss\n            \n            inference_times.append(end_time - start_time)\n            memory_usage.append(memory_after - memory_before)\n        \n        # Calculate statistics\n        stats = {\n            'mean_time': np.mean(inference_times),\n            'std_time': np.std(inference_times),\n            'min_time': np.min(inference_times),\n            'max_time': np.max(inference_times),\n            'fps': 1.0 / np.mean(inference_times),\n            'mean_memory': np.mean(memory_usage),\n            'max_memory': np.max(memory_usage)\n        }\n        \n        return stats\n    \n    def profile_model(self):\n        \"\"\"Profile model to identify bottlenecks\"\"\"\n        dummy_input = torch.randn(self.input_shape).to(self.device)\n        \n        with torch.profiler.profile(\n            activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n            record_shapes=True,\n            profile_memory=True,\n            with_stack=True\n        ) as profiler:\n            with torch.no_grad():\n                self.model(dummy_input)\n        \n        # Print profiling results\n        print(profiler.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n        \n        return profiler\n\n# Usage example\nbenchmark = ModelBenchmark(model, (1, 3, 224, 224), device='cpu')\nstats = benchmark.benchmark_inference()\nprint(f\"Average inference time: {stats['mean_time']:.4f}s\")\nprint(f\"FPS: {stats['fps']:.2f}\")\n```\n\n### Memory Optimization\n\n```python\ndef optimize_memory_usage(model):\n    \"\"\"Apply memory optimization techniques\"\"\"\n    \n    # Enable memory efficient attention (for transformers)\n    if hasattr(model, 'enable_memory_efficient_attention'):\n        model.enable_memory_efficient_attention()\n    \n    # Use gradient checkpointing during training\n    if hasattr(model, 'gradient_checkpointing_enable'):\n        model.gradient_checkpointing_enable()\n    \n    # Fuse operations where possible\n    model = torch.jit.optimize_for_inference(torch.jit.script(model))\n    \n    return model\n\ndef batch_inference(model, data_loader, batch_size=1):\n    \"\"\"Perform batch inference with memory management\"\"\"\n    model.eval()\n    results = []\n    \n    with torch.no_grad():\n        for batch in data_loader:\n            # Process in smaller chunks if needed\n            if batch.size(0) > batch_size:\n                for i in range(0, batch.size(0), batch_size):\n                    chunk = batch[i:i+batch_size]\n                    output = model(chunk)\n                    results.append(output.cpu())\n                    \n                    # Clear GPU cache\n                    if torch.cuda.is_available():\n                        torch.cuda.empty_cache()\n            else:\n                output = model(batch)\n                results.append(output.cpu())\n    \n    return torch.cat(results, dim=0)\n```\n\n## 7. Best Practices\n\n### Model Deployment Checklist\n\n```python\nclass DeploymentValidator:\n    def __init__(self, original_model, optimized_model, test_input):\n        self.original_model = original_model\n        self.optimized_model = optimized_model\n        self.test_input = test_input\n    \n    def validate_accuracy(self, tolerance=1e-3):\n        \"\"\"Validate that optimized model maintains accuracy\"\"\"\n        self.original_model.eval()\n        self.optimized_model.eval()\n        \n        with torch.no_grad():\n            original_output = self.original_model(self.test_input)\n            optimized_output = self.optimized_model(self.test_input)\n        \n        # Check if outputs are close\n        if torch.allclose(original_output, optimized_output, atol=tolerance):\n            print(\"✓ Accuracy validation passed\")\n            return True\n        else:\n            print(\"✗ Accuracy validation failed\")\n            diff = torch.abs(original_output - optimized_output).max().item()\n            print(f\"Maximum difference: {diff}\")\n            return False\n    \n    def validate_performance(self):\n        \"\"\"Compare performance metrics\"\"\"\n        # Benchmark both models\n        original_benchmark = ModelBenchmark(self.original_model, self.test_input.shape)\n        optimized_benchmark = ModelBenchmark(self.optimized_model, self.test_input.shape)\n        \n        original_stats = original_benchmark.benchmark_inference(num_runs=50)\n        optimized_stats = optimized_benchmark.benchmark_inference(num_runs=50)\n        \n        speedup = original_stats['mean_time'] / optimized_stats['mean_time']\n        memory_reduction = (original_stats['mean_memory'] - optimized_stats['mean_memory']) / original_stats['mean_memory'] * 100\n        \n        print(f\"Performance improvement: {speedup:.2f}x speedup\")\n        print(f\"Memory reduction: {memory_reduction:.1f}%\")\n        \n        return {\n            'speedup': speedup,\n            'memory_reduction': memory_reduction,\n            'original_fps': original_stats['fps'],\n            'optimized_fps': optimized_stats['fps']\n        }\n    \n    def check_model_size(self):\n        \"\"\"Compare model file sizes\"\"\"\n        # Save both models temporarily\n        torch.save(self.original_model.state_dict(), 'temp_original.pth')\n        torch.jit.save(torch.jit.script(self.optimized_model), 'temp_optimized.pt')\n        \n        import os\n        original_size = os.path.getsize('temp_original.pth')\n        optimized_size = os.path.getsize('temp_optimized.pt')\n        \n        size_reduction = (original_size - optimized_size) / original_size * 100\n        \n        print(f\"Original model size: {original_size / 1024 / 1024:.2f} MB\")\n        print(f\"Optimized model size: {optimized_size / 1024 / 1024:.2f} MB\")\n        print(f\"Size reduction: {size_reduction:.1f}%\")\n        \n        # Clean up temporary files\n        os.remove('temp_original.pth')\n        os.remove('temp_optimized.pt')\n        \n        return size_reduction\n\n# Example usage\nvalidator = DeploymentValidator(model, quantized_model, sample_input)\nvalidator.validate_accuracy()\nperformance_metrics = validator.validate_performance()\nsize_reduction = validator.check_model_size()\n```\n\n### Error Handling and Logging\n\n```python\nimport logging\nfrom functools import wraps\n\ndef setup_logging():\n    \"\"\"Setup logging for deployment\"\"\"\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.FileHandler('model_deployment.log'),\n            logging.StreamHandler()\n        ]\n    )\n    return logging.getLogger(__name__)\n\ndef handle_inference_errors(func):\n    \"\"\"Decorator for handling inference errors\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except torch.cuda.OutOfMemoryError:\n            logging.error(\"CUDA out of memory. Try reducing batch size.\")\n            torch.cuda.empty_cache()\n            raise\n        except Exception as e:\n            logging.error(f\"Inference error: {str(e)}\")\n            raise\n    return wrapper\n\nclass RobustInference:\n    def __init__(self, model_path, device='cpu'):\n        self.logger = setup_logging()\n        self.device = torch.device(device)\n        \n        try:\n            self.model = torch.jit.load(model_path, map_location=self.device)\n            self.model.eval()\n            self.logger.info(f\"Model loaded successfully on {device}\")\n        except Exception as e:\n            self.logger.error(f\"Failed to load model: {e}\")\n            raise\n    \n    @handle_inference_errors\n    def inference(self, input_data):\n        \"\"\"Robust inference with error handling\"\"\"\n        start_time = time.time()\n        \n        with torch.no_grad():\n            output = self.model(input_data)\n        \n        inference_time = time.time() - start_time\n        self.logger.info(f\"Inference completed in {inference_time:.3f}s\")\n        \n        return output\n```\n\n## Conclusion\n\nThis guide provides a comprehensive approach to deploying PyTorch models on edge devices. Key takeaways:\n\n1. **Model Optimization**: Always quantize and prune models before deployment\n2. **Format Selection**: Choose the right format (TorchScript, ONNX, TensorRT) based on your target device\n3. **Performance Monitoring**: Continuously monitor inference time, memory usage, and accuracy\n4. **Device-Specific Optimization**: Leverage device-specific optimizations (TensorRT for NVIDIA, Core ML for iOS)\n5. **Robust Deployment**: Implement proper error handling and logging for production systems\n\nRemember to validate your optimized models thoroughly before deployment and monitor their performance in production environments.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}