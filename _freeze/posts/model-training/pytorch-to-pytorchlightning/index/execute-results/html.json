{
  "hash": "1bbfd622ce6084a53f1e4fefd4622b1b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"PyTorch to PyTorch Lightning Migration Guide\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-05-25\"\ncategories: [code, tutorial, advanced]\nformat:\n  html:\n    code-fold: false\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# PyTorch to PyTorch Lightning Migration Guide\n\n![](pyt.png)\n\n## Introduction\n\nPyTorch Lightning is a lightweight wrapper around PyTorch that eliminates boilerplate code while maintaining full control over your models. It provides a structured approach to organizing PyTorch code and includes built-in support for distributed training, logging, and experiment management.\n\n### Why Migrate?\n- **Reduced Boilerplate**: Lightning handles training loops, device management, and distributed training\n- **Better Organization**: Standardized code structure improves readability and maintenance\n- **Built-in Features**: Automatic logging, checkpointing, early stopping, and more\n- **Scalability**: Easy multi-GPU and multi-node training\n- **Reproducibility**: Better experiment tracking and configuration management\n\n## Key Concepts\n\n### LightningModule\nThe core abstraction that wraps your PyTorch model. It defines:\n\n- Model architecture (`__init__`)\n- Forward pass (`forward`)\n- Training step (`training_step`)\n- Validation step (`validation_step`)\n- Optimizer configuration (`configure_optimizers`)\n\n### Trainer\nHandles the training loop, device management, and various training configurations.\n\n### DataModule\nEncapsulates data loading logic, including datasets and dataloaders.\n\n## Basic Migration Steps\n\n### Step 1: Convert Model to LightningModule\n\n**Before (PyTorch):**\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass SimpleModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n```\n\n**After (Lightning):**\n```python\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass LightningModel(pl.LightningModule):\n    def __init__(self, input_size, hidden_size, num_classes, learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        acc = (y_hat.argmax(dim=1) == y).float().mean()\n        self.log('val_loss', loss)\n        self.log('val_acc', acc)\n        \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n```\n\n### Step 2: Replace Training Loop with Trainer\n\n**Before (PyTorch):**\n```python\n# Training loop\nmodel = SimpleModel(input_size=784, hidden_size=128, num_classes=10)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        \n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        \n        if batch_idx % 100 == 0:\n            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}')\n    \n    # Validation\n    model.eval()\n    val_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in val_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            val_loss += criterion(output, target).item()\n            pred = output.argmax(dim=1)\n            correct += pred.eq(target).sum().item()\n    \n    print(f'Validation Loss: {val_loss/len(val_loader)}, Accuracy: {correct/len(val_dataset)}')\n```\n\n**After (Lightning):**\n```python\n# Training with Lightning\nmodel = LightningModel(input_size=784, hidden_size=128, num_classes=10)\ntrainer = pl.Trainer(max_epochs=10, accelerator='auto', devices='auto')\ntrainer.fit(model, train_loader, val_loader)\n```\n\n## Code Examples\n\n### Complete MNIST Example\n\n```python\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\n\nclass MNISTDataModule(pl.LightningDataModule):\n    def __init__(self, data_dir: str = \"data/\", batch_size: int = 64):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n\n    def prepare_data(self):\n        # Download data\n        datasets.MNIST(self.data_dir, train=True, download=True)\n        datasets.MNIST(self.data_dir, train=False, download=True)\n\n    def setup(self, stage: str):\n        # Assign train/val datasets\n        if stage == 'fit':\n            mnist_full = datasets.MNIST(\n                self.data_dir, train=True, transform=self.transform\n            )\n            self.mnist_train, self.mnist_val = random_split(\n                mnist_full, [55000, 5000]\n            )\n\n        if stage == 'test':\n            self.mnist_test = datasets.MNIST(\n                self.data_dir, train=False, transform=self.transform\n            )\n\n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=self.batch_size, shuffle=True)\n\n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n\n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=self.batch_size)\n\nclass MNISTClassifier(pl.LightningModule):\n    def __init__(self, learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        self.log(\"train_loss\", loss, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = torch.sum(preds == y).float() / len(y)\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = torch.sum(preds == y).float() / len(y)\n        self.log(\"test_loss\", loss)\n        self.log(\"test_acc\", acc)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n\n# Usage\nif __name__ == \"__main__\":\n    # Initialize data module and model\n    dm = MNISTDataModule()\n    model = MNISTClassifier()\n    \n    # Initialize trainer\n    trainer = pl.Trainer(\n        max_epochs=5,\n        accelerator='auto',\n        devices='auto',\n        logger=pl.loggers.TensorBoardLogger('lightning_logs/'),\n        callbacks=[\n            pl.callbacks.EarlyStopping(monitor='val_loss', patience=3),\n            pl.callbacks.ModelCheckpoint(monitor='val_loss', save_top_k=1)\n        ]\n    )\n    \n    # Train the model\n    trainer.fit(model, dm)\n    \n    # Test the model\n    trainer.test(model, dm)\n```\n\n### Advanced Model with Custom Metrics\n\n```python\nimport torchmetrics\n\nclass AdvancedClassifier(pl.LightningModule):\n    def __init__(self, num_classes=10, learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Model layers\n        self.model = nn.Sequential(\n            nn.Linear(784, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, num_classes)\n        )\n        \n        # Metrics\n        self.train_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n        self.val_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n        self.val_f1 = torchmetrics.F1Score(task='multiclass', num_classes=num_classes)\n        \n    def forward(self, x):\n        return self.model(x.view(x.size(0), -1))\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        \n        # Log metrics\n        self.train_accuracy(y_hat, y)\n        self.log('train_loss', loss, on_step=True, on_epoch=True)\n        self.log('train_acc', self.train_accuracy, on_step=True, on_epoch=True)\n        \n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        \n        # Update metrics\n        self.val_accuracy(y_hat, y)\n        self.val_f1(y_hat, y)\n        \n        # Log metrics\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', self.val_accuracy, prog_bar=True)\n        self.log('val_f1', self.val_f1, prog_bar=True)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(\n            self.parameters(), \n            lr=self.hparams.learning_rate,\n            weight_decay=1e-4\n        )\n        \n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer, T_max=100\n        )\n        \n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"monitor\": \"val_loss\",\n                \"frequency\": 1\n            }\n        }\n```\n\n## Advanced Features\n\n### 1. Multiple Optimizers and Schedulers\n\n```python\ndef configure_optimizers(self):\n    # Different learning rates for different parts\n    opt_g = torch.optim.Adam(self.generator.parameters(), lr=0.0002)\n    opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=0.0002)\n    \n    # Different schedulers\n    sch_g = torch.optim.lr_scheduler.StepLR(opt_g, step_size=50, gamma=0.5)\n    sch_d = torch.optim.lr_scheduler.StepLR(opt_d, step_size=50, gamma=0.5)\n    \n    return [opt_g, opt_d], [sch_g, sch_d]\n```\n\n### 2. Custom Callbacks\n\n```python\nclass CustomCallback(pl.Callback):\n    def on_train_epoch_end(self, trainer, pl_module):\n        # Custom logic at end of each epoch\n        if trainer.current_epoch % 10 == 0:\n            print(f\"Completed {trainer.current_epoch} epochs\")\n    \n    def on_validation_end(self, trainer, pl_module):\n        # Custom validation logic\n        val_loss = trainer.callback_metrics.get('val_loss')\n        if val_loss and val_loss < 0.1:\n            print(\"Excellent validation performance!\")\n\n# Usage\ntrainer = pl.Trainer(\n    callbacks=[CustomCallback(), pl.callbacks.EarlyStopping(monitor='val_loss')]\n)\n```\n\n### 3. Manual Optimization\n\n```python\nclass ManualOptimizationModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.automatic_optimization = False\n        # ... model definition\n        \n    def training_step(self, batch, batch_idx):\n        opt = self.optimizers()\n        \n        # Manual optimization\n        opt.zero_grad()\n        loss = self.compute_loss(batch)\n        self.manual_backward(loss)\n        opt.step()\n        \n        self.log('train_loss', loss)\n        return loss\n```\n\n## Best Practices\n\n### 1. Hyperparameter Management\n```python\nclass ConfigurableModel(pl.LightningModule):\n    def __init__(self, **kwargs):\n        super().__init__()\n        # Save all hyperparameters\n        self.save_hyperparameters()\n        \n        # Access with self.hparams\n        self.model = self._build_model()\n        \n    def _build_model(self):\n        return nn.Sequential(\n            nn.Linear(self.hparams.input_size, self.hparams.hidden_size),\n            nn.ReLU(),\n            nn.Linear(self.hparams.hidden_size, self.hparams.num_classes)\n        )\n```\n\n### 2. Proper Logging\n```python\ndef training_step(self, batch, batch_idx):\n    loss = self.compute_loss(batch)\n    \n    # Log to both step and epoch\n    self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n    \n    # Log learning rate\n    self.log('lr', self.trainer.optimizers[0].param_groups[0]['lr'])\n    \n    return loss\n```\n\n### 3. Model Organization\n```python\n# Separate model definition from Lightning logic\nclass ResNetBackbone(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        # Model architecture here\n        \nclass ResNetLightning(pl.LightningModule):\n    def __init__(self, num_classes, learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Use separate model class\n        self.model = ResNetBackbone(num_classes)\n        \n    def forward(self, x):\n        return self.model(x)\n    \n    # Training logic here...\n```\n\n### 4. Testing and Validation\n```python\ndef validation_step(self, batch, batch_idx):\n    # Always include validation metrics\n    loss, acc = self._shared_eval_step(batch, batch_idx)\n    self.log_dict({'val_loss': loss, 'val_acc': acc}, prog_bar=True)\n    \ndef test_step(self, batch, batch_idx):\n    # Comprehensive test metrics\n    loss, acc = self._shared_eval_step(batch, batch_idx)\n    self.log_dict({'test_loss': loss, 'test_acc': acc})\n    \ndef _shared_eval_step(self, batch, batch_idx):\n    x, y = batch\n    y_hat = self(x)\n    loss = F.cross_entropy(y_hat, y)\n    acc = (y_hat.argmax(dim=1) == y).float().mean()\n    return loss, acc\n```\n\n## Common Pitfalls\n\n### 1. Device Management\n**Wrong:**\n```python\ndef training_step(self, batch, batch_idx):\n    x, y = batch\n    x = x.cuda()  # Don't manually move to device\n    y = y.cuda()\n    # ...\n```\n\n**Correct:**\n```python\ndef training_step(self, batch, batch_idx):\n    x, y = batch  # Lightning handles device placement\n    # ...\n```\n\n### 2. Gradient Accumulation\n**Wrong:**\n```python\ndef training_step(self, batch, batch_idx):\n    loss = self.compute_loss(batch)\n    loss.backward()  # Don't call backward manually\n    return loss\n```\n\n**Correct:**\n```python\ndef training_step(self, batch, batch_idx):\n    loss = self.compute_loss(batch)\n    return loss  # Lightning handles backward pass\n```\n\n### 3. Metric Computation\n**Wrong:**\n```python\ndef validation_step(self, batch, batch_idx):\n    # Computing metrics inside step leads to incorrect averages\n    acc = compute_accuracy(batch)\n    self.log('val_acc', acc.mean())\n```\n\n**Correct:**\n```python\ndef __init__(self):\n    super().__init__()\n    self.val_acc = torchmetrics.Accuracy()\n\ndef validation_step(self, batch, batch_idx):\n    # Let torchmetrics handle the averaging\n    y_hat = self(x)\n    self.val_acc(y_hat, y)\n    self.log('val_acc', self.val_acc)\n```\n\n### 4. DataLoader in Model\n**Wrong:**\n```python\nclass Model(pl.LightningModule):\n    def train_dataloader(self):\n        # Don't put data loading in model\n        return DataLoader(...)\n```\n\n**Correct:**\n```python\n# Use separate DataModule\nclass DataModule(pl.LightningDataModule):\n    def train_dataloader(self):\n        return DataLoader(...)\n\n# Or pass dataloaders to trainer.fit()\ntrainer.fit(model, train_dataloader, val_dataloader)\n```\n\n## Migration Checklist\n\n- [ ] Convert model class to inherit from `pl.LightningModule`\n- [ ] Implement required methods: `training_step`, `configure_optimizers`\n- [ ] Add `validation_step` if you have validation data\n- [ ] Replace manual training loop with `pl.Trainer`\n- [ ] Move data loading to `pl.LightningDataModule` or separate functions\n- [ ] Add proper logging with `self.log()`\n- [ ] Use `self.save_hyperparameters()` for configuration\n- [ ] Add callbacks for checkpointing, early stopping, etc.\n- [ ] Remove manual device management (CUDA calls)\n- [ ] Test with different accelerators (CPU, GPU, multi-GPU)\n- [ ] Update any custom metrics to use torchmetrics\n- [ ] Verify logging and experiment tracking works\n- [ ] Add proper test methods if needed\n\nBy following this guide, you should be able to successfully migrate your PyTorch code to PyTorch Lightning while maintaining all functionality and gaining the benefits of Lightning's structured approach.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}