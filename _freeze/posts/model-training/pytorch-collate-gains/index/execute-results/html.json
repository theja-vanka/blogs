{
  "hash": "d04db54c1fb640347114ab7faba3a87e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"PyTorch Collate Function Speed-Up Guide\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-06-01\"\ncategories: [code, tutorial, advanced]\nformat:\n  html:\n    code-fold: false\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# PyTorch Collate Function Speed-Up Guide\n![](pytorch.jpg)\n\n## Introduction\n\nThe collate function in PyTorch is a crucial component for optimizing data loading performance. It determines how individual samples are combined into batches, and custom implementations can significantly speed up training by reducing data preprocessing overhead and memory operations.\n\n## Default vs Custom Collate Functions\n\n### Default Behavior\n\n::: {#d55afc85 .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport time\nimport numpy as np\n\nclass SimpleDataset(Dataset):\n    def __init__(self, size=1000):\n        self.data = [torch.randn(3, 224, 224) for _ in range(size)]\n        self.labels = [torch.randint(0, 10, (1,)).item() for _ in range(size)]\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\n# Using default collate function\ndataset = SimpleDataset(1000)\ndefault_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Timing default collate\nstart_time = time.time()\nfor batch_idx, (data, labels) in enumerate(default_loader):\n    if batch_idx >= 10:  # Test first 10 batches\n        break\ndefault_time = time.time() - start_time\nprint(f\"Default collate time: {default_time:.4f} seconds\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDefault collate time: 0.0108 seconds\n```\n:::\n:::\n\n\n### Custom Collate Function - Basic Optimization\n\n::: {#e7feb8ad .cell execution_count=2}\n``` {.python .cell-code}\ndef fast_collate(batch):\n    \"\"\"Optimized collate function for image data\"\"\"\n    # Separate data and labels\n    data, labels = zip(*batch)\n    \n    # Stack tensors directly (faster than default_collate for large tensors)\n    data_tensor = torch.stack(data, dim=0)\n    labels_tensor = torch.tensor(labels, dtype=torch.long)\n    \n    return data_tensor, labels_tensor\n\n# Using custom collate function\ncustom_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=fast_collate)\n\n# Timing custom collate\nstart_time = time.time()\nfor batch_idx, (data, labels) in enumerate(custom_loader):\n    if batch_idx >= 10:\n        break\ncustom_time = time.time() - start_time\nprint(f\"Custom collate time: {custom_time:.4f} seconds\")\nprint(f\"Speed improvement: {(default_time/custom_time - 1) * 100:.1f}%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCustom collate time: 0.0084 seconds\nSpeed improvement: 27.6%\n```\n:::\n:::\n\n\n## Advanced Optimizations\n\n### Memory-Efficient Collate for Variable-Length Sequences\n\n::: {#b9fdbb37 .cell execution_count=3}\n``` {.python .cell-code}\nimport torch.nn.utils.rnn as rnn_utils\n\nclass VariableLengthDataset(Dataset):\n    def __init__(self, size=1000):\n        # Simulate variable-length sequences\n        self.data = [torch.randn(np.random.randint(10, 100), 128) for _ in range(size)]\n        self.labels = [torch.randint(0, 5, (1,)).item() for _ in range(size)]\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\ndef efficient_variable_collate(batch):\n    \"\"\"Efficient collate for variable-length sequences\"\"\"\n    data, labels = zip(*batch)\n    \n    # Get sequence lengths for efficient packing\n    lengths = torch.tensor([len(seq) for seq in data])\n    \n    # Pad sequences efficiently\n    padded_data = rnn_utils.pad_sequence(data, batch_first=True, padding_value=0)\n    labels_tensor = torch.tensor(labels, dtype=torch.long)\n    \n    return padded_data, labels_tensor, lengths\n\n# Performance comparison\nvar_dataset = VariableLengthDataset(500)\n\n# Default collate (will fail for variable lengths, so we'll use a naive approach)\ndef naive_variable_collate(batch):\n    data, labels = zip(*batch)\n    max_len = max(len(seq) for seq in data)\n    \n    # Inefficient padding\n    padded_data = []\n    for seq in data:\n        if len(seq) < max_len:\n            padded_seq = torch.cat([seq, torch.zeros(max_len - len(seq), seq.size(1))])\n        else:\n            padded_seq = seq\n        padded_data.append(padded_seq)\n    \n    return torch.stack(padded_data), torch.tensor(labels, dtype=torch.long)\n\n# Timing comparison\nnaive_loader = DataLoader(var_dataset, batch_size=16, collate_fn=naive_variable_collate)\nefficient_loader = DataLoader(var_dataset, batch_size=16, collate_fn=efficient_variable_collate)\n\n# Naive approach timing\nstart_time = time.time()\nfor batch_idx, batch in enumerate(naive_loader):\n    if batch_idx >= 10:\n        break\nnaive_time = time.time() - start_time\n\n# Efficient approach timing\nstart_time = time.time()\nfor batch_idx, batch in enumerate(efficient_loader):\n    if batch_idx >= 10:\n        break\nefficient_time = time.time() - start_time\n\nprint(f\"Naive variable collate time: {naive_time:.4f} seconds\")\nprint(f\"Efficient variable collate time: {efficient_time:.4f} seconds\")\nprint(f\"Speed improvement: {(naive_time/efficient_time - 1) * 100:.1f}%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNaive variable collate time: 0.0016 seconds\nEfficient variable collate time: 0.0018 seconds\nSpeed improvement: -13.3%\n```\n:::\n:::\n\n\n### GPU-Accelerated Collate Function\n\n```python\ndef gpu_accelerated_collate(batch, device='cuda'):\n    \"\"\"Collate function that moves data to GPU during batching\"\"\"\n    if not torch.cuda.is_available():\n        device = 'cpu'\n    \n    data, labels = zip(*batch)\n    \n    # Stack and move to GPU in one operation\n    data_tensor = torch.stack(data, dim=0).to(device, non_blocking=True)\n    labels_tensor = torch.tensor(labels, dtype=torch.long).to(device, non_blocking=True)\n    \n    return data_tensor, labels_tensor\n\n# Performance comparison with GPU transfer\nif torch.cuda.is_available():\n    device = 'cuda'\n    \n    # Standard approach: CPU collate + GPU transfer\n    standard_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n    \n    # GPU-accelerated collate\n    gpu_loader = DataLoader(dataset, batch_size=32, shuffle=True, \n                           collate_fn=lambda batch: gpu_accelerated_collate(batch, device))\n    \n    # Timing standard approach\n    start_time = time.time()\n    for batch_idx, (data, labels) in enumerate(standard_loader):\n        data, labels = data.to(device), labels.to(device)\n        if batch_idx >= 10:\n            break\n    standard_gpu_time = time.time() - start_time\n    \n    # Timing GPU-accelerated collate\n    start_time = time.time()\n    for batch_idx, (data, labels) in enumerate(gpu_loader):\n        if batch_idx >= 10:\n            break\n    gpu_collate_time = time.time() - start_time\n    \n    print(f\"Standard CPU->GPU time: {standard_gpu_time:.4f} seconds\")\n    print(f\"GPU-accelerated collate time: {gpu_collate_time:.4f} seconds\")\n    print(f\"Speed improvement: {(standard_gpu_time/gpu_collate_time - 1) * 100:.1f}%\")\n```\n\n### Memory-Mapped File Collate for Large Datasets\n\n```python\nimport mmap\n\nclass MemoryMappedDataset(Dataset):\n    \"\"\"Dataset using memory-mapped files for efficient large data loading\"\"\"\n    def __init__(self, data_array, labels_array):\n        self.data = data_array\n        self.labels = labels_array\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        # Return views instead of copies when possible\n        return torch.from_numpy(self.data[idx].copy()), self.labels[idx]\n\ndef zero_copy_collate(batch):\n    \"\"\"Zero-copy collate function for numpy arrays\"\"\"\n    data, labels = zip(*batch)\n    \n    # Use torch.from_numpy for zero-copy conversion when possible\n    try:\n        # Stack numpy arrays first, then convert to tensor\n        data_array = np.stack([d.numpy() if isinstance(d, torch.Tensor) else d for d in data])\n        data_tensor = torch.from_numpy(data_array)\n    except:\n        # Fallback to regular stacking\n        data_tensor = torch.stack(data)\n    \n    labels_tensor = torch.tensor(labels, dtype=torch.long)\n    return data_tensor, labels_tensor\n\n# Create sample data for demonstration\nsample_data = np.random.randn(1000, 3, 224, 224).astype(np.float32)\nsample_labels = np.random.randint(0, 10, 1000)\n\nmmap_dataset = MemoryMappedDataset(sample_data, sample_labels)\nmmap_loader = DataLoader(mmap_dataset, batch_size=32, collate_fn=zero_copy_collate)\n```\n\n## Specialized Collate Functions\n\n### Multi-Modal Data Collate\n\n```python\nclass MultiModalDataset(Dataset):\n    def __init__(self, size=1000):\n        self.images = [torch.randn(3, 224, 224) for _ in range(size)]\n        self.text = [torch.randint(0, 1000, (np.random.randint(5, 50),)) for _ in range(size)]\n        self.labels = [torch.randint(0, 10, (1,)).item() for _ in range(size)]\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        return {\n            'image': self.images[idx],\n            'text': self.text[idx],\n            'label': self.labels[idx]\n        }\n\ndef multimodal_collate(batch):\n    \"\"\"Efficient collate for multi-modal data\"\"\"\n    # Separate different modalities\n    images = [item['image'] for item in batch]\n    texts = [item['text'] for item in batch]\n    labels = [item['label'] for item in batch]\n    \n    # Batch images\n    image_batch = torch.stack(images)\n    \n    # Batch variable-length text with padding\n    text_lengths = torch.tensor([len(text) for text in texts])\n    text_batch = rnn_utils.pad_sequence(texts, batch_first=True, padding_value=0)\n    \n    # Batch labels\n    label_batch = torch.tensor(labels, dtype=torch.long)\n    \n    return {\n        'images': image_batch,\n        'texts': text_batch,\n        'text_lengths': text_lengths,\n        'labels': label_batch\n    }\n\nmultimodal_dataset = MultiModalDataset(500)\nmultimodal_loader = DataLoader(multimodal_dataset, batch_size=16, collate_fn=multimodal_collate)\n\n# Test the multimodal loader\nsample_batch = next(iter(multimodal_loader))\nprint(\"Multimodal batch shapes:\")\nfor key, value in sample_batch.items():\n    print(f\"  {key}: {value.shape}\")\n```\n\n### Augmentation-Aware Collate\n\n```python\nimport torchvision.transforms as transforms\n\ndef augmentation_collate(batch, transform=None):\n    \"\"\"Collate function that applies augmentations during batching\"\"\"\n    data, labels = zip(*batch)\n    \n    if transform:\n        # Apply augmentations during collation\n        augmented_data = [transform(img) for img in data]\n        data_tensor = torch.stack(augmented_data)\n    else:\n        data_tensor = torch.stack(data)\n    \n    labels_tensor = torch.tensor(labels, dtype=torch.long)\n    return data_tensor, labels_tensor\n\n# Define augmentation pipeline\naugment_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2)\n])\n\n# Create collate function with augmentation\naug_collate_fn = lambda batch: augmentation_collate(batch, augment_transform)\n\naug_loader = DataLoader(dataset, batch_size=32, collate_fn=aug_collate_fn)\n```\n\n## Performance Tips and Best Practices\n\n### 1. Minimize Data Copying\n\n```python\ndef efficient_collate_tips(batch):\n    \"\"\"Demonstrates efficient collate practices\"\"\"\n    data, labels = zip(*batch)\n    \n    # TIP 1: Use torch.stack instead of torch.cat when possible\n    # torch.stack is faster for same-sized tensors\n    data_tensor = torch.stack(data, dim=0)  # Faster\n    # data_tensor = torch.cat([d.unsqueeze(0) for d in data], dim=0)  # Slower\n    \n    # TIP 2: Use appropriate dtypes to save memory\n    labels_tensor = torch.tensor(labels, dtype=torch.long)  # Use long for indices\n    \n    # TIP 3: Pre-allocate tensors when size is known\n    # This is more relevant for complex batching scenarios\n    \n    return data_tensor, labels_tensor\n```\n\n### 2. Memory Usage Optimization\n\n```python\ndef memory_efficient_collate(batch):\n    \"\"\"Memory-efficient collate function\"\"\"\n    data, labels = zip(*batch)\n    \n    # Pre-allocate output tensor to avoid multiple allocations\n    batch_size = len(data)\n    data_shape = data[0].shape\n    \n    # Allocate output tensor once\n    output_tensor = torch.empty((batch_size,) + data_shape, dtype=data[0].dtype)\n    \n    # Fill the tensor in-place\n    for i, tensor in enumerate(data):\n        output_tensor[i] = tensor\n    \n    labels_tensor = torch.tensor(labels, dtype=torch.long)\n    return output_tensor, labels_tensor\n```\n\n### 3. Benchmarking Your Collate Functions\n\n```python\ndef benchmark_collate_functions():\n    \"\"\"Comprehensive benchmarking of different collate approaches\"\"\"\n    dataset = SimpleDataset(1000)\n    batch_size = 32\n    num_batches = 20\n    \n    collate_functions = {\n        'default': None,\n        'fast_collate': fast_collate,\n        'efficient_tips': efficient_collate_tips,\n        'memory_efficient': memory_efficient_collate\n    }\n    \n    results = {}\n    \n    for name, collate_fn in collate_functions.items():\n        loader = DataLoader(dataset, batch_size=batch_size, \n                          collate_fn=collate_fn, shuffle=True)\n        \n        start_time = time.time()\n        for batch_idx, (data, labels) in enumerate(loader):\n            if batch_idx >= num_batches:\n                break\n        \n        elapsed_time = time.time() - start_time\n        results[name] = elapsed_time\n        print(f\"{name}: {elapsed_time:.4f} seconds\")\n    \n    # Calculate improvements\n    baseline = results['default']\n    for name, time_taken in results.items():\n        if name != 'default':\n            improvement = (baseline / time_taken - 1) * 100\n            print(f\"{name} improvement: {improvement:.1f}%\")\n\n# Run the benchmark\nbenchmark_collate_functions()\n```\n\n## Key Takeaways\n\n1. **Use `torch.stack()` instead of `torch.cat()`** for same-sized tensors\n2. **Minimize data copying** by working with tensor views when possible\n3. **Pre-allocate tensors** when batch sizes and shapes are known\n4. **Consider GPU transfer** during collation for better pipeline efficiency\n5. **Use appropriate data types** to optimize memory usage\n6. **Profile your specific use case** as optimal strategies vary by data type and size\n7. **Leverage specialized functions** like `pad_sequence` for variable-length data\n\nCustom collate functions can provide significant performance improvements, especially for large datasets or complex data structures. The key is to minimize unnecessary data operations and memory allocations while taking advantage of PyTorch's optimized tensor operations.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}