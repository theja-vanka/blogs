{
  "hash": "e6af844b2c95bce8ccc9d3297785e673",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"PyTorch Lightning: A Comprehensive Guide\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-03-29\"\ncategories: [code, tutorial, advanced]\nformat:\n  html:\n    code-fold: false\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# âš¡ PyTorch Lightning: A Comprehensive Guide\n\n![](pytl.png)\n\nPyTorch Lightning is a lightweight wrapper for PyTorch that helps organize code and reduce boilerplate while adding powerful features for research and production. This guide will walk you through the basics to advanced techniques.\n\n## Introduction to PyTorch Lightning\n\nPyTorch Lightning separates research code from engineering code, making models more:\n\n- **Reproducible**: The same code works across different hardware\n- **Readable**: Standard project structure makes collaboration easier\n- **Scalable**: Train on CPUs, GPUs, TPUs or clusters with no code changes\n\nLightning helps you focus on the science by handling the engineering details.\n\n## Installation\n\n```bash\npip install pytorch-lightning\n```\n\nFor the latest features, you can install from the source:\n\n```bash\npip install git+https://github.com/Lightning-AI/lightning.git\n```\n\n## Basic Structure: The LightningModule\n\nThe core component in Lightning is the `LightningModule`, which organizes your PyTorch code into a standardized structure:\n\n```python\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MNISTModel(pl.LightningModule):\n    def __init__(self, lr=0.001):\n        super().__init__()\n        self.save_hyperparameters()  # Saves learning_rate to hparams\n        self.layer_1 = nn.Linear(28 * 28, 128)\n        self.layer_2 = nn.Linear(128, 10)\n        self.lr = lr\n        \n    def forward(self, x):\n        batch_size, channels, width, height = x.size()\n        x = x.view(batch_size, -1)\n        x = self.layer_1(x)\n        x = F.relu(x)\n        x = self.layer_2(x)\n        return x\n        \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        self.log('train_loss', loss)\n        return loss\n        \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        self.log('val_loss', loss)\n        self.log('val_acc', acc)\n        \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        self.log('test_loss', loss)\n        self.log('test_acc', acc)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer\n```\n\nThis basic structure includes:\n\n- Model architecture (`__init__` and `forward`)\n- Training logic (`training_step`)\n- Validation logic (`validation_step`)\n- Test logic (`test_step`)\n- Optimization setup (`configure_optimizers`)\n\n## DataModules\n\nLightning's `LightningDataModule` encapsulates all data-related logic:\n\n```python\nfrom pytorch_lightning import LightningDataModule\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\n\nclass MNISTDataModule(LightningDataModule):\n    def __init__(self, data_dir='./data', batch_size=32):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n        \n    def prepare_data(self):\n        # Download data if needed\n        MNIST(self.data_dir, train=True, download=True)\n        MNIST(self.data_dir, train=False, download=True)\n        \n    def setup(self, stage=None):\n        # Assign train/val/test datasets\n        if stage == 'fit' or stage is None:\n            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n            \n        if stage == 'test' or stage is None:\n            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n            \n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=self.batch_size)\n        \n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n        \n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=self.batch_size)\n```\n\nBenefits of `LightningDataModule`:\n\n- Encapsulates all data preparation logic\n- Makes data pipeline portable and reproducible\n- Simplifies sharing data pipelines between projects\n\n## Training with Trainer\n\nThe Lightning `Trainer` handles the training loop and validation:\n\n```python\nfrom pytorch_lightning import Trainer\n\n# Create model and data module\nmodel = MNISTModel()\ndata_module = MNISTDataModule()\n\n# Initialize trainer\ntrainer = Trainer(\n    max_epochs=10,\n    accelerator=\"auto\",  # Automatically use available GPU\n    devices=\"auto\",\n    logger=True,         # Use TensorBoard logger by default\n)\n\n# Train model\ntrainer.fit(model, datamodule=data_module)\n\n# Test model\ntrainer.test(model, datamodule=data_module)\n```\n\nThe `Trainer` automatically handles:\n\n- Epoch and batch iteration\n- Optimizer steps\n- Logging metrics\n- Hardware acceleration (CPU, GPU, TPU)\n- Early stopping\n- Checkpointing\n- Multi-GPU training\n\n### Common Trainer Parameters\n\n```python\ntrainer = Trainer(\n    max_epochs=10,                    # Maximum number of epochs\n    min_epochs=1,                     # Minimum number of epochs\n    accelerator=\"cpu\",                # Use CPU acceleration\n    devices=1,                        # Use 1 CPUs\n    precision=\"16-mixed\",             # Use mixed precision for faster training\n    gradient_clip_val=0.5,            # Clip gradients\n    accumulate_grad_batches=4,        # Accumulate gradients over 4 batches\n    log_every_n_steps=50,             # Log metrics every 50 steps\n    val_check_interval=0.25,          # Run validation 4 times per epoch\n    fast_dev_run=False,               # Debug mode (only run a few batches)\n    deterministic=True,               # Make training deterministic\n)\n```\n\n## Callbacks\n\nCallbacks add functionality to the training loop without modifying the core code:\n\n```python\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n\n# Save the best model based on validation accuracy\ncheckpoint_callback = ModelCheckpoint(\n    monitor='val_acc',\n    dirpath='./checkpoints',\n    filename='mnist-{epoch:02d}-{val_acc:.2f}',\n    save_top_k=3,\n    mode='max',\n)\n\n# Stop training when validation loss stops improving\nearly_stop_callback = EarlyStopping(\n    monitor='val_loss',\n    patience=3,\n    verbose=True,\n    mode='min'\n)\n\n# Monitor learning rate\nlr_monitor = LearningRateMonitor(logging_interval='step')\n\n# Initialize trainer with callbacks\ntrainer = Trainer(\n    max_epochs=10,\n    callbacks=[checkpoint_callback, early_stop_callback, lr_monitor]\n)\n```\n\n### Custom Callback\n\nYou can create custom callbacks by extending the base `Callback` class:\n\n```python\nfrom pytorch_lightning.callbacks import Callback\n\nclass PrintingCallback(Callback):\n    def on_train_start(self, trainer, pl_module):\n        print(\"Training has started!\")\n        \n    def on_train_end(self, trainer, pl_module):\n        print(\"Training has finished!\")\n        \n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        if batch_idx % 100 == 0:\n            print(f\"Batch {batch_idx} completed\")\n```\n\n## Logging\n\nLightning supports various loggers:\n\n```python\nfrom pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n\n# TensorBoard logger\ntensorboard_logger = TensorBoardLogger(save_dir=\"logs/\", name=\"mnist\")\n\n# Initialize trainer with loggers\ntrainer = Trainer(\n    max_epochs=10,\n    logger=[tensorboard_logger]\n)\n```\n\nAdding metrics in your model:\n\n```python\ndef training_step(self, batch, batch_idx):\n    x, y = batch\n    logits = self(x)\n    loss = F.cross_entropy(logits, y)\n    \n    # Log scalar values\n    self.log('train_loss', loss)\n    \n    # Log learning rate\n    lr = self.optimizers().param_groups[0]['lr']\n    self.log('learning_rate', lr)\n    \n    # Log histograms (on GPU)\n    if batch_idx % 100 == 0:\n        self.logger.experiment.add_histogram('logits', logits, self.global_step)\n    \n    return loss\n```\n\n## Distributed Training\n\nLightning makes distributed training simple:\n\n```python\n# Single GPU\ntrainer = Trainer(accelerator=\"gpu\", devices=1)\n\n# Multiple GPUs (DDP strategy automatically selected)\ntrainer = Trainer(accelerator=\"gpu\", devices=4)\n\n# Specific GPU indices\ntrainer = Trainer(accelerator=\"gpu\", devices=[0, 1, 3])\n\n# TPU cores\ntrainer = Trainer(accelerator=\"tpu\", devices=8)\n\n# Advanced distributed training\ntrainer = Trainer(\n    accelerator=\"gpu\",\n    devices=4,\n    strategy=\"ddp\",  # Distributed Data Parallel\n    num_nodes=2      # Use 2 machines\n)\n```\n\nOther distribution strategies:\n\n- `ddp_spawn`: Similar to DDP but uses spawn for multiprocessing\n- `deepspeed`: For very large models using DeepSpeed\n- `fsdp`: Fully Sharded Data Parallel for huge models\n\n## Hyperparameter Tuning\n\nLightning integrates well with optimization libraries:\n\n```python\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.tuner import Tuner\n\n# Create model\nmodel = MNISTModel()\ndata_module = MNISTDataModule()\n\n# Create trainer\ntrainer = Trainer(max_epochs=10)\n\n# Use Lightning's Tuner for auto-scaling batch size\ntuner = Tuner(trainer)\ntuner.scale_batch_size(model, datamodule=data_module)\n\n# Find optimal learning rate\nlr_finder = tuner.lr_find(model, datamodule=data_module)\nmodel.learning_rate = lr_finder.suggestion()\n\n# Train with optimized parameters\ntrainer.fit(model, datamodule=data_module)\n```\n\n## Model Checkpointing\n\nSaving and loading model checkpoints:\n\n```python\n# Save checkpoints during training\ncheckpoint_callback = ModelCheckpoint(\n    dirpath='./checkpoints',\n    filename='{epoch}-{val_loss:.2f}',\n    monitor='val_loss',\n    save_top_k=3,\n    mode='min'\n)\n\ntrainer = Trainer(\n    max_epochs=10,\n    callbacks=[checkpoint_callback]\n)\n\ntrainer.fit(model, datamodule=data_module)\n\n# Path to best checkpoint\nbest_model_path = checkpoint_callback.best_model_path\n\n# Load checkpoint into model\nmodel = MNISTModel.load_from_checkpoint(best_model_path)\n\n# Continue training from checkpoint\ntrainer.fit(model, datamodule=data_module)\n```\n\nCheckpointing for production:\n\n```python\n# Save model in production-ready format\ntrainer.save_checkpoint(\"model.ckpt\")\n\n# Extract state dict for production\ncheckpoint = torch.load(\"model.ckpt\")\nmodel_state_dict = checkpoint[\"state_dict\"]\n\n# Save just the PyTorch model for production\nmodel = MNISTModel()\nmodel.load_state_dict(model_state_dict)\ntorch.save(model.state_dict(), \"production_model.pt\")\n```\n\n## Production Deployment\n\nConverting Lightning models to production:\n\n```python\n# Option 1: Use the Lightning model directly\nmodel = MNISTModel.load_from_checkpoint(\"model.ckpt\")\nmodel.eval()\nmodel.freeze()  # Freeze the model parameters\n\n# Make predictions\nwith torch.no_grad():\n    x = torch.randn(1, 1, 28, 28)\n    y_hat = model(x)\n\n# Option 2: Extract the PyTorch model\nclass ProductionModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(28 * 28, 128)\n        self.layer_2 = nn.Linear(128, 10)\n    \n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.layer_1(x))\n        x = self.layer_2(x)\n        return x\n\n# Load weights from Lightning model\nlightning_model = MNISTModel.load_from_checkpoint(\"model.ckpt\")\nproduction_model = ProductionModel()\n\n# Copy weights\nproduction_model.layer_1.weight.data = lightning_model.layer_1.weight.data\nproduction_model.layer_1.bias.data = lightning_model.layer_1.bias.data\nproduction_model.layer_2.weight.data = lightning_model.layer_2.weight.data\nproduction_model.layer_2.bias.data = lightning_model.layer_2.bias.data\n\n# Save production model\ntorch.save(production_model.state_dict(), \"production_model.pt\")\n\n# Export to ONNX\ndummy_input = torch.randn(1, 1, 28, 28)\ntorch.onnx.export(\n    production_model,\n    dummy_input,\n    \"model.onnx\",\n    export_params=True,\n    opset_version=11,\n    input_names=['input'],\n    output_names=['output']\n)\n```\n\n## Best Practices\n\n### Code Organization\n\nA well-organized Lightning project structure:\n\n```\nproject/\nâ”œâ”€â”€ configs/              # Configuration files\nâ”œâ”€â”€ data/                 # Data files\nâ”œâ”€â”€ lightning_logs/       # Generated logs\nâ”œâ”€â”€ models/               # Model definitions\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â””â”€â”€ mnist_model.py    # LightningModule\nâ”œâ”€â”€ data_modules/         # Data modules\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â””â”€â”€ mnist_data.py     # LightningDataModule\nâ”œâ”€â”€ callbacks/            # Custom callbacks\nâ”œâ”€â”€ utils/                # Utility functions\nâ”œâ”€â”€ main.py               # Training script\nâ””â”€â”€ README.md\n```\n\n### Lightning CLI\n\nLightning provides a CLI for running experiments from config files:\n\n```python\n# main.py\nfrom pytorch_lightning.cli import LightningCLI\n\ndef cli_main():\n    # Create CLI with LightningModule and LightningDataModule\n    cli = LightningCLI(\n        MNISTModel,\n        MNISTDataModule,\n        save_config_callback=None,\n    )\n\nif __name__ == \"__main__\":\n    cli_main()\n```\n\nRun with:\n\n```bash\npython main.py fit --config configs/mnist.yaml\n```\n\nExample config file (mnist.yaml):\n\n```{yaml}\nmodel:\n  learning_rate: 0.001\n  hidden_size: 128\ndata:\n  data_dir: ./data\n  batch_size: 64\ntrainer:\n  max_epochs: 10\n  accelerator: gpu\n  devices: 1\n```\n\n### Profiling\n\nLightning includes tools to profile your code:\n\n```python\nfrom pytorch_lightning.profilers import PyTorchProfiler, SimpleProfiler\n\n# PyTorch Profiler\nprofiler = PyTorchProfiler(\n    on_trace_ready=torch.profiler.tensorboard_trace_handler(\"logs/profiler\"),\n    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2)\n)\n\n# Simple Profiler\n# profiler = SimpleProfiler()\nmodel = MNISTModel()\ndata_module = MNISTDataModule()\ntrainer = Trainer(\n    max_epochs=5,\n    profiler=profiler\n)\n\ntrainer.fit(model, datamodule=data_module)\n```\n\n### Advanced Features\n\n#### Gradient Accumulation\n\n```python\ntrainer = Trainer(\n    accumulate_grad_batches=4  # Accumulate over 4 batches\n)\n```\n\n#### Learning Rate Scheduling\n\n```python\ndef configure_optimizers(self):\n    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, \n        mode='min', \n        factor=0.1, \n        patience=5\n    )\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\n            \"scheduler\": scheduler,\n            \"monitor\": \"val_loss\",\n            \"interval\": \"epoch\",\n            \"frequency\": 1\n        }\n    }\n```\n\n#### Mixed Precision Training\n\n```python\ntrainer = Trainer(\n    precision=\"16-mixed\"  # Use 16-bit mixed precision\n)\n```\n\n#### Transfer Learning\n\n```python\nclass TransferLearningModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        # Load pretrained model\n        self.feature_extractor = torchvision.models.resnet18(pretrained=True)\n        \n        # Freeze feature extractor\n        for param in self.feature_extractor.parameters():\n            param.requires_grad = False\n            \n        # Replace final layer\n        num_features = self.feature_extractor.fc.in_features\n        self.feature_extractor.fc = nn.Linear(num_features, 10)\n        \n    def unfreeze_features(self):\n        # Unfreeze model after some training\n        for param in self.feature_extractor.parameters():\n            param.requires_grad = True\n```\n\n## Conclusion\n\nPyTorch Lightning provides an organized framework that separates research code from engineering boilerplate, making deep learning projects easier to develop, share, and scale. It's especially valuable for research projects that need to be reproducible and scalable across different hardware configurations.\n\nFor further information:\n\n- [PyTorch Lightning Documentation](https://lightning.ai/docs/pytorch/stable/)\n- [Lightning GitHub Repository](https://github.com/Lightning-AI/lightning)\n- [Lightning Tutorials](https://lightning.ai/docs/pytorch/stable/tutorials.html)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}