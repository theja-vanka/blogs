{
  "hash": "8fd362582f80b21cca52a70dc8fee191",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Albumentations vs TorchVision Transforms: Complete Code Guide\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-05-27\"\ncategories: [code, tutorial, beginner]\nformat:\n  html:\n    code-fold: false\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# Albumentations vs TorchVision Transforms: Complete Code Guide\n\n![](albumentations.png)\n\n## Overview\n\nThis guide compares two popular image augmentation libraries for PyTorch:\n\n- **TorchVision Transforms**: Built-in PyTorch library for basic image transformations\n- **Albumentations**: Fast, flexible library with advanced augmentation techniques\n\n## Installation\n\n```bash\n# TorchVision (comes with PyTorch)\npip install torch torchvision\n\n# Albumentations\npip install albumentations\n```\n\n## Basic Setup and Imports\n\n::: {#a115ff4a .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport torchvision.transforms as T\nfrom torchvision.transforms import functional as TF\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n## Key Differences at a Glance\n\n| Feature | TorchVision | Albumentations |\n|---------|-------------|----------------|\n| **Input Format** | PIL Image, Tensor | NumPy array (OpenCV format) |\n| **Performance** | Moderate | Fast (optimized) |\n| **Augmentation Variety** | Basic to intermediate | Extensive advanced options |\n| **Bounding Box Support** | Limited | Excellent |\n| **Segmentation Masks** | Basic | Advanced |\n| **Keypoint Support** | No | Yes |\n| **Probability Control** | Limited | Fine-grained |\n\n## Basic Transformations Comparison\n\n### Image Loading and Format Differences\n\n::: {#d3c47867 .cell execution_count=2}\n``` {.python .cell-code}\n# TorchVision approach\ndef load_image_torchvision(path):\n    return Image.open(path).convert('RGB')\n\n# Albumentations approach  \ndef load_image_albumentations(path):\n    image = cv2.imread(path)\n    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# Example usage\nimg_path = \"cat.jpg\"\ntorch_img = load_image_torchvision(img_path)\nalbu_img = load_image_albumentations(img_path)\n```\n:::\n\n\n![](cat.jpg)\n\n### Basic Augmentations\n\n::: {#c0dc26eb .cell execution_count=3}\n``` {.python .cell-code}\n# TorchVision transforms\ntorchvision_transform = T.Compose([\n    T.Resize((224, 224)),\n    T.RandomHorizontalFlip(p=0.5),\n    T.RandomRotation(degrees=15),\n    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Albumentations equivalent\nalbumentations_transform = A.Compose([\n    A.Resize(224, 224),\n    A.HorizontalFlip(p=0.5),\n    A.Rotate(limit=15, p=1.0),\n    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=1.0),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2()\n])\n\n# Apply transforms\ntorch_result = torchvision_transform(torch_img)\nalbu_result = albumentations_transform(image=albu_img)['image']\n```\n:::\n\n\n## Advanced Augmentations\n\n### Albumentations Exclusive Features\n\n```python\n# Advanced geometric transformations\nadvanced_geometric = A.Compose([\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30, p=0.8),\n    A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.3),\n    A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3),\n    A.OpticalDistortion(distort_limit=0.5, shift_limit=0.5, p=0.3)\n])\n\n# Weather and lighting effects\nweather_effects = A.Compose([\n    A.RandomRain(slant_lower=-10, slant_upper=10, drop_length=20, p=0.3),\n    A.RandomSnow(snow_point_lower=0.1, snow_point_upper=0.3, p=0.3),\n    A.RandomFog(fog_coef_lower=0.3, fog_coef_upper=1, p=0.3),\n    A.RandomSunFlare(flare_roi=(0, 0, 1, 0.5), angle_lower=0, p=0.3)\n])\n\n# Noise and blur effects\nnoise_blur = A.Compose([\n    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n    A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=0.3),\n    A.MotionBlur(blur_limit=7, p=0.3),\n    A.MedianBlur(blur_limit=7, p=0.3),\n    A.Blur(blur_limit=7, p=0.3)\n])\n```\n\n### TorchVision v2 Enhanced Features\n\n```python\nimport torchvision.transforms.v2 as T2\n\n# TorchVision v2 with better functionality\ntorchvision_v2_transform = T2.Compose([\n    T2.Resize((224, 224)),\n    T2.RandomHorizontalFlip(p=0.5),\n    T2.RandomChoice([\n        T2.ColorJitter(brightness=0.3),\n        T2.ColorJitter(contrast=0.3),\n        T2.ColorJitter(saturation=0.3)\n    ]),\n    T2.RandomApply([T2.GaussianBlur(kernel_size=3)], p=0.3),\n    T2.ToTensor(),\n    T2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n```\n\n## Working with Bounding Boxes\n\n### Albumentations (Excellent Support)\n\n```python\n# Define bounding boxes in Pascal VOC format (x_min, y_min, x_max, y_max)\nbboxes = [[50, 50, 150, 150, 'person'], [200, 100, 300, 200, 'car']]\n\nbbox_transform = A.Compose([\n    A.Resize(416, 416),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.3),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),\n], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n\n# Apply transform\ntransformed = bbox_transform(\n    image=image, \n    bboxes=[[50, 50, 150, 150], [200, 100, 300, 200]], \n    class_labels=['person', 'car']\n)\n\ntransformed_image = transformed['image']\ntransformed_bboxes = transformed['bboxes']\ntransformed_labels = transformed['class_labels']\n```\n\n### TorchVision (Limited Support)\n\n```python\n# TorchVision v2 has some bbox support\nimport torchvision.transforms.v2 as T2\n\nbbox_torchvision = T2.Compose([\n    T2.Resize((416, 416)),\n    T2.RandomHorizontalFlip(p=0.5),\n    T2.ToTensor()\n])\n\n# Requires manual handling of bounding boxes\n# Less intuitive than Albumentations\n```\n\n## Working with Segmentation Masks\n\n### Albumentations\n\n```python\n# Segmentation mask handling\nsegmentation_transform = A.Compose([\n    A.Resize(512, 512),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.3),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),\n    A.Normalize(),\n    ToTensorV2()\n])\n\n# Apply to image and mask simultaneously\nresult = segmentation_transform(image=image, mask=mask)\ntransformed_image = result['image']\ntransformed_mask = result['mask']\n```\n\n### TorchVision\n\n```python\n# TorchVision requires separate handling\ndef apply_transform_to_mask(transform, image, mask):\n    # Manual synchronization needed\n    seed = torch.randint(0, 2**32, size=(1,)).item()\n    \n    torch.manual_seed(seed)\n    transformed_image = transform(image)\n    \n    torch.manual_seed(seed)\n    # Apply only geometric transforms to mask\n    mask_transform = T.Compose([\n        T.Resize((512, 512)),\n        T.RandomHorizontalFlip(p=0.5),\n        T.ToTensor()\n    ])\n    transformed_mask = mask_transform(mask)\n    \n    return transformed_image, transformed_mask\n```\n\n## Performance Comparison\n\n::: {#f70e8f88 .cell execution_count=4}\n``` {.python .cell-code}\nimport time\n\ndef benchmark_transforms(image, iterations=1000):\n    # TorchVision timing\n    start_time = time.time()\n    for _ in range(iterations):\n        _ = torchvision_transform(image.copy())\n    torch_time = time.time() - start_time\n    \n    # Convert to numpy for Albumentations\n    np_image = np.array(image)\n    \n    # Albumentations timing\n    start_time = time.time()\n    for _ in range(iterations):\n        _ = albumentations_transform(image=np_image.copy())\n    albu_time = time.time() - start_time\n    \n    print(f\"TorchVision: {torch_time:.3f}s ({iterations} iterations)\")\n    print(f\"Albumentations: {albu_time:.3f}s ({iterations} iterations)\")\n    print(f\"Speedup: {torch_time/albu_time:.2f}x\")\n\n# Run benchmark\nbenchmark_transforms(torch_img)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTorchVision: 37.706s (1000 iterations)\nAlbumentations: 2.810s (1000 iterations)\nSpeedup: 13.42x\n```\n:::\n:::\n\n\n## Custom Pipeline Examples\n\n### Data Science Pipeline with Albumentations\n\n```python\ndef create_training_pipeline():\n    return A.Compose([\n        # Geometric transformations\n        A.OneOf([\n            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30),\n            A.ElasticTransform(alpha=1, sigma=50),\n            A.GridDistortion(num_steps=5, distort_limit=0.3),\n        ], p=0.5),\n        \n        # Color augmentations\n        A.OneOf([\n            A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20),\n            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n        ], p=0.8),\n        \n        # Noise and blur\n        A.OneOf([\n            A.GaussNoise(var_limit=(10, 50)),\n            A.ISONoise(),\n            A.MultiplicativeNoise(),\n        ], p=0.3),\n        \n        A.OneOf([\n            A.MotionBlur(blur_limit=5),\n            A.MedianBlur(blur_limit=5),\n            A.GaussianBlur(blur_limit=5),\n        ], p=0.3),\n        \n        # Final processing\n        A.Resize(224, 224),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n\ndef create_validation_pipeline():\n    return A.Compose([\n        A.Resize(224, 224),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n```\n\n### TorchVision Pipeline for Simple Cases\n\n```python\ndef create_simple_training_pipeline():\n    return T.Compose([\n        T.Resize((224, 224)),\n        T.RandomHorizontalFlip(p=0.5),\n        T.RandomRotation(degrees=15),\n        T.RandomApply([T.ColorJitter(0.3, 0.3, 0.3, 0.1)], p=0.8),\n        T.RandomApply([T.GaussianBlur(kernel_size=3)], p=0.3),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\ndef create_simple_validation_pipeline():\n    return T.Compose([\n        T.Resize((224, 224)),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n```\n\n## Dataset Integration\n\n### PyTorch Dataset with Albumentations\n\n```python\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CustomDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Load image for Albumentations (OpenCV format)\n        image = cv2.imread(self.image_paths[idx])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.transform:\n            transformed = self.transform(image=image)\n            image = transformed['image']\n        \n        return image, self.labels[idx]\n\n# Usage\ntrain_dataset = CustomDataset(\n    image_paths=train_paths,\n    labels=train_labels,\n    transform=create_training_pipeline()\n)\n```\n\n### PyTorch Dataset with TorchVision\n\n```python\nclass TorchVisionDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Load image for TorchVision (PIL format)\n        image = Image.open(self.image_paths[idx]).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, self.labels[idx]\n\n# Usage\ntrain_dataset = TorchVisionDataset(\n    image_paths=train_paths,\n    labels=train_labels,\n    transform=create_simple_training_pipeline()\n)\n```\n\n## When to Use Which Library\n\n### Choose Albumentations When:\n\n- Working with object detection or segmentation tasks\n- Need advanced augmentation techniques (weather effects, distortions)\n- Performance is critical (processing large datasets)\n- Working with bounding boxes or keypoints\n- Need fine-grained control over augmentation probabilities\n- Dealing with medical or satellite imagery\n\n### Choose TorchVision When:\n\n- Building simple image classification models\n- Working within pure PyTorch ecosystem\n- Need basic augmentations only\n- Prototyping quickly\n- Following PyTorch tutorials or established workflows\n- Working with pre-trained models that expect specific preprocessing\n\n## Best Practices\n\n### Albumentations Best Practices\n\n```python\n# Use ReplayCompose for debugging\nreplay_transform = A.ReplayCompose([\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.3),\n])\n\nresult = replay_transform(image=image)\ntransformed_image = result['image']\nreplay_data = result['replay']\n\n# Apply same transforms to another image\nresult2 = A.ReplayCompose.replay(replay_data, image=another_image)\n\n# Efficient bbox handling\nbbox_params = A.BboxParams(\n    format='pascal_voc',\n    min_area=1024,  # Filter out small boxes\n    min_visibility=0.3,  # Filter out mostly occluded boxes\n    label_fields=['class_labels']\n)\n```\n\n### TorchVision Best Practices\n\n```python\n# Use functional API for custom control\ndef custom_transform(image):\n    if torch.rand(1) < 0.5:\n        image = TF.hflip(image)\n    \n    # Apply rotation with custom logic\n    angle = torch.randint(-30, 30, (1,)).item()\n    image = TF.rotate(image, angle)\n    \n    return image\n\n# Combine with standard transforms\ncombined_transform = T.Compose([\n    T.Lambda(custom_transform),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n```\n\n## Conclusion\n\nBoth libraries have their strengths:\n\n**Albumentations** excels in:\n\n- Advanced augmentation techniques\n- Performance optimization\n- Computer vision tasks beyond classification\n- Professional production environments\n\n**TorchVision** is ideal for:\n\n- Simple classification tasks\n- Learning and prototyping\n- Tight PyTorch integration\n- Basic augmentation needs\n\nChoose based on your specific requirements, with Albumentations being the go-to choice for advanced computer vision projects and TorchVision for simpler classification tasks.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}