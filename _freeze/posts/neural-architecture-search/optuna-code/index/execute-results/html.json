{
  "hash": "48692a819948ca5e981da3a3372a5c7b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Optuna for Deep Learning and Neural Architecture Search: A Comprehensive Guide\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-07-11\"\ncategories: [code, tutorial, advanced]\nformat:\n  html:\n    code-fold: false\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# Optuna for Deep Learning and Neural Architecture Search: A Comprehensive Guide\n![](optuna.png)\n\n## Introduction\n\nHyperparameter optimization is one of the most critical yet challenging aspects of deep learning. With the exponential growth in model complexity and the vast hyperparameter search spaces, manual tuning becomes impractical. Optuna, developed by Preferred Networks, emerges as a powerful automatic hyperparameter optimization framework that addresses these challenges with sophisticated algorithms and intuitive APIs.\n\nThis comprehensive guide explores how Optuna revolutionizes deep learning workflows, from basic hyperparameter tuning to advanced neural architecture search (NAS), providing practical implementations and real-world optimization strategies.\n\n## What is Optuna?\n\nOptuna is an open-source hyperparameter optimization framework designed for machine learning. It offers several key advantages:\n\n- **Efficient Sampling**: Uses Tree-structured Parzen Estimator (TPE) and other advanced algorithms\n- **Pruning**: Automatically stops unpromising trials early\n- **Distributed Optimization**: Supports parallel and distributed hyperparameter search\n- **Framework Agnostic**: Works with PyTorch, TensorFlow, Keras, and other ML frameworks\n- **Visualization**: Rich dashboard for monitoring optimization progress\n\n## Core Concepts\n\n### Studies and Trials\n\nIn Optuna terminology:\n\n- **Study**: An optimization session that tries to find optimal hyperparameters\n- **Trial**: A single execution of the objective function with specific hyperparameter values\n- **Objective Function**: The function to optimize (typically validation loss or accuracy)\n\n### Sampling Algorithms\n\nOptuna implements several sophisticated sampling strategies:\n\n1. **TPE (Tree-structured Parzen Estimator)**: Default algorithm that models the probability distribution of hyperparameters\n2. **Random Sampling**: Baseline method for comparison\n3. **Grid Search**: Exhaustive search over specified parameter combinations\n4. **CMA-ES**: Covariance Matrix Adaptation Evolution Strategy for continuous optimization\n\n### Pruning Algorithms\n\nPruning eliminates unpromising trials early:\n\n- **Median Pruner**: Prunes trials below the median performance\n- **Successive Halving**: Allocates resources progressively to promising trials\n- **Hyperband**: Combines successive halving with different resource allocations\n\n## Installation and Setup\n\n```bash\npip install optuna\npip install optuna-dashboard  # Optional: for visualization\n```\n\nFor specific deep learning frameworks:\n```bash\npip install torch torchvision  # PyTorch\npip install tensorflow  # TensorFlow\npip install optuna[integration]  # Framework integrations\n```\n\n## Basic Hyperparameter Optimization\n\n### Simple PyTorch Example\n\n```python\nimport optuna\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\ndef create_model(trial):\n    # Suggest hyperparameters\n    n_layers = trial.suggest_int('n_layers', 1, 3)\n    n_units = trial.suggest_int('n_units', 64, 512)\n    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n    \n    layers = []\n    in_features = 784  # MNIST input size\n    \n    for i in range(n_layers):\n        layers.append(nn.Linear(in_features, n_units))\n        layers.append(nn.ReLU())\n        layers.append(nn.Dropout(dropout_rate))\n        in_features = n_units\n    \n    layers.append(nn.Linear(in_features, 10))  # Output layer\n    \n    return nn.Sequential(*layers)\n\ndef objective(trial):\n    # Model hyperparameters\n    model = create_model(trial)\n    \n    # Optimizer hyperparameters\n    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n    \n    if optimizer_name == 'Adam':\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n    elif optimizer_name == 'SGD':\n        momentum = trial.suggest_float('momentum', 0.0, 0.99)\n        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n    else:  # RMSprop\n        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n    \n    # Data loading\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n    \n    train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n    \n    test_dataset = datasets.MNIST('data', train=False, transform=transform)\n    test_loader = DataLoader(test_dataset, batch_size=128)\n    \n    # Training\n    criterion = nn.CrossEntropyLoss()\n    model.train()\n    \n    for epoch in range(10):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.view(-1, 784), target\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n            \n            # Optional: Report intermediate values for pruning\n            if batch_idx % 100 == 0:\n                trial.report(loss.item(), epoch * len(train_loader) + batch_idx)\n                if trial.should_prune():\n                    raise optuna.exceptions.TrialPruned()\n    \n    # Validation\n    model.eval()\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.view(-1, 784), target\n            outputs = model(data)\n            _, predicted = torch.max(outputs.data, 1)\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n    \n    accuracy = correct / total\n    return accuracy\n\n# Create study and optimize\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\nprint(f\"Best trial: {study.best_trial.value}\")\nprint(f\"Best params: {study.best_params}\")\n```\n\n## Advanced Hyperparameter Optimization\n\n### Multi-Objective Optimization\n\n```python\ndef multi_objective_function(trial):\n    # Suggest hyperparameters\n    n_layers = trial.suggest_int('n_layers', 1, 5)\n    n_units = trial.suggest_int('n_units', 32, 512)\n    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n    \n    # Create and train model (simplified)\n    model = create_model(trial)\n    accuracy = train_and_evaluate(model)\n    \n    # Calculate model complexity (number of parameters)\n    model_size = sum(p.numel() for p in model.parameters())\n    \n    # Return multiple objectives\n    return accuracy, -model_size  # Maximize accuracy, minimize model size\n\n# Multi-objective study\nstudy = optuna.create_study(directions=['maximize', 'maximize'])\nstudy.optimize(multi_objective_function, n_trials=100)\n\n# Get Pareto front\npareto_front = study.best_trials\nfor trial in pareto_front:\n    print(f\"Trial {trial.number}: Accuracy={trial.values[0]:.3f}, \"\n          f\"Model Size={-trial.values[1]}\")\n```\n\n### Conditional Hyperparameters\n\n```python\ndef conditional_objective(trial):\n    # Main architecture choice\n    model_type = trial.suggest_categorical('model_type', ['CNN', 'ResNet', 'DenseNet'])\n    \n    if model_type == 'CNN':\n        # CNN-specific parameters\n        n_conv_layers = trial.suggest_int('n_conv_layers', 2, 4)\n        kernel_size = trial.suggest_categorical('kernel_size', [3, 5, 7])\n        n_filters = trial.suggest_int('n_filters', 32, 128)\n        \n        model = create_cnn(n_conv_layers, kernel_size, n_filters)\n        \n    elif model_type == 'ResNet':\n        # ResNet-specific parameters\n        depth = trial.suggest_categorical('depth', [18, 34, 50])\n        width_multiplier = trial.suggest_float('width_multiplier', 0.5, 2.0)\n        \n        model = create_resnet(depth, width_multiplier)\n        \n    else:  # DenseNet\n        # DenseNet-specific parameters\n        growth_rate = trial.suggest_int('growth_rate', 12, 48)\n        block_config = trial.suggest_categorical('block_config', \n                                               [(6, 12, 24, 16), (6, 12, 32, 32)])\n        \n        model = create_densenet(growth_rate, block_config)\n    \n    # Common hyperparameters\n    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n    \n    return train_and_evaluate(model, lr, batch_size)\n```\n\n## Neural Architecture Search (NAS)\n\n### Basic NAS Implementation\n\n```python\nimport torch.nn as nn\n\nclass SearchableBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, trial, block_id):\n        super().__init__()\n        self.block_id = block_id\n        \n        # Searchable operations\n        op_name = trial.suggest_categorical(f'op_{block_id}', [\n            'conv3x3', 'conv5x5', 'conv7x7', 'depthwise_conv', 'skip_connect'\n        ])\n        \n        if op_name == 'conv3x3':\n            self.op = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n        elif op_name == 'conv5x5':\n            self.op = nn.Conv2d(in_channels, out_channels, 5, padding=2)\n        elif op_name == 'conv7x7':\n            self.op = nn.Conv2d(in_channels, out_channels, 7, padding=3)\n        elif op_name == 'depthwise_conv':\n            self.op = nn.Sequential(\n                nn.Conv2d(in_channels, in_channels, 3, padding=1, groups=in_channels),\n                nn.Conv2d(in_channels, out_channels, 1)\n            )\n        else:  # skip_connect\n            self.op = nn.Identity() if in_channels == out_channels else nn.Conv2d(in_channels, out_channels, 1)\n        \n        # Activation and normalization\n        self.activation = trial.suggest_categorical(f'activation_{block_id}', \n                                                   ['relu', 'gelu', 'swish'])\n        self.use_batch_norm = trial.suggest_categorical(f'batch_norm_{block_id}', [True, False])\n        \n        if self.use_batch_norm:\n            self.bn = nn.BatchNorm2d(out_channels)\n        \n        if self.activation == 'relu':\n            self.act = nn.ReLU()\n        elif self.activation == 'gelu':\n            self.act = nn.GELU()\n        else:  # swish\n            self.act = nn.SiLU()\n    \n    def forward(self, x):\n        out = self.op(x)\n        if self.use_batch_norm:\n            out = self.bn(out)\n        out = self.act(out)\n        return out\n\nclass SearchableNet(nn.Module):\n    def __init__(self, trial, num_classes=10):\n        super().__init__()\n        \n        # Search for overall architecture\n        num_stages = trial.suggest_int('num_stages', 3, 5)\n        base_channels = trial.suggest_int('base_channels', 32, 128)\n        \n        # Build searchable architecture\n        self.stages = nn.ModuleList()\n        in_channels = 3\n        \n        for stage in range(num_stages):\n            # Number of blocks in this stage\n            num_blocks = trial.suggest_int(f'num_blocks_stage_{stage}', 1, 4)\n            \n            # Channel progression\n            out_channels = base_channels * (2 ** stage)\n            stage_blocks = nn.ModuleList()\n            \n            for block in range(num_blocks):\n                block_id = f'stage_{stage}_block_{block}'\n                stage_blocks.append(SearchableBlock(in_channels, out_channels, trial, block_id))\n                in_channels = out_channels\n            \n            self.stages.append(stage_blocks)\n            \n            # Downsampling between stages\n            if stage < num_stages - 1:\n                self.stages.append(nn.MaxPool2d(2))\n        \n        # Global pooling and classifier\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Linear(in_channels, num_classes)\n        \n        # Dropout\n        dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n        self.dropout = nn.Dropout(dropout_rate)\n    \n    def forward(self, x):\n        for stage in self.stages:\n            if isinstance(stage, nn.ModuleList):\n                for block in stage:\n                    x = block(x)\n            else:\n                x = stage(x)\n        \n        x = self.global_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = self.classifier(x)\n        return x\n\ndef nas_objective(trial):\n    # Create searchable model\n    model = SearchableNet(trial)\n    \n    # Training hyperparameters\n    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    \n    # Data augmentation search\n    use_cutmix = trial.suggest_categorical('use_cutmix', [True, False])\n    use_mixup = trial.suggest_categorical('use_mixup', [True, False])\n    \n    # Train and evaluate\n    accuracy = train_model_with_augmentation(model, optimizer, use_cutmix, use_mixup)\n    \n    return accuracy\n\n# Run NAS\nstudy = optuna.create_study(direction='maximize', \n                           pruner=optuna.pruners.MedianPruner())\nstudy.optimize(nas_objective, n_trials=200)\n```\n\n### Advanced NAS with Weight Sharing\n\n```python\nclass SuperNet(nn.Module):\n    \"\"\"Supernet that contains all possible operations\"\"\"\n    \n    def __init__(self, num_classes=10):\n        super().__init__()\n        \n        # Define all possible operations\n        self.operations = nn.ModuleDict({\n            'conv3x3': nn.Conv2d(64, 64, 3, padding=1),\n            'conv5x5': nn.Conv2d(64, 64, 5, padding=2),\n            'conv7x7': nn.Conv2d(64, 64, 7, padding=3),\n            'depthwise_conv': nn.Sequential(\n                nn.Conv2d(64, 64, 3, padding=1, groups=64),\n                nn.Conv2d(64, 64, 1)\n            ),\n            'skip_connect': nn.Identity()\n        })\n        \n        self.stem = nn.Conv2d(3, 64, 3, padding=1)\n        self.classifier = nn.Linear(64, num_classes)\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n    \n    def forward(self, x, architecture):\n        \"\"\"Forward pass with specific architecture\"\"\"\n        x = self.stem(x)\n        \n        for i, op_name in enumerate(architecture):\n            x = self.operations[op_name](x)\n            if i % 2 == 0:  # Add downsampling periodically\n                x = F.max_pool2d(x, 2)\n        \n        x = self.global_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\ndef progressive_nas_objective(trial):\n    \"\"\"NAS with progressive shrinking\"\"\"\n    \n    # Sample architecture\n    num_blocks = trial.suggest_int('num_blocks', 4, 8)\n    architecture = []\n    \n    for i in range(num_blocks):\n        op = trial.suggest_categorical(f'op_{i}', [\n            'conv3x3', 'conv5x5', 'conv7x7', 'depthwise_conv', 'skip_connect'\n        ])\n        architecture.append(op)\n    \n    # Create supernet (shared across trials)\n    if not hasattr(progressive_nas_objective, 'supernet'):\n        progressive_nas_objective.supernet = SuperNet()\n    \n    model = progressive_nas_objective.supernet\n    \n    # Training with early stopping\n    accuracy = train_with_early_stopping(model, architecture, trial)\n    \n    return accuracy\n```\n\n## Distributed Optimization\n\n### Multi-GPU Training with Optuna\n\n```python\nimport optuna\nfrom optuna.integration import PyTorchLightningPruningCallback\nimport pytorch_lightning as pl\n\nclass LightningModel(pl.LightningModule):\n    def __init__(self, trial):\n        super().__init__()\n        self.trial = trial\n        \n        # Architecture hyperparameters\n        self.lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n        self.batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n        \n        # Model definition\n        self.model = self.build_model(trial)\n        self.criterion = nn.CrossEntropyLoss()\n    \n    def build_model(self, trial):\n        # Build model based on trial suggestions\n        pass\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = self.criterion(y_hat, y)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = self.criterion(y_hat, y)\n        acc = (y_hat.argmax(dim=1) == y).float().mean()\n        self.log('val_loss', loss, sync_dist=True)\n        self.log('val_acc', acc, sync_dist=True)\n        return loss\n    \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.lr)\n\ndef distributed_objective(trial):\n    model = LightningModel(trial)\n    \n    # Pruning callback\n    pruning_callback = PyTorchLightningPruningCallback(trial, monitor='val_acc')\n    \n    # Multi-GPU trainer\n    trainer = pl.Trainer(\n        gpus=4,\n        strategy='ddp',\n        max_epochs=50,\n        callbacks=[pruning_callback],\n        enable_checkpointing=False\n    )\n    \n    trainer.fit(model, train_dataloader, val_dataloader)\n    \n    return trainer.callback_metrics['val_acc'].item()\n\n# Distributed study\nstudy = optuna.create_study(\n    direction='maximize',\n    storage='sqlite:///distributed_study.db',  # Shared storage\n    study_name='distributed_nas'\n)\n\nstudy.optimize(distributed_objective, n_trials=500)\n```\n\n## Advanced Techniques\n\n### Hyperband Integration\n\n```python\nclass HyperbandPruner(optuna.pruners.BasePruner):\n    def __init__(self, min_resource=1, max_resource=81, reduction_factor=3):\n        self.min_resource = min_resource\n        self.max_resource = max_resource\n        self.reduction_factor = reduction_factor\n    \n    def prune(self, study, trial):\n        # Hyperband logic implementation\n        pass\n\ndef hyperband_objective(trial):\n    # Suggest resource budget\n    resource_budget = trial.suggest_int('resource_budget', 1, 81)\n    \n    # Train for suggested epochs\n    model = create_model(trial)\n    accuracy = train_model(model, epochs=resource_budget)\n    \n    return accuracy\n\nstudy = optuna.create_study(\n    direction='maximize',\n    pruner=HyperbandPruner()\n)\n```\n\n### Population-Based Training\n\n```python\ndef population_based_optimization():\n    population_size = 20\n    generations = 10\n    \n    # Initialize population\n    population = []\n    for i in range(population_size):\n        trial = optuna.trial.create_trial(\n            params={\n                'lr': np.random.uniform(1e-5, 1e-1),\n                'batch_size': np.random.choice([16, 32, 64, 128]),\n                'weight_decay': np.random.uniform(1e-6, 1e-2)\n            }\n        )\n        population.append(trial)\n    \n    for generation in range(generations):\n        # Evaluate population\n        fitness_scores = []\n        for trial in population:\n            model = create_model(trial)\n            score = train_and_evaluate(model, trial.params)\n            fitness_scores.append(score)\n        \n        # Select top performers\n        top_indices = np.argsort(fitness_scores)[-population_size//2:]\n        \n        # Create new population\n        new_population = []\n        for idx in top_indices:\n            new_population.append(population[idx])\n        \n        # Mutate and add to population\n        for i in range(population_size - len(new_population)):\n            parent = np.random.choice(new_population)\n            child = mutate_hyperparameters(parent)\n            new_population.append(child)\n        \n        population = new_population\n    \n    return population\n```\n\n## Visualization and Analysis\n\n### Study Analysis\n\n```python\n# Basic study analysis\nprint(f\"Number of finished trials: {len(study.trials)}\")\nprint(f\"Best trial: {study.best_trial.number}\")\nprint(f\"Best value: {study.best_value}\")\nprint(f\"Best parameters: {study.best_params}\")\n\n# Parameter importance\nimportance = optuna.importance.get_param_importances(study)\nprint(\"Parameter importance:\")\nfor param, imp in importance.items():\n    print(f\"  {param}: {imp:.4f}\")\n\n# Visualization\nimport optuna.visualization as vis\n\n# Optimization history\nfig = vis.plot_optimization_history(study)\nfig.show()\n\n# Parameter importance plot\nfig = vis.plot_param_importances(study)\nfig.show()\n\n# Parameter relationships\nfig = vis.plot_parallel_coordinate(study)\nfig.show()\n\n# Hyperparameter slice plot\nfig = vis.plot_slice(study)\nfig.show()\n```\n\n### Custom Metrics Tracking\n\n```python\nclass CustomCallback:\n    def __init__(self):\n        self.metrics = {}\n    \n    def __call__(self, study, trial):\n        # Track custom metrics\n        self.metrics[trial.number] = {\n            'params': trial.params,\n            'value': trial.value,\n            'state': trial.state,\n            'duration': trial.duration\n        }\n        \n        # Custom analysis\n        if len(study.trials) % 10 == 0:\n            self.analyze_progress(study)\n    \n    def analyze_progress(self, study):\n        # Convergence analysis\n        values = [t.value for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n        if len(values) > 10:\n            improvement = values[-1] - values[-11]\n            print(f\"Improvement over last 10 trials: {improvement:.4f}\")\n\n# Use custom callback\ncallback = CustomCallback()\nstudy.optimize(objective, n_trials=100, callbacks=[callback])\n```\n\n## Best Practices and Tips\n\n### Study Configuration\n\n```python\n# Optimal study configuration\nstudy = optuna.create_study(\n    direction='maximize',\n    sampler=optuna.samplers.TPESampler(\n        n_startup_trials=20,  # Random trials before TPE\n        n_ei_candidates=24,   # Candidates for EI\n        multivariate=True,    # Consider parameter interactions\n        seed=42              # Reproducibility\n    ),\n    pruner=optuna.pruners.MedianPruner(\n        n_startup_trials=10,  # Minimum trials before pruning\n        n_warmup_steps=5,     # Steps before considering pruning\n        interval_steps=1      # Frequency of pruning checks\n    )\n)\n```\n\n### Memory Management\n\n```python\ndef memory_efficient_objective(trial):\n    # Clear GPU memory\n    torch.cuda.empty_cache()\n    \n    # Use gradient checkpointing\n    model = create_model(trial)\n    model.gradient_checkpointing_enable()\n    \n    # Mixed precision training\n    scaler = torch.cuda.amp.GradScaler()\n    \n    with torch.cuda.amp.autocast():\n        # Training loop\n        pass\n    \n    # Cleanup\n    del model\n    torch.cuda.empty_cache()\n    \n    return accuracy\n```\n\n### Reproducibility\n\n```python\ndef set_seed(seed=42):\n    import random\n    import numpy as np\n    \n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef reproducible_objective(trial):\n    # Set seed for reproducibility\n    set_seed(trial.suggest_int('seed', 0, 10000))\n    \n    # Rest of objective function\n    pass\n```\n\n## Real-World Applications\n\n### Computer Vision NAS\n\n```python\ndef vision_nas_objective(trial):\n    # Data augmentation search\n    augmentation_policy = {\n        'rotation': trial.suggest_float('rotation', 0, 30),\n        'brightness': trial.suggest_float('brightness', 0.8, 1.2),\n        'contrast': trial.suggest_float('contrast', 0.8, 1.2),\n        'saturation': trial.suggest_float('saturation', 0.8, 1.2),\n        'hue': trial.suggest_float('hue', -0.1, 0.1)\n    }\n    \n    # Architecture search\n    backbone = trial.suggest_categorical('backbone', ['resnet', 'efficientnet', 'mobilenet'])\n    \n    if backbone == 'resnet':\n        depth = trial.suggest_categorical('depth', [18, 34, 50, 101])\n        model = create_resnet(depth)\n    elif backbone == 'efficientnet':\n        version = trial.suggest_categorical('version', ['b0', 'b1', 'b2', 'b3'])\n        model = create_efficientnet(version)\n    else:\n        width_mult = trial.suggest_float('width_mult', 0.25, 2.0)\n        model = create_mobilenet(width_mult)\n    \n    # Training strategy\n    training_strategy = {\n        'optimizer': trial.suggest_categorical('optimizer', ['adam', 'sgd', 'adamw']),\n        'lr_schedule': trial.suggest_categorical('lr_schedule', ['cosine', 'step', 'exponential']),\n        'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n    }\n    \n    return train_vision_model(model, augmentation_policy, training_strategy)\n```\n\n### NLP Architecture Search\n\n```python\ndef nlp_nas_objective(trial):\n    # Transformer architecture search\n    config = {\n        'num_layers': trial.suggest_int('num_layers', 4, 12),\n        'num_heads': trial.suggest_categorical('num_heads', [4, 8, 12, 16]),\n        'hidden_size': trial.suggest_categorical('hidden_size', [256, 512, 768, 1024]),\n        'ffn_size': trial.suggest_categorical('ffn_size', [1024, 2048, 3072, 4096]),\n        'dropout': trial.suggest_float('dropout', 0.0, 0.3),\n        'attention_dropout': trial.suggest_float('attention_dropout', 0.0, 0.3)\n    }\n    \n    # Positional encoding\n    pos_encoding = trial.suggest_categorical('pos_encoding', ['learned', 'sinusoidal', 'rotary'])\n    \n    # Activation function\n    activation = trial.suggest_categorical('activation', ['gelu', 'relu', 'swish'])\n    \n    model = create_transformer(config, pos_encoding, activation)\n    \n    # Training hyperparameters\n    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n    warmup_steps = trial.suggest_int('warmup_steps', 1000, 10000)\n    \n    return train_nlp_model(model, lr, warmup_steps)\n```\n\n## Conclusion\n\nOptuna provides a powerful, flexible framework for hyperparameter optimization and neural architecture search in deep learning. Its sophisticated algorithms, pruning capabilities, and extensive integration ecosystem make it an essential tool for modern ML practitioners.\n\nKey takeaways:\n\n1. **Start Simple**: Begin with basic hyperparameter optimization before moving to complex NAS\n2. **Use Pruning**: Implement pruning to save computational resources\n3. **Leverage Distributed Computing**: Scale optimization across multiple GPUs/nodes\n4. **Monitor Progress**: Use visualization tools to understand optimization dynamics\n5. **Consider Multi-Objective**: Balance multiple criteria like accuracy and efficiency\n6. **Reproducibility**: Set seeds and use consistent evaluation protocols\n\nThe future of automated ML lies in intelligent optimization frameworks like Optuna, which democratize access to state-of-the-art hyperparameter tuning and architecture search techniques. By mastering these tools, practitioners can focus on higher-level design decisions while letting algorithms handle the tedious parameter optimization process.\n\nWhether you're working on computer vision, NLP, or other domains, Optuna's flexibility and power make it an invaluable addition to your deep learning toolkit. Start with the basic examples provided here, then gradually incorporate more advanced techniques as your optimization needs grow in complexity.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}