{
  "hash": "85db6328b2cbf6a6c1263cc6e19f9a10",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"The Mathematics Behind Neural Architecture Search\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-07-11\"\ncategories: [research, advanced]\nformat:\n  html:\n    code-fold: false\n    math: mathjax\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# The Mathematics Behind Neural Architecture Search\n![](nas-math.png)\n\nNeural Architecture Search (NAS) represents one of the most sophisticated applications of automated machine learning, where algorithms autonomously design neural network architectures. This field combines optimization theory, probability, and deep learning to solve the fundamental question: what is the optimal neural network architecture for a given task?\n\n## Problem Formulation\n\nThe core mathematical challenge in NAS can be formulated as a bilevel optimization problem. Given a dataset $D = \\{(x_i, y_i)\\}_{i=1}^N$, we seek to find the optimal architecture $\\alpha^*$ that minimizes the validation loss:\n\n$$\\alpha^* = \\arg \\min_\\alpha L_{\\text{val}}(w^*(\\alpha), \\alpha)$$\n\nwhere $w^*(\\alpha)$ is the optimal set of weights for architecture $\\alpha$, obtained by solving:\n\n$$w^*(\\alpha) = \\arg \\min_w L_{\\text{train}}(w, \\alpha)$$\n\nThis bilevel structure creates significant computational challenges, as evaluating each architecture requires full training to obtain $w^*(\\alpha)$.\n\n## Search Space Representation\n\n### Continuous Relaxation\n\nOne of the key mathematical innovations in NAS is the continuous relaxation of the discrete architecture search space. Instead of searching over discrete architectural choices, we can represent the search space as a continuous optimization problem.\n\nConsider a search space where each edge in the network can be one of $O$ operations from a set $\\mathcal{O} = \\{o^1, o^2, \\ldots, o^{|\\mathcal{O}|}\\}$. The continuous relaxation introduces architecture parameters $\\alpha = \\{\\alpha_{i,j}\\}_{i,j}$ where $\\alpha_{i,j} \\in \\mathbb{R}^{|\\mathcal{O}|}$.\n\nThe mixed operation at edge $(i,j)$ becomes:\n\n$$o^{\\text{mixed}}_{i,j}(x) = \\sum_{k=1}^{|\\mathcal{O}|} \\frac{\\exp(\\alpha_{i,j}^{(k)})}{\\sum_{l=1}^{|\\mathcal{O}|} \\exp(\\alpha_{i,j}^{(l)})} \\cdot o^{(k)}(x)$$\n\nThis softmax weighting allows gradient-based optimization while maintaining the constraint that weights sum to 1.\n\n### Graph-Based Representations\n\nNeural architectures can be represented as directed acyclic graphs (DAGs) $G = (V, E)$ where:\n\n- $V$ represents computational nodes (layers, operations)\n- $E$ represents data flow connections\n\nThe adjacency matrix $A \\in \\{0,1\\}^{|V|\\times|V|}$ encodes the connectivity, where $A_{i,j} = 1$ indicates a connection from node $i$ to node $j$.\n\nFor a node $j$ with incoming edges from nodes $i_1, i_2, \\ldots, i_k$, the output is:\n\n$$h_j = f_j\\left(\\sum_{i \\in \\text{pred}(j)} A_{i,j} \\cdot h_i\\right)$$\n\nwhere $f_j$ is the operation at node $j$ and $\\text{pred}(j)$ denotes the predecessor nodes.\n\n## Optimization Strategies\n\n### Gradient-Based Methods (DARTS)\n\nDifferentiable Architecture Search (DARTS) transforms the discrete search into a continuous optimization problem. The architecture parameters $\\alpha$ and network weights $w$ are optimized alternately:\n\n$$\\alpha_{t+1} = \\alpha_t - \\xi_\\alpha \\nabla_\\alpha L_{\\text{val}}(w_t, \\alpha_t)$$\n$$w_{t+1} = w_t - \\xi_w \\nabla_w L_{\\text{train}}(w_t, \\alpha_t)$$\n\nThe gradient with respect to architecture parameters is:\n\n$$\\nabla_\\alpha L_{\\text{val}} = \\sum_{i,j} \\nabla_\\alpha o^{\\text{mixed}}_{i,j} \\cdot \\nabla_{o^{\\text{mixed}}_{i,j}} L_{\\text{val}}$$\n\nThe chain rule application requires careful handling of the softmax operation:\n\n$$\\nabla_{\\alpha_{i,j}^{(k)}} o^{\\text{mixed}}_{i,j} = (\\delta_{k,l} - p_{i,j}^{(k)}) p_{i,j}^{(l)} \\cdot o^{(l)}$$\n\nwhere $p_{i,j}^{(k)} = \\frac{\\exp(\\alpha_{i,j}^{(k)})}{\\sum_l \\exp(\\alpha_{i,j}^{(l)})}$ and $\\delta_{k,l}$ is the Kronecker delta.\n\n### Evolutionary Approaches\n\nEvolutionary algorithms treat architecture search as a population-based optimization problem. Each architecture is represented as a genome $g$, and the fitness function is typically the validation accuracy.\n\nThe mutation operator $M: \\mathcal{G} \\to \\mathcal{G}$ modifies architectures:\n\n- **Node mutations**: Add/remove computational nodes\n- **Edge mutations**: Add/remove connections  \n- **Operation mutations**: Change operation types\n\nThe crossover operator $C: \\mathcal{G} \\times \\mathcal{G} \\to \\mathcal{G}$ combines two parent architectures:\n\n$$g_{\\text{child}} = C(g_{\\text{parent1}}, g_{\\text{parent2}})$$\n\nCommon crossover strategies include:\n\n- **Uniform crossover**: Each gene inherited from parent1 with probability $p$\n- **Graph crossover**: Combine subgraphs from both parents\n\n### Reinforcement Learning Formulation\n\nNAS can be formulated as a sequential decision problem where an agent (controller) generates architectures. The state space $\\mathcal{S}$ represents partial architectures, actions $\\mathcal{A}$ represent architectural choices, and rewards $\\mathcal{R}$ correspond to validation performance.\n\nThe policy $\\pi(a|s)$ gives the probability of selecting action $a$ in state $s$. The objective is to maximize expected reward:\n\n$$J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[R(\\tau)]$$\n\nwhere $\\tau$ is a trajectory (sequence of architectural decisions) and $\\theta$ are the controller parameters.\n\nUsing the REINFORCE algorithm, the gradient is:\n\n$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot (R(\\tau) - b)]$$\n\nwhere $b$ is a baseline to reduce variance.\n\n## Probability and Sampling\n\n### Architecture Sampling\n\nWhen using continuous relaxation, the final discrete architecture must be sampled. The Gumbel-Softmax trick provides a differentiable sampling mechanism:\n\n$$\\alpha_{\\text{sampled}} = \\text{softmax}\\left(\\frac{\\log(\\alpha) + g}{\\tau}\\right)$$\n\nwhere $g \\sim \\text{Gumbel}(0,1)$ and $\\tau$ is a temperature parameter controlling the sampling sharpness.\n\n### Bayesian Optimization\n\nSome NAS methods model the architecture performance as a Gaussian process. Given observed architectures and performances $\\{(\\alpha_i, y_i)\\}_{i=1}^n$, we model:\n\n$$f(\\alpha) \\sim \\mathcal{GP}(\\mu(\\alpha), k(\\alpha, \\alpha'))$$\n\nThe acquisition function guides the search:\n\n$$\\alpha_{\\text{next}} = \\arg \\max_\\alpha a(\\alpha|\\{(\\alpha_i, y_i)\\}_{i=1}^n)$$\n\nCommon acquisition functions include:\n\n- **Expected Improvement**: $\\text{EI}(\\alpha) = \\mathbb{E}[\\max(0, f(\\alpha) - f(\\alpha_{\\text{best}}))]$\n- **Upper Confidence Bound**: $\\text{UCB}(\\alpha) = \\mu(\\alpha) + \\beta \\cdot \\sigma(\\alpha)$\n\n## Weight Sharing and Supernets\n\n### One-Shot Architecture Search\n\nWeight sharing reduces computational cost by training a single \"supernet\" containing all possible architectures. The supernet weight tensor $W$ has dimensions accommodating all operations.\n\nFor a mixed operation with architecture weights $\\alpha$, the effective computation is:\n\n$$\\text{output} = \\sum_k \\alpha_k \\cdot \\text{op}_k(\\text{input}, W_k)$$\n\nThe challenge is ensuring that shared weights $W_k$ generalize across different architectural contexts.\n\n### Progressive Shrinking\n\nProgressive shrinking gradually reduces the search space by removing poorly-performing operations. The pruning criterion at iteration $t$ is:\n\n$$\\text{keep}_k = \\begin{cases}\n1 & \\text{if } \\alpha_k^{(t)} > \\text{threshold}_t \\\\\n0 & \\text{otherwise}\n\\end{cases}$$\n\nThis creates a sequence of nested search spaces: $\\mathcal{S}_0 \\supset \\mathcal{S}_1 \\supset \\ldots \\supset \\mathcal{S}_T$.\n\n## Performance Prediction\n\n### Learning Curves and Extrapolation\n\nEarly stopping strategies predict final performance from partial training curves. Common models include:\n\n**Power Law**: $f(x) = a \\cdot x^b + c$\n**Exponential**: $f(x) = a \\cdot e^{-bx} + c$\n**Logarithmic**: $f(x) = a \\cdot \\log(x) + b$\n\nThe parameters are fitted using least squares on early training data, then extrapolated to predict full training performance.\n\n### Neural Predictors\n\nNeural networks can predict architecture performance from structural features. Given an architecture encoding $\\phi(\\alpha)$, a predictor network estimates:\n\n$$\\hat{y} = f_\\theta(\\phi(\\alpha))$$\n\nwhere $\\phi(\\alpha)$ might include:\n\n- Graph neural network embeddings\n- Handcrafted features (depth, width, parameter count)\n- Learned representations\n\n## Multi-Objective Optimization\n\nReal-world NAS often involves multiple objectives: accuracy, latency, energy consumption, and memory usage. This creates a multi-objective optimization problem:\n\n$$\\min F(\\alpha) = (f_1(\\alpha), f_2(\\alpha), \\ldots, f_m(\\alpha))$$\n\n### Pareto Optimality\n\nAn architecture $\\alpha^*$ is Pareto optimal if there exists no $\\alpha$ such that:\n\n- $f_i(\\alpha) \\leq f_i(\\alpha^*)$ for all $i$\n- $f_j(\\alpha) < f_j(\\alpha^*)$ for at least one $j$\n\nThe Pareto front represents the set of all Pareto optimal solutions.\n\n### Scalarization Methods\n\n**Weighted Sum**: $\\min_\\alpha \\sum_i w_i \\cdot f_i(\\alpha)$\n**Îµ-Constraint**: $\\min_\\alpha f_1(\\alpha)$ subject to $f_i(\\alpha) \\leq \\varepsilon_i$ for $i > 1$\n**Chebyshev**: $\\min_\\alpha \\max_i w_i \\cdot |f_i(\\alpha) - z_i^*|$\n\nwhere $z_i^*$ is the ideal value for objective $i$.\n\n## Complexity Analysis\n\n### Search Space Size\n\nThe size of the discrete search space grows exponentially with the number of choices. For a search space with:\n\n- $L$ layers\n- $O$ operations per layer  \n- $C$ connections per layer\n\nThe total number of architectures is approximately $O^L \\cdot 2^{LC}$, making exhaustive search intractable for realistic problem sizes.\n\n### Computational Complexity\n\nDifferent NAS methods have varying computational requirements:\n\n**Exhaustive Search**: $\\mathcal{O}(|\\mathcal{S}| \\cdot T)$ where $|\\mathcal{S}|$ is search space size and $T$ is training time\n**Gradient-Based**: $\\mathcal{O}(K \\cdot T)$ where $K$ is number of gradient steps\n**Evolutionary**: $\\mathcal{O}(P \\cdot G \\cdot T)$ where $P$ is population size and $G$ is number of generations\n**One-Shot**: $\\mathcal{O}(T_{\\text{supernet}} + |\\mathcal{S}| \\cdot T_{\\text{eval}})$ where $T_{\\text{eval}} \\ll T$\n\n## Convergence Analysis\n\n### DARTS Convergence\n\nFor DARTS, convergence depends on the interplay between architecture and weight optimization. The coupled dynamics can be analyzed using:\n\n$$\\alpha_{t+1} = \\alpha_t - \\xi_\\alpha \\nabla_\\alpha L_{\\text{val}}(w^*(\\alpha_t), \\alpha_t)$$\n$$w_{t+1} = w_t - \\xi_w \\nabla_w L_{\\text{train}}(w_t, \\alpha_t)$$\n\nUnder certain conditions (convexity, smoothness), this alternating optimization converges to a stationary point. However, the bilevel nature and non-convexity of neural networks make theoretical guarantees challenging.\n\n### Evolutionary Algorithm Convergence\n\nFor evolutionary NAS, convergence analysis involves studying the transition probabilities between population states. The probability of finding the optimal architecture depends on:\n\n- Selection pressure\n- Mutation rates\n- Population diversity\n\nThe expected hitting time to the optimum can be bounded using Markov chain analysis.\n\n## Practical Considerations\n\n### Regularization\n\nArchitecture search often requires regularization to prevent overfitting:\n\n**Dropout on Architecture Parameters**: Randomly zero some $\\alpha$ values during training\n\n**Weight Decay**: Add L2 penalty $\\lambda ||\\alpha||^2$ to the loss\n\n**Early Stopping**: Stop search when validation performance plateaus\n\n### Search Space Design\n\nThe choice of search space significantly impacts results. Key considerations include:\n\n**Expressivity**: Can the space represent effective architectures?\n**Efficiency**: Can the space be searched efficiently?\n**Inductive Bias**: Does the space encode useful architectural priors?\n\nMathematical analysis of search spaces involves studying their geometric properties, connectivity, and the distribution of high-performing architectures.\n\n## Future Directions\n\nNeural Architecture Search continues to evolve, with emerging mathematical frameworks addressing:\n\n- **Theoretical foundations**: Convergence guarantees and optimality conditions\n- **Efficient search**: Better approximation algorithms and search strategies  \n- **Transferability**: Mathematical models for cross-domain architecture transfer\n- **Interpretability**: Understanding why certain architectures perform well\n\nThe mathematical sophistication of NAS continues to grow, drawing from diverse fields including optimization theory, probability, graph theory, and control theory. As the field matures, we expect to see more principled approaches that combine theoretical rigor with practical effectiveness.\n\n## Conclusion\n\nThe intersection of discrete optimization, continuous relaxation, and deep learning in NAS represents one of the most mathematically rich areas in modern machine learning, with applications extending far beyond neural network design to general automated algorithm design and meta-learning.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}