{
  "hash": "0e679f92ae4625acb74a2d97d6c57b42",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"The Mathematics Behind Neural Architecture Search\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-07-11\"\ncategories: [research, advanced]\nformat:\n  html:\n    code-fold: false\n    math: mathjax\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# The Mathematics Behind Neural Architecture Search\n![](nas-math.png)\n\nNeural Architecture Search (NAS) represents one of the most sophisticated applications of automated machine learning, where algorithms autonomously design neural network architectures. This field combines optimization theory, probability, and deep learning to solve the fundamental question: what is the optimal neural network architecture for a given task?\n\n## Problem Formulation\n\nThe core mathematical challenge in NAS can be formulated as a bilevel optimization problem.  \nGiven a dataset \\( \\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N \\), we seek to find the optimal architecture \\( \\alpha^* \\) that minimizes the validation loss:\n\n$$\n\\alpha^* = \\arg\\min_\\alpha \\; \\mathcal{L}_{\\text{val}}(w^*(\\alpha), \\alpha)\n$$\n\nwhere \\( w^*(\\alpha) \\) is the optimal set of weights for architecture \\( \\alpha \\), obtained by solving:\n\n$$\nw^*(\\alpha) = \\arg\\min_w \\; \\mathcal{L}_{\\text{train}}(w, \\alpha)\n$$\n\n\nThis bilevel structure creates significant computational challenges, as evaluating each architecture requires full training to obtain w*(Î±).\n\n## Search Space Representation\n\n### Continuous Relaxation\n\nOne of the key mathematical innovations in NAS is the **continuous relaxation** of the discrete architecture search space.  \nInstead of searching over discrete architectural choices, we represent the search space as a continuous optimization problem.\n\nConsider a search space where each edge in the network can be one of \\( \\mathcal{O} \\) operations from a set \\( \\mathcal{O} = \\{ o^1, o^2, \\ldots, o^{|\\mathcal{O}|} \\} \\).  \nThe continuous relaxation introduces architecture parameters \\( \\alpha = \\{ \\alpha_{i,j} \\}_{i,j} \\), where \\( \\alpha_{i,j} \\in \\mathbb{R}^{|\\mathcal{O}|} \\).\n\nThe **mixed operation** at edge \\( (i, j) \\) is defined as:\n\n$$\no^{\\text{mixed}}_{i,j}(x) = \\sum_{k=1}^{|\\mathcal{O}|} \\frac{\\exp(\\alpha_{i,j}^{(k)})}{\\sum_{l=1}^{|\\mathcal{O}|} \\exp(\\alpha_{i,j}^{(l)})} \\cdot o^{(k)}(x)\n$$\n\nThis softmax-based weighting enables **gradient-based optimization** while ensuring the weights over operations at each edge sum to 1.\n\n\n### Graph-Based Representations\n\nNeural architectures can be represented as **directed acyclic graphs (DAGs)**  \n\\( G = (V, E) \\), where:\n- \\( V \\) represents computational nodes (e.g., layers or operations)\n- \\( E \\) represents data flow connections\n\nThe **adjacency matrix** \\( A \\in \\{0,1\\}^{|V| \\times |V|} \\) encodes the connectivity,  \nwhere \\( A_{i,j} = 1 \\) indicates a connection from node \\( i \\) to node \\( j \\).\n\nFor a node \\( j \\) with incoming edges from nodes \\( i_1, i_2, \\ldots, i_k \\), the output is:\n\n$$\nh_j = f_j\\left( \\sum_{i \\in \\text{pred}(j)} A_{i,j} \\cdot h_i \\right)\n$$\n\nwhere \\( f_j \\) is the operation at node \\( j \\) and \\( \\text{pred}(j) \\) denotes the set of its predecessor nodes.\n\n\n## Optimization Strategies\n\n### Gradient-Based Methods (DARTS)\n\n**Differentiable Architecture Search (DARTS)** transforms the discrete search into a continuous optimization problem. The architecture parameters \\( \\alpha \\) and network weights \\( w \\) are optimized alternately:\n\n$$\n\\alpha_{t+1} = \\alpha_t - \\xi_\\alpha \\nabla_\\alpha \\mathcal{L}_{\\text{val}}(w_t, \\alpha_t)\n$$\n\n$$\nw_{t+1} = w_t - \\xi_w \\nabla_w \\mathcal{L}_{\\text{train}}(w_t, \\alpha_t)\n$$\n\nThe gradient with respect to architecture parameters is:\n\n$$\n\\nabla_\\alpha \\mathcal{L}_{\\text{val}} = \\sum_{i,j} \\nabla_\\alpha o^{\\text{mixed}}_{i,j} \\cdot \\nabla_{o^{\\text{mixed}}_{i,j}} \\mathcal{L}_{\\text{val}}\n$$\n\nThe chain rule application requires careful handling of the softmax operation:\n\n$$\n\\nabla_{\\alpha_{i,j}^{(k)}} o^{\\text{mixed}}_{i,j} = \\left( \\delta_{k,l} - p_{i,j}^{(k)} \\right) p_{i,j}^{(l)} \\cdot o^{(l)}\n$$\n\nwhere  \n\\( p_{i,j}^{(k)} = \\frac{\\exp(\\alpha_{i,j}^{(k)})}{\\sum_l \\exp(\\alpha_{i,j}^{(l)})} \\) and \\( \\delta_{k,l} \\) is the **Kronecker delta**.\n\n\n### Evolutionary Approaches\n\nEvolutionary algorithms treat architecture search as a population-based optimization problem. Each architecture is represented as a genome g, and the fitness function is typically the validation accuracy.\n\nThe mutation operator M: $\\mathcal{G} \\rightarrow \\mathcal{G}$ modifies architectures:\n- **Node mutations**: Add/remove computational nodes\n- **Edge mutations**: Add/remove connections  \n- **Operation mutations**: Change operation types\n\nThe **crossover operator** \\( C: \\mathcal{G} \\times \\mathcal{G} \\rightarrow \\mathcal{G} \\) combines two parent architectures:\n\n$$\ng_{\\text{child}} = C(g_{\\text{parent1}},\\ g_{\\text{parent2}})\n$$\n\n\nCommon crossover strategies include:\n- **Uniform crossover**: Each gene inherited from parent1 with probability p\n- **Graph crossover**: Combine subgraphs from both parents\n\n### Reinforcement Learning Formulation\n\nNeural Architecture Search (NAS) can be formulated as a **sequential decision problem**, where an agent (controller) generates architectures step by step.\n\n- The **state space** \\( \\mathcal{S} \\) represents partial architectures.\n- The **action space** \\( \\mathcal{A} \\) consists of architectural choices (e.g., layer types, connections).\n- The **reward** \\( \\mathcal{R} \\) corresponds to validation performance (e.g., accuracy or loss).\n\nThe policy \\( \\pi_\\theta(a \\mid s) \\) gives the probability of selecting action \\( a \\) in state \\( s \\), parameterized by \\( \\theta \\). The objective is to maximize the expected reward:\n\n$$\nJ(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ R(\\tau) \\right]\n$$\n\nwhere \\( \\tau \\) is a trajectory (i.e., a sequence of architectural decisions) and \\( \\theta \\) are the controller parameters.\n\nUsing the **REINFORCE** algorithm, the policy gradient is given by:\n\n$$\n\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a \\mid s) \\cdot \\left( R(\\tau) - b \\right) \\right]\n$$\n\nwhere \\( b \\) is a baseline (e.g., moving average of rewards) used to reduce variance in the gradient estimate.\n\n\n## Probability and Sampling\n\n### Architecture Sampling\n\nWhen using continuous relaxation, the final discrete architecture must be sampled. The **Gumbel-Softmax trick** provides a differentiable sampling mechanism:\n\n$$\n\\alpha_{\\text{sampled}} = \\text{softmax}\\left( \\frac{\\log(\\alpha) + g}{\\tau} \\right)\n$$\n\nwhere \\( g \\sim \\text{Gumbel}(0,1) \\) and \\( \\tau \\) is a temperature parameter controlling the sampling sharpness.\n\n\n### Bayesian Optimization\n\nSome NAS methods model the architecture performance as a **Gaussian Process (GP)**. Given observed architectures and performances \\( \\{(\\alpha_i, y_i)\\}_{i=1}^n \\), we model:\n\n$$\nf(\\alpha) \\sim \\mathcal{GP}(\\mu(\\alpha),\\ k(\\alpha, \\alpha'))\n$$\n\nThe **acquisition function** guides the search by selecting the next architecture:\n\n$$\n\\alpha_{\\text{next}} = \\arg\\max_\\alpha\\ a\\left(\\alpha \\mid \\{(\\alpha_i, y_i)\\}_{i=1}^n \\right)\n$$\n\nCommon acquisition functions include:\n\n- **Expected Improvement (EI)**:\n  \\[\n  \\text{EI}(\\alpha) = \\mathbb{E}\\left[ \\max(0,\\ f(\\alpha) - f(\\alpha_{\\text{best}})) \\right]\n  \\]\n\n- **Upper Confidence Bound (UCB)**:\n  \\[\n  \\text{UCB}(\\alpha) = \\mu(\\alpha) + \\beta \\cdot \\sigma(\\alpha)\n  \\]\n\nwhere \\( \\mu(\\alpha) \\) and \\( \\sigma(\\alpha) \\) are the predicted mean and standard deviation from the GP model, and \\( \\beta \\) controls the exploration-exploitation trade-off.\n\n\n## Weight Sharing and Supernets\n\n### One-Shot Architecture Search\n\nWeight sharing reduces computational cost by training a single \"supernet\" containing all possible architectures. The supernet weight tensor W has dimensions accommodating all operations.\n\nFor a **mixed operation** with architecture weights \\( \\alpha \\), the effective computation is:\n\n$$\n\\text{output} = \\sum_k \\alpha_k \\cdot \\text{op}_k(\\text{input},\\ W_k)\n$$\n\nwhere:\n- \\( \\alpha_k \\) is the weight assigned to operation \\( \\text{op}_k \\)\n- \\( W_k \\) are the learnable parameters of operation \\( \\text{op}_k \\)\n\nThe challenge lies in ensuring that the **shared weights** \\( W_k \\) can generalize across different architectural contexts, since they are updated under multiple competing configurations during training.\n\n\n### Progressive Shrinking\n\n**Progressive shrinking** gradually reduces the search space by removing poorly performing operations.  \nThe pruning criterion at iteration \\( t \\) is:\n\n$$\n\\text{keep}_k = \n\\begin{cases}\n1 & \\text{if } \\alpha_k^{(t)} > \\text{threshold}_t \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\n\nThis creates a sequence of **nested search spaces**:\n\n$$\n\\mathcal{S}_0 \\supset \\mathcal{S}_1 \\supset \\cdots \\supset \\mathcal{S}_T\n$$\n\n\n## Performance Prediction\n\n### Learning Curves and Extrapolation\n\nEarly stopping strategies predict final performance from partial training curves. Common models include:\n\n- **Power Law**:  \n  \\[\n  f(x) = a \\cdot x^b + c\n  \\]\n\n- **Exponential**:  \n  \\[\n  f(x) = a \\cdot e^{-bx} + c\n  \\]\n\n- **Logarithmic**:  \n  \\[\n  f(x) = a \\cdot \\log(x) + b\n  \\]\n\n\nThe parameters are fitted using least squares on early training data, then extrapolated to predict full training performance.\n\n### Neural Predictors\n\nNeural networks can be trained to **predict architecture performance** based on structural features.  \nGiven an architecture encoding \\( \\phi(\\alpha) \\), a predictor network estimates the performance:\n\n$$\n\\hat{y} = f_\\theta\\left( \\phi(\\alpha) \\right)\n$$\n\n\nwhere \\( \\phi(\\alpha) \\) might include:\n- Graph neural network embeddings\n- Handcrafted features (depth, width, parameter count)\n- Learned representations\n\n## Multi-Objective Optimization\n\nReal-world NAS often involves multiple objectives: accuracy, latency, energy consumption, and memory usage. This creates a multi-objective optimization problem:\n\n$$\n\\min F(\\alpha) = \\left( f_1(\\alpha),\\ f_2(\\alpha),\\ \\dots,\\ f_m(\\alpha) \\right)\n$$\n\n\n### Pareto Optimality\n\nAn architecture \\( \\alpha^* \\) is **Pareto optimal** if there does **not** exist any  \n\\( \\alpha \\) such that:\n\n- \\( f_i(\\alpha) \\leq f_i(\\alpha^*) \\quad \\text{for all } i \\)\n- \\( f_j(\\alpha) < f_j(\\alpha^*) \\quad \\text{for at least one } j \\)\n\n\nThe Pareto front represents the set of all Pareto optimal solutions.\n\n### Scalarization Methods\n\n**Weighted Sum**: $\\min_\\alpha \\sum_i w_i \\cdot f_i(\\alpha)$\n**Îµ-Constraint**: $\\min_\\alpha f_1(\\alpha)$ subject to $f_i(\\alpha) \\leq \\varepsilon_i$ for $i > 1$\n**Chebyshev**: $\\min_\\alpha \\max_i \\; w_i \\cdot |f_i(\\alpha) - z_i^*|$\n\nwhere $z_i^*$ is the ideal value for objective $i$.\n\n## Complexity Analysis\n\n### Search Space Size\n\nThe size of the discrete search space grows exponentially with the number of choices. For a search space with:\n- L layers\n- O operations per layer  \n- C connections per layer\n\nThe total number of architectures is approximately $O^L \\cdot 2^{LC}$, making exhaustive search intractable for realistic problem sizes.\n\n### Computational Complexity\n\nDifferent NAS methods have varying computational requirements:\n\n**Exhaustive Search**: $O(|S| \\cdot T)$ where $|S|$ is search space size and T is training time\n**Gradient-Based**: $O(K \\cdot T)$ where $K$ is number of gradient steps\n**Evolutionary**: $O(P \\cdot G \\cdot T)$ where $P$ is population size and $G$ is number of generations\n**One-Shot**: $O(T_{\\text{supernet}} + |S| \\cdot T_{\\text{eval}})$ where $T_{\\text{eval}}$ << $T$\n\n## Convergence Analysis\n\n### DARTS Convergence\n\nFor DARTS, convergence depends on the interplay between architecture and weight optimization. The coupled dynamics can be analyzed using:\n\n$$\n\\alpha_{t+1} = \\alpha_t - \\xi_\\alpha \\nabla_\\alpha \\mathcal{L}_{\\text{val}}(w^*(\\alpha_t), \\alpha_t)\n$$\n\n$$\nw_{t+1} = w_t - \\xi_w \\nabla_w \\mathcal{L}_{\\text{train}}(w_t, \\alpha_t)\n$$\n\n\nUnder certain conditions (convexity, smoothness), this alternating optimization converges to a stationary point. However, the bilevel nature and non-convexity of neural networks make theoretical guarantees challenging.\n\n### Evolutionary Algorithm Convergence\n\nFor evolutionary NAS, convergence analysis involves studying the transition probabilities between population states. The probability of finding the optimal architecture depends on:\n- Selection pressure\n- Mutation rates\n- Population diversity\n\nThe expected hitting time to the optimum can be bounded using Markov chain analysis.\n\n## Practical Considerations\n\n### Regularization\n\nArchitecture search often requires regularization to prevent overfitting:\n\n**Dropout on Architecture Parameters**: Randomly zero some Î± values during training\n**Weight Decay**: Add L2 penalty Î»||Î±||Â² to the loss\n**Early Stopping**: Stop search when validation performance plateaus\n\n### Search Space Design\n\nThe choice of search space significantly impacts results. Key considerations include:\n\n**Expressivity**: Can the space represent effective architectures?\n**Efficiency**: Can the space be searched efficiently?\n**Inductive Bias**: Does the space encode useful architectural priors?\n\nMathematical analysis of search spaces involves studying their geometric properties, connectivity, and the distribution of high-performing architectures.\n\n## Future Directions\n\nNeural Architecture Search continues to evolve, with emerging mathematical frameworks addressing:\n\n- **Theoretical foundations**: Convergence guarantees and optimality conditions\n- **Efficient search**: Better approximation algorithms and search strategies  \n- **Transferability**: Mathematical models for cross-domain architecture transfer\n- **Interpretability**: Understanding why certain architectures perform well\n\nThe mathematical sophistication of NAS continues to grow, drawing from diverse fields including optimization theory, probability, graph theory, and control theory. As the field matures, we expect to see more principled approaches that combine theoretical rigor with practical effectiveness.\n\nThe intersection of discrete optimization, continuous relaxation, and deep learning in NAS represents one of the most mathematically rich areas in modern machine learning, with applications extending far beyond neural network design to general automated algorithm design and meta-learning.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}