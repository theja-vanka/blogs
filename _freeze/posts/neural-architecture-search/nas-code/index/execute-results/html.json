{
  "hash": "23f6fffa18774f04140118bc80e1d6ed",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Neural Architecture Search: Complete Code Guide\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-07-11\"\ncategories: [code, tutorial, advanced]\nformat:\n  html:\n    code-fold: false\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# Neural Architecture Search: Complete Code Guide\n![](nas-code.png)\n\n## Introduction\n\nNeural Architecture Search (NAS) is an automated approach to designing neural network architectures. Instead of manually crafting network designs, NAS algorithms explore the space of possible architectures to find optimal configurations for specific tasks.\n\n### Why NAS Matters\n- **Automation**: Reduces human effort in architecture design\n- **Performance**: Can discover architectures that outperform human-designed ones\n- **Efficiency**: Optimizes for specific constraints (latency, memory, energy)\n- **Scalability**: Adapts to different tasks and domains\n\n## Theoretical Foundations\n\n### The NAS Framework\n\nNAS consists of three main components:\n\n1. **Search Space**: Defines the set of possible architectures\n2. **Search Strategy**: Determines how to explore the search space\n3. **Performance Estimation**: Evaluates architecture quality\n\n```python\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom typing import List, Dict, Tuple, Optional\nimport random\nfrom collections import defaultdict\n\nclass NASFramework:\n    def __init__(self, search_space, search_strategy, performance_estimator):\n        self.search_space = search_space\n        self.search_strategy = search_strategy\n        self.performance_estimator = performance_estimator\n        self.history = []\n    \n    def search(self, num_iterations: int):\n        \"\"\"Main NAS loop\"\"\"\n        for iteration in range(num_iterations):\n            # Sample architecture from search space\n            architecture = self.search_strategy.sample_architecture(\n                self.search_space, self.history\n            )\n            \n            # Evaluate architecture\n            performance = self.performance_estimator.evaluate(architecture)\n            \n            # Update history\n            self.history.append({\n                'architecture': architecture,\n                'performance': performance,\n                'iteration': iteration\n            })\n            \n            # Update search strategy\n            self.search_strategy.update(architecture, performance)\n        \n        return self.get_best_architecture()\n    \n    def get_best_architecture(self):\n        return max(self.history, key=lambda x: x['performance'])\n```\n\n## Search Space Design\n\n### Macro Search Space\n\nDefines the overall structure of the network (number of layers, skip connections, etc.).\n\n```python\nclass MacroSearchSpace:\n    def __init__(self, max_layers: int = 20, operations: List[str] = None):\n        self.max_layers = max_layers\n        self.operations = operations or [\n            'conv3x3', 'conv5x5', 'conv7x7', 'maxpool3x3', \n            'avgpool3x3', 'identity', 'zero'\n        ]\n    \n    def sample_architecture(self) -> Dict:\n        \"\"\"Sample a random architecture\"\"\"\n        num_layers = random.randint(8, self.max_layers)\n        architecture = {\n            'layers': [],\n            'skip_connections': []\n        }\n        \n        for i in range(num_layers):\n            layer = {\n                'operation': random.choice(self.operations),\n                'filters': random.choice([32, 64, 128, 256, 512]),\n                'kernel_size': random.choice([3, 5, 7]) if 'conv' in self.operations[0] else 3\n            }\n            architecture['layers'].append(layer)\n        \n        # Add skip connections\n        for i in range(1, num_layers):\n            if random.random() < 0.3:  # 30% chance of skip connection\n                source = random.randint(0, i-1)\n                architecture['skip_connections'].append((source, i))\n        \n        return architecture\n```\n\n### Micro Search Space (Cell-based)\n\nFocuses on designing building blocks (cells) that are repeated throughout the network.\n\n```python\nclass CellSearchSpace:\n    def __init__(self, num_nodes: int = 7, num_ops: int = 8):\n        self.num_nodes = num_nodes\n        self.operations = [\n            'none', 'max_pool_3x3', 'avg_pool_3x3', 'skip_connect',\n            'sep_conv_3x3', 'sep_conv_5x5', 'dil_conv_3x3', 'dil_conv_5x5'\n        ]\n        self.num_ops = len(self.operations)\n    \n    def sample_cell(self) -> Dict:\n        \"\"\"Sample a cell architecture\"\"\"\n        cell = {\n            'normal_cell': self._sample_single_cell(),\n            'reduction_cell': self._sample_single_cell()\n        }\n        return cell\n    \n    def _sample_single_cell(self) -> List[Tuple]:\n        \"\"\"Sample a single cell with intermediate nodes\"\"\"\n        cell = []\n        for i in range(2, self.num_nodes + 2):  # Nodes 2 to num_nodes+1\n            # Each node has two inputs\n            for j in range(2):\n                # Sample input node (0 to i-1)\n                input_node = random.randint(0, i-1)\n                # Sample operation\n                operation = random.choice(self.operations)\n                cell.append((input_node, operation))\n        return cell\n```\n\n### Differentiable Search Space\n\nEnables gradient-based optimization of architectures.\n\n```python\nclass DifferentiableSearchSpace(nn.Module):\n    def __init__(self, operations: List[str], num_nodes: int = 4):\n        super().__init__()\n        self.operations = operations\n        self.num_nodes = num_nodes\n        self.num_ops = len(operations)\n        \n        # Architecture parameters (alpha)\n        self.alpha = nn.Parameter(torch.randn(num_nodes, num_ops))\n        \n        # Operation modules\n        self.ops = nn.ModuleList([\n            self._get_operation(op) for op in operations\n        ])\n    \n    def _get_operation(self, op_name: str) -> nn.Module:\n        \"\"\"Get operation module by name\"\"\"\n        if op_name == 'conv3x3':\n            return nn.Conv2d(32, 32, 3, padding=1)\n        elif op_name == 'conv5x5':\n            return nn.Conv2d(32, 32, 5, padding=2)\n        elif op_name == 'maxpool3x3':\n            return nn.MaxPool2d(3, stride=1, padding=1)\n        elif op_name == 'avgpool3x3':\n            return nn.AvgPool2d(3, stride=1, padding=1)\n        elif op_name == 'identity':\n            return nn.Identity()\n        elif op_name == 'zero':\n            return Zero()\n        else:\n            raise ValueError(f\"Unknown operation: {op_name}\")\n    \n    def forward(self, x):\n        # Softmax over operations\n        weights = torch.softmax(self.alpha, dim=-1)\n        \n        # Mixed operation\n        output = 0\n        for i, op in enumerate(self.ops):\n            output += weights[0, i] * op(x)  # Simplified for single node\n        \n        return output\n    \n    def get_discrete_architecture(self):\n        \"\"\"Extract discrete architecture from continuous parameters\"\"\"\n        arch = []\n        for node in range(self.num_nodes):\n            best_op_idx = torch.argmax(self.alpha[node])\n            arch.append(self.operations[best_op_idx])\n        return arch\n\nclass Zero(nn.Module):\n    def forward(self, x):\n        return torch.zeros_like(x)\n```\n\n## Search Strategies\n\n### Random Search\n\nSimple baseline that samples architectures randomly.\n\n```python\nclass RandomSearch:\n    def __init__(self):\n        self.history = []\n    \n    def sample_architecture(self, search_space, history):\n        return search_space.sample_architecture()\n    \n    def update(self, architecture, performance):\n        self.history.append((architecture, performance))\n```\n\n### Evolutionary Search\n\nUses genetic algorithms to evolve architectures.\n\n```python\nclass EvolutionarySearch:\n    def __init__(self, population_size: int = 50, mutation_rate: float = 0.1):\n        self.population_size = population_size\n        self.mutation_rate = mutation_rate\n        self.population = []\n        self.fitness_scores = []\n    \n    def initialize_population(self, search_space):\n        \"\"\"Initialize random population\"\"\"\n        self.population = [\n            search_space.sample_architecture() \n            for _ in range(self.population_size)\n        ]\n    \n    def sample_architecture(self, search_space, history):\n        if not self.population:\n            self.initialize_population(search_space)\n            return self.population[0]\n        \n        # Tournament selection\n        return self._tournament_selection()\n    \n    def _tournament_selection(self, tournament_size: int = 3):\n        \"\"\"Select parent via tournament selection\"\"\"\n        tournament_indices = random.sample(\n            range(len(self.population)), tournament_size\n        )\n        tournament_fitness = [self.fitness_scores[i] for i in tournament_indices]\n        winner_idx = tournament_indices[np.argmax(tournament_fitness)]\n        return self.population[winner_idx]\n    \n    def update(self, architecture, performance):\n        \"\"\"Update population with new architecture\"\"\"\n        if len(self.population) < self.population_size:\n            self.population.append(architecture)\n            self.fitness_scores.append(performance)\n        else:\n            # Replace worst performing architecture\n            worst_idx = np.argmin(self.fitness_scores)\n            if performance > self.fitness_scores[worst_idx]:\n                self.population[worst_idx] = architecture\n                self.fitness_scores[worst_idx] = performance\n    \n    def mutate_architecture(self, architecture, search_space):\n        \"\"\"Mutate architecture\"\"\"\n        if random.random() < self.mutation_rate:\n            # Simple mutation: change random operation\n            if 'layers' in architecture:\n                layer_idx = random.randint(0, len(architecture['layers']) - 1)\n                architecture['layers'][layer_idx]['operation'] = random.choice(\n                    search_space.operations\n                )\n        return architecture\n```\n\n### Reinforcement Learning Search\n\nUses RL to learn architecture sampling policies.\n\n```python\nclass RLController(nn.Module):\n    def __init__(self, vocab_size: int, hidden_size: int = 64):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        \n        self.lstm = nn.LSTM(vocab_size, hidden_size, batch_first=True)\n        self.classifier = nn.Linear(hidden_size, vocab_size)\n        \n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        logits = self.classifier(lstm_out)\n        return logits\n    \n    def sample_architecture(self, max_length: int = 20):\n        \"\"\"Sample architecture using the controller\"\"\"\n        self.eval()\n        with torch.no_grad():\n            sequence = []\n            hidden = None\n            \n            # Start token\n            input_token = torch.zeros(1, 1, self.vocab_size)\n            \n            for _ in range(max_length):\n                logits, hidden = self.lstm(input_token, hidden)\n                logits = self.classifier(logits)\n                \n                # Sample next token\n                probs = torch.softmax(logits.squeeze(), dim=0)\n                next_token = torch.multinomial(probs, 1).item()\n                sequence.append(next_token)\n                \n                # Prepare input for next step\n                input_token = torch.zeros(1, 1, self.vocab_size)\n                input_token[0, 0, next_token] = 1\n        \n        return sequence\n\nclass ReinforcementLearningSearch:\n    def __init__(self, vocab_size: int, learning_rate: float = 0.001):\n        self.controller = RLController(vocab_size)\n        self.optimizer = torch.optim.Adam(\n            self.controller.parameters(), lr=learning_rate\n        )\n        self.baseline = 0\n        self.baseline_decay = 0.99\n        \n    def sample_architecture(self, search_space, history):\n        sequence = self.controller.sample_architecture()\n        return self._sequence_to_architecture(sequence, search_space)\n    \n    def _sequence_to_architecture(self, sequence, search_space):\n        \"\"\"Convert sequence to architecture\"\"\"\n        # Simplified conversion\n        architecture = {'layers': []}\n        for i in range(0, len(sequence), 2):\n            if i + 1 < len(sequence):\n                op_idx = sequence[i] % len(search_space.operations)\n                filter_idx = sequence[i + 1] % 4\n                \n                layer = {\n                    'operation': search_space.operations[op_idx],\n                    'filters': [32, 64, 128, 256][filter_idx]\n                }\n                architecture['layers'].append(layer)\n        \n        return architecture\n    \n    def update(self, architecture, performance):\n        \"\"\"Update controller using REINFORCE\"\"\"\n        # Update baseline\n        self.baseline = self.baseline_decay * self.baseline + \\\n                       (1 - self.baseline_decay) * performance\n        \n        # Calculate advantage\n        advantage = performance - self.baseline\n        \n        # Update controller (simplified)\n        self.optimizer.zero_grad()\n        # In practice, you'd compute the log probability of the sampled architecture\n        # and multiply by the advantage for the REINFORCE update\n        # loss = -log_prob * advantage\n        self.optimizer.step()\n```\n\n### Differentiable Architecture Search (DARTS)\n\nGradient-based search using continuous relaxation.\n\n```python\nclass DARTSSearch:\n    def __init__(self, model: DifferentiableSearchSpace, learning_rate: float = 0.025):\n        self.model = model\n        self.optimizer = torch.optim.SGD(\n            self.model.parameters(), lr=learning_rate, momentum=0.9\n        )\n        self.arch_optimizer = torch.optim.Adam(\n            [self.model.alpha], lr=3e-4\n        )\n    \n    def search_step(self, train_data, val_data, criterion):\n        \"\"\"Single search step in DARTS\"\"\"\n        # Update architecture parameters\n        self.arch_optimizer.zero_grad()\n        val_loss = self._compute_val_loss(val_data, criterion)\n        val_loss.backward()\n        self.arch_optimizer.step()\n        \n        # Update model parameters\n        self.optimizer.zero_grad()\n        train_loss = self._compute_train_loss(train_data, criterion)\n        train_loss.backward()\n        self.optimizer.step()\n        \n        return train_loss.item(), val_loss.item()\n    \n    def _compute_train_loss(self, data, criterion):\n        \"\"\"Compute training loss\"\"\"\n        inputs, targets = data\n        outputs = self.model(inputs)\n        return criterion(outputs, targets)\n    \n    def _compute_val_loss(self, data, criterion):\n        \"\"\"Compute validation loss\"\"\"\n        inputs, targets = data\n        outputs = self.model(inputs)\n        return criterion(outputs, targets)\n    \n    def get_final_architecture(self):\n        \"\"\"Extract final discrete architecture\"\"\"\n        return self.model.get_discrete_architecture()\n```\n\n## Performance Estimation\n\n### Full Training\n\nMost accurate but computationally expensive.\n\n```python\nclass FullTrainingEvaluator:\n    def __init__(self, dataset, num_epochs: int = 100):\n        self.dataset = dataset\n        self.num_epochs = num_epochs\n    \n    def evaluate(self, architecture) -> float:\n        \"\"\"Evaluate architecture by full training\"\"\"\n        model = self._build_model(architecture)\n        \n        # Training loop\n        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n        criterion = nn.CrossEntropyLoss()\n        \n        for epoch in range(self.num_epochs):\n            for batch in self.dataset:\n                inputs, targets = batch\n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                loss.backward()\n                optimizer.step()\n        \n        # Evaluate on validation set\n        return self._evaluate_model(model)\n    \n    def _build_model(self, architecture):\n        \"\"\"Build model from architecture description\"\"\"\n        # Implementation depends on architecture format\n        pass\n    \n    def _evaluate_model(self, model):\n        \"\"\"Evaluate model accuracy\"\"\"\n        # Implementation for model evaluation\n        pass\n```\n\n### Early Stopping\n\nReduces training time while maintaining correlation with full training.\n\n```python\nclass EarlyStoppingEvaluator:\n    def __init__(self, dataset, max_epochs: int = 20, patience: int = 5):\n        self.dataset = dataset\n        self.max_epochs = max_epochs\n        self.patience = patience\n    \n    def evaluate(self, architecture) -> float:\n        \"\"\"Evaluate with early stopping\"\"\"\n        model = self._build_model(architecture)\n        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n        criterion = nn.CrossEntropyLoss()\n        \n        best_val_acc = 0\n        patience_counter = 0\n        \n        for epoch in range(self.max_epochs):\n            # Training\n            train_loss = self._train_epoch(model, optimizer, criterion)\n            \n            # Validation\n            val_acc = self._validate_epoch(model)\n            \n            # Early stopping check\n            if val_acc > best_val_acc:\n                best_val_acc = val_acc\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                if patience_counter >= self.patience:\n                    break\n        \n        return best_val_acc\n```\n\n### Weight Sharing (One-Shot)\n\nTrains a super-network once and evaluates sub-networks by inheritance.\n\n```python\nclass WeightSharingEvaluator:\n    def __init__(self, supernet: nn.Module, dataset):\n        self.supernet = supernet\n        self.dataset = dataset\n        self.trained = False\n    \n    def train_supernet(self):\n        \"\"\"Train the supernet once\"\"\"\n        if self.trained:\n            return\n        \n        optimizer = torch.optim.SGD(self.supernet.parameters(), lr=0.01)\n        criterion = nn.CrossEntropyLoss()\n        \n        for epoch in range(50):  # Train supernet\n            for batch in self.dataset:\n                inputs, targets = batch\n                optimizer.zero_grad()\n                \n                # Sample random path through supernet\n                self.supernet.sample_active_subnet()\n                outputs = self.supernet(inputs)\n                loss = criterion(outputs, targets)\n                loss.backward()\n                optimizer.step()\n        \n        self.trained = True\n    \n    def evaluate(self, architecture) -> float:\n        \"\"\"Evaluate architecture using trained supernet\"\"\"\n        if not self.trained:\n            self.train_supernet()\n        \n        # Configure supernet for specific architecture\n        self.supernet.set_active_subnet(architecture)\n        \n        # Evaluate on validation set\n        return self._evaluate_subnet()\n    \n    def _evaluate_subnet(self):\n        \"\"\"Evaluate current subnet configuration\"\"\"\n        self.supernet.eval()\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for batch in self.dataset:\n                inputs, targets = batch\n                outputs = self.supernet(inputs)\n                _, predicted = torch.max(outputs.data, 1)\n                total += targets.size(0)\n                correct += (predicted == targets).sum().item()\n        \n        return correct / total\n```\n\n## Advanced Techniques\n\n### Progressive Search\n\nGradually increases search space complexity.\n\n```python\nclass ProgressiveSearch:\n    def __init__(self, base_search_space, max_complexity: int = 5):\n        self.base_search_space = base_search_space\n        self.max_complexity = max_complexity\n        self.current_complexity = 1\n        self.search_strategy = EvolutionarySearch()\n    \n    def search(self, iterations_per_stage: int = 100):\n        \"\"\"Progressive search with increasing complexity\"\"\"\n        best_architectures = []\n        \n        for complexity in range(1, self.max_complexity + 1):\n            self.current_complexity = complexity\n            \n            # Search at current complexity level\n            for _ in range(iterations_per_stage):\n                architecture = self._sample_architecture_at_complexity()\n                performance = self._evaluate_architecture(architecture)\n                self.search_strategy.update(architecture, performance)\n            \n            # Get best architecture at this complexity\n            best_arch = max(self.search_strategy.history, \n                          key=lambda x: x[1])\n            best_architectures.append(best_arch)\n        \n        return best_architectures\n    \n    def _sample_architecture_at_complexity(self):\n        \"\"\"Sample architecture with limited complexity\"\"\"\n        arch = self.base_search_space.sample_architecture()\n        # Limit architecture complexity\n        arch['layers'] = arch['layers'][:self.current_complexity * 3]\n        return arch\n```\n\n### Multi-Objective NAS\n\nOptimizes multiple objectives simultaneously.\n\n```python\nclass MultiObjectiveNAS:\n    def __init__(self, objectives: List[str]):\n        self.objectives = objectives  # e.g., ['accuracy', 'latency', 'flops']\n        self.pareto_front = []\n    \n    def evaluate_architecture(self, architecture) -> Dict[str, float]:\n        \"\"\"Evaluate architecture on multiple objectives\"\"\"\n        results = {}\n        \n        if 'accuracy' in self.objectives:\n            results['accuracy'] = self._evaluate_accuracy(architecture)\n        \n        if 'latency' in self.objectives:\n            results['latency'] = self._evaluate_latency(architecture)\n        \n        if 'flops' in self.objectives:\n            results['flops'] = self._evaluate_flops(architecture)\n        \n        return results\n    \n    def update_pareto_front(self, architecture, objectives):\n        \"\"\"Update Pareto front with new architecture\"\"\"\n        # Check if architecture is dominated\n        dominated = False\n        for pareto_arch, pareto_obj in self.pareto_front:\n            if self._dominates(pareto_obj, objectives):\n                dominated = True\n                break\n        \n        if not dominated:\n            # Remove dominated architectures\n            self.pareto_front = [\n                (arch, obj) for arch, obj in self.pareto_front\n                if not self._dominates(objectives, obj)\n            ]\n            # Add new architecture\n            self.pareto_front.append((architecture, objectives))\n    \n    def _dominates(self, obj1: Dict, obj2: Dict) -> bool:\n        \"\"\"Check if obj1 dominates obj2\"\"\"\n        better_in_all = True\n        strictly_better_in_one = False\n        \n        for objective in self.objectives:\n            if objective in ['accuracy']:  # Higher is better\n                if obj1[objective] < obj2[objective]:\n                    better_in_all = False\n                elif obj1[objective] > obj2[objective]:\n                    strictly_better_in_one = True\n            else:  # Lower is better (latency, flops)\n                if obj1[objective] > obj2[objective]:\n                    better_in_all = False\n                elif obj1[objective] < obj2[objective]:\n                    strictly_better_in_one = True\n        \n        return better_in_all and strictly_better_in_one\n```\n\n## Implementation Examples\n\n### Complete DARTS Implementation\n\n```python\nclass DARTSCell(nn.Module):\n    def __init__(self, num_nodes: int, channels: int):\n        super().__init__()\n        self.num_nodes = num_nodes\n        self.channels = channels\n        \n        # Mixed operations for each edge\n        self.mixed_ops = nn.ModuleList()\n        for i in range(num_nodes):\n            for j in range(2 + i):  # Each node connects to all previous nodes\n                self.mixed_ops.append(MixedOp(channels))\n        \n        # Architecture parameters\n        self.alpha = nn.Parameter(torch.randn(len(self.mixed_ops), 8))\n    \n    def forward(self, inputs):\n        # inputs[0] and inputs[1] are the two input nodes\n        states = [inputs[0], inputs[1]]\n        \n        offset = 0\n        for i in range(self.num_nodes):\n            # Collect inputs from all previous nodes\n            node_inputs = []\n            for j in range(len(states)):\n                op_idx = offset + j\n                node_inputs.append(self.mixed_ops[op_idx](states[j], self.alpha[op_idx]))\n            \n            # Sum all inputs to this node\n            state = sum(node_inputs)\n            states.append(state)\n            offset += len(states) - 1\n        \n        # Concatenate final nodes\n        return torch.cat(states[-self.num_nodes:], dim=1)\n\nclass MixedOp(nn.Module):\n    def __init__(self, channels: int):\n        super().__init__()\n        self.ops = nn.ModuleList([\n            SepConv(channels, channels, 3, 1, 1),\n            SepConv(channels, channels, 5, 1, 2),\n            DilConv(channels, channels, 3, 1, 2, 2),\n            DilConv(channels, channels, 5, 1, 4, 2),\n            nn.MaxPool2d(3, 1, 1),\n            nn.AvgPool2d(3, 1, 1),\n            Identity(),\n            Zero()\n        ])\n    \n    def forward(self, x, alpha):\n        # Apply weighted sum of operations\n        weights = torch.softmax(alpha, dim=0)\n        return sum(w * op(x) for w, op in zip(weights, self.ops))\n\nclass SepConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, \n                     groups=in_channels),\n            nn.Conv2d(in_channels, out_channels, 1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        return self.conv(x)\n\nclass DilConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, \n                     dilation=dilation),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        return self.conv(x)\n\nclass Identity(nn.Module):\n    def forward(self, x):\n        return x\n\nclass Zero(nn.Module):\n    def forward(self, x):\n        return torch.zeros_like(x)\n\n# Complete DARTS Network\nclass DARTSNetwork(nn.Module):\n    def __init__(self, num_classes: int, num_cells: int = 8, channels: int = 36):\n        super().__init__()\n        self.num_cells = num_cells\n        self.channels = channels\n        \n        # Stem\n        self.stem = nn.Sequential(\n            nn.Conv2d(3, channels, 3, 1, 1),\n            nn.BatchNorm2d(channels)\n        )\n        \n        # Cells\n        self.cells = nn.ModuleList()\n        for i in range(num_cells):\n            if i in [num_cells // 3, 2 * num_cells // 3]:\n                # Reduction cell\n                self.cells.append(DARTSCell(4, channels))\n                channels *= 2\n            else:\n                # Normal cell\n                self.cells.append(DARTSCell(4, channels))\n        \n        # Classifier\n        self.classifier = nn.Linear(channels, num_classes)\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n    \n    def forward(self, x):\n        x = self.stem(x)\n        \n        for cell in self.cells:\n            x = cell([x, x])  # Use same input for both inputs\n        \n        x = self.global_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        \n        return x\n```\n\n### Evolutionary Search Example\n\n```python\nclass EvolutionaryNAS:\n    def __init__(self, population_size: int = 50, generations: int = 100):\n        self.population_size = population_size\n        self.generations = generations\n        self.population = []\n        self.fitness_history = []\n    \n    def run_search(self, search_space, evaluator):\n        \"\"\"Run evolutionary search\"\"\"\n        # Initialize population\n        self.population = [\n            search_space.sample_architecture() \n            for _ in range(self.population_size)\n        ]\n        \n        for generation in range(self.generations):\n            # Evaluate population\n            fitness_scores = []\n            for individual in self.population:\n                fitness = evaluator.evaluate(individual)\n                fitness_scores.append(fitness)\n            \n            self.fitness_history.append(max(fitness_scores))\n            \n            # Selection and reproduction\n            new_population = []\n            for _ in range(self.population_size):\n                # Tournament selection\n                parent1 = self._tournament_selection(fitness_scores)\n                parent2 = self._tournament_selection(fitness_scores)\n                \n                # Crossover\n                child = self._crossover(parent1, parent2)\n                \n                # Mutation\n                child = self._mutate(child, search_space)\n                \n                new_population.append(child)\n            \n            self.population = new_population\n        \n        # Return best architecture\n        final_fitness = [evaluator.evaluate(ind) for ind in self.population]\n        best_idx = np.argmax(final_fitness)\n        return self.population[best_idx], final_fitness[best_idx]\n    \n    def _tournament_selection(self, fitness_scores, tournament_size: int = 3):\n        \"\"\"Tournament selection\"\"\"\n        tournament_indices = random.sample(range(len(fitness_scores)), tournament_size)\n        tournament_fitness = [fitness_scores[i] for i in tournament_indices]\n        winner_idx = tournament_indices[np.argmax(tournament_fitness)]\n        return self.population[winner_idx]\n    \n    def _crossover(self, parent1, parent2):\n        \"\"\"Single-point crossover\"\"\"\n        child = parent1.copy()\n        \n        if 'layers' in parent1 and 'layers' in parent2:\n            # Crossover layers\n            min_length = min(len(parent1['layers']), len(parent2['layers']))\n            if min_length > 1:\n                crossover_point = random.randint(1, min_length - 1)\n                child['layers'] = (parent1['layers'][:crossover_point] + \n                                 parent2['layers'][crossover_point:])\n        \n        return child\n    \n    def _mutate(self, individual, search_space, mutation_rate: float = 0.1):\n        \"\"\"Mutate individual\"\"\"\n        if random.random() < mutation_rate:\n            if 'layers' in individual and individual['layers']:\n                # Randomly mutate a layer\n                layer_idx = random.randint(0, len(individual['layers']) - 1)\n                layer = individual['layers'][layer_idx]\n                \n                # Mutate operation\n                if random.random() < 0.5:\n                    layer['operation'] = random.choice(search_space.operations)\n                \n                # Mutate filters\n                if random.random() < 0.5:\n                    layer['filters'] = random.choice([32, 64, 128, 256, 512])\n        \n        return individual\n\n\n### Reinforcement Learning NAS Example\n\n```python\nclass RLNASController(nn.Module):\n    def __init__(self, num_layers: int = 6, lstm_size: int = 32, \n                 num_branches: int = 6, out_filters: int = 48):\n        super().__init__()\n        self.num_layers = num_layers\n        self.lstm_size = lstm_size\n        self.num_branches = num_branches\n        self.out_filters = out_filters\n        \n        # LSTM controller\n        self.lstm = nn.LSTMCell(lstm_size, lstm_size)\n        \n        # Embedding layers for different architecture decisions\n        self.g_emb = nn.Embedding(1, lstm_size)  # Go embedding\n        self.encoder = nn.Linear(lstm_size, lstm_size)\n        \n        # Decision heads\n        self.conv_op = nn.Linear(lstm_size, len(CONV_OPS))\n        self.conv_ksize = nn.Linear(lstm_size, len(CONV_KERNEL_SIZES))\n        self.conv_filters = nn.Linear(lstm_size, len(CONV_FILTERS))\n        self.pooling_op = nn.Linear(lstm_size, len(POOLING_OPS))\n        self.pooling_ksize = nn.Linear(lstm_size, len(POOLING_KERNEL_SIZES))\n        \n        # Initialize parameters\n        self.reset_parameters()\n    \n    def reset_parameters(self):\n        init_range = 0.1\n        for param in self.parameters():\n            param.data.uniform_(-init_range, init_range)\n    \n    def forward(self, batch_size: int = 1):\n        \"\"\"Sample architecture using the controller\"\"\"\n        # Initialize hidden state\n        h = torch.zeros(batch_size, self.lstm_size)\n        c = torch.zeros(batch_size, self.lstm_size)\n        \n        # Start with go embedding\n        inputs = self.g_emb.weight.repeat(batch_size, 1)\n        \n        # Store sampled architecture\n        arc_seq = []\n        entropies = []\n        log_probs = []\n        \n        for layer_id in range(self.num_layers):\n            # LSTM step\n            h, c = self.lstm(inputs, (h, c))\n            \n            # Sample convolution operation\n            conv_op_logits = self.conv_op(h)\n            conv_op_prob = F.softmax(conv_op_logits, dim=-1)\n            conv_op_log_prob = F.log_softmax(conv_op_logits, dim=-1)\n            conv_op_entropy = -(conv_op_log_prob * conv_op_prob).sum(1, keepdim=True)\n            \n            conv_op_sample = torch.multinomial(conv_op_prob, 1)\n            conv_op_sample = conv_op_sample.view(-1)\n            \n            arc_seq.append(conv_op_sample)\n            entropies.append(conv_op_entropy)\n            log_probs.append(conv_op_log_prob.gather(1, conv_op_sample.unsqueeze(1)))\n            \n            # Sample kernel size\n            conv_ksize_logits = self.conv_ksize(h)\n            conv_ksize_prob = F.softmax(conv_ksize_logits, dim=-1)\n            conv_ksize_log_prob = F.log_softmax(conv_ksize_logits, dim=-1)\n            conv_ksize_entropy = -(conv_ksize_log_prob * conv_ksize_prob).sum(1, keepdim=True)\n            \n            conv_ksize_sample = torch.multinomial(conv_ksize_prob, 1)\n            conv_ksize_sample = conv_ksize_sample.view(-1)\n            \n            arc_seq.append(conv_ksize_sample)\n            entropies.append(conv_ksize_entropy)\n            log_probs.append(conv_ksize_log_prob.gather(1, conv_ksize_sample.unsqueeze(1)))\n            \n            # Continue for other decisions...\n            inputs = h  # Use current hidden state as input for next step\n        \n        return arc_seq, torch.cat(log_probs), torch.cat(entropies)\n\n# Constants for architecture choices\nCONV_OPS = ['conv', 'depthwise_conv', 'separable_conv']\nCONV_KERNEL_SIZES = [3, 5, 7]\nCONV_FILTERS = [24, 36, 48, 64]\nPOOLING_OPS = ['max_pool', 'avg_pool', 'no_pool']\nPOOLING_KERNEL_SIZES = [2, 3]\n\nclass RLNASTrainer:\n    def __init__(self, controller, child_model_builder, evaluator):\n        self.controller = controller\n        self.child_model_builder = child_model_builder\n        self.evaluator = evaluator\n        \n        # Controller optimizer\n        self.controller_optimizer = torch.optim.Adam(\n            controller.parameters(), lr=3.5e-4\n        )\n        \n        # Baseline for variance reduction\n        self.baseline = None\n        self.baseline_decay = 0.99\n        \n    def train_controller(self, num_epochs: int = 2000):\n        \"\"\"Train the controller using REINFORCE\"\"\"\n        for epoch in range(num_epochs):\n            # Sample architectures\n            arc_seq, log_probs, entropies = self.controller()\n            \n            # Build and evaluate child model\n            child_model = self.child_model_builder.build(arc_seq)\n            reward = self.evaluator.evaluate(child_model)\n            \n            # Update baseline\n            if self.baseline is None:\n                self.baseline = reward\n            else:\n                self.baseline = self.baseline_decay * self.baseline + \\\n                              (1 - self.baseline_decay) * reward\n            \n            # Compute advantage\n            advantage = reward - self.baseline\n            \n            # Controller loss (REINFORCE)\n            controller_loss = -log_probs * advantage\n            controller_loss = controller_loss.sum()\n            \n            # Add entropy regularization\n            entropy_penalty = -entropies.sum() * 1e-4\n            total_loss = controller_loss + entropy_penalty\n            \n            # Update controller\n            self.controller_optimizer.zero_grad()\n            total_loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.controller.parameters(), 5.0)\n            self.controller_optimizer.step()\n            \n            if epoch % 100 == 0:\n                print(f'Epoch {epoch}, Reward: {reward:.4f}, '\n                      f'Baseline: {self.baseline:.4f}, Loss: {total_loss.item():.4f}')\n```\n\n## Best Practices\n\n### 1. Search Space Design Guidelines\n\n```python\nclass SearchSpaceDesignPrinciples:\n    \"\"\"\n    Guidelines for designing effective search spaces\n    \"\"\"\n    \n    def __init__(self):\n        self.principles = {\n            'expressiveness': 'Include diverse operations and connections',\n            'efficiency': 'Balance search space size with computational cost',\n            'human_knowledge': 'Incorporate domain-specific insights',\n            'scalability': 'Design for different input sizes and tasks'\n        }\n    \n    def design_macro_space(self, task_type: str):\n        \"\"\"Design macro search space based on task\"\"\"\n        if task_type == 'image_classification':\n            return {\n                'operations': ['conv3x3', 'conv5x5', 'depthwise_conv', 'pointwise_conv',\n                              'max_pool', 'avg_pool', 'global_pool', 'identity'],\n                'max_layers': 20,\n                'channels': [16, 32, 64, 128, 256, 512],\n                'skip_connections': True,\n                'batch_norm': True,\n                'activation': ['relu', 'relu6', 'swish']\n            }\n        elif task_type == 'object_detection':\n            return {\n                'operations': ['conv3x3', 'conv5x5', 'depthwise_conv', 'atrous_conv',\n                              'max_pool', 'avg_pool', 'identity'],\n                'max_layers': 30,\n                'channels': [32, 64, 128, 256, 512, 1024],\n                'skip_connections': True,\n                'fpn_layers': True,\n                'anchor_scales': [32, 64, 128, 256, 512]\n            }\n    \n    def validate_search_space(self, search_space):\n        \"\"\"Validate search space design\"\"\"\n        issues = []\n        \n        # Check for minimal viable operations\n        if len(search_space.get('operations', [])) < 3:\n            issues.append(\"Too few operations - may limit expressiveness\")\n        \n        # Check for identity operation\n        if 'identity' not in search_space.get('operations', []):\n            issues.append(\"Missing identity operation - may hurt skip connections\")\n        \n        # Check channel progression\n        channels = search_space.get('channels', [])\n        if channels and not all(channels[i] <= channels[i+1] for i in range(len(channels)-1)):\n            issues.append(\"Non-monotonic channel progression\")\n        \n        return issues\n```\n\n### 2. Training Strategies\n\n```python\nclass NASTrainingStrategies:\n    \"\"\"Advanced training strategies for NAS\"\"\"\n    \n    def __init__(self):\n        self.strategies = {}\n    \n    def progressive_shrinking(self, supernet, dataset, stages: int = 4):\n        \"\"\"Progressive shrinking strategy\"\"\"\n        current_channels = supernet.max_channels\n        \n        for stage in range(stages):\n            # Reduce search space\n            target_channels = current_channels // (2 ** stage)\n            supernet.set_channel_constraint(target_channels)\n            \n            # Train for this stage\n            self._train_stage(supernet, dataset, epochs=50)\n            \n            print(f\"Stage {stage + 1}: Max channels = {target_channels}\")\n    \n    def sandwich_sampling(self, supernet, dataset):\n        \"\"\"Sandwich sampling for training efficiency\"\"\"\n        optimizer = torch.optim.SGD(supernet.parameters(), lr=0.01)\n        criterion = nn.CrossEntropyLoss()\n        \n        for epoch in range(100):\n            for batch in dataset:\n                inputs, targets = batch\n                \n                # Sample architectures: largest, smallest, and random\n                architectures = [\n                    supernet.largest_architecture(),\n                    supernet.smallest_architecture(),\n                    supernet.random_architecture(),\n                    supernet.random_architecture()\n                ]\n                \n                total_loss = 0\n                for arch in architectures:\n                    supernet.set_active_subnet(arch)\n                    optimizer.zero_grad()\n                    outputs = supernet(inputs)\n                    loss = criterion(outputs, targets)\n                    loss.backward()\n                    total_loss += loss.item()\n                \n                optimizer.step()\n                \n                if epoch % 10 == 0:\n                    print(f\"Epoch {epoch}, Loss: {total_loss/len(architectures):.4f}\")\n    \n    def knowledge_distillation(self, student_arch, teacher_model, dataset):\n        \"\"\"Knowledge distillation for architecture evaluation\"\"\"\n        student_model = self._build_model(student_arch)\n        \n        optimizer = torch.optim.SGD(student_model.parameters(), lr=0.01)\n        kd_loss = nn.KLDivLoss()\n        ce_loss = nn.CrossEntropyLoss()\n        \n        alpha = 0.7  # Distillation weight\n        temperature = 4.0\n        \n        for epoch in range(50):\n            for batch in dataset:\n                inputs, targets = batch\n                \n                # Teacher predictions\n                with torch.no_grad():\n                    teacher_outputs = teacher_model(inputs)\n                    teacher_probs = F.softmax(teacher_outputs / temperature, dim=1)\n                \n                # Student predictions\n                student_outputs = student_model(inputs)\n                student_log_probs = F.log_softmax(student_outputs / temperature, dim=1)\n                \n                # Combined loss\n                distill_loss = kd_loss(student_log_probs, teacher_probs)\n                hard_loss = ce_loss(student_outputs, targets)\n                \n                total_loss = alpha * distill_loss + (1 - alpha) * hard_loss\n                \n                optimizer.zero_grad()\n                total_loss.backward()\n                optimizer.step()\n        \n        return self._evaluate_model(student_model)\n```\n\n### 3. Evaluation and Benchmarking\n\n```python\nclass NASBenchmarking:\n    \"\"\"Benchmarking and evaluation utilities\"\"\"\n    \n    def __init__(self):\n        self.metrics = {}\n    \n    def comprehensive_evaluation(self, architecture, datasets):\n        \"\"\"Comprehensive evaluation across multiple metrics\"\"\"\n        results = {}\n        \n        # Build model\n        model = self._build_model(architecture)\n        \n        # Accuracy metrics\n        for dataset_name, dataset in datasets.items():\n            accuracy = self._evaluate_accuracy(model, dataset)\n            results[f'{dataset_name}_accuracy'] = accuracy\n        \n        # Efficiency metrics\n        results['params'] = self._count_parameters(model)\n        results['flops'] = self._count_flops(model)\n        results['latency'] = self._measure_latency(model)\n        results['memory'] = self._measure_memory(model)\n        \n        # Robustness metrics\n        results['adversarial_robustness'] = self._evaluate_adversarial_robustness(model)\n        results['noise_robustness'] = self._evaluate_noise_robustness(model)\n        \n        return results\n    \n    def _count_parameters(self, model):\n        \"\"\"Count model parameters\"\"\"\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \n    def _count_flops(self, model, input_size=(1, 3, 224, 224)):\n        \"\"\"Count FLOPs using a simple profiler\"\"\"\n        def flop_count_hook(module, input, output):\n            if isinstance(module, nn.Conv2d):\n                # Conv2d FLOPs\n                batch_size, in_channels, input_height, input_width = input[0].shape\n                output_height, output_width = output.shape[2], output.shape[3]\n                kernel_height, kernel_width = module.kernel_size\n                \n                flops = batch_size * in_channels * kernel_height * kernel_width * \\\n                       output_height * output_width * module.out_channels\n                \n                if hasattr(module, 'flops'):\n                    module.flops += flops\n                else:\n                    module.flops = flops\n        \n        # Register hooks\n        hooks = []\n        for module in model.modules():\n            if isinstance(module, (nn.Conv2d, nn.Linear)):\n                hooks.append(module.register_forward_hook(flop_count_hook))\n        \n        # Forward pass\n        model.eval()\n        with torch.no_grad():\n            dummy_input = torch.randn(input_size)\n            model(dummy_input)\n        \n        # Collect FLOPs\n        total_flops = 0\n        for module in model.modules():\n            if hasattr(module, 'flops'):\n                total_flops += module.flops\n        \n        # Remove hooks\n        for hook in hooks:\n            hook.remove()\n        \n        return total_flops\n    \n    def _measure_latency(self, model, input_size=(1, 3, 224, 224), runs=100):\n        \"\"\"Measure inference latency\"\"\"\n        model.eval()\n        dummy_input = torch.randn(input_size)\n        \n        # Warmup\n        for _ in range(10):\n            with torch.no_grad():\n                model(dummy_input)\n        \n        # Measure\n        torch.cuda.synchronize() if torch.cuda.is_available() else None\n        start_time = time.time()\n        \n        for _ in range(runs):\n            with torch.no_grad():\n                model(dummy_input)\n        \n        torch.cuda.synchronize() if torch.cuda.is_available() else None\n        end_time = time.time()\n        \n        return (end_time - start_time) / runs\n    \n    def compare_search_methods(self, methods, search_space, evaluator, runs=5):\n        \"\"\"Compare different search methods\"\"\"\n        results = {}\n        \n        for method_name, method in methods.items():\n            method_results = []\n            \n            for run in range(runs):\n                # Set random seed for reproducibility\n                torch.manual_seed(run)\n                random.seed(run)\n                np.random.seed(run)\n                \n                # Run search\n                best_arch, best_performance = method.search(search_space, evaluator)\n                method_results.append({\n                    'architecture': best_arch,\n                    'performance': best_performance,\n                    'run': run\n                })\n            \n            results[method_name] = method_results\n        \n        return self._analyze_comparison_results(results)\n    \n    def _analyze_comparison_results(self, results):\n        \"\"\"Analyze comparison results\"\"\"\n        analysis = {}\n        \n        for method_name, method_results in results.items():\n            performances = [r['performance'] for r in method_results]\n            \n            analysis[method_name] = {\n                'mean_performance': np.mean(performances),\n                'std_performance': np.std(performances),\n                'best_performance': np.max(performances),\n                'worst_performance': np.min(performances),\n                'median_performance': np.median(performances)\n            }\n        \n        # Rank methods\n        ranked_methods = sorted(analysis.items(), \n                              key=lambda x: x[1]['mean_performance'], \n                              reverse=True)\n        \n        return {\n            'detailed_results': analysis,\n            'ranking': ranked_methods,\n            'summary': self._generate_summary(ranked_methods)\n        }\n    \n    def _generate_summary(self, ranked_methods):\n        \"\"\"Generate summary of comparison\"\"\"\n        summary = []\n        summary.append(\"=== NAS Method Comparison Results ===\")\n        \n        for i, (method_name, stats) in enumerate(ranked_methods):\n            summary.append(f\"{i+1}. {method_name}:\")\n            summary.append(f\"   Mean: {stats['mean_performance']:.4f}\")\n            summary.append(f\"   Std:  {stats['std_performance']:.4f}\")\n            summary.append(f\"   Best: {stats['best_performance']:.4f}\")\n            summary.append(\"\")\n        \n        return \"\\n\".join(summary)\n```\n\n## Tools and Frameworks\n\n### Popular NAS Libraries\n\n```python\nclass NASFrameworkGuide:\n    \"\"\"Guide to popular NAS frameworks\"\"\"\n    \n    def __init__(self):\n        self.frameworks = {\n            'nni': {\n                'description': 'Microsoft\\'s Neural Network Intelligence toolkit',\n                'strengths': ['Easy to use', 'Multiple search algorithms', 'Good documentation'],\n                'installation': 'pip install nni',\n                'example_usage': '''\nfrom nni.nas.pytorch import DartsTrainer\nfrom nni.nas.pytorch.search_space_zoo import ENASMacroSearchSpace\n\n# Define search space\nsearch_space = ENASMacroSearchSpace()\n\n# Create trainer\ntrainer = DartsTrainer(\n    model=search_space,\n    loss=nn.CrossEntropyLoss(),\n    optimizer=torch.optim.SGD(search_space.parameters(), lr=0.1)\n)\n\n# Train\ntrainer.train()\n'''\n            },\n            'automl': {\n                'description': 'Google\\'s AutoML toolkit',\n                'strengths': ['State-of-the-art methods', 'Research-oriented'],\n                'installation': 'Custom installation from GitHub',\n                'example_usage': '''\n# Example for AdaNet\nimport adanet\n\n# Define search space and estimator\nestimator = adanet.Estimator(\n    head=head,\n    subnetwork_generator=generator,\n    max_iteration_steps=1000\n)\n\n# Train\nestimator.train(input_fn=train_input_fn)\n'''\n            },\n            'optuna': {\n                'description': 'Hyperparameter optimization framework',\n                'strengths': ['Flexible', 'Multiple optimization algorithms', 'Good for hyperparameter tuning'],\n                'installation': 'pip install optuna',\n                'example_usage': '''\nimport optuna\n\ndef objective(trial):\n    # Define architecture parameters\n    n_layers = trial.suggest_int('n_layers', 2, 8)\n    n_filters = trial.suggest_int('n_filters', 16, 128)\n    \n    # Build and train model\n    model = build_model(n_layers, n_filters)\n    accuracy = train_and_evaluate(model)\n    \n    return accuracy\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n'''\n            },\n            'ray_tune': {\n                'description': 'Distributed hyperparameter tuning',\n                'strengths': ['Scalable', 'Multiple search algorithms', 'Good for distributed training'],\n                'installation': 'pip install ray[tune]',\n                'example_usage': '''\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\n\ndef trainable(config):\n    model = build_model(config)\n    accuracy = train_model(model)\n    tune.report(accuracy=accuracy)\n\nscheduler = ASHAScheduler(metric=\"accuracy\", mode=\"max\")\nresult = tune.run(\n    trainable,\n    resources_per_trial={\"cpu\": 2, \"gpu\": 1},\n    config={\n        \"n_layers\": tune.choice([2, 4, 6, 8]),\n        \"n_filters\": tune.choice([16, 32, 64, 128])\n    },\n    scheduler=scheduler\n)\n'''\n            }\n        }\n    \n    def get_framework_recommendation(self, use_case: str):\n        \"\"\"Get framework recommendation based on use case\"\"\"\n        recommendations = {\n            'research': ['automl', 'custom_implementation'],\n            'production': ['nni', 'ray_tune'],\n            'hyperparameter_tuning': ['optuna', 'ray_tune'],\n            'distributed_training': ['ray_tune'],\n            'beginner_friendly': ['nni', 'optuna']\n        }\n        \n        return recommendations.get(use_case, ['nni'])\n\n# Example: Custom NAS implementation using PyTorch\nclass CustomNASExample:\n    def __init__(self):\n        self.search_space = None\n        self.search_strategy = None\n        self.evaluator = None\n    \n    def setup_cifar10_nas(self):\n        \"\"\"Setup NAS for CIFAR-10\"\"\"\n        # Define search space\n        self.search_space = CellSearchSpace(num_nodes=4)\n        \n        # Define search strategy\n        self.search_strategy = EvolutionarySearch(population_size=20)\n        \n        # Define evaluator\n        self.evaluator = EarlyStoppingEvaluator(\n            dataset=self._get_cifar10_dataset(),\n            max_epochs=10,\n            patience=3\n        )\n    \n    def run_nas_experiment(self):\n        \"\"\"Run complete NAS experiment\"\"\"\n        framework = NASFramework(\n            search_space=self.search_space,\n            search_strategy=self.search_strategy,\n            performance_estimator=self.evaluator\n        )\n        \n        # Run search\n        best_result = framework.search(num_iterations=100)\n        \n        # Analyze results\n        print(f\"Best architecture: {best_result['architecture']}\")\n        print(f\"Best performance: {best_result['performance']:.4f}\")\n        \n        return best_result\n    \n    def _get_cifar10_dataset(self):\n        \"\"\"Get CIFAR-10 dataset\"\"\"\n        # Implementation depends on your data loading setup\n        pass\n```\n\nThis comprehensive guide covers the essential aspects of Neural Architecture Search, from theoretical foundations to practical implementations. The code examples provide a solid foundation for understanding and implementing NAS algorithms, while the best practices and framework recommendations help guide practical applications.\n\nThe key takeaways from this guide are:\n\n1. **NAS Framework**: Understanding the three core components (search space, search strategy, performance estimation) is crucial\n2. **Search Space Design**: Careful design of search spaces balances expressiveness with computational efficiency\n3. **Search Strategies**: Different strategies have different trade-offs between exploration and exploitation\n4. **Performance Estimation**: Efficient evaluation methods are essential for practical NAS\n5. **Implementation**: Modern frameworks provide good starting points, but custom implementations offer more control\n6. **Best Practices**: Following established guidelines improves NAS effectiveness and reproducibility\n\nFor beginners, I recommend starting with existing frameworks like NNI or Optuna, then gradually moving to custom implementations as understanding deepens. For research applications, implementing methods from scratch using the patterns shown in this guide provides the most flexibility and insight into the algorithm\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}