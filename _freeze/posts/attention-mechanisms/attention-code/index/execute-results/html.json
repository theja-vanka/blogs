{
  "hash": "5d39f4be7b0762e04a359d81eec59392",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Attention Mechanisms: Transformers vs CNNs - Complete Code Guide\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-06-27\"\ncategories: [code, tutorial, intermediate]\nformat:\n  html:\n    code-fold: false\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# Attention Mechanisms: Transformers vs CNNs - Complete Code Guide\n![](attention.png)\n\n## Introduction\n\nAttention mechanisms have revolutionized deep learning by allowing models to focus on relevant parts of input data. While Transformers use self-attention as their core mechanism, CNNs incorporate attention as an enhancement to their convolutional operations.\n\n## Transformer Attention\n\n### Multi-Head Self-Attention Implementation\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads, dropout=0.1):\n        super().__init__()\n        assert d_model % num_heads == 0\n        \n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        \n        # Linear projections for Q, K, V\n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n        self.w_o = nn.Linear(d_model, d_model)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        \"\"\"\n        Compute scaled dot-product attention\n        Args:\n            Q: Query matrix [batch_size, num_heads, seq_len, d_k]\n            K: Key matrix [batch_size, num_heads, seq_len, d_k]\n            V: Value matrix [batch_size, num_heads, seq_len, d_k]\n            mask: Optional mask [batch_size, 1, seq_len, seq_len]\n        \"\"\"\n        # Calculate attention scores\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        \n        # Apply mask if provided\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        \n        # Softmax normalization\n        attention_weights = F.softmax(scores, dim=-1)\n        attention_weights = self.dropout(attention_weights)\n        \n        # Apply attention to values\n        output = torch.matmul(attention_weights, V)\n        \n        return output, attention_weights\n    \n    def forward(self, query, key, value, mask=None):\n        batch_size, seq_len = query.size(0), query.size(1)\n        \n        # Linear projections and reshape for multi-head\n        Q = self.w_q(query).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n        K = self.w_k(key).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n        V = self.w_v(value).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n        \n        # Apply attention\n        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n        \n        # Concatenate heads\n        attention_output = attention_output.transpose(1, 2).contiguous().view(\n            batch_size, seq_len, self.d_model\n        )\n        \n        # Final linear projection\n        output = self.w_o(attention_output)\n        \n        return output, attention_weights\n\n# Complete Transformer Block\nclass TransformerBlock(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        \n        self.feed_forward = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_ff, d_model)\n        )\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        # Self-attention with residual connection\n        attn_output, attn_weights = self.attention(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        \n        # Feed-forward with residual connection\n        ff_output = self.feed_forward(x)\n        x = self.norm2(x + self.dropout(ff_output))\n        \n        return x, attn_weights\n\n# Example usage\ndef transformer_example():\n    batch_size, seq_len, d_model = 2, 10, 512\n    num_heads, d_ff = 8, 2048\n    \n    # Create input\n    x = torch.randn(batch_size, seq_len, d_model)\n    \n    # Create transformer block\n    transformer = TransformerBlock(d_model, num_heads, d_ff)\n    \n    # Forward pass\n    output, attention_weights = transformer(x)\n    \n    print(f\"Input shape: {x.shape}\")\n    print(f\"Output shape: {output.shape}\")\n    print(f\"Attention weights shape: {attention_weights.shape}\")\n    \n    return output, attention_weights\n```\n\n### Positional Encoding for Transformers\n\n```python\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        \n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                           -(math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        self.register_buffer('pe', pe.unsqueeze(0))\n    \n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n```\n\n## CNN Attention\n\n### Spatial Attention Mechanism\n\n```python\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super().__init__()\n        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self, x):\n        # Compute spatial statistics\n        avg_pool = torch.mean(x, dim=1, keepdim=True)  # [B, 1, H, W]\n        max_pool, _ = torch.max(x, dim=1, keepdim=True)  # [B, 1, H, W]\n        \n        # Concatenate along channel dimension\n        spatial_info = torch.cat([avg_pool, max_pool], dim=1)  # [B, 2, H, W]\n        \n        # Generate attention map\n        attention_map = self.conv(spatial_info)  # [B, 1, H, W]\n        attention_map = self.sigmoid(attention_map)\n        \n        # Apply attention\n        return x * attention_map\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_channels, reduction_ratio=16):\n        super().__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        \n        self.fc = nn.Sequential(\n            nn.Linear(in_channels, in_channels // reduction_ratio, bias=False),\n            nn.ReLU(),\n            nn.Linear(in_channels // reduction_ratio, in_channels, bias=False)\n        )\n        \n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self, x):\n        b, c, h, w = x.size()\n        \n        # Global average pooling and max pooling\n        avg_pool = self.avg_pool(x).view(b, c)\n        max_pool = self.max_pool(x).view(b, c)\n        \n        # Channel attention\n        avg_out = self.fc(avg_pool)\n        max_out = self.fc(max_pool)\n        \n        # Combine and apply sigmoid\n        channel_attention = self.sigmoid(avg_out + max_out).view(b, c, 1, 1)\n        \n        return x * channel_attention\n\n# CBAM (Convolutional Block Attention Module)\nclass CBAM(nn.Module):\n    def __init__(self, in_channels, reduction_ratio=16, kernel_size=7):\n        super().__init__()\n        self.channel_attention = ChannelAttention(in_channels, reduction_ratio)\n        self.spatial_attention = SpatialAttention(kernel_size)\n    \n    def forward(self, x):\n        # Apply channel attention first\n        x = self.channel_attention(x)\n        # Then apply spatial attention\n        x = self.spatial_attention(x)\n        return x\n\n# Self-Attention for CNNs\nclass SelfAttention2D(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.in_channels = in_channels\n        \n        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, 1)\n        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, 1)\n        self.value_conv = nn.Conv2d(in_channels, in_channels, 1)\n        \n        self.gamma = nn.Parameter(torch.zeros(1))\n        self.softmax = nn.Softmax(dim=-1)\n    \n    def forward(self, x):\n        batch_size, channels, height, width = x.size()\n        \n        # Generate Q, K, V\n        proj_query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n        proj_key = self.key_conv(x).view(batch_size, -1, width * height)\n        proj_value = self.value_conv(x).view(batch_size, -1, width * height)\n        \n        # Compute attention\n        energy = torch.bmm(proj_query, proj_key)\n        attention = self.softmax(energy)\n        \n        # Apply attention to values\n        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n        out = out.view(batch_size, channels, height, width)\n        \n        # Residual connection with learnable weight\n        out = self.gamma * out + x\n        \n        return out\n\n# CNN with Attention\nclass AttentionCNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        \n        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n        self.cbam1 = CBAM(64)\n        \n        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n        self.cbam2 = CBAM(128)\n        \n        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n        self.self_attention = SelfAttention2D(256)\n        \n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Linear(256, num_classes)\n        \n    def forward(self, x):\n        # First block\n        x = F.relu(self.conv1(x))\n        x = self.cbam1(x)\n        x = F.max_pool2d(x, 2)\n        \n        # Second block\n        x = F.relu(self.conv2(x))\n        x = self.cbam2(x)\n        x = F.max_pool2d(x, 2)\n        \n        # Third block with self-attention\n        x = F.relu(self.conv3(x))\n        x = self.self_attention(x)\n        x = F.max_pool2d(x, 2)\n        \n        # Classification\n        x = self.pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        \n        return x\n\n# Example usage\ndef cnn_attention_example():\n    batch_size = 4\n    x = torch.randn(batch_size, 3, 224, 224)\n    \n    model = AttentionCNN(num_classes=1000)\n    output = model(x)\n    \n    print(f\"Input shape: {x.shape}\")\n    print(f\"Output shape: {output.shape}\")\n    \n    return output\n```\n\n## Key Differences\n\n### 1. Computational Complexity\n\n```python\ndef attention_complexity_comparison():\n    \"\"\"\n    Compare computational complexity of different attention mechanisms\n    \"\"\"\n    \n    # Transformer Self-Attention: O(n²d) where n=sequence length, d=dimension\n    def transformer_complexity(seq_len, d_model):\n        return seq_len * seq_len * d_model\n    \n    # CNN Spatial Attention: O(HW) where H=height, W=width\n    def spatial_attention_complexity(height, width):\n        return height * width\n    \n    # CNN Channel Attention: O(C) where C=channels\n    def channel_attention_complexity(channels):\n        return channels\n    \n    # Example calculations\n    seq_len, d_model = 512, 512\n    height, width, channels = 224, 224, 256\n    \n    transformer_ops = transformer_complexity(seq_len, d_model)\n    spatial_ops = spatial_attention_complexity(height, width)\n    channel_ops = channel_attention_complexity(channels)\n    \n    print(f\"Transformer attention operations: {transformer_ops:,}\")\n    print(f\"CNN spatial attention operations: {spatial_ops:,}\")\n    print(f\"CNN channel attention operations: {channel_ops:,}\")\n    \n    return {\n        'transformer': transformer_ops,\n        'spatial': spatial_ops,\n        'channel': channel_ops\n    }\n```\n\n### 2. Information Flow Patterns\n\n```python\nclass AttentionAnalysis:\n    @staticmethod\n    def analyze_transformer_attention(attention_weights):\n        \"\"\"\n        Analyze attention patterns in Transformers\n        Args:\n            attention_weights: [batch_size, num_heads, seq_len, seq_len]\n        \"\"\"\n        batch_size, num_heads, seq_len, _ = attention_weights.shape\n        \n        # Average attention across heads\n        avg_attention = attention_weights.mean(dim=1)  # [batch_size, seq_len, seq_len]\n        \n        # Compute attention statistics\n        max_attention = avg_attention.max(dim=-1)[0]  # Max attention per position\n        attention_entropy = -torch.sum(avg_attention * torch.log(avg_attention + 1e-8), dim=-1)\n        \n        return {\n            'max_attention': max_attention,\n            'attention_entropy': attention_entropy,\n            'global_connectivity': True,  # All positions can attend to all others\n            'pattern': 'sequence-to-sequence'\n        }\n    \n    @staticmethod\n    def analyze_cnn_attention(feature_map, attention_map):\n        \"\"\"\n        Analyze attention patterns in CNNs\n        Args:\n            feature_map: [batch_size, channels, height, width]\n            attention_map: [batch_size, 1, height, width] or [batch_size, channels, 1, 1]\n        \"\"\"\n        if attention_map.dim() == 4 and attention_map.size(2) == 1:\n            # Channel attention\n            attention_type = 'channel'\n            local_connectivity = False\n        else:\n            # Spatial attention\n            attention_type = 'spatial'\n            local_connectivity = True\n        \n        return {\n            'attention_type': attention_type,\n            'local_connectivity': local_connectivity,\n            'pattern': 'spatial-hierarchy' if attention_type == 'spatial' else 'channel-selection'\n        }\n```\n\n## Performance Comparison\n\n```python\nimport time\nimport torch.nn.functional as F\n\nclass PerformanceBenchmark:\n    def __init__(self):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    def benchmark_transformer_attention(self, batch_size=32, seq_len=512, d_model=512, num_heads=8):\n        \"\"\"Benchmark Transformer attention\"\"\"\n        model = MultiHeadAttention(d_model, num_heads).to(self.device)\n        x = torch.randn(batch_size, seq_len, d_model).to(self.device)\n        \n        # Warmup\n        for _ in range(10):\n            _ = model(x, x, x)\n        \n        # Benchmark\n        torch.cuda.synchronize() if torch.cuda.is_available() else None\n        start_time = time.time()\n        \n        for _ in range(100):\n            output, _ = model(x, x, x)\n        \n        torch.cuda.synchronize() if torch.cuda.is_available() else None\n        end_time = time.time()\n        \n        return (end_time - start_time) / 100\n    \n    def benchmark_cnn_attention(self, batch_size=32, channels=256, height=56, width=56):\n        \"\"\"Benchmark CNN attention\"\"\"\n        model = CBAM(channels).to(self.device)\n        x = torch.randn(batch_size, channels, height, width).to(self.device)\n        \n        # Warmup\n        for _ in range(10):\n            _ = model(x)\n        \n        # Benchmark\n        torch.cuda.synchronize() if torch.cuda.is_available() else None\n        start_time = time.time()\n        \n        for _ in range(100):\n            output = model(x)\n        \n        torch.cuda.synchronize() if torch.cuda.is_available() else None\n        end_time = time.time()\n        \n        return (end_time - start_time) / 100\n    \n    def run_comparison(self):\n        \"\"\"Run performance comparison\"\"\"\n        transformer_time = self.benchmark_transformer_attention()\n        cnn_time = self.benchmark_cnn_attention()\n        \n        print(f\"Transformer attention time: {transformer_time:.4f}s\")\n        print(f\"CNN attention time: {cnn_time:.4f}s\")\n        print(f\"Speedup: {transformer_time/cnn_time:.2f}x\")\n        \n        return {\n            'transformer_time': transformer_time,\n            'cnn_time': cnn_time,\n            'speedup': transformer_time/cnn_time\n        }\n\n# Memory usage comparison\ndef memory_comparison():\n    \"\"\"Compare memory usage of different attention mechanisms\"\"\"\n    \n    def get_memory_usage():\n        if torch.cuda.is_available():\n            return torch.cuda.memory_allocated() / 1024**2  # MB\n        return 0\n    \n    # Clear memory\n    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n    \n    # Transformer attention\n    transformer_model = MultiHeadAttention(512, 8)\n    x = torch.randn(32, 512, 512)\n    \n    if torch.cuda.is_available():\n        transformer_model = transformer_model.cuda()\n        x = x.cuda()\n    \n    transformer_memory = get_memory_usage()\n    _, _ = transformer_model(x, x, x)\n    transformer_memory = get_memory_usage() - transformer_memory\n    \n    # Clear memory\n    del transformer_model, x\n    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n    \n    # CNN attention\n    cnn_model = CBAM(256)\n    x = torch.randn(32, 256, 56, 56)\n    \n    if torch.cuda.is_available():\n        cnn_model = cnn_model.cuda()\n        x = x.cuda()\n    \n    cnn_memory = get_memory_usage()\n    _ = cnn_model(x)\n    cnn_memory = get_memory_usage() - cnn_memory\n    \n    print(f\"Transformer attention memory: {transformer_memory:.2f} MB\")\n    print(f\"CNN attention memory: {cnn_memory:.2f} MB\")\n    \n    return {\n        'transformer_memory': transformer_memory,\n        'cnn_memory': cnn_memory\n    }\n```\n\n## When to Use Each\n\n### Decision Framework\n\n```python\nclass AttentionSelector:\n    @staticmethod\n    def recommend_attention_type(data_type, sequence_length=None, spatial_dims=None, \n                                computational_budget='medium', task_type='classification'):\n        \"\"\"\n        Recommend attention mechanism based on requirements\n        \n        Args:\n            data_type: 'sequential', 'spatial', 'mixed'\n            sequence_length: Length of sequences (for sequential data)\n            spatial_dims: (height, width) for spatial data\n            computational_budget: 'low', 'medium', 'high'\n            task_type: 'classification', 'generation', 'detection'\n        \"\"\"\n        \n        recommendations = []\n        \n        # Sequential data\n        if data_type == 'sequential':\n            if sequence_length and sequence_length > 1000 and computational_budget == 'low':\n                recommendations.append({\n                    'type': 'Local Attention',\n                    'reason': 'Long sequences with limited compute',\n                    'implementation': 'sliding_window_attention'\n                })\n            else:\n                recommendations.append({\n                    'type': 'Transformer Self-Attention',\n                    'reason': 'Global context modeling for sequences',\n                    'implementation': 'MultiHeadAttention'\n                })\n        \n        # Spatial data\n        elif data_type == 'spatial':\n            if spatial_dims and spatial_dims[0] * spatial_dims[1] > 224 * 224:\n                recommendations.append({\n                    'type': 'CNN Spatial + Channel Attention',\n                    'reason': 'High-resolution spatial data',\n                    'implementation': 'CBAM'\n                })\n            else:\n                recommendations.append({\n                    'type': 'CNN Self-Attention',\n                    'reason': 'Moderate resolution with global context',\n                    'implementation': 'SelfAttention2D'\n                })\n        \n        # Mixed data\n        elif data_type == 'mixed':\n            recommendations.append({\n                'type': 'Hybrid Attention',\n                'reason': 'Combined sequential and spatial processing',\n                'implementation': 'transformer_cnn_hybrid'\n            })\n        \n        return recommendations\n    \n    @staticmethod\n    def create_hybrid_model(input_shape, num_classes):\n        \"\"\"Create a hybrid model combining both attention types\"\"\"\n        \n        class HybridAttentionModel(nn.Module):\n            def __init__(self, input_shape, num_classes):\n                super().__init__()\n                \n                # CNN backbone with attention\n                self.cnn_backbone = nn.Sequential(\n                    nn.Conv2d(input_shape[0], 64, 3, padding=1),\n                    nn.ReLU(),\n                    CBAM(64),\n                    nn.MaxPool2d(2),\n                    \n                    nn.Conv2d(64, 128, 3, padding=1),\n                    nn.ReLU(),\n                    CBAM(128),\n                    nn.MaxPool2d(2),\n                    \n                    nn.Conv2d(128, 256, 3, padding=1),\n                    nn.ReLU(),\n                    SelfAttention2D(256)\n                )\n                \n                # Flatten and prepare for transformer\n                self.flatten = nn.AdaptiveAvgPool2d(8)  # 8x8 spatial grid\n                self.embed_dim = 256\n                \n                # Transformer layers\n                self.transformer = nn.Sequential(\n                    *[TransformerBlock(self.embed_dim, 8, 1024) for _ in range(3)]\n                )\n                \n                # Classification head\n                self.classifier = nn.Linear(self.embed_dim, num_classes)\n            \n            def forward(self, x):\n                # CNN processing\n                x = self.cnn_backbone(x)\n                \n                # Reshape for transformer\n                batch_size = x.size(0)\n                x = self.flatten(x)  # [B, 256, 8, 8]\n                x = x.flatten(2).transpose(1, 2)  # [B, 64, 256]\n                \n                # Transformer processing\n                for transformer_block in self.transformer:\n                    x, _ = transformer_block(x)\n                \n                # Global average pooling and classification\n                x = x.mean(dim=1)  # [B, 256]\n                x = self.classifier(x)\n                \n                return x\n        \n        return HybridAttentionModel(input_shape, num_classes)\n\n# Usage examples\ndef usage_examples():\n    \"\"\"Demonstrate when to use each attention type\"\"\"\n    \n    selector = AttentionSelector()\n    \n    # Example 1: NLP task\n    nlp_rec = selector.recommend_attention_type(\n        data_type='sequential',\n        sequence_length=512,\n        computational_budget='high',\n        task_type='generation'\n    )\n    \n    # Example 2: Computer Vision task\n    cv_rec = selector.recommend_attention_type(\n        data_type='spatial',\n        spatial_dims=(224, 224),\n        computational_budget='medium',\n        task_type='classification'\n    )\n    \n    # Example 3: Video analysis\n    video_rec = selector.recommend_attention_type(\n        data_type='mixed',\n        sequence_length=30,\n        spatial_dims=(112, 112),\n        computational_budget='high',\n        task_type='detection'\n    )\n    \n    print(\"NLP Recommendation:\", nlp_rec)\n    print(\"Computer Vision Recommendation:\", cv_rec)\n    print(\"Video Analysis Recommendation:\", video_rec)\n    \n    return nlp_rec, cv_rec, video_rec\n```\n\n## Summary\n\n| Aspect | Transformer Attention | CNN Attention |\n|--------|----------------------|---------------|\n| **Scope** | Global, all-to-all | Local, spatial/channel-wise |\n| **Complexity** | O(n²) | O(HW) or O(C) |\n| **Best For** | Sequential data, language | Spatial data, images |\n| **Memory** | High | Moderate |\n| **Parallelization** | Limited by sequence length | Highly parallelizable |\n| **Interpretability** | Attention weights show relationships | Spatial/channel importance maps |\n\nChoose Transformer attention for tasks requiring global context modeling, and CNN attention for spatially-structured data where local relationships dominate. Consider hybrid approaches for complex multi-modal tasks.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}