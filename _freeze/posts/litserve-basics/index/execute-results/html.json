{
  "hash": "eb495595f28ca7b2c6699767f47d839e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"LitServe Code Guide\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-05-27\"\ncategories: [code, mlops, beginner]\nformat:\n  html:\n    code-fold: false\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# LitServe Code Guide\n![](litserve.jpg)\n\nLitServe is a high-performance, flexible AI model serving framework designed to deploy machine learning models with minimal code. It provides automatic batching, GPU acceleration, and easy scaling capabilities.\n\n## Table of Contents\n1. [Installation](#installation)\n2. [Basic Usage](#basic-usage)\n3. [Core Concepts](#core-concepts)\n4. [Advanced Features](#advanced-features)\n5. [Configuration Options](#configuration-options)\n6. [Examples](#examples)\n7. [Best Practices](#best-practices)\n8. [Troubleshooting](#troubleshooting)\n\n## Installation\n\n```bash\n# Install LitServe\npip install litserve\n\n# For GPU support\npip install litserve[gpu]\n\n# For development dependencies\npip install litserve[dev]\n```\n\n## Basic Usage\n\n### 1. Creating Your First LitServe API\n\n```python\nimport litserve as ls\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nclass SimpleTextGenerator(ls.LitAPI):\n    def setup(self, device):\n        # Load model and tokenizer during server startup\n        self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n        self.model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n        self.model.to(device)\n        self.model.eval()\n    \n    def decode_request(self, request):\n        # Process incoming request\n        return request[\"prompt\"]\n    \n    def predict(self, x):\n        # Run inference\n        inputs = self.tokenizer.encode(x, return_tensors=\"pt\")\n        with torch.no_grad():\n            outputs = self.model.generate(\n                inputs, \n                max_length=100, \n                num_return_sequences=1,\n                temperature=0.7\n            )\n        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    def encode_response(self, output):\n        # Format response\n        return {\"generated_text\": output}\n\n# Create and start server\nif __name__ == \"__main__\":\n    api = SimpleTextGenerator()\n    server = ls.LitServer(api, accelerator=\"auto\", max_batch_size=4)\n    server.run(port=8000)\n```\n\n### 2. Making Requests to Your API\n\n```python\nimport requests\n\n# Test the API\nresponse = requests.post(\n    \"http://localhost:8000/predict\",\n    json={\"prompt\": \"The future of AI is\"}\n)\nprint(response.json())\n```\n\n## Core Concepts\n\n### LitAPI Class Structure\n\nEvery LitServe API must inherit from `ls.LitAPI` and implement these core methods:\n\n```python\nclass MyAPI(ls.LitAPI):\n    def setup(self, device):\n        \"\"\"Initialize models, load weights, set up preprocessing\"\"\"\n        pass\n    \n    def decode_request(self, request):\n        \"\"\"Parse and validate incoming requests\"\"\"\n        pass\n    \n    def predict(self, x):\n        \"\"\"Run model inference\"\"\"\n        pass\n    \n    def encode_response(self, output):\n        \"\"\"Format model output for HTTP response\"\"\"\n        pass\n```\n\n### Optional Methods\n\n```python\nclass AdvancedAPI(ls.LitAPI):\n    def batch(self, inputs):\n        \"\"\"Custom batching logic (optional)\"\"\"\n        return inputs\n    \n    def unbatch(self, output):\n        \"\"\"Custom unbatching logic (optional)\"\"\"\n        return output\n    \n    def preprocess(self, input_data):\n        \"\"\"Additional preprocessing (optional)\"\"\"\n        return input_data\n    \n    def postprocess(self, output):\n        \"\"\"Additional postprocessing (optional)\"\"\"\n        return output\n```\n\n## Advanced Features\n\n### 1. Custom Batching\n\n```python\nclass BatchedImageClassifier(ls.LitAPI):\n    def setup(self, device):\n        from torchvision import models, transforms\n        self.model = models.resnet50(pretrained=True)\n        self.model.to(device)\n        self.model.eval()\n        \n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                               std=[0.229, 0.224, 0.225])\n        ])\n    \n    def decode_request(self, request):\n        from PIL import Image\n        import base64\n        import io\n        \n        # Decode base64 image\n        image_data = base64.b64decode(request[\"image\"])\n        image = Image.open(io.BytesIO(image_data))\n        return self.transform(image).unsqueeze(0)\n    \n    def batch(self, inputs):\n        # Custom batching for images\n        return torch.cat(inputs, dim=0)\n    \n    def predict(self, batch):\n        with torch.no_grad():\n            outputs = self.model(batch)\n            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n        return probabilities\n    \n    def unbatch(self, output):\n        # Split batch back to individual predictions\n        return [pred.unsqueeze(0) for pred in output]\n    \n    def encode_response(self, output):\n        # Get top prediction\n        confidence, predicted = torch.max(output, 1)\n        return {\n            \"class_id\": predicted.item(),\n            \"confidence\": confidence.item()\n        }\n```\n\n### 2. Streaming Responses\n\n```python\nclass StreamingChatAPI(ls.LitAPI):\n    def setup(self, device):\n        from transformers import AutoTokenizer, AutoModelForCausalLM\n        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n        self.model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n        self.model.to(device)\n    \n    def decode_request(self, request):\n        return request[\"message\"]\n    \n    def predict(self, x):\n        # Generator for streaming\n        inputs = self.tokenizer.encode(x, return_tensors=\"pt\")\n        \n        for i in range(50):  # Generate up to 50 tokens\n            with torch.no_grad():\n                outputs = self.model(inputs)\n                next_token_logits = outputs.logits[:, -1, :]\n                next_token = torch.multinomial(\n                    torch.softmax(next_token_logits, dim=-1), \n                    num_samples=1\n                )\n                inputs = torch.cat([inputs, next_token], dim=-1)\n                \n                # Yield each token\n                token_text = self.tokenizer.decode(next_token[0])\n                yield {\"token\": token_text}\n                \n                # Stop if end token\n                if next_token.item() == self.tokenizer.eos_token_id:\n                    break\n    \n    def encode_response(self, output):\n        return output\n\n# Enable streaming\nserver = ls.LitServer(api, accelerator=\"auto\", stream=True)\n```\n\n### 3. Multiple GPU Support\n\n```python\n# Automatic multi-GPU scaling\nserver = ls.LitServer(\n    api, \n    accelerator=\"auto\",\n    devices=\"auto\",  # Use all available GPUs\n    max_batch_size=8\n)\n\n# Specify specific GPUs\nserver = ls.LitServer(\n    api,\n    accelerator=\"gpu\",\n    devices=[0, 1, 2],  # Use GPUs 0, 1, and 2\n    max_batch_size=16\n)\n```\n\n### 4. Authentication and Middleware\n\n```python\nclass AuthenticatedAPI(ls.LitAPI):\n    def setup(self, device):\n        # Your model setup\n        pass\n    \n    def authenticate(self, request):\n        \"\"\"Custom authentication logic\"\"\"\n        api_key = request.headers.get(\"Authorization\")\n        if not api_key or not self.validate_api_key(api_key):\n            raise ls.AuthenticationError(\"Invalid API key\")\n        return True\n    \n    def validate_api_key(self, api_key):\n        # Your API key validation logic\n        valid_keys = [\"your-secret-key-1\", \"your-secret-key-2\"]\n        return api_key.replace(\"Bearer \", \"\") in valid_keys\n    \n    def decode_request(self, request):\n        return request[\"data\"]\n    \n    def predict(self, x):\n        # Your prediction logic\n        return f\"Processed: {x}\"\n    \n    def encode_response(self, output):\n        return {\"result\": output}\n```\n\n## Configuration Options\n\n### Server Configuration\n\n```python\nserver = ls.LitServer(\n    api=api,\n    accelerator=\"auto\",           # \"auto\", \"cpu\", \"gpu\", \"mps\"\n    devices=\"auto\",               # Device selection\n    max_batch_size=4,            # Maximum batch size\n    batch_timeout=0.1,           # Batch timeout in seconds\n    workers_per_device=1,        # Workers per device\n    timeout=30,                  # Request timeout\n    stream=False,                # Enable streaming\n    spec=None,                   # Custom OpenAPI spec\n)\n```\n\n### Environment Variables\n\n```bash\n# Set device preferences\nexport CUDA_VISIBLE_DEVICES=0,1,2\n\n# Set batch configuration\nexport LITSERVE_MAX_BATCH_SIZE=8\nexport LITSERVE_BATCH_TIMEOUT=0.05\n\n# Set worker configuration\nexport LITSERVE_WORKERS_PER_DEVICE=2\n```\n\n### Custom Configuration File\n\n```python\n# config.py\nclass Config:\n    MAX_BATCH_SIZE = 8\n    BATCH_TIMEOUT = 0.1\n    WORKERS_PER_DEVICE = 2\n    ACCELERATOR = \"auto\"\n    TIMEOUT = 60\n\n# Use in your API\nfrom config import Config\n\nserver = ls.LitServer(\n    api, \n    max_batch_size=Config.MAX_BATCH_SIZE,\n    batch_timeout=Config.BATCH_TIMEOUT,\n    workers_per_device=Config.WORKERS_PER_DEVICE\n)\n```\n\n## Examples\n\n### 1. Image Classification API\n\n```python\nimport litserve as ls\nimport torch\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport base64\nimport io\n\nclass ImageClassificationAPI(ls.LitAPI):\n    def setup(self, device):\n        # Load pre-trained ResNet model\n        self.model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n        self.model.to(device)\n        self.model.eval()\n        \n        # Image preprocessing\n        self.preprocess = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                               std=[0.229, 0.224, 0.225]),\n        ])\n        \n        # Load ImageNet class labels\n        with open('imagenet_classes.txt') as f:\n            self.classes = [line.strip() for line in f.readlines()]\n    \n    def decode_request(self, request):\n        # Decode base64 image\n        encoded_image = request[\"image\"]\n        image_bytes = base64.b64decode(encoded_image)\n        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n        return self.preprocess(image).unsqueeze(0)\n    \n    def predict(self, x):\n        with torch.no_grad():\n            output = self.model(x)\n            probabilities = torch.nn.functional.softmax(output[0], dim=0)\n        return probabilities\n    \n    def encode_response(self, output):\n        # Get top 5 predictions\n        top5_prob, top5_catid = torch.topk(output, 5)\n        results = []\n        for i in range(top5_prob.size(0)):\n            results.append({\n                \"class\": self.classes[top5_catid[i]],\n                \"probability\": float(top5_prob[i])\n            })\n        return {\"predictions\": results}\n\nif __name__ == \"__main__\":\n    api = ImageClassificationAPI()\n    server = ls.LitServer(api, accelerator=\"auto\", max_batch_size=4)\n    server.run(port=8000)\n```\n\n### 2. Text Embedding API\n\n```python\nimport litserve as ls\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\n\nclass TextEmbeddingAPI(ls.LitAPI):\n    def setup(self, device):\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        self.model.to(device)\n    \n    def decode_request(self, request):\n        texts = request.get(\"texts\", [])\n        if isinstance(texts, str):\n            texts = [texts]\n        return texts\n    \n    def predict(self, texts):\n        embeddings = self.model.encode(texts)\n        return embeddings\n    \n    def encode_response(self, embeddings):\n        return {\n            \"embeddings\": embeddings.tolist(),\n            \"dimension\": embeddings.shape[1] if len(embeddings.shape) > 1 else len(embeddings)\n        }\n\nif __name__ == \"__main__\":\n    api = TextEmbeddingAPI()\n    server = ls.LitServer(api, accelerator=\"auto\", max_batch_size=32)\n    server.run(port=8000)\n```\n\n### 3. Multi-Modal API (Text + Image)\n\n```python\nimport litserve as ls\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\nimport base64\nimport io\n\nclass ImageCaptioningAPI(ls.LitAPI):\n    def setup(self, device):\n        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n        self.model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n        self.model.to(device)\n    \n    def decode_request(self, request):\n        # Handle both image and optional text input\n        encoded_image = request[\"image\"]\n        text_prompt = request.get(\"text\", \"\")\n        \n        image_bytes = base64.b64decode(encoded_image)\n        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n        \n        return {\"image\": image, \"text\": text_prompt}\n    \n    def predict(self, inputs):\n        processed = self.processor(\n            images=inputs[\"image\"], \n            text=inputs[\"text\"], \n            return_tensors=\"pt\"\n        )\n        \n        with torch.no_grad():\n            outputs = self.model.generate(**processed, max_length=50)\n        \n        caption = self.processor.decode(outputs[0], skip_special_tokens=True)\n        return caption\n    \n    def encode_response(self, caption):\n        return {\"caption\": caption}\n```\n\n## Best Practices\n\n### 1. Resource Management\n\n```python\nclass OptimizedAPI(ls.LitAPI):\n    def setup(self, device):\n        # Use torch.jit.script for optimization\n        self.model = torch.jit.script(your_model)\n        \n        # Enable mixed precision if using GPU\n        if device.type == 'cuda':\n            self.scaler = torch.cuda.amp.GradScaler()\n        \n        # Pre-allocate tensors for common shapes\n        self.common_shapes = {}\n    \n    def predict(self, x):\n        # Use autocast for mixed precision\n        if hasattr(self, 'scaler'):\n            with torch.cuda.amp.autocast():\n                return self.model(x)\n        return self.model(x)\n```\n\n### 2. Error Handling\n\n```python\nclass RobustAPI(ls.LitAPI):\n    def decode_request(self, request):\n        try:\n            # Validate required fields\n            if \"input\" not in request:\n                raise ls.ValidationError(\"Missing required field: input\")\n            \n            data = request[\"input\"]\n            \n            # Type validation\n            if not isinstance(data, (str, list)):\n                raise ls.ValidationError(\"Input must be string or list\")\n            \n            return data\n        except Exception as e:\n            raise ls.ValidationError(f\"Request parsing failed: {str(e)}\")\n    \n    def predict(self, x):\n        try:\n            result = self.model(x)\n            return result\n        except torch.cuda.OutOfMemoryError:\n            raise ls.ServerError(\"GPU memory exhausted\")\n        except Exception as e:\n            raise ls.ServerError(f\"Prediction failed: {str(e)}\")\n```\n\n### 3. Monitoring and Logging\n\n```python\nimport logging\nimport time\n\nclass MonitoredAPI(ls.LitAPI):\n    def setup(self, device):\n        self.logger = logging.getLogger(__name__)\n        self.request_count = 0\n        # Your model setup\n    \n    def decode_request(self, request):\n        self.request_count += 1\n        self.logger.info(f\"Processing request #{self.request_count}\")\n        return request[\"data\"]\n    \n    def predict(self, x):\n        start_time = time.time()\n        result = self.model(x)\n        inference_time = time.time() - start_time\n        \n        self.logger.info(f\"Inference completed in {inference_time:.3f}s\")\n        return result\n```\n\n### 4. Model Versioning\n\n```python\nclass VersionedAPI(ls.LitAPI):\n    def setup(self, device):\n        self.version = \"1.0.0\"\n        self.model_path = f\"models/model_v{self.version}\"\n        # Load versioned model\n    \n    def encode_response(self, output):\n        return {\n            \"result\": output,\n            \"model_version\": self.version,\n            \"timestamp\": time.time()\n        }\n```\n\n## Troubleshooting\n\n### Common Issues and Solutions\n\n#### 1. CUDA Out of Memory\n```python\n# Solution: Reduce batch size or implement gradient checkpointing\nserver = ls.LitServer(api, max_batch_size=2)  # Reduce batch size\n\n# Or clear cache in your predict method\ndef predict(self, x):\n    torch.cuda.empty_cache()  # Clear unused memory\n    result = self.model(x)\n    return result\n```\n\n#### 2. Slow Inference\n```python\n# Enable model optimization\ndef setup(self, device):\n    self.model.eval()  # Set to evaluation mode\n    self.model = torch.jit.script(self.model)  # JIT compilation\n    \n    # Use half precision if supported\n    if device.type == 'cuda':\n        self.model.half()\n```\n\n#### 3. Request Timeout\n```python\n# Increase timeout settings\nserver = ls.LitServer(\n    api, \n    timeout=60,  # Increase request timeout\n    batch_timeout=1.0  # Increase batch timeout\n)\n```\n\n#### 4. Port Already in Use\n```python\n# Check and kill existing processes\nimport subprocess\nsubprocess.run([\"lsof\", \"-ti:8000\", \"|\", \"xargs\", \"kill\", \"-9\"], shell=True)\n\n# Or use a different port\nserver.run(port=8001)\n```\n\n### Performance Optimization Tips\n\n1. **Use appropriate batch sizes**: Start with small batches and gradually increase\n2. **Enable GPU acceleration**: Use `accelerator=\"auto\"` for automatic GPU detection\n3. **Optimize model loading**: Load models once in `setup()`, not in `predict()`\n4. **Use mixed precision**: Enable autocast for GPU inference\n5. **Profile your code**: Use tools like `torch.profiler` to identify bottlenecks\n6. **Cache preprocessed data**: Store frequently used transformations\n\n### Deployment Checklist\n\n- [ ] Test API locally with various input types\n- [ ] Validate error handling for malformed requests\n- [ ] Check memory usage under load\n- [ ] Verify GPU utilization (if using GPUs)\n- [ ] Test with maximum expected batch size\n- [ ] Implement proper logging and monitoring\n- [ ] Set up health check endpoints\n- [ ] Configure appropriate timeouts\n- [ ] Test authentication (if implemented)\n- [ ] Verify response format consistency\n\nThis guide covers the essential aspects of using LitServe for deploying AI models. For the most up-to-date information, always refer to the official LitServe documentation.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}