{
  "hash": "7d4f39ef93995f520333f8bf97b3acef",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Distributed Training with PyTorch - Complete Code Guide\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-06-21\"\ncategories: [code, tutorial, intermediate]\nformat:\n  html:\n    code-fold: false\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# Distributed Training with PyTorch - Complete Code Guide\n\n![](distributed.png)\n\n## Introduction\n\nDistributed training allows you to scale PyTorch models across multiple GPUs and machines, dramatically reducing training time for large models and datasets. This guide covers practical implementation patterns from basic data parallelism to advanced distributed strategies.\n\n## Core Concepts\n\n### Key Terminology\n- **World Size**: Total number of processes participating in training\n- **Rank**: Unique identifier for each process (0 to world_size-1)\n- **Local Rank**: Process identifier within a single node/machine\n- **Process Group**: Collection of processes that can communicate with each other\n- **Backend**: Communication backend (NCCL for GPU, Gloo for CPU)\n\n### Communication Patterns\n- **All-Reduce**: Combine values from all processes and distribute the result\n- **Broadcast**: Send data from one process to all others\n- **Gather**: Collect data from all processes to one process\n- **Scatter**: Distribute data from one process to all others\n\n## Setup and Initialization\n\n### Basic Environment Setup\n\n```python\nimport os\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\n\ndef setup_distributed(rank, world_size, backend='nccl'):\n    \"\"\"Initialize distributed training environment\"\"\"\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    \n    # Initialize process group\n    dist.init_process_group(\n        backend=backend,\n        rank=rank,\n        world_size=world_size\n    )\n    \n    # Set device for current process\n    torch.cuda.set_device(rank)\n\ndef cleanup_distributed():\n    \"\"\"Clean up distributed training\"\"\"\n    dist.destroy_process_group()\n```\n\n### Multi-Node Setup\n\n```python\ndef setup_multinode(rank, world_size, master_addr, master_port):\n    \"\"\"Setup for multi-node distributed training\"\"\"\n    os.environ['MASTER_ADDR'] = master_addr\n    os.environ['MASTER_PORT'] = str(master_port)\n    os.environ['RANK'] = str(rank)\n    os.environ['WORLD_SIZE'] = str(world_size)\n    \n    dist.init_process_group(\n        backend='nccl',\n        init_method='env://',\n        rank=rank,\n        world_size=world_size\n    )\n```\n\n## Data Parallel Training\n\n### Simple DataParallel (Single Node)\n\n```python\nimport torch.nn as nn\n\nclass SimpleModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\ndef train_dataparallel():\n    \"\"\"Basic DataParallel training\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Create model and wrap with DataParallel\n    model = SimpleModel(784, 256, 10)\n    if torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model)\n    model.to(device)\n    \n    # Setup optimizer and loss\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n    \n    # Training loop\n    for epoch in range(num_epochs):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(device), target.to(device)\n            \n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n```\n\n## Distributed Data Parallel (DDP)\n\n### Basic DDP Implementation\n\n```python\ndef train_ddp(rank, world_size):\n    \"\"\"Distributed Data Parallel training function\"\"\"\n    # Setup distributed environment\n    setup_distributed(rank, world_size)\n    \n    # Create model and move to GPU\n    model = SimpleModel(784, 256, 10).to(rank)\n    \n    # Wrap model with DDP\n    ddp_model = DDP(model, device_ids=[rank])\n    \n    # Setup distributed sampler\n    train_sampler = DistributedSampler(\n        train_dataset,\n        num_replicas=world_size,\n        rank=rank,\n        shuffle=True\n    )\n    \n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        sampler=train_sampler,\n        num_workers=4,\n        pin_memory=True\n    )\n    \n    # Setup optimizer and loss\n    optimizer = torch.optim.Adam(ddp_model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n    \n    # Training loop\n    for epoch in range(num_epochs):\n        train_sampler.set_epoch(epoch)  # Important for shuffling\n        \n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(rank), target.to(rank)\n            \n            optimizer.zero_grad()\n            output = ddp_model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n            \n            if rank == 0 and batch_idx % 100 == 0:\n                print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n    \n    cleanup_distributed()\n\ndef main():\n    \"\"\"Main function to spawn distributed processes\"\"\"\n    world_size = torch.cuda.device_count()\n    mp.spawn(train_ddp, args=(world_size,), nprocs=world_size, join=True)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Complete Training Script with Validation\n\n```python\nimport time\nfrom torch.utils.tensorboard import SummaryWriter\n\nclass DistributedTrainer:\n    def __init__(self, model, rank, world_size, train_loader, val_loader=None):\n        self.model = model\n        self.rank = rank\n        self.world_size = world_size\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        \n        # Setup DDP\n        self.ddp_model = DDP(model, device_ids=[rank])\n        \n        # Setup optimizer and scheduler\n        self.optimizer = torch.optim.AdamW(\n            self.ddp_model.parameters(),\n            lr=0.001,\n            weight_decay=0.01\n        )\n        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            self.optimizer, T_max=100\n        )\n        self.criterion = nn.CrossEntropyLoss()\n        \n        # Logging (only on rank 0)\n        if rank == 0:\n            self.writer = SummaryWriter('runs/distributed_training')\n    \n    def train_epoch(self, epoch):\n        \"\"\"Train for one epoch\"\"\"\n        self.ddp_model.train()\n        total_loss = 0\n        num_batches = 0\n        \n        for batch_idx, (data, target) in enumerate(self.train_loader):\n            data, target = data.to(self.rank), target.to(self.rank)\n            \n            self.optimizer.zero_grad()\n            output = self.ddp_model(data)\n            loss = self.criterion(output, target)\n            loss.backward()\n            \n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(self.ddp_model.parameters(), max_norm=1.0)\n            \n            self.optimizer.step()\n            \n            total_loss += loss.item()\n            num_batches += 1\n            \n            if self.rank == 0 and batch_idx % 100 == 0:\n                print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n        \n        avg_loss = total_loss / num_batches\n        return avg_loss\n    \n    def validate(self):\n        \"\"\"Validate the model\"\"\"\n        if self.val_loader is None:\n            return None\n            \n        self.ddp_model.eval()\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for data, target in self.val_loader:\n                data, target = data.to(self.rank), target.to(self.rank)\n                output = self.ddp_model(data)\n                loss = self.criterion(output, target)\n                \n                total_loss += loss.item()\n                pred = output.argmax(dim=1)\n                correct += pred.eq(target).sum().item()\n                total += target.size(0)\n        \n        # Gather metrics from all processes\n        total_loss_tensor = torch.tensor(total_loss).to(self.rank)\n        correct_tensor = torch.tensor(correct).to(self.rank)\n        total_tensor = torch.tensor(total).to(self.rank)\n        \n        dist.all_reduce(total_loss_tensor, op=dist.ReduceOp.SUM)\n        dist.all_reduce(correct_tensor, op=dist.ReduceOp.SUM)\n        dist.all_reduce(total_tensor, op=dist.ReduceOp.SUM)\n        \n        avg_loss = total_loss_tensor.item() / self.world_size\n        accuracy = correct_tensor.item() / total_tensor.item()\n        \n        return avg_loss, accuracy\n    \n    def save_checkpoint(self, epoch, loss):\n        \"\"\"Save model checkpoint (only on rank 0)\"\"\"\n        if self.rank == 0:\n            checkpoint = {\n                'epoch': epoch,\n                'model_state_dict': self.ddp_model.module.state_dict(),\n                'optimizer_state_dict': self.optimizer.state_dict(),\n                'scheduler_state_dict': self.scheduler.state_dict(),\n                'loss': loss,\n            }\n            torch.save(checkpoint, f'checkpoint_epoch_{epoch}.pth')\n    \n    def train(self, num_epochs):\n        \"\"\"Complete training loop\"\"\"\n        for epoch in range(num_epochs):\n            start_time = time.time()\n            \n            # Set epoch for distributed sampler\n            if hasattr(self.train_loader.sampler, 'set_epoch'):\n                self.train_loader.sampler.set_epoch(epoch)\n            \n            # Train\n            train_loss = self.train_epoch(epoch)\n            \n            # Validate\n            val_metrics = self.validate()\n            \n            # Step scheduler\n            self.scheduler.step()\n            \n            # Logging and checkpointing (rank 0 only)\n            if self.rank == 0:\n                epoch_time = time.time() - start_time\n                print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, '\n                      f'Time: {epoch_time:.2f}s')\n                \n                if val_metrics:\n                    val_loss, val_acc = val_metrics\n                    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n                    \n                    # TensorBoard logging\n                    self.writer.add_scalar('Loss/Train', train_loss, epoch)\n                    self.writer.add_scalar('Loss/Val', val_loss, epoch)\n                    self.writer.add_scalar('Accuracy/Val', val_acc, epoch)\n                \n                # Save checkpoint\n                if epoch % 10 == 0:\n                    self.save_checkpoint(epoch, train_loss)\n```\n\n## Advanced Patterns\n\n### Mixed Precision Training\n\n```python\nfrom torch.cuda.amp import GradScaler, autocast\n\nclass MixedPrecisionTrainer(DistributedTrainer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.scaler = GradScaler()\n    \n    def train_epoch(self, epoch):\n        \"\"\"Train with mixed precision\"\"\"\n        self.ddp_model.train()\n        total_loss = 0\n        num_batches = 0\n        \n        for batch_idx, (data, target) in enumerate(self.train_loader):\n            data, target = data.to(self.rank), target.to(self.rank)\n            \n            self.optimizer.zero_grad()\n            \n            # Forward pass with autocast\n            with autocast():\n                output = self.ddp_model(data)\n                loss = self.criterion(output, target)\n            \n            # Backward pass with scaled gradients\n            self.scaler.scale(loss).backward()\n            \n            # Gradient clipping with scaled gradients\n            self.scaler.unscale_(self.optimizer)\n            torch.nn.utils.clip_grad_norm_(self.ddp_model.parameters(), max_norm=1.0)\n            \n            # Optimizer step with scaler\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n            \n            total_loss += loss.item()\n            num_batches += 1\n        \n        return total_loss / num_batches\n```\n\n### Model Sharding with FSDP\n\n```python\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\nfrom torch.distributed.fsdp.wrap import size_based_auto_wrap_policy\n\ndef create_fsdp_model(model, rank):\n    \"\"\"Create FSDP wrapped model\"\"\"\n    wrap_policy = size_based_auto_wrap_policy(min_num_params=100000)\n    \n    fsdp_model = FSDP(\n        model,\n        auto_wrap_policy=wrap_policy,\n        mixed_precision=torch.distributed.fsdp.MixedPrecision(\n            param_dtype=torch.float16,\n            reduce_dtype=torch.float16,\n            buffer_dtype=torch.float16\n        ),\n        device_id=rank,\n        sync_module_states=True,\n        sharding_strategy=torch.distributed.fsdp.ShardingStrategy.FULL_SHARD\n    )\n    \n    return fsdp_model\n```\n\n### Pipeline Parallelism\n\n```python\nimport torch.distributed.pipeline.sync as Pipe\n\nclass PipelineModel(nn.Module):\n    def __init__(self, layers_per_partition=2):\n        super().__init__()\n        \n        # Define layers\n        layers = []\n        layers.append(nn.Linear(784, 512))\n        layers.append(nn.ReLU())\n        layers.append(nn.Linear(512, 256))\n        layers.append(nn.ReLU())\n        layers.append(nn.Linear(256, 128))\n        layers.append(nn.ReLU())\n        layers.append(nn.Linear(128, 10))\n        \n        # Create pipeline\n        self.pipe = Pipe.Pipe(\n            nn.Sequential(*layers),\n            balance=[layers_per_partition] * (len(layers) // layers_per_partition),\n            devices=[0, 1],  # GPU devices\n            chunks=8  # Number of micro-batches\n        )\n    \n    def forward(self, x):\n        return self.pipe(x)\n```\n\n## Monitoring and Debugging\n\n### Performance Profiling\n\n```python\nimport torch.profiler\n\ndef profile_training(trainer, num_steps=100):\n    \"\"\"Profile distributed training performance\"\"\"\n    with torch.profiler.profile(\n        activities=[\n            torch.profiler.ProfilerActivity.CPU,\n            torch.profiler.ProfilerActivity.CUDA,\n        ],\n        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/profiler'),\n        record_shapes=True,\n        profile_memory=True,\n        with_stack=True\n    ) as prof:\n        for step, (data, target) in enumerate(trainer.train_loader):\n            if step >= num_steps:\n                break\n                \n            data, target = data.to(trainer.rank), target.to(trainer.rank)\n            \n            trainer.optimizer.zero_grad()\n            output = trainer.ddp_model(data)\n            loss = trainer.criterion(output, target)\n            loss.backward()\n            trainer.optimizer.step()\n            \n            prof.step()\n```\n\n### Communication Debugging\n\n```python\ndef debug_communication():\n    \"\"\"Debug distributed communication\"\"\"\n    rank = dist.get_rank()\n    world_size = dist.get_world_size()\n    \n    # Test all-reduce\n    tensor = torch.randn(10).cuda()\n    print(f\"Rank {rank}: Before all-reduce: {tensor.sum().item():.4f}\")\n    \n    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n    print(f\"Rank {rank}: After all-reduce: {tensor.sum().item():.4f}\")\n    \n    # Test broadcast\n    if rank == 0:\n        broadcast_tensor = torch.randn(5).cuda()\n    else:\n        broadcast_tensor = torch.zeros(5).cuda()\n    \n    dist.broadcast(broadcast_tensor, src=0)\n    print(f\"Rank {rank}: Broadcast result: {broadcast_tensor.sum().item():.4f}\")\n```\n\n## Best Practices\n\n### 1. Data Loading Optimization\n\n```python\ndef create_efficient_dataloader(dataset, batch_size, world_size, rank):\n    \"\"\"Create optimized distributed data loader\"\"\"\n    sampler = DistributedSampler(\n        dataset,\n        num_replicas=world_size,\n        rank=rank,\n        shuffle=True,\n        drop_last=True  # Ensures consistent batch sizes\n    )\n    \n    loader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        sampler=sampler,\n        num_workers=4,  # Adjust based on system\n        pin_memory=True,\n        persistent_workers=True,  # Reuse worker processes\n        prefetch_factor=2\n    )\n    \n    return loader\n```\n\n### 2. Gradient Synchronization\n\n```python\ndef train_with_gradient_accumulation(model, optimizer, criterion, data_loader, \n                                   accumulation_steps=4):\n    \"\"\"Training with gradient accumulation\"\"\"\n    model.train()\n    \n    for batch_idx, (data, target) in enumerate(data_loader):\n        data, target = data.cuda(), target.cuda()\n        \n        # Forward pass\n        output = model(data)\n        loss = criterion(output, target) / accumulation_steps\n        \n        # Backward pass\n        loss.backward()\n        \n        # Update parameters every accumulation_steps\n        if (batch_idx + 1) % accumulation_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n```\n\n### 3. Dynamic Loss Scaling\n\n```python\nclass DynamicLossScaler:\n    def __init__(self, init_scale=2.**16, scale_factor=2., scale_window=2000):\n        self.scale = init_scale\n        self.scale_factor = scale_factor\n        self.scale_window = scale_window\n        self.counter = 0\n        \n    def update(self, overflow):\n        if overflow:\n            self.scale /= self.scale_factor\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.scale_window:\n                self.scale *= self.scale_factor\n                self.counter = 0\n```\n\n### 4. Launch Script Example\n\n```bash\n#!/bin/bash\n# launch_distributed.sh\n\n# Single node, multiple GPUs\npython -m torch.distributed.launch \\\n    --nproc_per_node=4 \\\n    --nnodes=1 \\\n    --node_rank=0 \\\n    --master_addr=\"localhost\" \\\n    --master_port=12345 \\\n    train_distributed.py\n\n# Multi-node setup\n# Node 0:\npython -m torch.distributed.launch \\\n    --nproc_per_node=4 \\\n    --nnodes=2 \\\n    --node_rank=0 \\\n    --master_addr=\"192.168.1.100\" \\\n    --master_port=12345 \\\n    train_distributed.py\n\n# Node 1:\npython -m torch.distributed.launch \\\n    --nproc_per_node=4 \\\n    --nnodes=2 \\\n    --node_rank=1 \\\n    --master_addr=\"192.168.1.100\" \\\n    --master_port=12345 \\\n    train_distributed.py\n```\n\n### 5. Error Handling and Recovery\n\n```python\ndef robust_train_loop(trainer, num_epochs, checkpoint_dir):\n    \"\"\"Training loop with error handling and recovery\"\"\"\n    start_epoch = 0\n    \n    # Load checkpoint if exists\n    latest_checkpoint = find_latest_checkpoint(checkpoint_dir)\n    if latest_checkpoint:\n        start_epoch = load_checkpoint(trainer, latest_checkpoint)\n    \n    for epoch in range(start_epoch, num_epochs):\n        try:\n            trainer.train_epoch(epoch)\n            \n            # Save checkpoint\n            if epoch % 5 == 0:\n                save_checkpoint(trainer, epoch, checkpoint_dir)\n                \n        except RuntimeError as e:\n            if \"out of memory\" in str(e):\n                print(f\"OOM error at epoch {epoch}, reducing batch size\")\n                # Implement batch size reduction logic\n                torch.cuda.empty_cache()\n                continue\n            else:\n                raise e\n        except Exception as e:\n            print(f\"Error at epoch {epoch}: {e}\")\n            # Save emergency checkpoint\n            save_checkpoint(trainer, epoch, checkpoint_dir, emergency=True)\n            raise e\n```\n\nThis guide provides a comprehensive foundation for implementing distributed training with PyTorch. Start with basic DDP for single-node multi-GPU setups, then progress to more advanced techniques like FSDP and pipeline parallelism as your models and datasets grow larger.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}