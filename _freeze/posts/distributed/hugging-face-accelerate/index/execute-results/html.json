{
  "hash": "eed1dd0782dd327ddcac819505facb72",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Hugging Face Accelerate Code Guide\"\nauthor: \"Krishnatheja Vanka\"\ndate: \"2025-06-03\"\ncategories: [code, tutorial, beginner]\nformat:\n  html:\n    code-fold: false\nexecute:\n  echo: true\n  timing: true\njupyter: python3\n---\n\n# Hugging Face Accelerate Code Guide\n![](accelerate.png)\n\n## Overview\n\nThis comprehensive code guide covers everything you need to know about Hugging Face Accelerate, from basic setup to advanced features like DeepSpeed integration. Accelerate simplifies distributed training and mixed precision training across multiple GPUs and nodes.\n\n## Installation and Setup\n\n### Installation\n\n```{.bash}\npip install accelerate\n```\n\n### Configuration\n\nRun the configuration wizard to set up your training environment:\n\n```{.bash}\naccelerate config\n```\n\nOr create a config file programmatically:\n\n::: {#a271a62a .cell execution_count=1}\n``` {.python .cell-code}\nfrom accelerate import Accelerator\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config(mixed_precision=\"fp16\")  # or \"bf16\", \"no\"\n```\n:::\n\n\n## Basic Concepts\n\n### The Accelerator Object\n\nThe `Accelerator` is the main class that handles device placement, gradient synchronization, and other distributed training concerns.\n\n::: {#27e8f2ac .cell execution_count=2}\n``` {.python .cell-code}\nfrom accelerate import Accelerator\n\n# Initialize accelerator\naccelerator = Accelerator()\n\n# Key properties\ndevice = accelerator.device\nis_main_process = accelerator.is_main_process\nnum_processes = accelerator.num_processes\n```\n:::\n\n\n### Device Placement\n\nAccelerate automatically handles device placement:\n\n::: {#24e1bd68 .cell execution_count=3}\n``` {.python .cell-code}\n# Manual device placement (old way)\nmodel = model.to(device)\nbatch = {k: v.to(device) for k, v in batch.items()}\n\n# Accelerate way (automatic)\nmodel, optimizer, dataloader = accelerator.prepare(model, optimizer, dataloader)\n# No need to move batch to device - accelerate handles it\n```\n:::\n\n\n::: {.callout-tip}\n## Key Benefit\nWith Accelerate, you don't need to manually handle device placement. The `prepare()` method takes care of moving your model, optimizer, and dataloader to the appropriate devices.\n:::\n\n## Simple Training Loop\n\n### Basic Example\n\n::: {#1bbdf928 .cell execution_count=4}\n``` {.python .cell-code}\nimport torch\nfrom torch.utils.data import DataLoader\nfrom accelerate import Accelerator\nfrom transformers import AutoModel, AutoTokenizer, AdamW\n\ndef train_model():\n    # Initialize accelerator\n    accelerator = Accelerator()\n    \n    # Load model and tokenizer\n    model = AutoModel.from_pretrained(\"bert-base-uncased\")\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n    \n    # Create optimizer\n    optimizer = AdamW(model.parameters(), lr=5e-5)\n    \n    # Create dataloader (your dataset here)\n    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    \n    # Prepare everything with accelerator\n    model, optimizer, train_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader\n    )\n    \n    # Training loop\n    model.train()\n    for epoch in range(3):\n        for batch in train_dataloader:\n            # Forward pass\n            outputs = model(**batch)\n            loss = outputs.loss\n            \n            # Backward pass\n            accelerator.backward(loss)\n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # Print loss (only on main process)\n            if accelerator.is_main_process:\n                print(f\"Loss: {loss.item():.4f}\")\n\nif __name__ == \"__main__\":\n    train_model()\n```\n:::\n\n\n### Running the Training\n\n```{.bash}\n# Single GPU\npython train.py\n\n# Multiple GPUs\naccelerate launch --num_processes=2 train.py\n\n# With config file\naccelerate launch --config_file config.yaml train.py\n```\n\n## Advanced Features\n\n### Logging and Tracking\n\n::: {#6d4db596 .cell execution_count=5}\n``` {.python .cell-code}\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\n\n# Initialize with logging\naccelerator = Accelerator(log_with=\"tensorboard\", project_dir=\"./logs\")\n\n# Get logger\nlogger = get_logger(__name__)\n\n# Start tracking\naccelerator.init_trackers(\"my_experiment\")\n\n# Log metrics\naccelerator.log({\"train_loss\": loss.item(), \"epoch\": epoch})\n\n# End tracking\naccelerator.end_training()\n```\n:::\n\n\n### Saving and Loading Models\n\n::: {#961e44d1 .cell execution_count=6}\n``` {.python .cell-code}\n# Save model\naccelerator.save_model(model, \"path/to/save\")\n\n# Or save state dict\naccelerator.save(model.state_dict(), \"model.pt\")\n\n# Load model\naccelerator.load_state(\"model.pt\")\n\n# Save complete training state\naccelerator.save_state(\"checkpoint_dir\")\n\n# Load complete training state\naccelerator.load_state(\"checkpoint_dir\")\n```\n:::\n\n\n### Evaluation Loop\n\n::: {#b363330b .cell execution_count=7}\n``` {.python .cell-code}\ndef evaluate_model(model, eval_dataloader, accelerator):\n    model.eval()\n    total_loss = 0\n    total_samples = 0\n    \n    with torch.no_grad():\n        for batch in eval_dataloader:\n            outputs = model(**batch)\n            loss = outputs.loss\n            \n            # Gather losses from all processes\n            gathered_loss = accelerator.gather(loss)\n            total_loss += gathered_loss.sum().item()\n            total_samples += gathered_loss.shape[0]\n    \n    avg_loss = total_loss / total_samples\n    return avg_loss\n```\n:::\n\n\n## Multi-GPU Training\n\n### Data Parallel Training\n\n::: {#13503e82 .cell execution_count=8}\n``` {.python .cell-code}\nfrom accelerate import Accelerator\n\ndef train_multi_gpu():\n    accelerator = Accelerator()\n    \n    # Model will be replicated across GPUs\n    model = MyModel()\n    optimizer = torch.optim.Adam(model.parameters())\n    \n    # Prepare for multi-GPU\n    model, optimizer, train_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader\n    )\n    \n    # Training loop remains the same\n    for batch in train_dataloader:\n        outputs = model(**batch)\n        loss = outputs.loss\n        \n        # Accelerate handles gradient synchronization\n        accelerator.backward(loss)\n        optimizer.step()\n        optimizer.zero_grad()\n```\n:::\n\n\n### Launch Commands\n\n```{.bash}\n# Launch on 4 GPUs\naccelerate launch --num_processes=4 --multi_gpu train.py\n\n# Launch with specific GPUs\nCUDA_VISIBLE_DEVICES=0,1,3 accelerate launch --num_processes=3 train.py\n\n# Launch on multiple nodes\naccelerate launch --num_processes=8 --num_machines=2 --main_process_ip=192.168.1.1 train.py\n```\n\n::: {.callout-note}\n## Multi-Node Training\nFor multi-node training, ensure all nodes can communicate with each other and specify the correct IP address of the main process.\n:::\n\n## Mixed Precision Training\n\n### Automatic Mixed Precision\n\n::: {#a4dc7fa7 .cell execution_count=9}\n``` {.python .cell-code}\n# Enable mixed precision in config or during initialization\naccelerator = Accelerator(mixed_precision=\"fp16\")  # or \"bf16\"\n\n# Training loop remains exactly the same\nfor batch in train_dataloader:\n    outputs = model(**batch)\n    loss = outputs.loss\n    \n    # Accelerate handles scaling automatically\n    accelerator.backward(loss)\n    optimizer.step()\n    optimizer.zero_grad()\n```\n:::\n\n\n### Manual Mixed Precision Control\n\n::: {#4ddf06e4 .cell execution_count=10}\n``` {.python .cell-code}\n# Access the scaler if needed\nif accelerator.mixed_precision == \"fp16\":\n    scaler = accelerator.scaler\n    \n    # Manual scaling (usually not needed)\n    scaled_loss = scaler.scale(loss)\n    scaled_loss.backward()\n    scaler.step(optimizer)\n    scaler.update()\n```\n:::\n\n\n::: {.callout-important}\n## Precision Types\n- **fp16**: Good for most cases, significant speedup\n- **bf16**: Better numerical stability, requires newer hardware\n- **no**: Full precision, slower but most stable\n:::\n\n## Gradient Accumulation\n\n### Basic Gradient Accumulation\n\n::: {#c6eb015e .cell execution_count=11}\n``` {.python .cell-code}\naccelerator = Accelerator(gradient_accumulation_steps=4)\n\nfor batch in train_dataloader:\n    # Use accumulate context manager\n    with accelerator.accumulate(model):\n        outputs = model(**batch)\n        loss = outputs.loss\n        \n        accelerator.backward(loss)\n        optimizer.step()\n        optimizer.zero_grad()\n```\n:::\n\n\n### Dynamic Gradient Accumulation\n\n::: {#eb654bca .cell execution_count=12}\n``` {.python .cell-code}\ndef train_with_dynamic_accumulation():\n    accumulation_steps = 2\n    \n    for i, batch in enumerate(train_dataloader):\n        outputs = model(**batch)\n        loss = outputs.loss / accumulation_steps  # Scale loss\n        \n        accelerator.backward(loss)\n        \n        if (i + 1) % accumulation_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n```\n:::\n\n\n## DeepSpeed Integration\n\n### DeepSpeed Configuration\n\nCreate a DeepSpeed config file (`ds_config.json`):\n\n```{.json}\n{\n    \"train_batch_size\": 32,\n    \"gradient_accumulation_steps\": 1,\n    \"optimizer\": {\n        \"type\": \"Adam\",\n        \"params\": {\n            \"lr\": 5e-5\n        }\n    },\n    \"fp16\": {\n        \"enabled\": true\n    },\n    \"zero_optimization\": {\n        \"stage\": 2\n    }\n}\n```\n\n### Using DeepSpeed\n\n::: {#46dace9e .cell execution_count=13}\n``` {.python .cell-code}\nfrom accelerate import Accelerator\n\n# Initialize with DeepSpeed\naccelerator = Accelerator(deepspeed_plugin=\"ds_config.json\")\n\n# Or programmatically\nfrom accelerate import DeepSpeedPlugin\n\nds_plugin = DeepSpeedPlugin(\n    gradient_accumulation_steps=4,\n    zero_stage=2,\n    offload_optimizer_device=\"cpu\"\n)\naccelerator = Accelerator(deepspeed_plugin=ds_plugin)\n\n# Training code remains the same\nmodel, optimizer = accelerator.prepare(model, optimizer)\n```\n:::\n\n\n### Launch with DeepSpeed\n\n```{.bash}\naccelerate launch --config_file ds_config.yaml train.py\n```\n\n## Troubleshooting\n\n### Common Issues and Solutions\n\n#### Memory Issues\n\n::: {#f8fb08fb .cell execution_count=14}\n``` {.python .cell-code}\n# Clear cache regularly\nif accelerator.is_main_process:\n    torch.cuda.empty_cache()\n\n# Use gradient checkpointing\nmodel.gradient_checkpointing_enable()\n\n# Reduce batch size or increase gradient accumulation\n```\n:::\n\n\n#### Synchronization Issues\n\n::: {#7b499b25 .cell execution_count=15}\n``` {.python .cell-code}\n# Wait for all processes\naccelerator.wait_for_everyone()\n\n# Gather data from all processes\nall_losses = accelerator.gather(loss)\n\n# Reduce across processes\navg_loss = accelerator.reduce(loss, reduction=\"mean\")\n```\n:::\n\n\n#### Debugging\n\n::: {#3f1eed30 .cell execution_count=16}\n``` {.python .cell-code}\n# Enable debug mode\naccelerator = Accelerator(debug=True)\n\n# Check if running in distributed mode\nif accelerator.distributed_type != \"NO\":\n    print(f\"Running on {accelerator.num_processes} processes\")\n\n# Print only on main process\naccelerator.print(\"This will only print once\")\n```\n:::\n\n\n### Performance Tips\n\n::: {.callout-tip icon=\"lightning\"}\n## Optimization Strategies\n\n1. **Use appropriate batch sizes**: Larger batch sizes generally improve GPU utilization\n2. **Enable mixed precision**: Use fp16 or bf16 for faster training\n3. **Gradient accumulation**: Simulate larger batch sizes without memory issues\n4. **DataLoader optimization**: Use `num_workers` and `pin_memory=True`\n5. **Compile models**: Use `torch.compile()` for PyTorch 2.0+\n:::\n\n::: {#e9e3b453 .cell execution_count=17}\n``` {.python .cell-code}\n# Optimized setup\naccelerator = Accelerator(\n    mixed_precision=\"bf16\",\n    gradient_accumulation_steps=4\n)\n\n# Compile model (PyTorch 2.0+)\nmodel = torch.compile(model)\n\n# Optimized DataLoader\ntrain_dataloader = DataLoader(\n    dataset,\n    batch_size=32,\n    num_workers=4,\n    pin_memory=True,\n    shuffle=True\n)\n```\n:::\n\n\n## Complete Example: BERT Fine-tuning\n\nHere's a complete example showing how to fine-tune BERT for sequence classification:\n\n::: {#c638064d .cell execution_count=18}\n``` {.python .cell-code}\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\nfrom accelerate import Accelerator\nfrom datasets import load_dataset\n\ndef main():\n    # Initialize\n    accelerator = Accelerator(mixed_precision=\"fp16\")\n    \n    # Load data\n    dataset = load_dataset(\"imdb\")\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n    \n    def tokenize_function(examples):\n        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\")\n    \n    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n    train_dataset = tokenized_datasets[\"train\"].with_format(\"torch\")\n    \n    # Model and optimizer\n    model = AutoModelForSequenceClassification.from_pretrained(\n        \"bert-base-uncased\", num_labels=2\n    )\n    optimizer = AdamW(model.parameters(), lr=5e-5)\n    \n    # DataLoader\n    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    \n    # Prepare everything\n    model, optimizer, train_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader\n    )\n    \n    # Training loop\n    num_epochs = 3\n    model.train()\n    \n    for epoch in range(num_epochs):\n        total_loss = 0\n        for step, batch in enumerate(train_dataloader):\n            outputs = model(**batch)\n            loss = outputs.loss\n            \n            accelerator.backward(loss)\n            optimizer.step()\n            optimizer.zero_grad()\n            \n            total_loss += loss.item()\n            \n            if step % 100 == 0 and accelerator.is_main_process:\n                print(f\"Epoch {epoch}, Step {step}, Loss: {loss.item():.4f}\")\n        \n        if accelerator.is_main_process:\n            avg_loss = total_loss / len(train_dataloader)\n            print(f\"Epoch {epoch} completed. Average loss: {avg_loss:.4f}\")\n    \n    # Save model\n    accelerator.wait_for_everyone()\n    unwrapped_model = accelerator.unwrap_model(model)\n    unwrapped_model.save_pretrained(\"./fine_tuned_bert\")\n    \n    if accelerator.is_main_process:\n        tokenizer.save_pretrained(\"./fine_tuned_bert\")\n\nif __name__ == \"__main__\":\n    main()\n```\n:::\n\n\n## Summary\n\nThis guide covers the essential aspects of using Hugging Face Accelerate for distributed training. The library abstracts away much of the complexity while providing fine-grained control when needed. Key takeaways:\n\n- **Simplicity**: Minimal code changes required for distributed training\n- **Flexibility**: Works with any PyTorch model and training loop\n- **Performance**: Built-in support for mixed precision and gradient accumulation\n- **Scalability**: Easy scaling from single GPU to multi-node training\n- **Integration**: Seamless integration with popular frameworks like DeepSpeed\n\n::: {.callout-note}\n## Next Steps\n- Explore the [official Accelerate documentation](https://huggingface.co/docs/accelerate/)\n- Try the examples with your own models and datasets\n- Experiment with different optimization strategies\n- Consider advanced features like FSDP for very large models\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}