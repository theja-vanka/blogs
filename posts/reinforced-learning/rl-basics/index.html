<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Krishnatheja Vanka">
<meta name="dcterms.date" content="2025-08-22">

<title>Complete Guide to Reinforcement Learning – Krishnatheja Vanka’s Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-bc185b5c5bdbcb35c2eb49d8a876ef70.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-62c28fcd870f55f61984f019219cbd7c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-5a86c4bd0c1f9981a70f893fdae069f2.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-62c28fcd870f55f61984f019219cbd7c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles/styles.css">
</head>

<body class="floating nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Krishnatheja Vanka’s Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/theja-vanka"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-right">
      <h1 class="title">Complete Guide to Reinforcement Learning</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">research</div>
                <div class="quarto-category">intermediate</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta column-page-right">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Krishnatheja Vanka </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 22, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#complete-guide-to-reinforcement-learning" id="toc-complete-guide-to-reinforcement-learning" class="nav-link active" data-scroll-target="#complete-guide-to-reinforcement-learning">Complete Guide to Reinforcement Learning</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#core-concepts" id="toc-core-concepts" class="nav-link" data-scroll-target="#core-concepts">Core Concepts</a>
  <ul class="collapse">
  <li><a href="#agent-and-environment" id="toc-agent-and-environment" class="nav-link" data-scroll-target="#agent-and-environment">Agent and Environment</a></li>
  <li><a href="#key-elements" id="toc-key-elements" class="nav-link" data-scroll-target="#key-elements">Key Elements</a></li>
  <li><a href="#the-rl-loop" id="toc-the-rl-loop" class="nav-link" data-scroll-target="#the-rl-loop">The RL Loop</a></li>
  <li><a href="#exploration-vs-exploitation" id="toc-exploration-vs-exploitation" class="nav-link" data-scroll-target="#exploration-vs-exploitation">Exploration vs Exploitation</a></li>
  </ul></li>
  <li><a href="#mathematical-foundations" id="toc-mathematical-foundations" class="nav-link" data-scroll-target="#mathematical-foundations">Mathematical Foundations</a>
  <ul class="collapse">
  <li><a href="#markov-decision-process-mdp" id="toc-markov-decision-process-mdp" class="nav-link" data-scroll-target="#markov-decision-process-mdp">Markov Decision Process (MDP)</a></li>
  <li><a href="#bellman-equations" id="toc-bellman-equations" class="nav-link" data-scroll-target="#bellman-equations">Bellman Equations</a></li>
  <li><a href="#convergence-and-optimality" id="toc-convergence-and-optimality" class="nav-link" data-scroll-target="#convergence-and-optimality">Convergence and Optimality</a></li>
  </ul></li>
  <li><a href="#key-algorithms" id="toc-key-algorithms" class="nav-link" data-scroll-target="#key-algorithms">Key Algorithms</a>
  <ul class="collapse">
  <li><a href="#model-based-methods" id="toc-model-based-methods" class="nav-link" data-scroll-target="#model-based-methods">Model-Based Methods</a></li>
  <li><a href="#model-free-methods" id="toc-model-free-methods" class="nav-link" data-scroll-target="#model-free-methods">Model-Free Methods</a></li>
  <li><a href="#monte-carlo-methods" id="toc-monte-carlo-methods" class="nav-link" data-scroll-target="#monte-carlo-methods">Monte Carlo Methods</a></li>
  </ul></li>
  <li><a href="#deep-reinforcement-learning" id="toc-deep-reinforcement-learning" class="nav-link" data-scroll-target="#deep-reinforcement-learning">Deep Reinforcement Learning</a>
  <ul class="collapse">
  <li><a href="#deep-q-networks-dqn" id="toc-deep-q-networks-dqn" class="nav-link" data-scroll-target="#deep-q-networks-dqn">Deep Q-Networks (DQN)</a></li>
  <li><a href="#policy-gradient-methods" id="toc-policy-gradient-methods" class="nav-link" data-scroll-target="#policy-gradient-methods">Policy Gradient Methods</a></li>
  <li><a href="#deep-deterministic-policy-gradient-ddpg" id="toc-deep-deterministic-policy-gradient-ddpg" class="nav-link" data-scroll-target="#deep-deterministic-policy-gradient-ddpg">Deep Deterministic Policy Gradient (DDPG)</a></li>
  </ul></li>
  <li><a href="#advanced-topics" id="toc-advanced-topics" class="nav-link" data-scroll-target="#advanced-topics">Advanced Topics</a>
  <ul class="collapse">
  <li><a href="#multi-agent-reinforcement-learning-marl" id="toc-multi-agent-reinforcement-learning-marl" class="nav-link" data-scroll-target="#multi-agent-reinforcement-learning-marl">Multi-Agent Reinforcement Learning (MARL)</a></li>
  <li><a href="#hierarchical-reinforcement-learning" id="toc-hierarchical-reinforcement-learning" class="nav-link" data-scroll-target="#hierarchical-reinforcement-learning">Hierarchical Reinforcement Learning</a></li>
  <li><a href="#transfer-learning-and-meta-learning" id="toc-transfer-learning-and-meta-learning" class="nav-link" data-scroll-target="#transfer-learning-and-meta-learning">Transfer Learning and Meta-Learning</a></li>
  <li><a href="#partial-observability" id="toc-partial-observability" class="nav-link" data-scroll-target="#partial-observability">Partial Observability</a></li>
  <li><a href="#safety-and-robustness" id="toc-safety-and-robustness" class="nav-link" data-scroll-target="#safety-and-robustness">Safety and Robustness</a></li>
  </ul></li>
  <li><a href="#applications" id="toc-applications" class="nav-link" data-scroll-target="#applications">Applications</a>
  <ul class="collapse">
  <li><a href="#game-playing" id="toc-game-playing" class="nav-link" data-scroll-target="#game-playing">Game Playing</a></li>
  <li><a href="#robotics" id="toc-robotics" class="nav-link" data-scroll-target="#robotics">Robotics</a></li>
  <li><a href="#autonomous-systems" id="toc-autonomous-systems" class="nav-link" data-scroll-target="#autonomous-systems">Autonomous Systems</a></li>
  <li><a href="#finance-and-trading" id="toc-finance-and-trading" class="nav-link" data-scroll-target="#finance-and-trading">Finance and Trading</a></li>
  <li><a href="#healthcare" id="toc-healthcare" class="nav-link" data-scroll-target="#healthcare">Healthcare</a></li>
  <li><a href="#natural-language-processing" id="toc-natural-language-processing" class="nav-link" data-scroll-target="#natural-language-processing">Natural Language Processing</a></li>
  <li><a href="#resource-management" id="toc-resource-management" class="nav-link" data-scroll-target="#resource-management">Resource Management</a></li>
  </ul></li>
  <li><a href="#implementation-considerations" id="toc-implementation-considerations" class="nav-link" data-scroll-target="#implementation-considerations">Implementation Considerations</a>
  <ul class="collapse">
  <li><a href="#environment-design" id="toc-environment-design" class="nav-link" data-scroll-target="#environment-design">Environment Design</a></li>
  <li><a href="#hyperparameter-tuning" id="toc-hyperparameter-tuning" class="nav-link" data-scroll-target="#hyperparameter-tuning">Hyperparameter Tuning</a></li>
  <li><a href="#evaluation-and-testing" id="toc-evaluation-and-testing" class="nav-link" data-scroll-target="#evaluation-and-testing">Evaluation and Testing</a></li>
  <li><a href="#debugging-rl-systems" id="toc-debugging-rl-systems" class="nav-link" data-scroll-target="#debugging-rl-systems">Debugging RL Systems</a></li>
  <li><a href="#computational-considerations" id="toc-computational-considerations" class="nav-link" data-scroll-target="#computational-considerations">Computational Considerations</a></li>
  </ul></li>
  <li><a href="#resources-and-tools" id="toc-resources-and-tools" class="nav-link" data-scroll-target="#resources-and-tools">Resources and Tools</a>
  <ul class="collapse">
  <li><a href="#frameworks-and-libraries" id="toc-frameworks-and-libraries" class="nav-link" data-scroll-target="#frameworks-and-libraries">Frameworks and Libraries</a></li>
  <li><a href="#simulation-environments" id="toc-simulation-environments" class="nav-link" data-scroll-target="#simulation-environments">Simulation Environments</a></li>
  <li><a href="#books-and-courses" id="toc-books-and-courses" class="nav-link" data-scroll-target="#books-and-courses">Books and Courses</a></li>
  <li><a href="#research-venues" id="toc-research-venues" class="nav-link" data-scroll-target="#research-venues">Research Venues</a></li>
  <li><a href="#best-practices" id="toc-best-practices" class="nav-link" data-scroll-target="#best-practices">Best Practices</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-right" id="quarto-document-content">






<section id="complete-guide-to-reinforcement-learning" class="level1">
<h1>Complete Guide to Reinforcement Learning</h1>
<p><img src="rl.png" class="img-fluid"></p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Reinforcement Learning (RL) is a paradigm of machine learning where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. Unlike supervised learning, where the correct answers are provided, or unsupervised learning, where patterns are discovered in data, reinforcement learning involves learning through trial and error based on feedback from the environment.</p>
<p>The inspiration for RL comes from behavioral psychology and how animals learn through rewards and punishments. This approach has proven remarkably effective for complex decision-making problems where the optimal strategy isn’t immediately apparent.</p>
</section>
<section id="core-concepts" class="level2">
<h2 class="anchored" data-anchor-id="core-concepts">Core Concepts</h2>
<section id="agent-and-environment" class="level3">
<h3 class="anchored" data-anchor-id="agent-and-environment">Agent and Environment</h3>
<p>The fundamental setup of RL involves two main components:</p>
<p><strong>Agent</strong>: The learner or decision-maker that takes actions in the environment. The agent’s goal is to learn a policy that maximizes expected cumulative reward.</p>
<p><strong>Environment</strong>: Everything the agent interacts with. It receives actions from the agent and returns observations (states) and rewards.</p>
</section>
<section id="key-elements" class="level3">
<h3 class="anchored" data-anchor-id="key-elements">Key Elements</h3>
<p><strong>State (S)</strong>: A representation of the current situation in the environment. States can be fully observable (agent sees complete state) or partially observable (agent has limited information).</p>
<p><strong>Action (A)</strong>: Choices available to the agent at any given state. Actions can be discrete (finite set of options) or continuous (infinite possibilities within a range).</p>
<p><strong>Reward (R)</strong>: Numerical feedback from the environment indicating the immediate value of the agent’s action. Rewards can be sparse (only at terminal states) or dense (at every step).</p>
<p><strong>Policy (π)</strong>: The agent’s strategy for choosing actions given states. Can be deterministic (always same action for same state) or stochastic (probability distribution over actions).</p>
<p><strong>Value Function</strong>: Estimates the expected cumulative reward from a given state or state-action pair under a particular policy.</p>
</section>
<section id="the-rl-loop" class="level3">
<h3 class="anchored" data-anchor-id="the-rl-loop">The RL Loop</h3>
<ol type="1">
<li>Agent observes current state</li>
<li>Agent selects action based on current policy</li>
<li>Environment transitions to new state</li>
<li>Environment provides reward signal</li>
<li>Agent updates its knowledge/policy</li>
<li>Process repeats</li>
</ol>
</section>
<section id="exploration-vs-exploitation" class="level3">
<h3 class="anchored" data-anchor-id="exploration-vs-exploitation">Exploration vs Exploitation</h3>
<p>One of the central challenges in RL is balancing exploration (trying new actions to discover better strategies) with exploitation (using current knowledge to maximize immediate reward). This tradeoff is crucial because:</p>
<ul>
<li>Pure exploitation may miss better long-term strategies</li>
<li>Pure exploration wastes opportunities to use known good strategies</li>
<li>The optimal balance depends on the problem and learning phase</li>
</ul>
</section>
</section>
<section id="mathematical-foundations" class="level2">
<h2 class="anchored" data-anchor-id="mathematical-foundations">Mathematical Foundations</h2>
<section id="markov-decision-process-mdp" class="level3">
<h3 class="anchored" data-anchor-id="markov-decision-process-mdp">Markov Decision Process (MDP)</h3>
<p>Most RL problems are formalized as MDPs, defined by the tuple (S, A, P, R, γ):</p>
<ul>
<li>S: Set of states</li>
<li>A: Set of actions<br>
</li>
<li>P: State transition probabilities P(s’|s,a)</li>
<li>R: Reward function R(s,a,s’)</li>
<li>γ: Discount factor (0 ≤ γ ≤ 1)</li>
</ul>
<p>The Markov property states that the future depends only on the current state, not the history of how we arrived there.</p>
</section>
<section id="bellman-equations" class="level3">
<h3 class="anchored" data-anchor-id="bellman-equations">Bellman Equations</h3>
<p>The Bellman equations provide the foundation for many RL algorithms:</p>
<p><strong>State Value Function</strong>: <span class="math display">\[
V^π(s) = \mathbb{E}[R_{t+1} + γV^π(S_{t+1}) | S_t = s]
\]</span></p>
<p><strong>Action Value Function (Q-function)</strong>: <span class="math display">\[
Q^π(s,a) = \mathbb{E}[R_{t+1} + γQ^π(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]
\]</span></p>
<p><strong>Optimal Bellman Equations</strong>: <span class="math display">\[
V^*(s) = \max_a \sum_{s'} P(s'|s,a)[R(s,a,s') + γV^*(s')]
\]</span></p>
<p><span class="math display">\[
Q^*(s,a) = \sum_{s'} P(s'|s,a)[R(s,a,s') + γ \max_{a'} Q^*(s',a')]
\]</span></p>
</section>
<section id="convergence-and-optimality" class="level3">
<h3 class="anchored" data-anchor-id="convergence-and-optimality">Convergence and Optimality</h3>
<p>Under certain conditions (finite state/action spaces, proper discount factor), RL algorithms are guaranteed to converge to optimal policies. The policy improvement theorem provides theoretical backing for iterative policy improvement methods.</p>
</section>
</section>
<section id="key-algorithms" class="level2">
<h2 class="anchored" data-anchor-id="key-algorithms">Key Algorithms</h2>
<section id="model-based-methods" class="level3">
<h3 class="anchored" data-anchor-id="model-based-methods">Model-Based Methods</h3>
<p><strong>Dynamic Programming</strong></p>
<ul>
<li><strong>Policy Iteration</strong>: Alternates between policy evaluation and policy improvement</li>
<li><strong>Value Iteration</strong>: Directly computes optimal value function, then derives policy</li>
<li>Requires complete knowledge of environment dynamics</li>
<li>Guaranteed convergence but computationally expensive for large state spaces</li>
</ul>
</section>
<section id="model-free-methods" class="level3">
<h3 class="anchored" data-anchor-id="model-free-methods">Model-Free Methods</h3>
<p><strong>Temporal Difference Learning</strong></p>
<ul>
<li><strong>Q-Learning</strong>: Off-policy method that learns optimal action values
<ul>
<li>Update rule: <span class="math inline">\(Q(s,a) \leftarrow Q(s,a) + α[r + γ \max_{a'} Q(s',a') - Q(s,a)]\)</span></li>
<li>Explores using ε-greedy or other exploration strategies</li>
<li>Proven to converge to optimal Q-function</li>
</ul></li>
<li><strong>SARSA (State-Action-Reward-State-Action)</strong>: On-policy method
<ul>
<li>Update rule: <span class="math inline">\(Q(s,a) \leftarrow Q(s,a) + α[r + γ Q(s',a') - Q(s,a)]\)</span></li>
<li>Uses actual next action taken by current policy</li>
<li>More conservative than Q-learning</li>
</ul></li>
</ul>
<p><strong>Policy Gradient Methods</strong></p>
<ul>
<li>Directly optimize policy parameters using gradient ascent</li>
<li><strong>REINFORCE</strong>: Basic policy gradient algorithm using Monte Carlo returns</li>
<li><strong>Actor-Critic</strong>: Combines value function estimation with policy optimization
<ul>
<li>Actor: Updates policy parameters</li>
<li>Critic: Estimates value function to reduce variance</li>
</ul></li>
<li>Better for continuous action spaces and stochastic policies</li>
</ul>
</section>
<section id="monte-carlo-methods" class="level3">
<h3 class="anchored" data-anchor-id="monte-carlo-methods">Monte Carlo Methods</h3>
<ul>
<li>Learn from complete episodes</li>
<li>No bootstrapping (unlike TD methods)</li>
<li>High variance but unbiased estimates</li>
<li>Suitable when episodes are short and environment is episodic</li>
</ul>
</section>
</section>
<section id="deep-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="deep-reinforcement-learning">Deep Reinforcement Learning</h2>
<section id="deep-q-networks-dqn" class="level3">
<h3 class="anchored" data-anchor-id="deep-q-networks-dqn">Deep Q-Networks (DQN)</h3>
<p>Combines Q-learning with deep neural networks to handle high-dimensional state spaces:</p>
<p><strong>Key Innovations</strong>:</p>
<ul>
<li><strong>Experience Replay</strong>: Store and randomly sample past experiences to break correlation</li>
<li><strong>Target Network</strong>: Use separate network for computing targets to stabilize learning</li>
<li><strong>Function Approximation</strong>: Neural networks approximate Q-values for large state spaces</li>
</ul>
<p><strong>Improvements</strong>:</p>
<ul>
<li><strong>Double DQN</strong>: Addresses overestimation bias in Q-learning</li>
<li><strong>Dueling DQN</strong>: Separates state value and advantage estimation</li>
<li><strong>Prioritized Experience Replay</strong>: Sample important experiences more frequently</li>
<li><strong>Rainbow DQN</strong>: Combines multiple improvements for state-of-the-art performance</li>
</ul>
</section>
<section id="policy-gradient-methods" class="level3">
<h3 class="anchored" data-anchor-id="policy-gradient-methods">Policy Gradient Methods</h3>
<p><strong>Proximal Policy Optimization (PPO)</strong></p>
<ul>
<li>Clips policy updates to prevent destructive large changes</li>
<li>Simpler and more stable than other policy gradient methods</li>
<li>Widely used in practice due to reliability</li>
</ul>
<p><strong>Trust Region Policy Optimization (TRPO)</strong></p>
<ul>
<li>Constrains policy updates within trust region</li>
<li>Provides theoretical guarantees on policy improvement</li>
<li>More complex than PPO but stronger theoretical foundation</li>
</ul>
<p><strong>Actor-Critic Methods</strong></p>
<ul>
<li><strong>A3C (Asynchronous Actor-Critic)</strong>: Parallel training with multiple agents</li>
<li><strong>A2C (Advantage Actor-Critic)</strong>: Synchronous version of A3C</li>
<li><strong>SAC (Soft Actor-Critic)</strong>: Off-policy method with entropy regularization</li>
</ul>
</section>
<section id="deep-deterministic-policy-gradient-ddpg" class="level3">
<h3 class="anchored" data-anchor-id="deep-deterministic-policy-gradient-ddpg">Deep Deterministic Policy Gradient (DDPG)</h3>
<ul>
<li>Extends DQN to continuous action spaces</li>
<li>Uses actor-critic architecture with deterministic policies</li>
<li>Employs target networks and experience replay like DQN</li>
</ul>
</section>
</section>
<section id="advanced-topics" class="level2">
<h2 class="anchored" data-anchor-id="advanced-topics">Advanced Topics</h2>
<section id="multi-agent-reinforcement-learning-marl" class="level3">
<h3 class="anchored" data-anchor-id="multi-agent-reinforcement-learning-marl">Multi-Agent Reinforcement Learning (MARL)</h3>
<p>When multiple agents interact in the same environment:</p>
<ul>
<li><strong>Cooperative</strong>: Agents share common goal</li>
<li><strong>Competitive</strong>: Zero-sum or adversarial setting<br>
</li>
<li><strong>Mixed-Motive</strong>: Combination of cooperation and competition</li>
</ul>
<p>Challenges include non-stationarity (other agents are learning too), credit assignment, and communication.</p>
</section>
<section id="hierarchical-reinforcement-learning" class="level3">
<h3 class="anchored" data-anchor-id="hierarchical-reinforcement-learning">Hierarchical Reinforcement Learning</h3>
<p>Structures learning across multiple temporal scales:</p>
<ul>
<li><strong>Options Framework</strong>: Semi-Markov decision processes with temporal abstractions</li>
<li><strong>Feudal Networks</strong>: Hierarchical structure with managers and workers</li>
<li><strong>HAM (Hierarchy of Abstract Machines)</strong>: Formal framework for hierarchical policies</li>
</ul>
<p>Benefits include faster learning, better exploration, and transferable skills.</p>
</section>
<section id="transfer-learning-and-meta-learning" class="level3">
<h3 class="anchored" data-anchor-id="transfer-learning-and-meta-learning">Transfer Learning and Meta-Learning</h3>
<ul>
<li><strong>Transfer Learning</strong>: Apply knowledge from one task to related tasks</li>
<li><strong>Meta-Learning</strong>: Learn how to learn quickly on new tasks</li>
<li><strong>Few-Shot Learning</strong>: Quickly adapt to new tasks with minimal data</li>
</ul>
</section>
<section id="partial-observability" class="level3">
<h3 class="anchored" data-anchor-id="partial-observability">Partial Observability</h3>
<p>When agents can’t observe complete state:</p>
<ul>
<li><strong>POMDPs (Partially Observable MDPs)</strong>: Formal framework with belief states</li>
<li><strong>Recurrent Networks</strong>: Use memory to maintain state estimates</li>
<li><strong>Attention Mechanisms</strong>: Focus on relevant parts of observation history</li>
</ul>
</section>
<section id="safety-and-robustness" class="level3">
<h3 class="anchored" data-anchor-id="safety-and-robustness">Safety and Robustness</h3>
<p>Critical considerations for real-world deployment:</p>
<ul>
<li><strong>Safe Exploration</strong>: Avoid dangerous actions during learning</li>
<li><strong>Robust RL</strong>: Handle uncertainty and distribution shift</li>
<li><strong>Constrained RL</strong>: Satisfy safety constraints while optimizing rewards</li>
<li><strong>Interpretability</strong>: Understanding agent decision-making process</li>
</ul>
</section>
</section>
<section id="applications" class="level2">
<h2 class="anchored" data-anchor-id="applications">Applications</h2>
<section id="game-playing" class="level3">
<h3 class="anchored" data-anchor-id="game-playing">Game Playing</h3>
<ul>
<li><strong>Board Games</strong>: Chess (Deep Blue), Go (AlphaGo, AlphaZero)</li>
<li><strong>Video Games</strong>: Atari games (DQN), StarCraft II (AlphaStar), Dota 2 (OpenAI Five)</li>
<li><strong>Card Games</strong>: Poker (Libratus, Pluribus)</li>
</ul>
</section>
<section id="robotics" class="level3">
<h3 class="anchored" data-anchor-id="robotics">Robotics</h3>
<ul>
<li><strong>Manipulation</strong>: Grasping, assembly, dexterous manipulation</li>
<li><strong>Navigation</strong>: Path planning, obstacle avoidance, SLAM</li>
<li><strong>Locomotion</strong>: Walking, running, jumping for legged robots</li>
<li><strong>Human-Robot Interaction</strong>: Social robots, collaborative robots</li>
</ul>
</section>
<section id="autonomous-systems" class="level3">
<h3 class="anchored" data-anchor-id="autonomous-systems">Autonomous Systems</h3>
<ul>
<li><strong>Self-Driving Cars</strong>: Path planning, decision making in traffic</li>
<li><strong>Drones</strong>: Navigation, surveillance, delivery</li>
<li><strong>Traffic Management</strong>: Optimizing traffic flow, signal control</li>
</ul>
</section>
<section id="finance-and-trading" class="level3">
<h3 class="anchored" data-anchor-id="finance-and-trading">Finance and Trading</h3>
<ul>
<li><strong>Algorithmic Trading</strong>: Portfolio management, execution strategies</li>
<li><strong>Risk Management</strong>: Dynamic hedging, capital allocation</li>
<li><strong>Market Making</strong>: Optimal bid-ask spread management</li>
</ul>
</section>
<section id="healthcare" class="level3">
<h3 class="anchored" data-anchor-id="healthcare">Healthcare</h3>
<ul>
<li><strong>Treatment Planning</strong>: Personalized therapy recommendations</li>
<li><strong>Drug Discovery</strong>: Molecular design, clinical trial optimization</li>
<li><strong>Medical Imaging</strong>: Automated diagnosis, treatment planning</li>
</ul>
</section>
<section id="natural-language-processing" class="level3">
<h3 class="anchored" data-anchor-id="natural-language-processing">Natural Language Processing</h3>
<ul>
<li><strong>Dialogue Systems</strong>: Conversational AI, customer service bots</li>
<li><strong>Machine Translation</strong>: Optimizing translation quality</li>
<li><strong>Text Generation</strong>: Content creation, summarization</li>
</ul>
</section>
<section id="resource-management" class="level3">
<h3 class="anchored" data-anchor-id="resource-management">Resource Management</h3>
<ul>
<li><strong>Cloud Computing</strong>: Resource allocation, auto-scaling</li>
<li><strong>Energy Systems</strong>: Smart grid management, battery optimization<br>
</li>
<li><strong>Supply Chain</strong>: Inventory management, logistics optimization</li>
</ul>
</section>
</section>
<section id="implementation-considerations" class="level2">
<h2 class="anchored" data-anchor-id="implementation-considerations">Implementation Considerations</h2>
<section id="environment-design" class="level3">
<h3 class="anchored" data-anchor-id="environment-design">Environment Design</h3>
<ul>
<li><strong>Reward Engineering</strong>: Design rewards that incentivize desired behavior</li>
<li><strong>State Representation</strong>: Choose appropriate features and observations</li>
<li><strong>Action Space</strong>: Balance expressiveness with computational complexity</li>
<li><strong>Simulation Fidelity</strong>: Trade-off between realism and computational speed</li>
</ul>
</section>
<section id="hyperparameter-tuning" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameter-tuning">Hyperparameter Tuning</h3>
<p>Critical parameters affecting performance:</p>
<ul>
<li><strong>Learning Rate</strong>: Too high causes instability, too low slows convergence</li>
<li><strong>Exploration Rate</strong>: Balance exploration and exploitation</li>
<li><strong>Discount Factor</strong>: Determines importance of future rewards</li>
<li><strong>Network Architecture</strong>: Layer sizes, activation functions, regularization</li>
<li><strong>Batch Size</strong>: Affects stability and computational efficiency</li>
</ul>
</section>
<section id="evaluation-and-testing" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-and-testing">Evaluation and Testing</h3>
<ul>
<li><strong>Sample Efficiency</strong>: How much data needed to learn effective policy</li>
<li><strong>Final Performance</strong>: Quality of learned policy on test environments</li>
<li><strong>Robustness</strong>: Performance under distribution shift or adversarial conditions</li>
<li><strong>Safety</strong>: Avoiding dangerous or harmful actions</li>
</ul>
</section>
<section id="debugging-rl-systems" class="level3">
<h3 class="anchored" data-anchor-id="debugging-rl-systems">Debugging RL Systems</h3>
<p>Common issues and solutions:</p>
<ul>
<li><strong>Learning Instability</strong>: Use target networks, gradient clipping, proper initialization</li>
<li><strong>Poor Exploration</strong>: Adjust exploration strategies, use curiosity-driven methods</li>
<li><strong>Reward Hacking</strong>: Careful reward design, use auxiliary objectives</li>
<li><strong>Overfitting</strong>: Regularization, diverse training environments</li>
</ul>
</section>
<section id="computational-considerations" class="level3">
<h3 class="anchored" data-anchor-id="computational-considerations">Computational Considerations</h3>
<ul>
<li><strong>Parallel Training</strong>: Distributed computing, asynchronous updates</li>
<li><strong>Memory Requirements</strong>: Experience replay buffers, model storage</li>
<li><strong>Training Time</strong>: Sample efficiency vs wall-clock time trade-offs</li>
<li><strong>Hardware</strong>: GPUs for neural networks, CPUs for environment simulation</li>
</ul>
</section>
</section>
<section id="resources-and-tools" class="level2">
<h2 class="anchored" data-anchor-id="resources-and-tools">Resources and Tools</h2>
<section id="frameworks-and-libraries" class="level3">
<h3 class="anchored" data-anchor-id="frameworks-and-libraries">Frameworks and Libraries</h3>
<ul>
<li><strong>Stable-Baselines3</strong>: High-quality implementations of RL algorithms</li>
<li><strong>Ray RLlib</strong>: Scalable reinforcement learning library</li>
<li><strong>OpenAI Gym</strong>: Standard environment interface for RL research</li>
<li><strong>PyBullet</strong>: Physics simulation for robotics applications</li>
<li><strong>Unity ML-Agents</strong>: RL framework for Unity game engine</li>
<li><strong>TensorFlow Agents</strong>: RL library built on TensorFlow</li>
<li><strong>Dopamine</strong>: Research framework for fast prototyping</li>
</ul>
</section>
<section id="simulation-environments" class="level3">
<h3 class="anchored" data-anchor-id="simulation-environments">Simulation Environments</h3>
<ul>
<li><strong>Atari</strong>: Classic video games for testing RL algorithms</li>
<li><strong>MuJoCo</strong>: Physics simulation for continuous control</li>
<li><strong>CarRacing</strong>: Autonomous driving simulation</li>
<li><strong>Roboschool</strong>: Open-source physics simulation</li>
<li><strong>StarCraft II Learning Environment</strong>: Real-time strategy game</li>
<li><strong>Procgen</strong>: Procedurally generated environments for generalization</li>
</ul>
</section>
<section id="books-and-courses" class="level3">
<h3 class="anchored" data-anchor-id="books-and-courses">Books and Courses</h3>
<ul>
<li>“Reinforcement Learning: An Introduction” by Sutton &amp; Barto</li>
<li>“Deep Reinforcement Learning” by Aske Plaat</li>
<li>CS294 Deep Reinforcement Learning (UC Berkeley)</li>
<li>DeepMind &amp; UCL Reinforcement Learning Course</li>
<li>OpenAI Spinning Up in Deep RL</li>
</ul>
</section>
<section id="research-venues" class="level3">
<h3 class="anchored" data-anchor-id="research-venues">Research Venues</h3>
<ul>
<li><strong>Conferences</strong>: ICML, NeurIPS, ICLR, AAAI, IJCAI</li>
<li><strong>Journals</strong>: JMLR, Machine Learning, Artificial Intelligence</li>
<li><strong>Workshops</strong>: Deep RL Workshop, Multi-Agent RL Workshop</li>
</ul>
</section>
<section id="best-practices" class="level3">
<h3 class="anchored" data-anchor-id="best-practices">Best Practices</h3>
<ol type="1">
<li><strong>Start Simple</strong>: Begin with basic algorithms before moving to complex methods</li>
<li><strong>Understand the Environment</strong>: Analyze state/action spaces and reward structure</li>
<li><strong>Baseline Comparison</strong>: Compare against random and heuristic policies</li>
<li><strong>Ablation Studies</strong>: Test individual components to understand their contribution</li>
<li><strong>Reproducibility</strong>: Use seeds, version control, and detailed logging</li>
<li><strong>Incremental Development</strong>: Add complexity gradually while maintaining functionality</li>
<li><strong>Monitor Training</strong>: Track learning curves, exploration metrics, and environment statistics</li>
</ol>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Reinforcement learning represents a powerful paradigm for solving complex sequential decision-making problems. While it presents unique challenges in terms of sample efficiency, exploration, and stability, the field continues to advance rapidly with new algorithms, applications, and theoretical insights. Success in RL requires careful consideration of problem formulation, algorithm selection, implementation details, and thorough evaluation practices.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/theja-vanka\.github\.io\/blogs\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>© 2025 Krishnatheja Vanka</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>