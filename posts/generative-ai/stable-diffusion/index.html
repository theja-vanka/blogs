<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Krishnatheja Vanka">
<meta name="dcterms.date" content="2025-07-22">

<title>Stable Diffusion: A Complete Guide to Text-to-Image Generation – Krishnatheja Vanka’s Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-2fef5ea3f8957b3e4ecc936fc74692ca.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-62c28fcd870f55f61984f019219cbd7c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-5a86c4bd0c1f9981a70f893fdae069f2.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-62c28fcd870f55f61984f019219cbd7c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles/styles.css">
</head>

<body class="floating nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Krishnatheja Vanka’s Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/theja-vanka"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-right">
      <h1 class="title">Stable Diffusion: A Complete Guide to Text-to-Image Generation</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">research</div>
                <div class="quarto-category">intermediate</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta column-page-right">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Krishnatheja Vanka </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 22, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#stable-diffusion-a-complete-guide-to-text-to-image-generation" id="toc-stable-diffusion-a-complete-guide-to-text-to-image-generation" class="nav-link active" data-scroll-target="#stable-diffusion-a-complete-guide-to-text-to-image-generation">Stable Diffusion: A Complete Guide to Text-to-Image Generation</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#technical-foundation" id="toc-technical-foundation" class="nav-link" data-scroll-target="#technical-foundation">Technical Foundation</a>
  <ul class="collapse">
  <li><a href="#sec-diffusion-process" id="toc-sec-diffusion-process" class="nav-link" data-scroll-target="#sec-diffusion-process">The Diffusion Process</a></li>
  <li><a href="#latent-space-innovation" id="toc-latent-space-innovation" class="nav-link" data-scroll-target="#latent-space-innovation">Latent Space Innovation</a></li>
  <li><a href="#model-architecture-components" id="toc-model-architecture-components" class="nav-link" data-scroll-target="#model-architecture-components">Model Architecture Components</a></li>
  </ul></li>
  <li><a href="#training-and-data" id="toc-training-and-data" class="nav-link" data-scroll-target="#training-and-data">Training and Data</a>
  <ul class="collapse">
  <li><a href="#dataset-composition" id="toc-dataset-composition" class="nav-link" data-scroll-target="#dataset-composition">Dataset Composition</a></li>
  <li><a href="#training-process" id="toc-training-process" class="nav-link" data-scroll-target="#training-process">Training Process</a></li>
  </ul></li>
  <li><a href="#inference-and-generation-process" id="toc-inference-and-generation-process" class="nav-link" data-scroll-target="#inference-and-generation-process">Inference and Generation Process</a>
  <ul class="collapse">
  <li><a href="#the-sampling-pipeline" id="toc-the-sampling-pipeline" class="nav-link" data-scroll-target="#the-sampling-pipeline">The Sampling Pipeline</a></li>
  <li><a href="#sampling-algorithms" id="toc-sampling-algorithms" class="nav-link" data-scroll-target="#sampling-algorithms">Sampling Algorithms</a></li>
  <li><a href="#guidance-and-control" id="toc-guidance-and-control" class="nav-link" data-scroll-target="#guidance-and-control">Guidance and Control</a></li>
  </ul></li>
  <li><a href="#advanced-techniques-and-applications" id="toc-advanced-techniques-and-applications" class="nav-link" data-scroll-target="#advanced-techniques-and-applications">Advanced Techniques and Applications</a>
  <ul class="collapse">
  <li><a href="#image-to-image-generation" id="toc-image-to-image-generation" class="nav-link" data-scroll-target="#image-to-image-generation">Image-to-Image Generation</a></li>
  <li><a href="#inpainting-and-outpainting" id="toc-inpainting-and-outpainting" class="nav-link" data-scroll-target="#inpainting-and-outpainting">Inpainting and Outpainting</a></li>
  <li><a href="#controlnet-and-conditioning" id="toc-controlnet-and-conditioning" class="nav-link" data-scroll-target="#controlnet-and-conditioning">ControlNet and Conditioning</a></li>
  <li><a href="#fine-tuning-and-customization" id="toc-fine-tuning-and-customization" class="nav-link" data-scroll-target="#fine-tuning-and-customization">Fine-tuning and Customization</a></li>
  </ul></li>
  <li><a href="#performance-and-hardware-considerations" id="toc-performance-and-hardware-considerations" class="nav-link" data-scroll-target="#performance-and-hardware-considerations">Performance and Hardware Considerations</a>
  <ul class="collapse">
  <li><a href="#system-requirements" id="toc-system-requirements" class="nav-link" data-scroll-target="#system-requirements">System Requirements</a></li>
  <li><a href="#cloud-and-edge-deployment" id="toc-cloud-and-edge-deployment" class="nav-link" data-scroll-target="#cloud-and-edge-deployment">Cloud and Edge Deployment</a></li>
  </ul></li>
  <li><a href="#sec-ethics" id="toc-sec-ethics" class="nav-link" data-scroll-target="#sec-ethics">Ethical Considerations and Societal Impact</a>
  <ul class="collapse">
  <li><a href="#copyright-and-intellectual-property" id="toc-copyright-and-intellectual-property" class="nav-link" data-scroll-target="#copyright-and-intellectual-property">Copyright and Intellectual Property</a></li>
  <li><a href="#bias-and-representation" id="toc-bias-and-representation" class="nav-link" data-scroll-target="#bias-and-representation">Bias and Representation</a></li>
  <li><a href="#misuse-and-safety-concerns" id="toc-misuse-and-safety-concerns" class="nav-link" data-scroll-target="#misuse-and-safety-concerns">Misuse and Safety Concerns</a></li>
  </ul></li>
  <li><a href="#the-open-source-ecosystem" id="toc-the-open-source-ecosystem" class="nav-link" data-scroll-target="#the-open-source-ecosystem">The Open Source Ecosystem</a>
  <ul class="collapse">
  <li><a href="#community-contributions" id="toc-community-contributions" class="nav-link" data-scroll-target="#community-contributions">Community Contributions</a></li>
  <li><a href="#commercial-applications" id="toc-commercial-applications" class="nav-link" data-scroll-target="#commercial-applications">Commercial Applications</a></li>
  </ul></li>
  <li><a href="#future-developments-and-research-directions" id="toc-future-developments-and-research-directions" class="nav-link" data-scroll-target="#future-developments-and-research-directions">Future Developments and Research Directions</a>
  <ul class="collapse">
  <li><a href="#technical-improvements" id="toc-technical-improvements" class="nav-link" data-scroll-target="#technical-improvements">Technical Improvements</a></li>
  <li><a href="#architectural-innovations" id="toc-architectural-innovations" class="nav-link" data-scroll-target="#architectural-innovations">Architectural Innovations</a></li>
  <li><a href="#emerging-applications" id="toc-emerging-applications" class="nav-link" data-scroll-target="#emerging-applications">Emerging Applications</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-right" id="quarto-document-content">






<section id="stable-diffusion-a-complete-guide-to-text-to-image-generation" class="level1">
<h1>Stable Diffusion: A Complete Guide to Text-to-Image Generation</h1>
<p><img src="sd.png" class="img-fluid"></p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Stable Diffusion represents a watershed moment in artificial intelligence and creative technology. Released in August 2022 by Stability AI in collaboration with the CompVis Group at Ludwig Maximilian University of Munich and Runway, this open-source text-to-image model democratized AI-powered image generation in unprecedented ways. Unlike its predecessors that required massive computational resources and were locked behind proprietary APIs, Stable Diffusion can run on consumer hardware while producing remarkable results.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Innovation
</div>
</div>
<div class="callout-body-container callout-body">
<p>The model’s ability to run on consumer hardware while producing high-quality results marked a significant departure from previous text-to-image models that required massive computational resources.</p>
</div>
</div>
<p>The model’s impact extends far beyond technical achievements. It has sparked conversations about creativity, copyright, artistic authenticity, and the future of visual media. From independent artists experimenting with new forms of expression to major studios integrating AI into production pipelines, Stable Diffusion has become a foundational technology in the rapidly evolving landscape of generative AI.</p>
</section>
<section id="technical-foundation" class="level2">
<h2 class="anchored" data-anchor-id="technical-foundation">Technical Foundation</h2>
<section id="sec-diffusion-process" class="level3">
<h3 class="anchored" data-anchor-id="sec-diffusion-process">The Diffusion Process</h3>
<p>At its core, Stable Diffusion employs a diffusion model architecture, a class of generative models that learns to reverse a gradual noising process. The fundamental concept involves two phases: a forward process that systematically adds noise to clean images until they become pure noise, and a reverse process that learns to denoise these images step by step.</p>
<p>The forward process follows a Markov chain where at each timestep, Gaussian noise is added to the image according to a predefined noise schedule. This process is deterministic and can be expressed mathematically as:</p>
<p><span id="eq-forward-process"><span class="math display">\[q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t} x_{t-1}, \beta_t I) \tag{1}\]</span></span></p>
<p>Where <span class="math inline">\(\beta_t\)</span> represents the noise schedule, controlling how much noise is added at each step. The brilliance of diffusion models lies in the reverse process, where a neural network learns to predict and remove the noise that was added at each step.</p>
</section>
<section id="latent-space-innovation" class="level3">
<h3 class="anchored" data-anchor-id="latent-space-innovation">Latent Space Innovation</h3>
<p>What sets Stable Diffusion apart from earlier diffusion models like DALL-E 2 is its operation in latent space rather than pixel space. This architectural decision, inspired by the work on Latent Diffusion Models (LDMs), provides several crucial advantages:</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">Computational Efficiency</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">Semantic Coherence</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" role="tab" aria-controls="tabset-1-3" aria-selected="false">Training Stability</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<p>By working in a compressed latent representation, the model reduces computational requirements by factors of 4-8 compared to pixel-space diffusion. This compression is achieved through a Variational Autoencoder (VAE) that maps images to and from the latent space.</p>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<p>The latent space captures high-level semantic features while abstracting away pixel-level details. This allows the model to focus on meaningful image composition rather than getting caught up in low-level noise patterns.</p>
</div>
<div id="tabset-1-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-3-tab">
<p>The reduced dimensionality and semantic organization of latent space leads to more stable training dynamics and better convergence properties.</p>
</div>
</div>
</div>
</section>
<section id="model-architecture-components" class="level3">
<h3 class="anchored" data-anchor-id="model-architecture-components">Model Architecture Components</h3>
<p>Stable Diffusion consists of three main components working in harmony:</p>
<p><strong>Text Encoder</strong>: The model uses CLIP’s text encoder to transform textual prompts into high-dimensional embeddings. These embeddings capture semantic relationships between words and concepts, enabling the model to understand complex prompt instructions. The text encoder processes prompts up to 77 tokens, with longer prompts being truncated.</p>
<p><strong>U-Net Denoising Network</strong>: The heart of the diffusion process is a U-Net architecture that predicts noise to be removed at each denoising step. This network incorporates cross-attention mechanisms to condition the denoising process on the text embeddings, allowing for precise control over image generation based on textual descriptions.</p>
<p><strong>Variational Autoencoder (VAE)</strong>: The VAE handles the conversion between pixel space and latent space. The encoder compresses 512×512 pixel images into 64×64 latent representations, while the decoder reconstructs high-resolution images from these compressed representations.</p>
</section>
</section>
<section id="training-and-data" class="level2">
<h2 class="anchored" data-anchor-id="training-and-data">Training and Data</h2>
<section id="dataset-composition" class="level3">
<h3 class="anchored" data-anchor-id="dataset-composition">Dataset Composition</h3>
<p>Stable Diffusion was trained on a subset of LAION-5B, a massive dataset containing 5.85 billion image-text pairs scraped from the internet. The training set consisted of approximately 2.3 billion images, filtered and processed to ensure quality and relevance. This enormous scale allows the model to learn diverse visual concepts, artistic styles, and the relationships between textual descriptions and visual content.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Dataset Scale
</div>
</div>
<div class="callout-body-container callout-body">
<p>The training dataset of 2.3 billion images from LAION-5B represents one of the largest collections of image-text pairs used for training generative models at the time.</p>
</div>
</div>
<p>The dataset’s diversity is both a strength and a source of ongoing discussion. It includes artwork, photographs, diagrams, memes, and virtually every category of visual content found online. This comprehensive coverage enables the model’s remarkable versatility but also raises questions about copyright, consent, and the ethics of training on web-scraped content.</p>
</section>
<section id="training-process" class="level3">
<h3 class="anchored" data-anchor-id="training-process">Training Process</h3>
<p>The training process involves several stages and techniques designed to produce a robust and capable model:</p>
<p><strong>Noise Scheduling</strong>: The model learns to denoise images across different noise levels, from heavily corrupted images to nearly clean ones. This teaches the network to handle various levels of corruption and enables the flexible sampling procedures used during inference.</p>
<p><strong>Classifier-Free Guidance</strong>: During training, the model learns to generate images both with and without text conditioning. This technique, known as classifier-free guidance, allows for better control over how closely the generated image follows the text prompt during inference.</p>
<p><strong>Progressive Training</strong>: The training process often employs progressive techniques, starting with lower resolutions and gradually increasing to the full 512×512 resolution. This approach improves training efficiency and helps the model learn both coarse and fine-grained features.</p>
</section>
</section>
<section id="inference-and-generation-process" class="level2">
<h2 class="anchored" data-anchor-id="inference-and-generation-process">Inference and Generation Process</h2>
<section id="the-sampling-pipeline" class="level3">
<h3 class="anchored" data-anchor-id="the-sampling-pipeline">The Sampling Pipeline</h3>
<p>Image generation in Stable Diffusion follows a carefully orchestrated sampling pipeline that transforms random noise into coherent images:</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    A[Random Noise] --&gt; B[Text Encoding]
    B --&gt; C[Iterative Denoising]
    C --&gt; D[VAE Decoding]
    D --&gt; E[Final Image]
    
    B --&gt; F[CLIP Text Encoder]
    C --&gt; G[U-Net Denoising]
    D --&gt; H[VAE Decoder]
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<ol type="1">
<li><p><strong>Initialization</strong>: The process begins with pure random noise in the latent space, typically sampled from a standard Gaussian distribution.</p></li>
<li><p><strong>Text Processing</strong>: The input prompt is tokenized and encoded using the CLIP text encoder, producing conditioning embeddings that guide the generation process.</p></li>
<li><p><strong>Iterative Denoising</strong>: Over multiple timesteps (typically 20-50), the U-Net predicts and removes noise from the latent representation. Each step brings the latent closer to representing a coherent image that matches the text prompt.</p></li>
<li><p><strong>Decoding</strong>: The final denoised latent representation is passed through the VAE decoder to produce the final high-resolution image.</p></li>
</ol>
</section>
<section id="sampling-algorithms" class="level3">
<h3 class="anchored" data-anchor-id="sampling-algorithms">Sampling Algorithms</h3>
<p>Various sampling algorithms can be employed during inference, each with different trade-offs between speed and quality:</p>
<div id="tbl-samplers" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-samplers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Comparison of sampling algorithms
</figcaption>
<div aria-describedby="tbl-samplers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Algorithm</th>
<th>Speed</th>
<th>Quality</th>
<th>Deterministic</th>
<th>Best Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>DDPM</td>
<td>Slow</td>
<td>High</td>
<td>No</td>
<td>High-quality generation</td>
</tr>
<tr class="even">
<td>DDIM</td>
<td>Fast</td>
<td>High</td>
<td>Yes</td>
<td>Reproducible results</td>
</tr>
<tr class="odd">
<td>Euler</td>
<td>Medium</td>
<td>Good</td>
<td>No</td>
<td>Balanced approach</td>
</tr>
<tr class="even">
<td>DPM++</td>
<td>Fast</td>
<td>High</td>
<td>Yes</td>
<td>Production workflows</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="guidance-and-control" class="level3">
<h3 class="anchored" data-anchor-id="guidance-and-control">Guidance and Control</h3>
<p><strong>Classifier-Free Guidance (CFG)</strong>: This technique allows users to control how closely the generated image follows the text prompt. Higher CFG values produce images that more strictly adhere to the prompt but may sacrifice diversity and naturalness.</p>
<p><strong>Negative Prompting</strong>: By specifying what should NOT appear in the image, users can steer generation away from unwanted elements or styles.</p>
<p><strong>Seed Control</strong>: Random seeds provide reproducibility and enable users to generate variations of the same basic composition.</p>
</section>
</section>
<section id="advanced-techniques-and-applications" class="level2">
<h2 class="anchored" data-anchor-id="advanced-techniques-and-applications">Advanced Techniques and Applications</h2>
<section id="image-to-image-generation" class="level3">
<h3 class="anchored" data-anchor-id="image-to-image-generation">Image-to-Image Generation</h3>
<p>Beyond text-to-image generation, Stable Diffusion supports image-to-image transformation, where an existing image serves as a starting point rather than random noise. This technique enables:</p>
<ul>
<li><strong>Style Transfer</strong>: Transforming images into different artistic styles while preserving content structure</li>
<li><strong>Image Editing</strong>: Making targeted modifications to existing images based on textual descriptions</li>
<li><strong>Variation Generation</strong>: Creating multiple variations of a base image with controlled differences</li>
</ul>
</section>
<section id="inpainting-and-outpainting" class="level3">
<h3 class="anchored" data-anchor-id="inpainting-and-outpainting">Inpainting and Outpainting</h3>
<p>Specialized versions of Stable Diffusion can fill in masked regions of images (inpainting) or extend images beyond their original boundaries (outpainting). These capabilities enable sophisticated image editing workflows and creative applications.</p>
</section>
<section id="controlnet-and-conditioning" class="level3">
<h3 class="anchored" data-anchor-id="controlnet-and-conditioning">ControlNet and Conditioning</h3>
<p>ControlNet represents a significant advancement in controllable generation, allowing users to guide image generation using structural inputs like edge maps, depth maps, pose information, or segmentation masks. This level of control bridges the gap between random generation and precise artistic intent.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
ControlNet Applications
</div>
</div>
<div class="callout-body-container callout-body">
<p>ControlNet enables precise control over composition, pose, and structure while maintaining the creative power of text-to-image generation.</p>
</div>
</div>
</section>
<section id="fine-tuning-and-customization" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-and-customization">Fine-tuning and Customization</h3>
<p>The open-source nature of Stable Diffusion has spawned numerous fine-tuning techniques:</p>
<p><strong>DreamBooth</strong>: Enables training the model to generate images of specific subjects or styles using just a few example images.</p>
<p><strong>Textual Inversion</strong>: Learns new tokens that represent specific concepts, styles, or objects not well-represented in the original training data.</p>
<p><strong>LoRA (Low-Rank Adaptation)</strong>: An efficient fine-tuning method that requires minimal computational resources while enabling significant customization.</p>
</section>
</section>
<section id="performance-and-hardware-considerations" class="level2">
<h2 class="anchored" data-anchor-id="performance-and-hardware-considerations">Performance and Hardware Considerations</h2>
<section id="system-requirements" class="level3">
<h3 class="anchored" data-anchor-id="system-requirements">System Requirements</h3>
<p>Stable Diffusion’s hardware requirements vary significantly based on the desired generation speed and image quality:</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true">Minimum Requirements</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false">Recommended Specifications</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-3" role="tab" aria-controls="tabset-2-3" aria-selected="false">Optimization Strategies</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<ul>
<li>6GB VRAM (for basic 512×512 generation)</li>
<li>16GB system RAM</li>
<li>Modern CPU (any architecture from the last 5 years)</li>
</ul>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<ul>
<li>12GB+ VRAM (enables higher resolutions and faster generation)</li>
<li>32GB system RAM (for complex workflows and batch processing)</li>
<li>High-end GPU (RTX 3080/4070 or better)</li>
</ul>
</div>
<div id="tabset-2-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-3-tab">
<ul>
<li>Half-precision (FP16) inference reduces memory usage significantly</li>
<li>Attention optimization techniques (xFormers, Flash Attention)</li>
<li>Model quantization for further memory reduction</li>
<li>Tiled VAE for generating images larger than native resolution</li>
</ul>
</div>
</div>
</div>
</section>
<section id="cloud-and-edge-deployment" class="level3">
<h3 class="anchored" data-anchor-id="cloud-and-edge-deployment">Cloud and Edge Deployment</h3>
<p>The model’s relatively modest requirements have enabled deployment across various platforms:</p>
<p><strong>Cloud Platforms</strong>: Services like RunPod, Vast.ai, and Google Colab provide accessible cloud-based generation.</p>
<p><strong>Edge Deployment</strong>: Optimized versions can run on mobile devices and embedded systems, though with reduced capability.</p>
<p><strong>Web Interfaces</strong>: Numerous web-based interfaces democratize access without requiring technical setup.</p>
</section>
</section>
<section id="sec-ethics" class="level2">
<h2 class="anchored" data-anchor-id="sec-ethics">Ethical Considerations and Societal Impact</h2>
<section id="copyright-and-intellectual-property" class="level3">
<h3 class="anchored" data-anchor-id="copyright-and-intellectual-property">Copyright and Intellectual Property</h3>
<p>Stable Diffusion’s training on web-scraped imagery has sparked significant debate about copyright, fair use, and intellectual property rights. Key concerns include:</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Copyright Concerns
</div>
</div>
<div class="callout-body-container callout-body">
<p>The use of copyrighted material in training data without explicit consent raises ongoing legal and ethical questions about fair use and artist rights.</p>
</div>
</div>
<p><strong>Artist Rights</strong>: Many artists’ works were included in training data without explicit consent, raising questions about compensation and attribution.</p>
<p><strong>Style Mimicry</strong>: The model’s ability to generate images “in the style of” specific artists has led to discussions about artistic authenticity and economic impact.</p>
<p><strong>Commercial Use</strong>: The boundaries between transformative use and copyright infringement remain legally unclear in many jurisdictions.</p>
</section>
<section id="bias-and-representation" class="level3">
<h3 class="anchored" data-anchor-id="bias-and-representation">Bias and Representation</h3>
<p>Like many AI systems trained on internet data, Stable Diffusion exhibits various biases:</p>
<ul>
<li><strong>Demographic Bias</strong>: Default representations often skew toward certain demographics, reflecting biases present in the training data</li>
<li><strong>Cultural Bias</strong>: The model’s understanding of concepts can be influenced by Western-centric perspectives prevalent in English-language internet content</li>
<li><strong>Historical Bias</strong>: Temporal biases in training data can lead to outdated or stereotypical representations</li>
</ul>
</section>
<section id="misuse-and-safety-concerns" class="level3">
<h3 class="anchored" data-anchor-id="misuse-and-safety-concerns">Misuse and Safety Concerns</h3>
<p>The democratization of high-quality image generation raises several safety considerations:</p>
<p><strong>Deepfakes and Misinformation</strong>: While not specifically designed for photorealistic human faces, the technology contributes to broader concerns about synthetic media and misinformation.</p>
<p><strong>Harmful Content</strong>: Despite built-in safety filters, determined users may find ways to generate inappropriate or harmful content.</p>
<p><strong>Economic Disruption</strong>: The technology’s impact on creative industries continues to evolve, with both opportunities and challenges for traditional creative professions.</p>
</section>
</section>
<section id="the-open-source-ecosystem" class="level2">
<h2 class="anchored" data-anchor-id="the-open-source-ecosystem">The Open Source Ecosystem</h2>
<section id="community-contributions" class="level3">
<h3 class="anchored" data-anchor-id="community-contributions">Community Contributions</h3>
<p>The open-source release of Stable Diffusion catalyzed an unprecedented wave of community innovation:</p>
<p><strong>User Interfaces</strong>: Projects like AUTOMATIC1111’s WebUI, ComfyUI, and InvokeAI provide accessible interfaces for non-technical users.</p>
<p><strong>Extensions and Plugins</strong>: Thousands of community-developed extensions add functionality ranging from advanced sampling methods to integration with other AI models.</p>
<p><strong>Model Variants</strong>: The community has created countless fine-tuned versions optimized for specific use cases, artistic styles, or quality improvements.</p>
</section>
<section id="commercial-applications" class="level3">
<h3 class="anchored" data-anchor-id="commercial-applications">Commercial Applications</h3>
<p>Despite being open-source, Stable Diffusion has enabled numerous commercial applications:</p>
<ul>
<li><strong>Creative Tools</strong>: Integration into professional creative software like Photoshop, Blender, and specialized AI art platforms</li>
<li><strong>Marketing and Advertising</strong>: Rapid prototyping of visual concepts and personalized content generation</li>
<li><strong>Gaming and Entertainment</strong>: Asset generation for games, concept art creation, and virtual world building</li>
<li><strong>Education and Research</strong>: Teaching aids, scientific visualization, and research tool development</li>
</ul>
</section>
</section>
<section id="future-developments-and-research-directions" class="level2">
<h2 class="anchored" data-anchor-id="future-developments-and-research-directions">Future Developments and Research Directions</h2>
<section id="technical-improvements" class="level3">
<h3 class="anchored" data-anchor-id="technical-improvements">Technical Improvements</h3>
<p>Active areas of research and development include:</p>
<p><strong>Higher Resolution Generation</strong>: Techniques for generating images at resolutions significantly higher than the training resolution of 512×512.</p>
<p><strong>Improved Consistency</strong>: Better temporal consistency for video generation and improved coherence across multiple images.</p>
<p><strong>Efficiency Optimizations</strong>: Faster sampling methods, more efficient architectures, and better hardware utilization.</p>
<p><strong>Multi-modal Integration</strong>: Better integration with other modalities like audio, 3D geometry, and temporal sequences.</p>
</section>
<section id="architectural-innovations" class="level3">
<h3 class="anchored" data-anchor-id="architectural-innovations">Architectural Innovations</h3>
<p><strong>Transformer-based Diffusion</strong>: Exploring alternatives to the U-Net architecture using transformer models for potentially better scalability and performance.</p>
<p><strong>Continuous Diffusion</strong>: Moving beyond discrete timesteps to continuous-time formulations that may offer theoretical and practical advantages.</p>
<p><strong>Hierarchical Generation</strong>: Multi-scale approaches that generate images at multiple resolutions simultaneously for better detail and consistency.</p>
</section>
<section id="emerging-applications" class="level3">
<h3 class="anchored" data-anchor-id="emerging-applications">Emerging Applications</h3>
<p><strong>3D Generation</strong>: Extensions of diffusion models to 3D object and scene generation, opening new possibilities for content creation.</p>
<p><strong>Video Generation</strong>: Temporal extensions that enable consistent video generation from text descriptions.</p>
<p><strong>Interactive Generation</strong>: Real-time generation and editing capabilities that enable new forms of creative interaction.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Stable Diffusion represents more than just a technical achievement; it embodies a paradigm shift in how we think about creativity, accessibility, and the democratization of advanced AI capabilities. By making high-quality text-to-image generation freely available and runnable on consumer hardware, it has lowered barriers to entry that previously restricted such capabilities to well-funded research labs and major technology companies.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Impact Summary
</div>
</div>
<div class="callout-body-container callout-body">
<p>Stable Diffusion’s open-source approach has democratized access to advanced AI image generation, sparking innovation while raising important questions about creativity, copyright, and the future of visual media.</p>
</div>
</div>
<p>The model’s impact extends across multiple domains, from empowering individual creators with new tools for expression to enabling businesses to rapidly prototype visual concepts. It has accelerated research in generative AI, inspired countless derivative works and improvements, and sparked important conversations about the future of human creativity in an age of artificial intelligence.</p>
<p>However, this democratization also brings challenges. Questions about copyright, consent, bias, and the economic impact on creative industries remain largely unresolved. As the technology continues to evolve, balancing innovation with ethical considerations will be crucial for realizing its positive potential while mitigating potential harms.</p>
<p>Looking forward, Stable Diffusion has established a foundation that will likely influence AI development for years to come. Its open-source ethos has proven that powerful AI capabilities need not remain locked behind corporate walls, while its technical innovations continue to inspire new research directions and applications.</p>
<p>The story of Stable Diffusion is still being written, with each new fine-tuned model, innovative application, and community contribution adding new chapters to this remarkable technological narrative. As we stand at this inflection point in the history of AI and creativity, Stable Diffusion serves as both a powerful tool and a glimpse into a future where the boundaries between human and artificial creativity continue to blur and evolve.</p>
<p>Whether one views it as a revolutionary creative tool, a concerning disruption to traditional industries, or simply an impressive technical achievement, Stable Diffusion undeniably represents a significant milestone in the ongoing evolution of artificial intelligence and its integration into human creative processes. Its legacy will likely be measured not just in the images it generates, but in the broader conversations, innovations, and transformations it has catalyzed across society.</p>



</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/theja-vanka\.github\.io\/blogs\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>© 2025 Krishnatheja Vanka</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>