---
title: "Complete MobileNet Code Guide"
author: "Krishnatheja Vanka"
date: "2025-07-19"
categories: [code, research, intermediate]
format:
  html:
    code-fold: false
    math: mathjax
execute:
  echo: true
  timing: true
jupyter: python3
---

# Complete MobileNet Code Guide
![](mobnet.png)

## Introduction

MobileNet is a family of efficient neural network architectures designed specifically for mobile and embedded devices. The key innovation is the use of **depthwise separable convolutions** which dramatically reduce the number of parameters and computational cost while maintaining reasonable accuracy.

## Prerequisites and Setup

::: {.callout-important}
## Requirements
Before diving into MobileNet implementation, ensure you have the following prerequisites:

**Software Requirements:**

- Python 3.8+ 
- PyTorch 1.12+
- torchvision 0.13+
- CUDA (optional, for GPU acceleration)

**Hardware Recommendations:**

- 8GB+ RAM for training
- NVIDIA GPU with 4GB+ VRAM (recommended)
- SSD storage for faster data loading
:::

### Installation

```{python}
#| label: installation
#| eval: false
#| caption: "Install required packages"

# Core dependencies
pip install torch torchvision torchaudio
pip install numpy matplotlib seaborn
pip install pillow opencv-python

# Optional dependencies for advanced features
pip install tensorboard  # For training visualization
pip install ptflops      # For FLOPs calculation  
pip install onnx onnxruntime  # For model export
pip install coremltools      # For iOS deployment
pip install tensorflow-lite  # For Android deployment

# Development tools
pip install jupyter notebook
pip install black isort      # Code formatting
```

### Quick Start Example

Here's a minimal example to get you started with MobileNet:

```{python}
#| label: quick-start
#| caption: "Quick start example with pre-trained MobileNet"
#| warning: false

import torch
import torchvision.models as models
from torchvision import transforms
import numpy as np

# Load pre-trained model
model = models.mobilenet_v2(pretrained=True)
model.eval()

print(f"âœ… Model loaded successfully!")
print(f"ðŸ“Š Total parameters: {sum(p.numel() for p in model.parameters()):,}")
print(f"ðŸ’¾ Model size: {sum(p.numel() * p.element_size() for p in model.parameters()) / 1024**2:.1f} MB")

# Test with random input
dummy_input = torch.randn(1, 3, 224, 224)
with torch.no_grad():
    output = model(dummy_input)
    
print(f"ðŸŽ¯ Output shape: {output.shape}")
print(f"ðŸ”¥ Top prediction: Class {torch.argmax(output).item()}")
```


### Key Features

- **Efficient**: 50x fewer parameters than AlexNet
- **Fast**: Optimized for mobile inference
- **Flexible**: Width and resolution multipliers for different use cases
- **Accurate**: Competitive performance on ImageNet

::: {.callout-note}
## MobileNet Efficiency
MobileNet achieves its efficiency through depthwise separable convolutions, which split standard convolutions into two operations: depthwise and pointwise convolutions.
:::

### Architecture Comparison Table

| Architecture | Parameters (M) | FLOPs (M) | Top-1 Accuracy (%) | Model Size (MB) | Target Device |
|--------------|----------------|-----------|-------------------|-----------------|---------------|
| AlexNet | 61.0 | 714 | 56.5 | 233 | Desktop |
| VGG-16 | 138.0 | 15500 | 71.5 | 528 | Desktop |
| ResNet-50 | 25.6 | 4100 | 76.1 | 98 | Server |
| MobileNet-V1 | 4.2 | 569 | 70.6 | 16 | Mobile |
| MobileNet-V2 | 3.4 | 300 | 72.0 | 14 | Mobile |
| EfficientNet-B0 | 5.3 | 390 | 77.3 | 21 | Mobile |

::: {.column-margin}
**Note:** Accuracy values are for ImageNet classification. FLOPs calculated for 224Ã—224 input images.
:::

## MobileNet Architecture

The MobileNet architecture consists of:

1. **Standard 3Ã—3 convolution** (first layer)
2. **13 depthwise separable convolution blocks**
3. **Average pooling and fully connected layer**

### Architecture Overview

```{python}
#| label: mobilenet-architecture
#| caption: "Complete MobileNet architecture implementation"

import torch
import torch.nn as nn
import torch.nn.functional as F

class MobileNet(nn.Module):
    def __init__(self, num_classes=1000, width_mult=1.0):
        super(MobileNet, self).__init__()
        
        # First standard convolution
        self.conv1 = nn.Conv2d(3, int(32 * width_mult), 3, stride=2, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(int(32 * width_mult))
        
        # Depthwise separable convolution blocks
        self.layers = nn.ModuleList([
            self._make_layer(int(32 * width_mult), int(64 * width_mult), 1),
            self._make_layer(int(64 * width_mult), int(128 * width_mult), 2),
            self._make_layer(int(128 * width_mult), int(128 * width_mult), 1),
            self._make_layer(int(128 * width_mult), int(256 * width_mult), 2),
            self._make_layer(int(256 * width_mult), int(256 * width_mult), 1),
            self._make_layer(int(256 * width_mult), int(512 * width_mult), 2),
            # 5 layers with stride 1
            *[self._make_layer(int(512 * width_mult), int(512 * width_mult), 1) for _ in range(5)],
            self._make_layer(int(512 * width_mult), int(1024 * width_mult), 2),
            self._make_layer(int(1024 * width_mult), int(1024 * width_mult), 1),
        ])
        
        # Global average pooling and classifier
        self.avgpool = nn.AdaptiveAvgPool2d(1)
        self.classifier = nn.Linear(int(1024 * width_mult), num_classes)
        
    def _make_layer(self, in_channels, out_channels, stride):
        return DepthwiseSeparableConv(in_channels, out_channels, stride)
    
    def forward(self, x):
        x = F.relu6(self.bn1(self.conv1(x)))
        
        for layer in self.layers:
            x = layer(x)
            
        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        
        return x
```

## Depthwise Separable Convolutions

The core innovation of MobileNet is the depthwise separable convolution, which splits a standard convolution into two operations:

### Depthwise Convolution

Applies a single filter per input channel (spatial filtering):

```{python}
#| label: depthwise-conv
#| caption: "Depthwise convolution implementation"

class DepthwiseConv(nn.Module):
    def __init__(self, in_channels, kernel_size=3, stride=1, padding=1):
        super(DepthwiseConv, self).__init__()
        self.depthwise = nn.Conv2d(
            in_channels, in_channels, 
            kernel_size=kernel_size, 
            stride=stride, 
            padding=padding, 
            groups=in_channels,  # Key: groups = in_channels
            bias=False
        )
        self.bn = nn.BatchNorm2d(in_channels)
    
    def forward(self, x):
        return F.relu6(self.bn(self.depthwise(x)))
```

### Pointwise Convolution

Applies 1Ã—1 convolution to combine features (channel mixing):

```{python}
#| label: pointwise-conv
#| caption: "Pointwise convolution implementation"

class PointwiseConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(PointwiseConv, self).__init__()
        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, bias=False)
        self.bn = nn.BatchNorm2d(out_channels)
    
    def forward(self, x):
        return F.relu6(self.bn(self.pointwise(x)))
```

### Complete Depthwise Separable Block

```{python}
#| label: depthwise-separable-block
#| caption: "Complete depthwise separable convolution block"

class DepthwiseSeparableConv(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(DepthwiseSeparableConv, self).__init__()
        
        self.depthwise = DepthwiseConv(in_channels, stride=stride)
        self.pointwise = PointwiseConv(in_channels, out_channels)
    
    def forward(self, x):
        x = self.depthwise(x)
        x = self.pointwise(x)
        return x
```

### Computational Efficiency

::: {.callout-important}
## Efficiency Gains

**Standard Convolution:**

- Parameters: `Dk Ã— Dk Ã— M Ã— N`
- Computation: `Dk Ã— Dk Ã— M Ã— N Ã— Df Ã— Df`

**Depthwise Separable Convolution:**

- Parameters: `Dk Ã— Dk Ã— M + M Ã— N`  
- Computation: `Dk Ã— Dk Ã— M Ã— Df Ã— Df + M Ã— N Ã— Df Ã— Df`

**Reduction Factor:**    `1/N + 1/DkÂ²` (typically 8-9x reduction)
:::

### Efficiency Visualization

Let's visualize the efficiency gains of depthwise separable convolutions:

```{python}
#| label: fig-efficiency-comparison
#| fig-cap: "Computational cost comparison: Standard vs Depthwise Separable Convolutions"
#| fig-width: 12
#| fig-height: 8
#| echo: false

import matplotlib.pyplot as plt
import numpy as np

# Parameters for comparison
input_channels = [32, 64, 128, 256, 512]
output_channels = [64, 128, 256, 512, 1024]
kernel_size = 3
feature_size = 56  # Assuming 56x56 feature maps

fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))

# Calculate FLOPs for each layer
standard_flops = []
depthwise_flops = []
params_standard = []
params_depthwise = []

for i, (in_ch, out_ch) in enumerate(zip(input_channels, output_channels)):
    # Standard convolution
    std_flops = kernel_size * kernel_size * in_ch * out_ch * feature_size * feature_size
    std_params = kernel_size * kernel_size * in_ch * out_ch
    
    # Depthwise separable convolution  
    dw_flops = (kernel_size * kernel_size * in_ch * feature_size * feature_size) + \
               (in_ch * out_ch * feature_size * feature_size)
    dw_params = (kernel_size * kernel_size * in_ch) + (in_ch * out_ch)
    
    standard_flops.append(std_flops / 1e6)  # Convert to millions
    depthwise_flops.append(dw_flops / 1e6)
    params_standard.append(std_params / 1e3)  # Convert to thousands
    params_depthwise.append(dw_params / 1e3)

layers = [f'L{i+1}\n{ic}â†’{oc}' for i, (ic, oc) in enumerate(zip(input_channels, output_channels))]

# FLOPs comparison
x = np.arange(len(layers))
width = 0.35

ax1.bar(x - width/2, standard_flops, width, label='Standard Conv', alpha=0.8, color='red')
ax1.bar(x + width/2, depthwise_flops, width, label='Depthwise Separable', alpha=0.8, color='green')
ax1.set_ylabel('FLOPs (Millions)')
ax1.set_title('Computational Cost Comparison')
ax1.set_xticks(x)
ax1.set_xticklabels(layers)
ax1.legend()
ax1.grid(True, alpha=0.3)

# Parameters comparison
ax2.bar(x - width/2, params_standard, width, label='Standard Conv', alpha=0.8, color='red')
ax2.bar(x + width/2, params_depthwise, width, label='Depthwise Separable', alpha=0.8, color='green')
ax2.set_ylabel('Parameters (Thousands)')
ax2.set_title('Parameter Count Comparison')
ax2.set_xticks(x)
ax2.set_xticklabels(layers)
ax2.legend()
ax2.grid(True, alpha=0.3)

# Efficiency ratio
efficiency_flops = [std/dw for std, dw in zip(standard_flops, depthwise_flops)]
efficiency_params = [std/dw for std, dw in zip(params_standard, params_depthwise)]

ax3.plot(layers, efficiency_flops, 'o-', label='FLOPs Reduction', linewidth=2, markersize=8)
ax3.plot(layers, efficiency_params, 's-', label='Parameters Reduction', linewidth=2, markersize=8)
ax3.set_ylabel('Reduction Factor (Ã—)')
ax3.set_title('Efficiency Gains')
ax3.legend()
ax3.grid(True, alpha=0.3)

# Cumulative savings
cumulative_std_flops = np.cumsum(standard_flops)
cumulative_dw_flops = np.cumsum(depthwise_flops)

ax4.plot(layers, cumulative_std_flops, 'o-', label='Standard Conv', linewidth=2)
ax4.plot(layers, cumulative_dw_flops, 's-', label='Depthwise Separable', linewidth=2)  
ax4.set_ylabel('Cumulative FLOPs (Millions)')
ax4.set_title('Cumulative Computational Cost')
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Print summary statistics
total_std_flops = sum(standard_flops)
total_dw_flops = sum(depthwise_flops)
print(f"\nðŸ“Š **Efficiency Summary:**")
print(f"   â€¢ Total FLOPs - Standard: {total_std_flops:.1f}M")
print(f"   â€¢ Total FLOPs - Depthwise: {total_dw_flops:.1f}M") 
print(f"   â€¢ **Overall Reduction: {total_std_flops/total_dw_flops:.1f}Ã—**")
```

## Implementation from Scratch

Here's a complete implementation with detailed explanations:

```{python}
#| label: mobilenet-complete
#| caption: "Complete MobileNetV1 implementation with configurable parameters"

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional

class MobileNetV1(nn.Module):
    """
    MobileNetV1 implementation with configurable width and resolution multipliers.
    
    Args:
        num_classes: Number of output classes
        width_mult: Width multiplier for channels (0.25, 0.5, 0.75, 1.0)
        resolution_mult: Resolution multiplier for input size
        dropout_rate: Dropout rate before classifier
    """
    
    def __init__(self, 
                 num_classes: int = 1000, 
                 width_mult: float = 1.0,
                 dropout_rate: float = 0.2):
        super(MobileNetV1, self).__init__()
        
        self.width_mult = width_mult
        
        # Helper function to make channels divisible by 8
        def _make_divisible(v, divisor=8):
            new_v = max(divisor, int(v + divisor / 2) // divisor * divisor)
            if new_v < 0.9 * v:
                new_v += divisor
            return new_v
        
        # Define channel configurations
        input_channel = _make_divisible(32 * width_mult)
        
        # First standard convolution
        self.conv1 = nn.Sequential(
            nn.Conv2d(3, input_channel, 3, 2, 1, bias=False),
            nn.BatchNorm2d(input_channel),
            nn.ReLU6(inplace=True)
        )
        
        # Configuration: [output_channels, stride]
        configs = [
            [64, 1],
            [128, 2], [128, 1],
            [256, 2], [256, 1],
            [512, 2], [512, 1], [512, 1], [512, 1], [512, 1], [512, 1],
            [1024, 2], [1024, 1]
        ]
        
        # Build depthwise separable layers
        layers = []
        for output_channel, stride in configs:
            output_channel = _make_divisible(output_channel * width_mult)
            layers.append(
                DepthwiseSeparableConv(input_channel, output_channel, stride)
            )
            input_channel = output_channel
        
        self.features = nn.Sequential(*layers)
        
        # Classifier
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.dropout = nn.Dropout(dropout_rate)
        self.classifier = nn.Linear(input_channel, num_classes)
        
        # Initialize weights
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Initialize weights using He initialization for ReLU networks."""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.zeros_(m.bias)
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.features(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.dropout(x)
        x = self.classifier(x)
        return x

# Optimized Depthwise Separable Convolution with better efficiency
class DepthwiseSeparableConv(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(DepthwiseSeparableConv, self).__init__()
        
        self.conv = nn.Sequential(
            # Depthwise convolution
            nn.Conv2d(in_channels, in_channels, 3, stride, 1, 
                     groups=in_channels, bias=False),
            nn.BatchNorm2d(in_channels),
            nn.ReLU6(inplace=True),
            
            # Pointwise convolution
            nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU6(inplace=True),
        )
    
    def forward(self, x):
        return self.conv(x)

# Model factory function
def mobilenet_v1(num_classes=1000, width_mult=1.0, pretrained=False):
    """
    Create MobileNetV1 model.
    
    Args:
        num_classes: Number of classes for classification
        width_mult: Width multiplier (0.25, 0.5, 0.75, 1.0)
        pretrained: Load pretrained weights (if available)
    """
    model = MobileNetV1(num_classes=num_classes, width_mult=width_mult)
    
    if pretrained:
        # In practice, you would load pretrained weights here
        print(f"Loading pretrained MobileNetV1 with width_mult={width_mult}")
    
    return model
```

## Using Pre-trained MobileNet

### With PyTorch (torchvision)

```{python}
#| label: pretrained-usage
#| caption: "Using pre-trained MobileNet for inference"
#| warning: false

import torch
import torchvision.models as models
from torchvision import transforms
from PIL import Image

# Load pre-trained MobileNetV2
model = models.mobilenet_v2(pretrained=True)
model.eval()

# Preprocessing pipeline
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                        std=[0.229, 0.224, 0.225]),
])

# Inference function
def predict_image(image_path, model, preprocess, top_k=5):
    """Predict top-k classes for an image."""
    
    # Load and preprocess image
    image = Image.open(image_path).convert('RGB')
    input_tensor = preprocess(image)
    input_batch = input_tensor.unsqueeze(0)  # Add batch dimension
    
    # Predict
    with torch.no_grad():
        output = model(input_batch)
        probabilities = torch.nn.functional.softmax(output[0], dim=0)
    
    # Get top-k predictions
    top_prob, top_indices = torch.topk(probabilities, top_k)
    
    return [(idx.item(), prob.item()) for idx, prob in zip(top_indices, top_prob)]

# Example usage
# predictions = predict_image('cat.jpg', model, preprocess)
# print(predictions)
```

### Fine-tuning Pre-trained MobileNet

```{python}
#| label: fine-tuning
#| caption: "Fine-tuning MobileNet for custom classification tasks"

import torch.optim as optim
import torch.nn as nn

def create_mobilenet_classifier(num_classes, pretrained=True):
    """Create MobileNet for custom classification task."""
    
    # Load pre-trained model
    model = models.mobilenet_v2(pretrained=pretrained)
    
    # Modify classifier for custom number of classes
    model.classifier = nn.Sequential(
        nn.Dropout(0.2),
        nn.Linear(model.last_channel, num_classes),
    )
    
    return model

# Training setup for fine-tuning
def setup_training(model, num_classes, learning_rate=0.001):
    """Setup optimizer and loss function for fine-tuning."""
    
    # Freeze feature extraction layers (optional)
    for param in model.features.parameters():
        param.requires_grad = False
    
    # Only train classifier
    optimizer = optim.Adam(model.classifier.parameters(), lr=learning_rate)
    criterion = nn.CrossEntropyLoss()
    
    return optimizer, criterion

# Training loop
def train_epoch(model, dataloader, optimizer, criterion, device):
    """Train model for one epoch."""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    for inputs, labels in dataloader:
        inputs, labels = inputs.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    
    return running_loss / len(dataloader), 100 * correct / total
```

## Training MobileNet

### Complete Training Pipeline

```{python}
#| label: training-pipeline
#| caption: "Complete training pipeline for MobileNet"

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import time

class MobileNetTrainer:
    def __init__(self, model, device='cuda'):
        self.model = model.to(device)
        self.device = device
        self.history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}
    
    def train(self, train_loader, val_loader, epochs=10, lr=0.001):
        """Complete training pipeline."""
        
        # Setup optimizer and scheduler
        optimizer = optim.RMSprop(self.model.parameters(), lr=lr, weight_decay=1e-4)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
        criterion = nn.CrossEntropyLoss()
        
        best_acc = 0.0
        
        for epoch in range(epochs):
            print(f'Epoch {epoch+1}/{epochs}')
            print('-' * 10)
            
            # Training phase
            train_loss, train_acc = self._train_epoch(train_loader, optimizer, criterion)
            
            # Validation phase
            val_loss, val_acc = self._validate_epoch(val_loader, criterion)
            
            # Update scheduler
            scheduler.step()
            
            # Save best model
            if val_acc > best_acc:
                best_acc = val_acc
                torch.save(self.model.state_dict(), 'best_mobilenet.pth')
            
            # Update history
            self.history['train_loss'].append(train_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_loss'].append(val_loss)
            self.history['val_acc'].append(val_acc)
            
            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')
            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
            print()
    
    def _train_epoch(self, dataloader, optimizer, criterion):
        """Train for one epoch."""
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(self.device), labels.to(self.device)
            
            optimizer.zero_grad()
            outputs = self.model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        
        return running_loss / len(dataloader), 100 * correct / total
    
    def _validate_epoch(self, dataloader, criterion):
        """Validate for one epoch."""
        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for inputs, labels in dataloader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                
                outputs = self.model(inputs)
                loss = criterion(outputs, labels)
                
                running_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        return running_loss / len(dataloader), 100 * correct / total
```

### Data Loading and Augmentation

```{python}
#| label: data-loading
#| caption: "Data loading and augmentation pipeline"

# Data loading and augmentation
def get_dataloaders(data_dir, batch_size=32, num_workers=4):
    """Create training and validation dataloaders."""
    
    # Data augmentation for training
    train_transforms = transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    # Validation transforms
    val_transforms = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    # Create datasets
    train_dataset = datasets.ImageFolder(f'{data_dir}/train', train_transforms)
    val_dataset = datasets.ImageFolder(f'{data_dir}/val', val_transforms)
    
    # Create dataloaders
    train_loader = DataLoader(train_dataset, batch_size=batch_size, 
                             shuffle=True, num_workers=num_workers)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, 
                           shuffle=False, num_workers=num_workers)
    
    return train_loader, val_loader, len(train_dataset.classes)

# Example usage
if __name__ == "__main__":
    # Setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Load data
    # train_loader, val_loader, num_classes = get_dataloaders('path/to/data')
    
    # Create model
    # model = mobilenet_v1(num_classes=num_classes, width_mult=1.0)
    
    # Train
    # trainer = MobileNetTrainer(model, device)
    # trainer.train(train_loader, val_loader, epochs=20)
    pass
```

## MobileNet Variants

### MobileNetV2 Implementation

```{python}
#| label: mobilenetv2
#| caption: "MobileNetV2 with inverted residual blocks"

class InvertedResidual(nn.Module):
    """Inverted residual block for MobileNetV2."""
    
    def __init__(self, in_channels, out_channels, stride, expand_ratio):
        super(InvertedResidual, self).__init__()
        
        hidden_dim = int(round(in_channels * expand_ratio))
        self.use_residual = stride == 1 and in_channels == out_channels
        
        layers = []
        
        # Expansion phase
        if expand_ratio != 1:
            layers.extend([
                nn.Conv2d(in_channels, hidden_dim, 1, bias=False),
                nn.BatchNorm2d(hidden_dim),
                nn.ReLU6(inplace=True),
            ])
        
        # Depthwise convolution
        layers.extend([
            nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, 
                     groups=hidden_dim, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU6(inplace=True),
            
            # Pointwise linear projection
            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
        ])
        
        self.conv = nn.Sequential(*layers)
    
    def forward(self, x):
        if self.use_residual:
            return x + self.conv(x)
        else:
            return self.conv(x)

class MobileNetV2(nn.Module):
    """MobileNetV2 with inverted residuals and linear bottlenecks."""
    
    def __init__(self, num_classes=1000, width_mult=1.0):
        super(MobileNetV2, self).__init__()
        
        input_channel = 32
        last_channel = 1280
        
        # Inverted residual settings
        # t: expansion factor, c: output channels, n: number of blocks, s: stride
        inverted_residual_setting = [
            # t, c, n, s
            [1, 16, 1, 1],
            [6, 24, 2, 2],
            [6, 32, 3, 2],
            [6, 64, 4, 2],
            [6, 96, 3, 1],
            [6, 160, 3, 2],
            [6, 320, 1, 1],
        ]
        
        # Apply width multiplier
        input_channel = int(input_channel * width_mult)
        self.last_channel = int(last_channel * max(1.0, width_mult))
        
        # First convolution
        features = [nn.Sequential(
            nn.Conv2d(3, input_channel, 3, 2, 1, bias=False),
            nn.BatchNorm2d(input_channel),
            nn.ReLU6(inplace=True)
        )]
        
        # Inverted residual blocks
        for t, c, n, s in inverted_residual_setting:
            output_channel = int(c * width_mult)
            for i in range(n):
                stride = s if i == 0 else 1
                features.append(InvertedResidual(input_channel, output_channel, 
                                               stride, t))
                input_channel = output_channel
        
        # Last convolution
        features.append(nn.Sequential(
            nn.Conv2d(input_channel, self.last_channel, 1, bias=False),
            nn.BatchNorm2d(self.last_channel),
            nn.ReLU6(inplace=True)
        ))
        
        self.features = nn.Sequential(*features)
        
        # Classifier
        self.classifier = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(self.last_channel, num_classes),
        )
    
    def forward(self, x):
        x = self.features(x)
        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x
```

### MobileNetV3 Features

```{python}
#| label: mobilenetv3-components
#| caption: "Key components of MobileNetV3"

class SEBlock(nn.Module):
    """Squeeze-and-Excitation block."""
    
    def __init__(self, in_channels, reduction=4):
        super(SEBlock, self).__init__()
        
        self.se = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(in_channels, in_channels // reduction, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels // reduction, in_channels, 1),
            nn.Hardsigmoid(inplace=True)
        )
    
    def forward(self, x):
        return x * self.se(x)

class HardSwish(nn.Module):
    """Hard Swish activation function."""
    
    def forward(self, x):
        return x * F.hardsigmoid(x)

# MobileNetV3 would use these components along with:
# - Neural Architecture Search (NAS) for optimal architecture
# - Hard Swish activation instead of ReLU6
# - Squeeze-and-Excitation blocks
# - Optimized last layers
```

::: {.callout-tip}
## MobileNetV3 Improvements
MobileNetV3 incorporates several advanced techniques:

- **Neural Architecture Search** for optimal layer configurations
- **Squeeze-and-Excitation blocks** for attention mechanisms
- **Hard Swish activation** for better performance
- **Optimized head and tail** layers for efficiency
:::

## Optimization Techniques

### Quantization

```{python}
#| label: quantization
#| caption: "Post-training and dynamic quantization techniques"

import torch.quantization as quantization

def quantize_mobilenet(model, calibration_loader):
    """Apply post-training quantization to MobileNet."""
    
    # Set model to evaluation mode
    model.eval()
    
    # Fuse modules for better quantization
    model_fused = torch.quantization.fuse_modules(model, [
        ['conv', 'bn', 'relu'] for conv, bn, relu in model.named_modules()
        if isinstance(conv, nn.Conv2d) and isinstance(bn, nn.BatchNorm2d)
    ])
    
    # Set quantization config
    model_fused.qconfig = quantization.get_default_qconfig('qnnpack')
    
    # Prepare model for quantization
    model_prepared = quantization.prepare(model_fused)
    
    # Calibrate with representative data
    with torch.no_grad():
        for inputs, _ in calibration_loader:
            model_prepared(inputs)
    
    # Convert to quantized model
    model_quantized = quantization.convert(model_prepared)
    
    return model_quantized

# Dynamic quantization (easier but less optimal)
def dynamic_quantize_mobilenet(model):
    """Apply dynamic quantization."""
    return quantization.quantize_dynamic(
        model, 
        {nn.Linear, nn.Conv2d}, 
        dtype=torch.qint8
    )
```

### Pruning

```{python}
#| label: pruning
#| caption: "Magnitude-based and structured pruning techniques"

import torch.nn.utils.prune as prune

def prune_mobilenet(model, pruning_ratio=0.2):
    """Apply magnitude-based pruning to MobileNet."""
    
    parameters_to_prune = []
    
    # Collect Conv2d and Linear layers for pruning
    for name, module in model.named_modules():
        if isinstance(module, (nn.Conv2d, nn.Linear)):
            parameters_to_prune.append((module, 'weight'))
    
    # Apply global magnitude pruning
    prune.global_unstructured(
        parameters_to_prune,
        pruning_method=prune.L1Unstructured,
        amount=pruning_ratio,
    )
    
    # Make pruning permanent
    for module, param_name in parameters_to_prune:
        prune.remove(module, param_name)
    
    return model

# Structured pruning example
def structured_prune_mobilenet(model, pruning_ratio=0.2):
    """Apply structured channel pruning."""
    
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv2d) and module.groups == 1:  # Skip depthwise
            prune.ln_structured(
                module, 
                name='weight', 
                amount=pruning_ratio, 
                n=2, 
                dim=0  # Prune output channels
            )
    
    return model
```

## Deployment Considerations

### ONNX Export

```{python}
#| label: onnx-export
#| caption: "Exporting MobileNet to ONNX format for cross-platform deployment"

import torch.onnx

def export_to_onnx(model, input_shape=(1, 3, 224, 224), onnx_path="mobilenet.onnx"):
    """Export MobileNet to ONNX format."""
    
    model.eval()
    dummy_input = torch.randn(input_shape)
    
    torch.onnx.export(
        model,
        dummy_input,
        onnx_path,
        export_params=True,
        opset_version=11,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={
            'input': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        }
    )
    
    print(f"Model exported to {onnx_path}")

# TensorRT optimization (requires tensorrt)
def optimize_with_tensorrt(onnx_path):
    """Optimize ONNX model with TensorRT."""
    try:
        import tensorrt as trt
        
        # Create TensorRT logger and builder
        logger = trt.Logger(trt.Logger.WARNING)
        builder = trt.Builder(logger)
        
        # Parse ONNX model
        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
        parser = trt.OnnxParser(network, logger)
        
        with open(onnx_path, 'rb') as model:
            parser.parse(model.read())
        
        # Build optimized engine
        config = builder.create_builder_config()
        config.max_workspace_size = 1 << 28  # 256MB
        config.set_flag(trt.BuilderFlag.FP16)  # Enable FP16 precision
        
        engine = builder.build_engine(network, config)
        
        # Save engine
        with open("mobilenet.trt", "wb") as f:
            f.write(engine.serialize())
        
        return engine
    except ImportError:
        print("TensorRT not installed. Please install TensorRT for optimization.")
        return None
```

### Mobile Deployment

```{python}
#| label: mobile-deployment
#| caption: "Converting models for mobile deployment platforms"

# TensorFlow Lite conversion (if using TensorFlow)
def convert_to_tflite(model_path, tflite_path="mobilenet.tflite"):
    """Convert model to TensorFlow Lite format."""
    try:
        import tensorflow as tf
        
        # Load model (assuming saved as TensorFlow model)
        converter = tf.lite.TFLiteConverter.from_saved_model(model_path)
        
        # Optimization settings
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.target_spec.supported_types = [tf.float16]
        
        # Convert
        tflite_model = converter.convert()
        
        # Save
        with open(tflite_path, 'wb') as f:
            f.write(tflite_model)
        
        print(f"TFLite model saved to {tflite_path}")
    except ImportError:
        print("TensorFlow not installed. Install with: pip install tensorflow")

# CoreML conversion (for iOS)
def convert_to_coreml(model, input_shape=(1, 3, 224, 224)):
    """Convert PyTorch model to CoreML format."""
    try:
        import coremltools as ct
        
        model.eval()
        example_input = torch.rand(input_shape)
        
        # Trace the model
        traced_model = torch.jit.trace(model, example_input)
        
        # Convert to CoreML
        coreml_model = ct.convert(
            traced_model,
            inputs=[ct.ImageType(shape=input_shape, bias=[-1, -1, -1], scale=1/127.5)]
        )
        
        # Save
        coreml_model.save("MobileNet.mlmodel")
        print("CoreML model saved successfully")
        
    except ImportError:
        print("coremltools not installed. Install with: pip install coremltools")
```

### Edge Deployment with Optimization

```{python}
#| label: edge-deployment
#| caption: "Optimized inference class for edge deployment"

# Edge deployment with optimization
class OptimizedMobileNetInference:
    """Optimized inference class for edge deployment."""
    
    def __init__(self, model_path, device='cpu'):
        self.device = device
        self.model = self.load_optimized_model(model_path)
        self.preprocess = self.get_preprocessing()
    
    def load_optimized_model(self, model_path):
        """Load and optimize model for inference."""
        model = torch.load(model_path, map_location=self.device)
        model.eval()
        
        # Apply optimizations
        if self.device == 'cpu':
            # Optimize for CPU inference
            model = torch.jit.optimize_for_inference(torch.jit.script(model))
        
        return model
    
    def get_preprocessing(self):
        """Get optimized preprocessing pipeline."""
        return transforms.Compose([
            transforms.Resize(256, interpolation=transforms.InterpolationMode.BILINEAR),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
    
    @torch.no_grad()
    def predict(self, image):
        """Fast inference on single image."""
        if isinstance(image, str):
            from PIL import Image
            image = Image.open(image).convert('RGB')
        
        # Preprocess
        input_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
        
        # Inference
        output = self.model(input_tensor)
        probabilities = F.softmax(output[0], dim=0)
        
        return probabilities.cpu().numpy()
    
    def batch_predict(self, images, batch_size=32):
        """Batch inference for multiple images."""
        results = []
        
        for i in range(0, len(images), batch_size):
            batch = images[i:i+batch_size]
            batch_tensor = torch.stack([
                self.preprocess(img) for img in batch
            ]).to(self.device)
            
            outputs = self.model(batch_tensor)
            probabilities = F.softmax(outputs, dim=1)
            results.extend(probabilities.cpu().numpy())
        
        return results
```

## Performance Analysis

### Benchmarking Tools

```{python}
#| label: benchmarking
#| caption: "Comprehensive benchmarking suite for MobileNet"

import time
import numpy as np
from contextlib import contextmanager

class MobileNetBenchmark:
    """Comprehensive benchmarking suite for MobileNet."""
    
    def __init__(self, model, device='cpu'):
        self.model = model.to(device)
        self.device = device
        self.model.eval()
    
    @contextmanager
    def timer(self):
        """Context manager for timing operations."""
        start = time.time()
        yield
        end = time.time()
        self.last_time = end - start
    
    def benchmark_inference(self, input_shape=(1, 3, 224, 224), num_runs=100, warmup=10):
        """Benchmark inference speed."""
        dummy_input = torch.randn(input_shape).to(self.device)
        
        # Warmup
        with torch.no_grad():
            for _ in range(warmup):
                _ = self.model(dummy_input)
        
        # Benchmark
        times = []
        with torch.no_grad():
            for _ in range(num_runs):
                with self.timer():
                    _ = self.model(dummy_input)
                times.append(self.last_time)
        
        return {
            'mean_time': np.mean(times),
            'std_time': np.std(times),
            'min_time': np.min(times),
            'max_time': np.max(times),
            'fps': 1.0 / np.mean(times)
        }
    
    def benchmark_memory(self, input_shape=(1, 3, 224, 224)):
        """Benchmark memory usage."""
        if self.device == 'cuda':
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            
            dummy_input = torch.randn(input_shape).to(self.device)
            
            with torch.no_grad():
                _ = self.model(dummy_input)
            
            memory_stats = {
                'peak_memory_mb': torch.cuda.max_memory_allocated() / 1024**2,
                'current_memory_mb': torch.cuda.memory_allocated() / 1024**2
            }
            
            return memory_stats
        else:
            return {'message': 'Memory benchmarking only available for CUDA'}
    
    def profile_layers(self, input_shape=(1, 3, 224, 224)):
        """Profile individual layers."""
        dummy_input = torch.randn(input_shape).to(self.device)
        
        with torch.profiler.profile(
            activities=[torch.profiler.ProfilerActivity.CPU, 
                       torch.profiler.ProfilerActivity.CUDA],
            record_shapes=True,
            profile_memory=True,
            with_stack=True
        ) as prof:
            with torch.no_grad():
                _ = self.model(dummy_input)
        
        return prof
    
    def compare_models(self, models_dict, input_shape=(1, 3, 224, 224)):
        """Compare multiple model variants."""
        results = {}
        
        for name, model in models_dict.items():
            benchmark = MobileNetBenchmark(model, self.device)
            results[name] = {
                'inference': benchmark.benchmark_inference(input_shape),
                'memory': benchmark.benchmark_memory(input_shape),
                'parameters': sum(p.numel() for p in model.parameters()),
                'model_size_mb': sum(p.numel() * p.element_size() 
                                   for p in model.parameters()) / 1024**2
            }
        
        return results
```

### Model Comparison

```{python}
#| label: model-comparison
#| caption: "Comparing different MobileNet variants"
#| echo: false

# Model comparison example
def compare_mobilenet_variants():
    """Compare different MobileNet configurations."""
    
    models = {
        'MobileNet_1.0': mobilenet_v1(width_mult=1.0),
        'MobileNet_0.75': mobilenet_v1(width_mult=0.75),
        'MobileNet_0.5': mobilenet_v1(width_mult=0.5),
        'MobileNet_0.25': mobilenet_v1(width_mult=0.25),
    }
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    benchmark = MobileNetBenchmark(models['MobileNet_1.0'], device)
    
    results = benchmark.compare_models(models)
    
    # Print comparison table
    print(f"{'Model':<15} {'Params (M)':<12} {'Size (MB)':<10} {'FPS':<8} {'Peak Mem (MB)':<15}")
    print("-" * 70)
    
    for name, stats in results.items():
        params_m = stats['parameters'] / 1e6
        size_mb = stats['model_size_mb']
        fps = stats['inference']['fps']
        memory = stats['memory'].get('peak_memory_mb', 'N/A')
        
        print(f"{name:<15} {params_m:<12.2f} {size_mb:<10.2f} {fps:<8.1f} {memory:<15}")
    
    return models

models = compare_mobilenet_variants()

# Advanced profiling
def detailed_profiling(model, input_shape=(1, 3, 224, 224)):
    """Detailed profiling with visualization."""
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = model.to(device)
    dummy_input = torch.randn(input_shape).to(device)
    
    # Profile with different batch sizes
    batch_sizes = [1, 4, 8, 16, 32]
    results = {}
    
    for batch_size in batch_sizes:
        input_tensor = torch.randn(batch_size, 3, 224, 224).to(device)
        benchmark = MobileNetBenchmark(model, device)
        
        stats = benchmark.benchmark_inference((batch_size, 3, 224, 224))
        results[batch_size] = {
            'latency_ms': stats['mean_time'] * 1000,
            'throughput_imgs_per_sec': batch_size / stats['mean_time'],
            'fps': stats['fps']
        }
    
    return results

# FLOPs calculation
def calculate_flops(model, input_shape=(1, 3, 224, 224)):
    """Calculate FLOPs for MobileNet."""
    try:
        from ptflops import get_model_complexity_info
        
        macs, params = get_model_complexity_info(
            model, 
            input_shape[1:],  # Remove batch dimension
            as_strings=True,
            print_per_layer_stat=True,
            verbose=True
        )
        
        return {
            'MACs': macs,
            'Params': params
        }
    except ImportError:
        print("ptflops not installed. Install with: pip install ptflops")
        return None
```

### Accuracy vs Efficiency Analysis

```{python}
#| label: accuracy-efficiency
#| caption: "Analyzing trade-offs between accuracy and efficiency"
#| echo: false

import matplotlib.pyplot as plt
import seaborn as sns

def accuracy_efficiency_analysis():
    """Analyze trade-offs between accuracy and efficiency."""
    
    # Example data (in practice, you'd measure actual accuracy)
    model_data = {
        'MobileNet_1.0': {'accuracy': 70.6, 'latency_ms': 15.2, 'params_m': 4.2, 'flops_m': 569},
        'MobileNet_0.75': {'accuracy': 68.4, 'latency_ms': 11.8, 'params_m': 2.6, 'flops_m': 325},
        'MobileNet_0.5': {'accuracy': 63.7, 'latency_ms': 8.1, 'params_m': 1.3, 'flops_m': 149},
        'MobileNet_0.25': {'accuracy': 50.6, 'latency_ms': 4.9, 'params_m': 0.5, 'flops_m': 41},
        'MobileNetV2_1.0': {'accuracy': 72.0, 'latency_ms': 18.1, 'params_m': 3.4, 'flops_m': 300},
        'ResNet50': {'accuracy': 76.1, 'latency_ms': 42.3, 'params_m': 25.6, 'flops_m': 4100},
    }
    
    # Create visualization
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    models = list(model_data.keys())
    accuracies = [model_data[m]['accuracy'] for m in models]
    latencies = [model_data[m]['latency_ms'] for m in models]
    params = [model_data[m]['params_m'] for m in models]
    flops = [model_data[m]['flops_m'] for m in models]
    
    # Accuracy vs Latency
    ax1.scatter(latencies, accuracies, s=100, alpha=0.7)
    for i, model in enumerate(models):
        ax1.annotate(model, (latencies[i], accuracies[i]), 
                    xytext=(5, 5), textcoords='offset points')
    ax1.set_xlabel('Latency (ms)')
    ax1.set_ylabel('Accuracy (%)')
    ax1.set_title('Accuracy vs Latency Trade-off')
    ax1.grid(True, alpha=0.3)
    
    # Accuracy vs Parameters
    ax2.scatter(params, accuracies, s=100, alpha=0.7, color='orange')
    for i, model in enumerate(models):
        ax2.annotate(model, (params[i], accuracies[i]), 
                    xytext=(5, 5), textcoords='offset points')
    ax2.set_xlabel('Parameters (M)')
    ax2.set_ylabel('Accuracy (%)')
    ax2.set_title('Accuracy vs Model Size')
    ax2.grid(True, alpha=0.3)
    
    # Efficiency Score (Accuracy / Latency)
    efficiency_scores = [acc / lat for acc, lat in zip(accuracies, latencies)]
    ax3.bar(models, efficiency_scores, alpha=0.7, color='green')
    ax3.set_ylabel('Efficiency Score (Accuracy/Latency)')
    ax3.set_title('Model Efficiency Ranking')
    ax3.tick_params(axis='x', rotation=45)
    
    # FLOPs comparison
    ax4.bar(models, flops, alpha=0.7, color='red')
    ax4.set_ylabel('FLOPs (M)')
    ax4.set_title('Computational Complexity')
    ax4.tick_params(axis='x', rotation=45)
    ax4.set_yscale('log')
    
    plt.tight_layout()
    plt.savefig('mobilenet_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return model_data

model_data = accuracy_efficiency_analysis()
```

### Real-world Deployment Simulation

```{python}
#| label: deployment-simulation
#| caption: "Simulating real-world deployment scenarios"

# Real-world deployment simulation
class DeploymentSimulator:
    """Simulate real-world deployment scenarios."""
    
    def __init__(self, model):
        self.model = model
    
    def simulate_mobile_inference(self, num_images=1000, target_fps=30):
        """Simulate mobile device inference."""
        device = 'cpu'  # Mobile devices typically use CPU
        model = self.model.to(device)
        model.eval()
        
        # Simulate various image sizes
        image_sizes = [(224, 224), (320, 320), (416, 416)]
        results = {}
        
        for size in image_sizes:
            input_tensor = torch.randn(1, 3, *size)
            
            # Measure inference time
            times = []
            with torch.no_grad():
                for _ in range(100):  # Warmup and measurement
                    start = time.time()
                    _ = model(input_tensor)
                    times.append(time.time() - start)
            
            avg_time = np.mean(times[10:])  # Skip first 10 for warmup
            fps = 1.0 / avg_time
            
            results[f'{size[0]}x{size[1]}'] = {
                'fps': fps,
                'meets_target': fps >= target_fps,
                'latency_ms': avg_time * 1000
            }
        
        return results
    
    def battery_consumption_estimate(self, inference_time_ms, device_type='mobile'):
        """Estimate battery consumption per inference."""
        
        # Rough estimates based on device type
        power_consumption = {
            'mobile': 2.0,  # Watts during inference
            'edge': 5.0,    # Edge devices
            'embedded': 0.5  # Low-power embedded
        }
        
        power_w = power_consumption.get(device_type, 2.0)
        energy_per_inference = (inference_time_ms / 1000) * power_w  # Joules
        
        # Convert to more meaningful metrics
        battery_capacity_wh = 15  # Typical smartphone battery ~15 Wh
        inferences_per_battery = (battery_capacity_wh * 3600) / energy_per_inference
        
        return {
            'energy_per_inference_j': energy_per_inference,
            'estimated_inferences_per_battery': int(inferences_per_battery),
            'power_consumption_w': power_w
        }
```

### Performance Metrics Dashboard

```{python}
#| label: performance-dashboard
#| caption: "Comprehensive performance analysis dashboard"
#| echo: false
#| warning: false

def create_performance_dashboard(benchmark_results):
    """Create a comprehensive performance dashboard."""
    
    fig = plt.figure(figsize=(16, 12))
    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
    
    models = list(benchmark_results.keys())
    
    # Extract metrics
    fps_data = [benchmark_results[model]['inference']['fps'] for model in models]
    memory_data = [benchmark_results[model]['memory'].get('peak_memory_mb', 0) for model in models]
    params_data = [benchmark_results[model]['parameters'] / 1e6 for model in models]  # In millions
    size_data = [benchmark_results[model]['model_size_mb'] for model in models]
    
    # 1. FPS Comparison (Bar Chart)
    ax1 = fig.add_subplot(gs[0, 0])
    bars1 = ax1.bar(models, fps_data, color='skyblue', alpha=0.8)
    ax1.set_title('Inference Speed (FPS)', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Frames Per Second')
    ax1.tick_params(axis='x', rotation=45)
    
    # Add value labels on bars
    for bar, fps in zip(bars1, fps_data):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                f'{fps:.1f}', ha='center', va='bottom', fontsize=9)
    
    # 2. Memory Usage (Bar Chart)
    ax2 = fig.add_subplot(gs[0, 1])
    bars2 = ax2.bar(models, memory_data, color='lightcoral', alpha=0.8)
    ax2.set_title('Peak Memory Usage', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Memory (MB)')
    ax2.tick_params(axis='x', rotation=45)
    
    for bar, mem in zip(bars2, memory_data):
        if mem > 0:
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, 
                    f'{mem:.0f}MB', ha='center', va='bottom', fontsize=9)
    
    # 3. Model Size vs Parameters (Scatter)
    ax3 = fig.add_subplot(gs[0, 2])
    scatter = ax3.scatter(params_data, size_data, s=100, alpha=0.7, c=fps_data, cmap='viridis')
    ax3.set_xlabel('Parameters (Millions)')
    ax3.set_ylabel('Model Size (MB)')
    ax3.set_title('Size vs Parameters', fontsize=12, fontweight='bold')
    
    # Add model labels
    for i, model in enumerate(models):
        ax3.annotate(model, (params_data[i], size_data[i]), 
                    xytext=(5, 5), textcoords='offset points', fontsize=8)
    
    # Add colorbar for FPS
    plt.colorbar(scatter, ax=ax3, label='FPS')
    
    # 4. Efficiency Score Radar Chart
    ax4 = fig.add_subplot(gs[1, :], projection='polar')
    
    # Normalize metrics for radar chart (0-1 scale)
    metrics = ['FPS', 'Memory Efficiency', 'Parameter Efficiency', 'Size Efficiency']
    
    # Calculate efficiency scores (higher is better)
    max_fps = max(fps_data)
    max_memory = max(memory_data) if max(memory_data) > 0 else 1
    max_params = max(params_data)
    max_size = max(size_data)
    
    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
    angles += angles[:1]  # Close the circle
    
    colors = plt.cm.Set3(np.linspace(0, 1, len(models)))
    
    for i, model in enumerate(models):
        # Normalize scores (invert memory, params, size as smaller is better)
        scores = [
            fps_data[i] / max_fps,  # FPS (higher is better)
            1 - (memory_data[i] / max_memory) if memory_data[i] > 0 else 1,  # Memory efficiency
            1 - (params_data[i] / max_params),  # Parameter efficiency
            1 - (size_data[i] / max_size)  # Size efficiency
        ]
        scores += scores[:1]  # Close the circle
        
        ax4.plot(angles, scores, 'o-', label=model, color=colors[i], linewidth=2)
        ax4.fill(angles, scores, alpha=0.25, color=colors[i])
    
    ax4.set_xticks(angles[:-1])
    ax4.set_xticklabels(metrics)
    ax4.set_ylim(0, 1)
    ax4.set_title('Model Efficiency Comparison', fontsize=14, fontweight='bold', pad=20)
    ax4.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
    
    # 5. Deployment Suitability Matrix
    ax5 = fig.add_subplot(gs[2, 0])
    
    # Create suitability matrix (example scoring)
    devices = ['Mobile', 'Edge', 'Cloud', 'Embedded']
    suitability_matrix = []
    
    for model in models:
        fps = fps_data[models.index(model)]
        params = params_data[models.index(model)]
        memory = memory_data[models.index(model)]
        
        # Simple scoring logic (0-5 scale)
        mobile_score = 5 if params < 2 and fps > 20 else (3 if params < 4 else 1)
        edge_score = 5 if params < 5 and fps > 15 else (3 if params < 10 else 1)
        cloud_score = 5  # All models suitable for cloud
        embedded_score = 5 if params < 1 and memory < 50 else (3 if params < 2 else 1)
        
        suitability_matrix.append([mobile_score, edge_score, cloud_score, embedded_score])
    
    im = ax5.imshow(suitability_matrix, cmap='RdYlGn', aspect='auto')
    ax5.set_xticks(range(len(devices)))
    ax5.set_xticklabels(devices)
    ax5.set_yticks(range(len(models)))
    ax5.set_yticklabels(models)
    ax5.set_title('Deployment Suitability', fontsize=12, fontweight='bold')
    
    # Add text annotations
    for i in range(len(models)):
        for j in range(len(devices)):
            ax5.text(j, i, f'{suitability_matrix[i][j]}', 
                    ha='center', va='center', fontweight='bold')
    
    plt.colorbar(im, ax=ax5, label='Suitability Score (1-5)')
    
    # 6. Latency Distribution
    ax6 = fig.add_subplot(gs[2, 1])
    latencies = [1000/fps for fps in fps_data]  # Convert FPS to latency in ms
    
    ax6.barh(models, latencies, color='lightgreen', alpha=0.8)
    ax6.set_xlabel('Latency (ms)')
    ax6.set_title('Inference Latency', fontsize=12, fontweight='bold')
    
    # Add target lines
    ax6.axvline(x=33.33, color='red', linestyle='--', alpha=0.7, label='30 FPS target')
    ax6.axvline(x=16.67, color='blue', linestyle='--', alpha=0.7, label='60 FPS target')
    ax6.legend()
    
    # 7. Resource Utilization
    ax7 = fig.add_subplot(gs[2, 2])
    
    # Create bubble chart: x=params, y=memory, size=model_size, color=fps
    scatter = ax7.scatter(params_data, memory_data, s=[s*20 for s in size_data], 
                         c=fps_data, alpha=0.6, cmap='plasma')
    
    ax7.set_xlabel('Parameters (M)')
    ax7.set_ylabel('Memory (MB)')
    ax7.set_title('Resource Utilization', fontsize=12, fontweight='bold')
    
    # Add legend for bubble sizes
    sizes = [min(size_data), np.mean(size_data), max(size_data)]
    size_labels = [f'{s:.1f}MB' for s in sizes]
    legend_sizes = [s*20 for s in sizes]
    
    for i, (size, label) in enumerate(zip(legend_sizes, size_labels)):
        ax7.scatter([], [], s=size, c='gray', alpha=0.6, label=label)
    
    ax7.legend(title='Model Size', loc='upper left')
    plt.colorbar(scatter, ax=ax7, label='FPS')
    
    plt.suptitle('MobileNet Performance Dashboard', fontsize=16, fontweight='bold')
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()
    
    # Generate recommendations
    generate_deployment_recommendations(benchmark_results)

def generate_deployment_recommendations(benchmark_results):
    """Generate deployment recommendations based on benchmark results."""
    
    print(f"\nðŸŽ¯ **Deployment Recommendations:**\n")
    
    models = list(benchmark_results.keys())
    
    # Find best models for different scenarios
    fps_scores = [(model, benchmark_results[model]['inference']['fps']) for model in models]
    memory_scores = [(model, benchmark_results[model]['memory'].get('peak_memory_mb', float('inf'))) for model in models]
    size_scores = [(model, benchmark_results[model]['model_size_mb']) for model in models]
    param_scores = [(model, benchmark_results[model]['parameters']) for model in models]
    
    # Sort by different criteria
    best_fps = max(fps_scores, key=lambda x: x[1])
    best_memory = min(memory_scores, key=lambda x: x[1] if x[1] > 0 else float('inf'))
    best_size = min(size_scores, key=lambda x: x[1])
    best_params = min(param_scores, key=lambda x: x[1])
    
    print(f"ðŸš€ **Fastest Inference:** {best_fps[0]} ({best_fps[1]:.1f} FPS)")
    print(f"   âœ… Best for: Real-time applications, video processing")
    print(f"   ðŸ“± Recommended: High-end mobile devices, edge servers\n")
    
    print(f"ðŸ’¾ **Most Memory Efficient:** {best_memory[0]} ({best_memory[1]:.0f} MB peak)")
    print(f"   âœ… Best for: Memory-constrained devices")
    print(f"   ðŸ“± Recommended: Budget smartphones, IoT devices\n")
    
    print(f"ðŸ“¦ **Smallest Model:** {best_size[0]} ({best_size[1]:.1f} MB)")
    print(f"   âœ… Best for: App size constraints, OTA updates")
    print(f"   ðŸ“± Recommended: Mobile apps with size limits\n")
    
    print(f"âš¡ **Fewest Parameters:** {best_params[0]} ({best_params[1]/1e6:.1f}M params)")
    print(f"   âœ… Best for: Ultra-low power devices")
    print(f"   ðŸ“± Recommended: Microcontrollers, embedded systems\n")
    
    # Overall recommendation
    efficiency_scores = {}
    for model in models:
        fps = benchmark_results[model]['inference']['fps']
        size = benchmark_results[model]['model_size_mb']
        params = benchmark_results[model]['parameters'] / 1e6
        
        # Combined efficiency score (higher is better)
        efficiency = fps / (size * params)
        efficiency_scores[model] = efficiency
    
    best_overall = max(efficiency_scores.items(), key=lambda x: x[1])
    
    print(f"ðŸ† **Best Overall Balance:** {best_overall[0]}")
    print(f"   ðŸ’¡ Efficiency Score: {best_overall[1]:.3f}")
    print(f"   âœ… Best for: General-purpose mobile AI applications")
    print(f"   ðŸ“± Recommended: Production deployments")

# Example usage and integration
def run_comprehensive_analysis():
    """Run comprehensive MobileNet analysis."""
    
    print("ðŸ”¬ **Running Comprehensive MobileNet Analysis...**\n")
    
    # Create models for comparison
    models = {
        'MobileNet_1.0': mobilenet_v1(width_mult=1.0),
        'MobileNet_0.75': mobilenet_v1(width_mult=0.75),
        'MobileNet_0.5': mobilenet_v1(width_mult=0.5),
        'MobileNet_0.25': mobilenet_v1(width_mult=0.25),
    }
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ðŸ–¥ï¸  Using device: {device}")
    
    # Run benchmarks
    benchmark = MobileNetBenchmark(models['MobileNet_1.0'], device)
    results = benchmark.compare_models(models)
    
    # Create dashboard
    create_performance_dashboard(results)
    
    return results

# Uncomment to run the analysis
results = run_comprehensive_analysis()
```



## Advanced Topics

### Neural Architecture Search (NAS) for MobileNet

```{python}
#| label: nas-mobilenet
#| caption: "Neural Architecture Search for MobileNet optimization"

class MobileNetSearchSpace:
    """Define search space for MobileNet architecture optimization."""
    
    def __init__(self):
        self.width_multipliers = [0.25, 0.35, 0.5, 0.75, 1.0, 1.4]
        self.depth_multipliers = [0.5, 0.75, 1.0, 1.25]
        self.kernel_sizes = [3, 5, 7]
        self.activation_functions = ['relu6', 'swish', 'hard_swish']
    
    def sample_architecture(self):
        """Sample a random architecture from search space."""
        import random
        
        return {
            'width_mult': random.choice(self.width_multipliers),
            'depth_mult': random.choice(self.depth_multipliers),
            'kernel_size': random.choice(self.kernel_sizes),
            'activation': random.choice(self.activation_functions)
        }
    
    def evaluate_architecture(self, arch_config, train_loader, val_loader):
        """Evaluate a sampled architecture."""
        
        # Create model with sampled configuration
        model = self.create_model_from_config(arch_config)
        
        # Quick training (few epochs for NAS efficiency)
        trainer = MobileNetTrainer(model)
        trainer.train(train_loader, val_loader, epochs=5)
        
        # Calculate efficiency metrics
        benchmark = MobileNetBenchmark(model)
        perf_stats = benchmark.benchmark_inference()
        
        # Return multi-objective score
        accuracy = trainer.history['val_acc'][-1]
        latency = perf_stats['mean_time']
        
        # Pareto efficiency score
        score = accuracy / (latency * 1000)  # Accuracy per ms
        
        return {
            'score': score,
            'accuracy': accuracy,
            'latency': latency,
            'config': arch_config
        }
    
    def create_model_from_config(self, config):
        """Create MobileNet model from configuration."""
        # Simplified - in practice would build full architecture
        return mobilenet_v1(
            width_mult=config['width_mult'],
            num_classes=1000
        )
```

### Knowledge Distillation for MobileNet {#sec-knowledge-distillation}

```{python}
#| label: knowledge-distillation
#| caption: "Knowledge distillation to improve MobileNet performance"

# Knowledge Distillation for MobileNet
class KnowledgeDistillation:
    """Knowledge distillation to improve MobileNet performance."""
    
    def __init__(self, teacher_model, student_model, temperature=4.0, alpha=0.3):
        self.teacher = teacher_model
        self.student = student_model
        self.temperature = temperature
        self.alpha = alpha  # Weight for distillation loss
        
        # Freeze teacher model
        for param in self.teacher.parameters():
            param.requires_grad = False
        self.teacher.eval()
    
    def distillation_loss(self, student_outputs, teacher_outputs, labels):
        """Calculate knowledge distillation loss."""
        
        # Soft targets from teacher
        teacher_probs = F.softmax(teacher_outputs / self.temperature, dim=1)
        student_log_probs = F.log_softmax(student_outputs / self.temperature, dim=1)
        
        # KL divergence loss
        distillation_loss = F.kl_div(
            student_log_probs, teacher_probs, reduction='batchmean'
        ) * (self.temperature ** 2)
        
        # Standard cross-entropy loss
        student_loss = F.cross_entropy(student_outputs, labels)
        
        # Combined loss
        total_loss = (
            self.alpha * distillation_loss + 
            (1 - self.alpha) * student_loss
        )
        
        return total_loss
    
    def train_with_distillation(self, train_loader, val_loader, epochs=10):
        """Train student model with knowledge distillation."""
        
        optimizer = optim.Adam(self.student.parameters(), lr=0.001)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
        
        for epoch in range(epochs):
            self.student.train()
            running_loss = 0.0
            
            for inputs, labels in train_loader:
                optimizer.zero_grad()
                
                # Get predictions from both models
                with torch.no_grad():
                    teacher_outputs = self.teacher(inputs)
                
                student_outputs = self.student(inputs)
                
                # Calculate distillation loss
                loss = self.distillation_loss(student_outputs, teacher_outputs, labels)
                
                loss.backward()
                optimizer.step()
                running_loss += loss.item()
            
            scheduler.step()
            
            # Validation
            val_acc = self.validate(val_loader)
            print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Val Acc: {val_acc:.2f}%')
    
    def validate(self, val_loader):
        """Validate student model."""
        self.student.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for inputs, labels in val_loader:
                outputs = self.student(inputs)
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        return 100 * correct / total
```

## Conclusion

This comprehensive guide has covered MobileNet from fundamental concepts to production deployment. The journey through depthwise separable convolutions, implementation details, optimization techniques, and real-world deployment strategies provides a complete foundation for building efficient mobile AI applications.

### Key Takeaways

::: {.callout-note}
## ðŸŽ¯ **Essential Insights**

**Architectural Innovation:**
- Depthwise separable convolutions reduce computation by 8-9Ã— with minimal accuracy loss
- Width multipliers provide flexible trade-offs between accuracy and efficiency
- The architecture scales gracefully across different hardware constraints

**Implementation Best Practices:**
- Always profile on target hardware before deployment
- Use appropriate data augmentation for robust training
- Consider knowledge distillation for improved student model performance
- Apply quantization and pruning strategically based on deployment requirements
:::

### Performance Summary

Based on our comprehensive analysis, here are the recommended MobileNet configurations:

| **Use Case** | **Configuration** | **Expected Performance** | **Deployment Target** |
|--------------|-------------------|--------------------------|------------------------|
| **Real-time Video** | MobileNet-V2 1.0Ã— | 30+ FPS, 72% accuracy | High-end mobile devices |
| **General Mobile AI** | MobileNet-V1 0.75Ã— | 45+ FPS, 68% accuracy | Mid-range smartphones |
| **Edge Computing** | MobileNet-V1 0.5Ã— | 60+ FPS, 64% accuracy | Edge servers, IoT hubs |
| **Embedded Systems** | MobileNet-V1 0.25Ã— | 80+ FPS, 51% accuracy | Microcontrollers, sensors |

### Deployment Recommendations

::: {.callout-tip}
## ðŸš€ **Production Deployment Checklist**

**Pre-deployment:**

- [ ] Benchmark on actual target hardware
- [ ] Validate accuracy on representative test data  
- [ ] Measure memory usage under realistic conditions
- [ ] Test battery consumption (for mobile devices)
- [ ] Verify model export/conversion pipeline

**Optimization Pipeline:**

- [ ] Apply appropriate quantization (dynamic/static)
- [ ] Consider structured pruning for further compression
- [ ] Export to platform-specific formats (ONNX, TFLite, CoreML)
- [ ] Implement efficient preprocessing pipelines
- [ ] Add monitoring and performance tracking

**Platform Integration:**

- [ ] Handle model loading and initialization efficiently
- [ ] Implement proper error handling and fallbacks
- [ ] Use background threads for inference
- [ ] Cache models and avoid repeated loading
- [ ] Plan for model updates and versioning
:::

### Common Pitfalls and Solutions

::: {.callout-warning}
## âš ï¸ **Avoid These Mistakes**

**Performance Issues:**

- **Problem**: Model runs slower on device than benchmarks suggest
- **Solution**: Always test with realistic input pipelines and preprocessing

**Memory Problems:**

- **Problem**: Out of memory errors during inference  
- **Solution**: Monitor peak memory usage, not just model size

**Accuracy Degradation:**

- **Problem**: Significant accuracy drop after optimization
- **Solution**: Use quantization-aware training and gradual pruning

**Integration Challenges:**

- **Problem**: Model format incompatibility with deployment platform
- **Solution**: Test export pipeline early and validate outputs
:::

### Future Directions

The field of efficient neural networks continues to evolve rapidly:

**Next-Generation Architectures:**

- **EfficientNet** and **EfficientNetV2**: Better scaling strategies with compound scaling
- **MobileViT**: Combining CNNs with Vision Transformers for mobile deployment
- **Once-for-All Networks**: Single networks supporting multiple deployment scenarios

**Advanced Optimization Techniques:**

- **Neural Architecture Search (NAS)**: Automated architecture optimization
- **Differentiable Architecture Search**: End-to-end learnable architectures  
- **Hardware-aware NAS**: Optimizing specifically for target hardware

**Deployment Innovations:**

- **Edge AI Accelerators**: Custom silicon for mobile AI (Apple Neural Engine, Google Edge TPU)
- **Federated Learning**: Training models across distributed mobile devices
- **Model Compression**: Advanced techniques beyond pruning and quantization

### Resources and Further Reading

::: {.callout-note}
## ðŸ“š **Additional Resources**

**Essential Papers:**

- [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)
- [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381)
- [Searching for MobileNetV3](https://arxiv.org/abs/1905.02244)

**Implementation Resources:**

- [PyTorch Mobile Documentation](https://pytorch.org/mobile/home/)
- [TensorFlow Lite Guide](https://www.tensorflow.org/lite)
- [ONNX Runtime Mobile](https://onnxruntime.ai/docs/tutorials/mobile/)

**Community and Support:**

- [PyTorch Forums - Mobile](https://discuss.pytorch.org/c/mobile/19)
- [TensorFlow Community](https://www.tensorflow.org/community)
- [Papers With Code - Mobile AI](https://paperswithcode.com/task/mobile-ai)
:::

### Final Thoughts {#sec-final-thoughts}

MobileNet represents a paradigm shift in how we approach deep learning for resource-constrained environments. The techniques and principles covered in this guide extend beyond MobileNet itself â€“ they form the foundation for understanding and implementing efficient AI systems across a wide range of applications.

As mobile and edge AI continues to grow, the ability to design, implement, and deploy efficient neural networks becomes increasingly valuable. Whether you're building the next generation of mobile apps, edge computing solutions, or embedded AI systems, the concepts and code in this guide provide a solid foundation for success.

::: {.callout-important}
## ðŸŽ¯ **Remember**
The best model is not necessarily the most accurate one, but the one that best serves your users within the constraints of your deployment environment. Always optimize for the complete user experience, not just benchmark metrics.
:::

## References

- Howard, A. G., et al. (2017). MobileNets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861.
- Sandler, M., et al. (2018). MobileNetV2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4510-4520).
- Howard, A., et al. (2019). Searching for mobilenetv3. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1314-1324).