<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Krishnatheja Vanka">
<meta name="dcterms.date" content="2025-06-27">

<title>Attention Mechanisms: Transformers vs Convolutional Neural Networks – Krishnatheja Vanka’s Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-7cf12f9d5c5caf5e13008aedb6606350.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Krishnatheja Vanka’s Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/theja-vanka"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Attention Mechanisms: Transformers vs Convolutional Neural Networks</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">research</div>
                <div class="quarto-category">intermediate</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Krishnatheja Vanka </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 27, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="attention-mechanisms-transformers-vs-convolutional-neural-networks" class="level1">
<h1>Attention Mechanisms: Transformers vs Convolutional Neural Networks</h1>
<p><img src="attention.png" class="img-fluid"></p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Attention mechanisms have revolutionized deep learning by enabling models to focus on relevant parts of the input data. While originally popularized in Transformers, attention has also been successfully integrated into Convolutional Neural Networks (CNNs). This article explores the fundamental differences, applications, and trade-offs between attention mechanisms in these two architectural paradigms.</p>
</section>
<section id="attention-in-transformers" class="level2">
<h2 class="anchored" data-anchor-id="attention-in-transformers">Attention in Transformers</h2>
<section id="core-concept" class="level3">
<h3 class="anchored" data-anchor-id="core-concept">Core Concept</h3>
<p>The attention mechanism in Transformers is based on the concept of <strong>self-attention</strong> or <strong>scaled dot-product attention</strong>. The fundamental idea is to allow each position in a sequence to attend to all positions in both the input and output sequences.</p>
</section>
<section id="mathematical-foundation" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-foundation">Mathematical Foundation</h3>
<p>The attention mechanism in Transformers computes attention weights using three key components:</p>
<ul>
<li><strong>Query (Q)</strong>: What information we’re looking for</li>
<li><strong>Key (K)</strong>: What information is available</li>
<li><strong>Value (V)</strong>: The actual information content</li>
</ul>
<p>The attention score is calculated as:</p>
<pre><code>Attention(Q, K, V) = softmax(QK^T / √d_k)V</code></pre>
<p>Where <code>d_k</code> is the dimension of the key vectors, used for scaling to prevent the softmax function from having extremely small gradients.</p>
</section>
<section id="multi-head-attention" class="level3">
<h3 class="anchored" data-anchor-id="multi-head-attention">Multi-Head Attention</h3>
<p>Transformers employ <strong>multi-head attention</strong>, which runs multiple attention mechanisms in parallel:</p>
<pre><code>MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O</code></pre>
<p>Where each <code>head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)</code></p>
<p>This allows the model to attend to information from different representation subspaces simultaneously.</p>
</section>
<section id="key-characteristics" class="level3">
<h3 class="anchored" data-anchor-id="key-characteristics">Key Characteristics</h3>
<ol type="1">
<li><strong>Global Context</strong>: Every token can attend to every other token in the sequence</li>
<li><strong>Position Agnostic</strong>: Inherently permutation-invariant (requires positional encoding)</li>
<li><strong>Parallel Processing</strong>: All attention computations can be performed simultaneously</li>
<li><strong>Quadratic Complexity</strong>: O(n²) memory and computational complexity with sequence length</li>
<li><strong>Dynamic Weights</strong>: Attention weights are computed dynamically based on input content</li>
</ol>
</section>
<section id="applications" class="level3">
<h3 class="anchored" data-anchor-id="applications">Applications</h3>
<ul>
<li>Natural Language Processing (BERT, GPT, T5)</li>
<li>Computer Vision (Vision Transformer - ViT)</li>
<li>Multimodal tasks (CLIP, DALL-E)</li>
<li>Time series analysis</li>
<li>Graph neural networks</li>
</ul>
</section>
</section>
<section id="attention-in-convolutional-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="attention-in-convolutional-neural-networks">Attention in Convolutional Neural Networks</h2>
<section id="core-concept-1" class="level3">
<h3 class="anchored" data-anchor-id="core-concept-1">Core Concept</h3>
<p>Attention in CNNs is typically implemented as <strong>channel attention</strong> or <strong>spatial attention</strong> mechanisms that help the network focus on important features or spatial locations. Unlike Transformers, CNN attention is usually applied to feature maps rather than sequence elements.</p>
</section>
<section id="types-of-cnn-attention" class="level3">
<h3 class="anchored" data-anchor-id="types-of-cnn-attention">Types of CNN Attention</h3>
<section id="channel-attention-se-net-eca-net" class="level4">
<h4 class="anchored" data-anchor-id="channel-attention-se-net-eca-net">1. Channel Attention (SE-Net, ECA-Net)</h4>
<p>Channel attention mechanisms adaptively recalibrate channel-wise feature responses by modeling interdependencies between channels.</p>
<p><strong>Squeeze-and-Excitation (SE) Block</strong>:</p>
<pre><code>1. Global Average Pooling: z_c = 1/(H×W) Σ Σ u_c(i,j)
2. Excitation: s = σ(W_2 δ(W_1 z))
3. Scale: x̃_c = s_c × u_c</code></pre>
</section>
<section id="spatial-attention-cbam-sam" class="level4">
<h4 class="anchored" data-anchor-id="spatial-attention-cbam-sam">2. Spatial Attention (CBAM, SAM)</h4>
<p>Spatial attention focuses on “where” informative parts are located in the feature map.</p>
<p><strong>Spatial Attention Module</strong>:</p>
<pre><code>1. Channel-wise statistics: F_avg, F_max
2. Convolution: M_s = σ(conv([F_avg; F_max]))
3. Element-wise multiplication: F' = M_s ⊗ F</code></pre>
</section>
<section id="self-attention-in-cnns-non-local-networks" class="level4">
<h4 class="anchored" data-anchor-id="self-attention-in-cnns-non-local-networks">3. Self-Attention in CNNs (Non-Local Networks)</h4>
<p>Some CNNs incorporate self-attention mechanisms similar to Transformers but adapted for spatial data:</p>
<pre><code>y_i = 1/C(x) Σ f(x_i, x_j) g(x_j)</code></pre>
<p>Where <code>f</code> computes affinity between positions <code>i</code> and <code>j</code>, and <code>g</code> computes representation of input at position <code>j</code>.</p>
</section>
</section>
<section id="key-characteristics-1" class="level3">
<h3 class="anchored" data-anchor-id="key-characteristics-1">Key Characteristics</h3>
<ol type="1">
<li><strong>Local and Global Context</strong>: Can focus on both local patterns and global dependencies</li>
<li><strong>Spatial Awareness</strong>: Naturally preserves spatial relationships in 2D/3D data</li>
<li><strong>Efficient Computation</strong>: Generally more computationally efficient than Transformer attention</li>
<li><strong>Feature Enhancement</strong>: Primarily used to enhance existing convolutional features</li>
<li><strong>Lightweight</strong>: Usually adds minimal parameters to the base model</li>
</ol>
</section>
<section id="applications-1" class="level3">
<h3 class="anchored" data-anchor-id="applications-1">Applications</h3>
<ul>
<li>Image classification (ResNet + SE, EfficientNet)</li>
<li>Object detection (Feature Pyramid Networks with attention)</li>
<li>Semantic segmentation (attention-based skip connections)</li>
<li>Medical image analysis</li>
<li>Video understanding</li>
</ul>
</section>
</section>
<section id="comparative-analysis" class="level2">
<h2 class="anchored" data-anchor-id="comparative-analysis">Comparative Analysis</h2>
<section id="computational-complexity" class="level3">
<h3 class="anchored" data-anchor-id="computational-complexity">Computational Complexity</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 47%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>Transformer Attention</th>
<th>CNN Attention</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Time Complexity</strong></td>
<td>O(n²d) for sequence length n</td>
<td>O(HWd) for spatial dimensions H×W</td>
</tr>
<tr class="even">
<td><strong>Space Complexity</strong></td>
<td>O(n²) attention matrix</td>
<td>O(HW) or O(d) depending on type</td>
</tr>
<tr class="odd">
<td><strong>Scalability</strong></td>
<td>Challenging for long sequences</td>
<td>Scales well with image resolution</td>
</tr>
</tbody>
</table>
</section>
<section id="architectural-differences" class="level3">
<h3 class="anchored" data-anchor-id="architectural-differences">Architectural Differences</h3>
<section id="information-flow" class="level4">
<h4 class="anchored" data-anchor-id="information-flow">Information Flow</h4>
<ul>
<li><strong>Transformers</strong>: Global information exchange from the start</li>
<li><strong>CNNs</strong>: Hierarchical feature learning with attention refinement</li>
</ul>
</section>
<section id="inductive-biases" class="level4">
<h4 class="anchored" data-anchor-id="inductive-biases">Inductive Biases</h4>
<ul>
<li><strong>Transformers</strong>: Minimal inductive bias, relies on data and scale</li>
<li><strong>CNNs</strong>: Strong spatial inductive bias through convolution operations</li>
</ul>
</section>
<section id="interpretability" class="level4">
<h4 class="anchored" data-anchor-id="interpretability">Interpretability</h4>
<ul>
<li><strong>Transformers</strong>: Attention weights provide interpretable focus patterns</li>
<li><strong>CNNs</strong>: Channel/spatial attention maps show feature importance</li>
</ul>
</section>
</section>
<section id="performance-characteristics" class="level3">
<h3 class="anchored" data-anchor-id="performance-characteristics">Performance Characteristics</h3>
<section id="data-efficiency" class="level4">
<h4 class="anchored" data-anchor-id="data-efficiency">Data Efficiency</h4>
<ul>
<li><strong>Transformers</strong>: Require large datasets to learn effectively</li>
<li><strong>CNNs</strong>: More data-efficient due to built-in inductive biases</li>
</ul>
</section>
<section id="generalization" class="level4">
<h4 class="anchored" data-anchor-id="generalization">Generalization</h4>
<ul>
<li><strong>Transformers</strong>: Excel at capturing long-range dependencies</li>
<li><strong>CNNs</strong>: Better at learning local patterns and spatial hierarchies</li>
</ul>
</section>
<section id="training-stability" class="level4">
<h4 class="anchored" data-anchor-id="training-stability">Training Stability</h4>
<ul>
<li><strong>Transformers</strong>: Can be unstable, require careful initialization and learning rates</li>
<li><strong>CNNs</strong>: Generally more stable training dynamics</li>
</ul>
</section>
</section>
</section>
<section id="hybrid-approaches" class="level2">
<h2 class="anchored" data-anchor-id="hybrid-approaches">Hybrid Approaches</h2>
<p>Recent research has explored combining both attention mechanisms:</p>
<section id="convnets-with-transformer-blocks" class="level3">
<h3 class="anchored" data-anchor-id="convnets-with-transformer-blocks">ConvNets with Transformer Blocks</h3>
<ul>
<li><strong>ConvNeXt</strong>: Modernized CNNs inspired by Transformer design principles</li>
<li><strong>CoAtNet</strong>: Combines convolution and self-attention in a unified architecture</li>
</ul>
</section>
<section id="vision-transformers-with-convolutional-elements" class="level3">
<h3 class="anchored" data-anchor-id="vision-transformers-with-convolutional-elements">Vision Transformers with Convolutional Elements</h3>
<ul>
<li><strong>CvT</strong>: Convolutional Vision Transformer with convolutional token embedding</li>
<li><strong>CeiT</strong>: Incorporating convolutional inductive bias into ViTs</li>
</ul>
</section>
<section id="advantages-of-hybrid-models" class="level3">
<h3 class="anchored" data-anchor-id="advantages-of-hybrid-models">Advantages of Hybrid Models</h3>
<ol type="1">
<li><strong>Best of Both Worlds</strong>: Local pattern recognition + global context modeling</li>
<li><strong>Improved Efficiency</strong>: Reduced computational complexity while maintaining performance</li>
<li><strong>Better Inductive Bias</strong>: Combines spatial awareness with flexible attention</li>
</ol>
</section>
</section>
<section id="use-case-recommendations" class="level2">
<h2 class="anchored" data-anchor-id="use-case-recommendations">Use Case Recommendations</h2>
<section id="choose-transformer-attention-when" class="level3">
<h3 class="anchored" data-anchor-id="choose-transformer-attention-when">Choose Transformer Attention When:</h3>
<ul>
<li>Working with sequential data (NLP, time series)</li>
<li>Need to model long-range dependencies</li>
<li>Have access to large datasets</li>
<li>Computational resources are abundant</li>
<li>Interpretability of attention patterns is important</li>
</ul>
</section>
<section id="choose-cnn-attention-when" class="level3">
<h3 class="anchored" data-anchor-id="choose-cnn-attention-when">Choose CNN Attention When:</h3>
<ul>
<li>Working with spatial data (images, videos)</li>
<li>Limited computational resources</li>
<li>Smaller datasets available</li>
<li>Need faster inference times</li>
<li>Spatial relationships are crucial for the task</li>
</ul>
</section>
<section id="consider-hybrid-approaches-when" class="level3">
<h3 class="anchored" data-anchor-id="consider-hybrid-approaches-when">Consider Hybrid Approaches When:</h3>
<ul>
<li>Working with complex visual tasks requiring both local and global understanding</li>
<li>Need to balance performance and efficiency</li>
<li>Have moderate computational resources</li>
<li>Want to leverage benefits of both paradigms</li>
</ul>
</section>
</section>
<section id="future-directions" class="level2">
<h2 class="anchored" data-anchor-id="future-directions">Future Directions</h2>
<p>The field continues to evolve with several promising directions:</p>
<ol type="1">
<li><strong>Efficient Attention</strong>: Linear attention mechanisms for Transformers</li>
<li><strong>Dynamic Attention</strong>: Adaptive attention mechanisms that adjust based on input complexity</li>
<li><strong>Cross-Modal Attention</strong>: Attention mechanisms that work across different data modalities</li>
<li><strong>Learnable Attention Patterns</strong>: Meta-learning approaches for attention mechanism design</li>
<li><strong>Hardware-Optimized Attention</strong>: Attention mechanisms designed for specific hardware accelerators</li>
</ol>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Both Transformer and CNN attention mechanisms serve distinct but complementary purposes in modern deep learning. Transformer attention excels at modeling global dependencies and complex relationships in sequential data, while CNN attention provides efficient feature enhancement for spatial data. The choice between them depends on specific use case requirements, available resources, and the nature of the data being processed.</p>
<p>The ongoing convergence of these approaches through hybrid architectures suggests that the future of attention mechanisms lies not in choosing one over the other, but in thoughtfully combining their strengths to create more powerful and efficient models. As the field continues to advance, we can expect to see more sophisticated attention mechanisms that bridge the gap between these two paradigms while addressing their respective limitations.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/theja-vanka\.github\.io\/blogs\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>