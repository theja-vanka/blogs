<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Krishnatheja Vanka">
<meta name="dcterms.date" content="2025-05-10">

<title>Vision Transformers: Revolutionizing Computer Vision – Krishnatheja Vanka’s Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-7cf12f9d5c5caf5e13008aedb6606350.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Krishnatheja Vanka’s Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/theja-vanka"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Vision Transformers: Revolutionizing Computer Vision</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">research</div>
                <div class="quarto-category">advanced</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Krishnatheja Vanka </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 10, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="vision-transformers-revolutionizing-computer-vision" class="level1">
<h1>Vision Transformers: Revolutionizing Computer Vision</h1>
<p><img src="vit.png" class="img-fluid"></p>
<section id="the-rise-of-vision-transformers" class="level2">
<h2 class="anchored" data-anchor-id="the-rise-of-vision-transformers">The Rise of Vision Transformers</h2>
<p>For nearly a decade, Convolutional Neural Networks (CNNs) dominated the field of computer vision. Then, in 2020, a groundbreaking paper titled “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale” introduced Vision Transformers (ViT), dramatically changing the landscape. This visual guide breaks down how Vision Transformers work, their advantages, and why they’ve become so important in modern AI.</p>
</section>
<section id="from-nlp-to-computer-vision-the-transformer-journey" class="level2">
<h2 class="anchored" data-anchor-id="from-nlp-to-computer-vision-the-transformer-journey">From NLP to Computer Vision: The Transformer Journey</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://api.placeholder.com/800/400.png" class="img-fluid figure-img"></p>
<figcaption>NLP to CV Transformation</figcaption>
</figure>
</div>
<p>Transformers first revolutionized Natural Language Processing (NLP) through models like BERT and GPT. The key innovation was replacing recurrent neural networks with <strong>attention mechanisms</strong> that could process sequences in parallel and capture long-range dependencies. Vision Transformers apply this same powerful architecture to images.</p>
</section>
<section id="how-vision-transformers-work-a-visual-breakdown" class="level2">
<h2 class="anchored" data-anchor-id="how-vision-transformers-work-a-visual-breakdown">How Vision Transformers Work: A Visual Breakdown</h2>
<section id="step-1-image-patching" class="level3">
<h3 class="anchored" data-anchor-id="step-1-image-patching">Step 1: Image Patching</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://api.placeholder.com/800/400.png" class="img-fluid figure-img"></p>
<figcaption>Image Patching Process</figcaption>
</figure>
</div>
<p>Unlike CNNs that process images through convolutional filters, Vision Transformers first divide an image into fixed-size patches (typically 16×16 pixels). These patches become the “tokens” of our sequence – similar to words in NLP transformers.</p>
</section>
<section id="step-2-linear-embedding" class="level3">
<h3 class="anchored" data-anchor-id="step-2-linear-embedding">Step 2: Linear Embedding</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://api.placeholder.com/800/400.png" class="img-fluid figure-img"></p>
<figcaption>Linear Embedding</figcaption>
</figure>
</div>
<p>Each image patch is flattened into a 1D vector and linearly projected to an embedding dimension (typically 768 or 1024). This transforms our patches into a sequence of embedded tokens.</p>
</section>
<section id="step-3-position-embeddings" class="level3">
<h3 class="anchored" data-anchor-id="step-3-position-embeddings">Step 3: Position Embeddings</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://api.placeholder.com/800/400.png" class="img-fluid figure-img"></p>
<figcaption>Position Embeddings</figcaption>
</figure>
</div>
<p>Since transformers don’t inherently understand the spatial relationships between patches, we add <strong>position embeddings</strong> to each token. These positional encodings help the model understand the 2D structure of the image.</p>
</section>
<section id="step-4-class-token-and-transformer-encoder" class="level3">
<h3 class="anchored" data-anchor-id="step-4-class-token-and-transformer-encoder">Step 4: Class Token and Transformer Encoder</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://api.placeholder.com/800/400.png" class="img-fluid figure-img"></p>
<figcaption>Class Token and Transformer</figcaption>
</figure>
</div>
<p>A special learnable <strong>[CLS] token</strong> is prepended to the sequence. This token will eventually contain the representation used for classification. The entire sequence is then processed through multiple transformer encoder blocks.</p>
</section>
<section id="step-5-multi-head-self-attention" class="level3">
<h3 class="anchored" data-anchor-id="step-5-multi-head-self-attention">Step 5: Multi-Head Self-Attention</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://api.placeholder.com/800/400.png" class="img-fluid figure-img"></p>
<figcaption>Multi-Head Self-Attention</figcaption>
</figure>
</div>
<p>The heart of the transformer is the <strong>multi-head self-attention mechanism</strong>. Each patch can attend to all other patches, allowing the model to capture global relationships across the entire image. This is fundamentally different from CNNs that build features hierarchically.</p>
<p>In the visualization above: - Lighter colors show stronger attention - Notice how the model can focus on distant but related parts of the image simultaneously</p>
</section>
<section id="step-6-mlp-head-for-classification" class="level3">
<h3 class="anchored" data-anchor-id="step-6-mlp-head-for-classification">Step 6: MLP Head for Classification</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://api.placeholder.com/800/400.png" class="img-fluid figure-img"></p>
<figcaption>MLP Classification Head</figcaption>
</figure>
</div>
<p>For image classification, the final representation of the [CLS] token is passed through a Multi-Layer Perceptron (MLP) head to produce class predictions.</p>
</section>
</section>
<section id="the-attention-mechanism-visualized" class="level2">
<h2 class="anchored" data-anchor-id="the-attention-mechanism-visualized">The Attention Mechanism Visualized</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://api.placeholder.com/800/400.png" class="img-fluid figure-img"></p>
<figcaption>Attention Visualization</figcaption>
</figure>
</div>
<p>The self-attention mechanism allows each patch to “look at” all other patches when forming its representation. The heat map above shows which parts of the image a particular patch is paying attention to. Notice how semantically related regions have stronger connections.</p>
</section>
<section id="vit-architecture-diagram" class="level2">
<h2 class="anchored" data-anchor-id="vit-architecture-diagram">ViT Architecture Diagram</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://api.placeholder.com/800/600.png" class="img-fluid figure-img"></p>
<figcaption>Complete ViT Architecture</figcaption>
</figure>
</div>
<p>The complete Vision Transformer architecture combines all these elements into a powerful model capable of state-of-the-art performance on computer vision tasks.</p>
</section>
<section id="comparing-cnn-vs.-vit-feature-maps" class="level2">
<h2 class="anchored" data-anchor-id="comparing-cnn-vs.-vit-feature-maps">Comparing CNN vs.&nbsp;ViT Feature Maps</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://api.placeholder.com/800/400.png" class="img-fluid figure-img"></p>
<figcaption>CNN vs ViT Feature Maps</figcaption>
</figure>
</div>
<p>The fundamental difference between CNNs and Vision Transformers is how they process information: - <strong>CNNs</strong> build features hierarchically, starting with local patterns and gradually integrating them into more complex structures - <strong>ViTs</strong> process the entire image simultaneously, allowing each part to interact with every other part from the earliest layers</p>
</section>
<section id="the-scale-effect-why-data-matters" class="level2">
<h2 class="anchored" data-anchor-id="the-scale-effect-why-data-matters">The Scale Effect: Why Data Matters</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://api.placeholder.com/800/400.png" class="img-fluid figure-img"></p>
<figcaption>Scale Effect</figcaption>
</figure>
</div>
<p>A key finding from the original ViT paper was that transformers need more data than CNNs to perform well. The graph shows how ViT performance scales with dataset size: - With small datasets, CNNs outperform ViTs - With large datasets, ViTs surpass CNNs</p>
<p>This is because transformers have fewer built-in inductive biases about images, making them more flexible but also more data-hungry.</p>
</section>
<section id="attention-maps-across-layers" class="level2">
<h2 class="anchored" data-anchor-id="attention-maps-across-layers">Attention Maps Across Layers</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://api.placeholder.com/800/400.png" class="img-fluid figure-img"></p>
<figcaption>Attention Maps Across Layers</figcaption>
</figure>
</div>
<p>As we move deeper into the transformer, the attention patterns evolve: - <strong>Early layers</strong>: Attention tends to be more local or focus on simple patterns - <strong>Middle layers</strong>: Attention starts to connect related regions across the image - <strong>Later layers</strong>: Attention maps become more specialized to the task</p>
</section>
<section id="vit-variants-and-improvements" class="level2">
<h2 class="anchored" data-anchor-id="vit-variants-and-improvements">ViT Variants and Improvements</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://api.placeholder.com/800/400.png" class="img-fluid figure-img"></p>
<figcaption>ViT Variants</figcaption>
</figure>
</div>
<p>Since the original ViT, numerous improvements have emerged:</p>
<section id="hierarchical-vits" class="level3">
<h3 class="anchored" data-anchor-id="hierarchical-vits">Hierarchical ViTs</h3>
<p>Integrate multi-scale processing similar to CNNs</p>
</section>
<section id="swin-transformer" class="level3">
<h3 class="anchored" data-anchor-id="swin-transformer">Swin Transformer</h3>
<p>Uses shifted windows to efficiently capture both local and global information</p>
</section>
<section id="deit-data-efficient-image-transformer" class="level3">
<h3 class="anchored" data-anchor-id="deit-data-efficient-image-transformer">DeiT (Data-efficient Image Transformer)</h3>
<p>Improves training efficiency through distillation from CNN teachers</p>
</section>
<section id="mobile-vit" class="level3">
<h3 class="anchored" data-anchor-id="mobile-vit">Mobile-ViT</h3>
<p>Optimized for mobile devices with reduced computation</p>
</section>
</section>
<section id="how-to-choose-between-cnn-and-vit" class="level2">
<h2 class="anchored" data-anchor-id="how-to-choose-between-cnn-and-vit">How to Choose Between CNN and ViT?</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://api.placeholder.com/800/400.png" class="img-fluid figure-img"></p>
<figcaption>CNN vs ViT Decision Chart</figcaption>
</figure>
</div>
<p>When should you use a Vision Transformer versus a CNN? - <strong>Use CNN when</strong>: You have limited data, need interpreTable features, or require real-time performance on limited hardware - <strong>Use ViT when</strong>: You have large datasets, need to capture long-range dependencies, or want state-of-the-art accuracy</p>
</section>
<section id="vit-code-visualization" class="level2">
<h2 class="anchored" data-anchor-id="vit-code-visualization">ViT Code Visualization</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://api.placeholder.com/800/400.png" class="img-fluid figure-img"></p>
<figcaption>ViT Code Preview</figcaption>
</figure>
</div>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VisionTransformer(nn.Module):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim):</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate number of patches</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        num_patches <span class="op">=</span> (image_size <span class="op">//</span> patch_size) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Patch embedding</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patch_embed <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, dim, kernel_size<span class="op">=</span>patch_size, stride<span class="op">=</span>patch_size)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Class token and position embeddings</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cls_token <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, <span class="dv">1</span>, dim))</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embed <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, num_patches <span class="op">+</span> <span class="dv">1</span>, dim))</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Transformer encoder</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transformer <span class="op">=</span> Transformer(dim, depth, heads, mlp_dim)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Classification head</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp_head <span class="op">=</span> nn.Linear(dim, num_classes)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract patches</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.patch_embed(x)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.flatten(<span class="dv">2</span>).transpose(<span class="dv">1</span>, <span class="dv">2</span>)  <span class="co"># [B, num_patches, dim]</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add class token</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        cls_token <span class="op">=</span> <span class="va">self</span>.cls_token.expand(x.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.cat((cls_token, x), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add position embeddings</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pos_embed</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply transformer</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.transformer(x)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Classification from CLS token</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x[:, <span class="dv">0</span>]</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.mlp_head(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="applications-beyond-classification" class="level2">
<h2 class="anchored" data-anchor-id="applications-beyond-classification">Applications Beyond Classification</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://api.placeholder.com/800/400.png" class="img-fluid figure-img"></p>
<figcaption>ViT Applications</figcaption>
</figure>
</div>
<p>Vision Transformers have expanded far beyond image classification:</p>
<section id="object-detection" class="level3">
<h3 class="anchored" data-anchor-id="object-detection">Object Detection</h3>
<p>Models like DETR use transformers to eliminate the need for hand-designed components</p>
</section>
<section id="image-segmentation" class="level3">
<h3 class="anchored" data-anchor-id="image-segmentation">Image Segmentation</h3>
<p>ViT-based models achieve pixel-perfect segmentation</p>
</section>
<section id="image-generation" class="level3">
<h3 class="anchored" data-anchor-id="image-generation">Image Generation</h3>
<p>Transformers power cutting-edge generative models like DALL-E and Stable Diffusion</p>
</section>
<section id="medical-imaging" class="level3">
<h3 class="anchored" data-anchor-id="medical-imaging">Medical Imaging</h3>
<p>Specialized ViTs detect anomalies in X-rays and MRIs with doctor-level accuracy</p>
</section>
</section>
<section id="the-future-of-vision-transformers" class="level2">
<h2 class="anchored" data-anchor-id="the-future-of-vision-transformers">The Future of Vision Transformers</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://api.placeholder.com/800/400.png" class="img-fluid figure-img"></p>
<figcaption>Future Directions</figcaption>
</figure>
</div>
<p>Where is the field heading?</p>
<section id="hybrid-architectures" class="level3">
<h3 class="anchored" data-anchor-id="hybrid-architectures">Hybrid Architectures</h3>
<p>Combining the strengths of both CNNs and transformers</p>
</section>
<section id="multimodal-models" class="level3">
<h3 class="anchored" data-anchor-id="multimodal-models">Multimodal Models</h3>
<p>Processing images alongside text, audio, or video</p>
</section>
<section id="foundation-models" class="level3">
<h3 class="anchored" data-anchor-id="foundation-models">Foundation Models</h3>
<p>Massive pre-trained ViTs that can be fine-tuned for many tasks</p>
</section>
<section id="efficient-attention" class="level3">
<h3 class="anchored" data-anchor-id="efficient-attention">Efficient Attention</h3>
<p>New mechanisms that reduce the quadratic complexity of self-attention</p>
</section>
</section>
<section id="getting-started-with-vision-transformers" class="level2">
<h2 class="anchored" data-anchor-id="getting-started-with-vision-transformers">Getting Started with Vision Transformers</h2>
<p>Ready to implement your own Vision Transformer? Here are the key steps:</p>
<ol type="1">
<li><strong>Prepare your data</strong>: Ensure proper image preprocessing</li>
<li><strong>Choose a variant</strong>: Select the appropriate ViT architecture for your task</li>
<li><strong>Training strategy</strong>: Consider pre-training on large datasets followed by fine-tuning</li>
<li><strong>Visualization tools</strong>: Use attention map visualizations to understand model behavior</li>
</ol>
</section>
<section id="conclusion-the-transformer-revolution-continues" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-the-transformer-revolution-continues">Conclusion: The Transformer Revolution Continues</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://api.placeholder.com/800/400.png" class="img-fluid figure-img"></p>
<figcaption>Conclusion</figcaption>
</figure>
</div>
<p>Vision Transformers have permanently changed how we approach computer vision. By bringing the power of self-attention to images, they’ve enabled models to capture complex relationships that were previously difficult to model. As hardware continues to improve and training data grows, we can expect Vision Transformers to continue leading the way in computer vision innovation.</p>
<hr>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ol type="1">
<li>Dosovitskiy, A., et al.&nbsp;(2020). “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.”</li>
<li>Touvron, H., et al.&nbsp;(2021). “Training data-efficient image transformers &amp; distillation through attention.”</li>
<li>Liu, Z., et al.&nbsp;(2021). “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.”</li>
<li>Carion, N., et al.&nbsp;(2020). “End-to-End Object Detection with Transformers.”</li>
<li>Ranftl, R., et al.&nbsp;(2021). “Vision Transformers for Dense Prediction.”</li>
</ol>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/theja-vanka\.github\.io\/blogs\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>