[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Hi 👋, I’m Krishnatheja Vanka\n\nMachine Learning Engineer (Applied Computer Vision)"
  },
  {
    "objectID": "posts/dinov2/index.html",
    "href": "posts/dinov2/index.html",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "DINOv2 is a state-of-the-art self-supervised vision model developed by Meta AI Research that builds upon the original DINO (Self-Distillation with No Labels) framework. This guide will walk you through understanding, implementing, and leveraging DINOv2 for various computer vision tasks.\n\n\n\nIntroduction to DINOv2\nInstallation and Setup\nLoading Pre-trained Models\nFeature Extraction\nFine-tuning for Downstream Tasks\nImage Classification Example\nSemantic Segmentation Example\nObject Detection Example\nAdvanced Usage and Customization\nPerformance Benchmarks\nTroubleshooting\n\n\n\n\nDINOv2 is a self-supervised learning method for vision that produces high-quality visual features without requiring labeled data. It extends the original DINO architecture with several improvements:\n\nTraining on a large and diverse dataset of images\nEnhanced teacher-student architecture\nImproved augmentation strategy\nMulti-scale feature learning\nSupport for various Vision Transformer (ViT) backbones\n\nThe result is a versatile foundation model that can be adapted to numerous vision tasks with minimal fine-tuning.\n\n\n\nTo use DINOv2, you’ll need to install the official implementation:\n# Install PyTorch first if not already installed\n# pip install torch torchvision\n\n# Install DINOv2\npip install git+https://github.com/facebookresearch/dinov2\nAlternatively, you can clone the repository and install it locally:\ngit clone https://github.com/facebookresearch/dinov2.git\ncd dinov2\npip install -e .\n\n\nDINOv2 requires:\n\nPython 3.8+\nPyTorch 1.12+\ntorchvision\nCUDA (for GPU acceleration)\n\n\n\n\n\nDINOv2 provides several pre-trained models with different sizes and capabilities:\nimport torch\nfrom dinov2.models import build_model_from_cfg\nfrom dinov2.configs import get_config\n\n# Available model sizes: 'small', 'base', 'large', 'giant'\nmodel_size = 'base'\ncfg = get_config(f\"dinov2_{model_size}\")\nmodel = build_model_from_cfg(cfg)\n\n# Load pre-trained weights\ncheckpoint_path = f\"dinov2_{model_size}_pretrain.pth\"  # Download this from Meta AI's repository\ncheckpoint = torch.load(checkpoint_path, map_location=\"cpu\")\nmodel.load_state_dict(checkpoint[\"model\"])\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nmodel.eval()  # Set to evaluation mode\nYou can also use the Hugging Face Transformers library for an easier integration:\nfrom transformers import AutoImageProcessor, AutoModel\n\n# Available model sizes: 'small', 'base', 'large', 'giant'\nmodel_name = \"facebook/dinov2-base\"\nprocessor = AutoImageProcessor.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n\n\nOne of DINOv2’s key strengths is its ability to extract powerful visual features:\nimport torch\nfrom PIL import Image\nimport torchvision.transforms as T\nfrom transformers import AutoImageProcessor, AutoModel\n\n# Load model\nmodel_name = \"facebook/dinov2-base\"\nprocessor = AutoImageProcessor.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\nmodel.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\nmodel.eval()\n\n# Load and preprocess image\nimage = Image.open(\"path/to/your/image.jpg\").convert(\"RGB\")\ninputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n\n# Extract features\nwith torch.no_grad():\n    outputs = model(**inputs)\n    \n# Get CLS token features (useful for classification tasks)\ncls_features = outputs.last_hidden_state[:, 0]\n\n# Get patch features (useful for dense prediction tasks like segmentation)\npatch_features = outputs.last_hidden_state[:, 1:]\n\nprint(f\"CLS features shape: {cls_features.shape}\")\nprint(f\"Patch features shape: {patch_features.shape}\")\n\n\n\nDINOv2 can be fine-tuned for specific vision tasks:\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModel\n\n# Load pre-trained DINOv2 model\nbackbone = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n\n# Create a custom classification head\nclass ClassificationHead(nn.Module):\n    def __init__(self, backbone, num_classes=1000):\n        super().__init__()\n        self.backbone = backbone\n        self.classifier = nn.Linear(backbone.config.hidden_size, num_classes)\n        \n    def forward(self, x):\n        outputs = self.backbone(x)\n        cls_token = outputs.last_hidden_state[:, 0]\n        return self.classifier(cls_token)\n\n# Create the complete model\nmodel = ClassificationHead(backbone, num_classes=100)  # For 100 classes\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop example\ndef train_one_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    \n    for batch in dataloader:\n        images = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(dataloader)\n\n\n\nHere’s a complete example for image classification using DINOv2:\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as transforms\nfrom transformers import AutoImageProcessor, AutoModel\n\n# Define the dataset and transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Load your dataset (adjust the path)\ntrain_dataset = ImageFolder(root=\"path/to/train\", transform=transform)\nval_dataset = ImageFolder(root=\"path/to/val\", transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n\n# Create the model\nclass DINOv2Classifier(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.dinov2 = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n        self.classifier = nn.Linear(768, num_classes)  # 768 is the hidden size for base model\n        \n    def forward(self, x):\n        # Extract features\n        with torch.set_grad_enabled(self.training):\n            features = self.dinov2(x).last_hidden_state[:, 0]  # Get CLS token\n        \n        # Classify\n        logits = self.classifier(features)\n        return logits\n\n# Initialize model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DINOv2Classifier(num_classes=len(train_dataset.classes))\nmodel = model.to(device)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW([\n    {'params': model.classifier.parameters(), 'lr': 1e-3},\n    {'params': model.dinov2.parameters(), 'lr': 1e-5}\n])\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    # Training\n    model.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    \n    for inputs, targets in train_loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n    \n    train_accuracy = 100 * correct / total\n    \n    # Validation\n    model.eval()\n    val_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            \n            val_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    \n    val_accuracy = 100 * correct / total\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    print(f\"Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_accuracy:.2f}%\")\n    print(f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_accuracy:.2f}%\")\n\n\n\nDINOv2 is particularly powerful for segmentation tasks:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModel\n\nclass DINOv2Segmenter(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        # Load DINOv2 backbone\n        self.backbone = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n        \n        # Define segmentation head\n        hidden_dim = self.backbone.config.hidden_size\n        self.segmentation_head = nn.Sequential(\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(hidden_dim, num_classes, kernel_size=1)\n        )\n        \n        # Image size and patch size for reshaping\n        self.image_size = 224\n        self.patch_size = 14  # For ViT-Base\n        self.num_patches = (self.image_size // self.patch_size) ** 2\n        \n    def forward(self, x):\n        # Get patch features\n        outputs = self.backbone(x)\n        patch_features = outputs.last_hidden_state[:, 1:]  # Remove CLS token\n        \n        # Reshape to 2D spatial layout\n        B = x.shape[0]\n        H = W = self.image_size // self.patch_size\n        patch_features = patch_features.reshape(B, H, W, -1).permute(0, 3, 1, 2)\n        \n        # Apply segmentation head\n        segmentation_logits = self.segmentation_head(patch_features)\n        \n        # Upsample to original image size\n        segmentation_logits = F.interpolate(\n            segmentation_logits, \n            size=(self.image_size, self.image_size), \n            mode='bilinear', \n            align_corners=False\n        )\n        \n        return segmentation_logits\n\n# Create model and move to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DINOv2Segmenter(num_classes=21)  # 21 classes for Pascal VOC\nmodel = model.to(device)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW([\n    {'params': model.segmentation_head.parameters(), 'lr': 1e-3},\n    {'params': model.backbone.parameters(), 'lr': 1e-5}\n])\ncriterion = nn.CrossEntropyLoss(ignore_index=255)  # 255 is typically the ignore index\n\n# Rest of the training code would be similar to the classification example\n\n\n\nHere’s how to use DINOv2 features for object detection with a simple detection head:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModel\n\nclass DINOv2Detector(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        # Load DINOv2 backbone\n        self.backbone = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n        hidden_dim = self.backbone.config.hidden_size\n        \n        # Detection heads\n        self.box_predictor = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, 4)  # (x1, y1, x2, y2)\n        )\n        \n        self.class_predictor = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, num_classes + 1)  # +1 for background\n        )\n        \n        # Image size and patch size for feature map creation\n        self.image_size = 224\n        self.patch_size = 14  # For ViT-Base\n        \n    def forward(self, x):\n        # Get features\n        outputs = self.backbone(x)\n        features = outputs.last_hidden_state[:, 1:]  # Remove CLS token\n        \n        # Reshape to 2D spatial layout\n        B = x.shape[0]\n        H = W = self.image_size // self.patch_size\n        features = features.reshape(B, H, W, -1)\n        \n        # Flatten for prediction heads\n        features_flat = features.reshape(B, -1, features.shape[-1])\n        \n        # Predict boxes and classes\n        boxes = self.box_predictor(features_flat)\n        classes = self.class_predictor(features_flat)\n        \n        return {'boxes': boxes, 'classes': classes, 'features_map': features}\n\n# Create model and move to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DINOv2Detector(num_classes=80)  # 80 classes for COCO\nmodel = model.to(device)\n\n# Training would require a more complex detection pipeline with NMS, etc.\n\n\n\n\n\nYou can customize the DINOv2 model architecture:\nfrom dinov2.configs import get_config\nfrom dinov2.models import build_model_from_cfg\n\n# Get default configuration and modify it\ncfg = get_config(\"dinov2_base\")\n\n# Modify configuration\ncfg.student.drop_path_rate = 0.2  # Change stochastic depth rate\ncfg.student.num_registers = 16    # Change the number of registers\n\n# Build model from modified config\nmodel = build_model_from_cfg(cfg)\n\n\n\nFor some applications, you might want to extract features from intermediate layers:\nimport torch\nfrom transformers import AutoModel\nfrom torch.utils.hooks import RemovableHandle\n\nclass FeatureExtractor:\n    def __init__(self, model, layers=None):\n        self.model = model\n        self.features = {}\n        self.hooks = []\n        \n        # Default to extracting from the last block if no layers specified\n        self.layers = layers if layers is not None else [11]  # Base model has 12 blocks (0-11)\n        \n        # Register hooks\n        for idx in self.layers:\n            hook = self.model.encoder.layer[idx].register_forward_hook(\n                lambda module, input, output, idx=idx: self.features.update({f\"layer_{idx}\": output})\n            )\n            self.hooks.append(hook)\n    \n    def __call__(self, x):\n        self.features.clear()\n        with torch.no_grad():\n            outputs = self.model(x)\n        return self.features\n    \n    def remove_hooks(self):\n        for hook in self.hooks:\n            hook.remove()\n\n# Usage\nmodel = AutoModel.from_pretrained(\"facebook/dinov2-base\")\nextractor = FeatureExtractor(model, layers=[3, 7, 11])\n\n# Extract features\nfeatures = extractor(input_image)\nlayer_3_features = features[\"layer_3\"]\nlayer_7_features = features[\"layer_7\"]\nlayer_11_features = features[\"layer_11\"]\n\n# Clean up\nextractor.remove_hooks()\n\n\n\n\nDINOv2 achieves excellent results across various vision tasks. Here are typical performance metrics:\n\nImageNet-1K Classification (top-1 accuracy):\n\nDINOv2-Small: ~80.0%\nDINOv2-Base: ~84.5%\nDINOv2-Large: ~86.3%\nDINOv2-Giant: ~87.0%\n\nSemantic Segmentation (ADE20K) (mIoU):\n\nDINOv2-Small: ~47.5%\nDINOv2-Base: ~50.2%\nDINOv2-Large: ~52.5%\nDINOv2-Giant: ~53.8%\n\nObject Detection (COCO) (AP):\n\nDINOv2-Small: ~48.5%\nDINOv2-Base: ~51.3%\nDINOv2-Large: ~53.2%\nDINOv2-Giant: ~54.5%\n\n\n\n\n\n\n\n\nOut of Memory Errors\n\nReduce batch size\nUse gradient accumulation\nUse a smaller model variant (Small or Base)\nUse mixed precision training\n\n\n# Example of mixed precision training\nfrom torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nfor inputs, targets in train_loader:\n    inputs, targets = inputs.to(device), targets.to(device)\n    \n    optimizer.zero_grad()\n    \n    # Use autocast for mixed precision\n    with autocast():\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n    \n    # Scale loss and backprop\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n\nSlow Inference\n\nUse batch processing\nUse model.eval() and torch.no_grad()\nConsider model distillation or quantization\n\nPoor Performance on Downstream Tasks\n\nEnsure proper data preprocessing\nAdjust learning rates (lower for backbone, higher for heads)\nUse appropriate augmentations\nConsider using a larger variant of DINOv2\n\n\n\n\n\n\nVisualize model attention maps to understand what the model focuses on:\n\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom PIL import Image\nimport torchvision.transforms as T\n\ndef get_attention_map(model, img_tensor):\n    model.eval()\n    with torch.no_grad():\n        outputs = model(img_tensor.unsqueeze(0), output_attentions=True)\n    \n    # Get attention weights from the last layer\n    att_mat = outputs.attentions[-1]\n    \n    # Average attention across heads\n    att_mat = att_mat.mean(dim=1)\n    \n    # Extract attention for cls token to patch tokens\n    cls_att_map = att_mat[0, 0, 1:].reshape(14, 14)\n    \n    return cls_att_map.cpu().numpy()\n\n# Load and preprocess image\nimage = Image.open(\"path/to/image.jpg\").convert(\"RGB\")\ntransform = T.Compose([\n    T.Resize((224, 224)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\nimg_tensor = transform(image).to(device)\n\n# Get attention map\nfrom transformers import AutoModel\nmodel = AutoModel.from_pretrained(\"facebook/dinov2-base\", output_attentions=True)\nmodel.to(device)\nattention_map = get_attention_map(model, img_tensor)\n\n# Visualize\nplt.figure(figsize=(10, 10))\nplt.imshow(image.resize((224, 224)))\nplt.imshow(attention_map, alpha=0.5, cmap='jet')\nplt.axis('off')\nplt.colorbar()\nplt.savefig('attention_map.png')\nplt.close()\nThis guide should help you get started with DINOv2 and explore its capabilities for various computer vision tasks. As a self-supervised vision foundation model, DINOv2 provides a strong starting point for numerous applications with minimal labeled data requirements."
  },
  {
    "objectID": "posts/dinov2/index.html#table-of-contents",
    "href": "posts/dinov2/index.html#table-of-contents",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "Introduction to DINOv2\nInstallation and Setup\nLoading Pre-trained Models\nFeature Extraction\nFine-tuning for Downstream Tasks\nImage Classification Example\nSemantic Segmentation Example\nObject Detection Example\nAdvanced Usage and Customization\nPerformance Benchmarks\nTroubleshooting"
  },
  {
    "objectID": "posts/dinov2/index.html#introduction-to-dinov2",
    "href": "posts/dinov2/index.html#introduction-to-dinov2",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "DINOv2 is a self-supervised learning method for vision that produces high-quality visual features without requiring labeled data. It extends the original DINO architecture with several improvements:\n\nTraining on a large and diverse dataset of images\nEnhanced teacher-student architecture\nImproved augmentation strategy\nMulti-scale feature learning\nSupport for various Vision Transformer (ViT) backbones\n\nThe result is a versatile foundation model that can be adapted to numerous vision tasks with minimal fine-tuning."
  },
  {
    "objectID": "posts/dinov2/index.html#installation-and-setup",
    "href": "posts/dinov2/index.html#installation-and-setup",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "To use DINOv2, you’ll need to install the official implementation:\n# Install PyTorch first if not already installed\n# pip install torch torchvision\n\n# Install DINOv2\npip install git+https://github.com/facebookresearch/dinov2\nAlternatively, you can clone the repository and install it locally:\ngit clone https://github.com/facebookresearch/dinov2.git\ncd dinov2\npip install -e .\n\n\nDINOv2 requires:\n\nPython 3.8+\nPyTorch 1.12+\ntorchvision\nCUDA (for GPU acceleration)"
  },
  {
    "objectID": "posts/dinov2/index.html#loading-pre-trained-models",
    "href": "posts/dinov2/index.html#loading-pre-trained-models",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "DINOv2 provides several pre-trained models with different sizes and capabilities:\nimport torch\nfrom dinov2.models import build_model_from_cfg\nfrom dinov2.configs import get_config\n\n# Available model sizes: 'small', 'base', 'large', 'giant'\nmodel_size = 'base'\ncfg = get_config(f\"dinov2_{model_size}\")\nmodel = build_model_from_cfg(cfg)\n\n# Load pre-trained weights\ncheckpoint_path = f\"dinov2_{model_size}_pretrain.pth\"  # Download this from Meta AI's repository\ncheckpoint = torch.load(checkpoint_path, map_location=\"cpu\")\nmodel.load_state_dict(checkpoint[\"model\"])\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nmodel.eval()  # Set to evaluation mode\nYou can also use the Hugging Face Transformers library for an easier integration:\nfrom transformers import AutoImageProcessor, AutoModel\n\n# Available model sizes: 'small', 'base', 'large', 'giant'\nmodel_name = \"facebook/dinov2-base\"\nprocessor = AutoImageProcessor.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)"
  },
  {
    "objectID": "posts/dinov2/index.html#feature-extraction",
    "href": "posts/dinov2/index.html#feature-extraction",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "One of DINOv2’s key strengths is its ability to extract powerful visual features:\nimport torch\nfrom PIL import Image\nimport torchvision.transforms as T\nfrom transformers import AutoImageProcessor, AutoModel\n\n# Load model\nmodel_name = \"facebook/dinov2-base\"\nprocessor = AutoImageProcessor.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\nmodel.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\nmodel.eval()\n\n# Load and preprocess image\nimage = Image.open(\"path/to/your/image.jpg\").convert(\"RGB\")\ninputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n\n# Extract features\nwith torch.no_grad():\n    outputs = model(**inputs)\n    \n# Get CLS token features (useful for classification tasks)\ncls_features = outputs.last_hidden_state[:, 0]\n\n# Get patch features (useful for dense prediction tasks like segmentation)\npatch_features = outputs.last_hidden_state[:, 1:]\n\nprint(f\"CLS features shape: {cls_features.shape}\")\nprint(f\"Patch features shape: {patch_features.shape}\")"
  },
  {
    "objectID": "posts/dinov2/index.html#fine-tuning-for-downstream-tasks",
    "href": "posts/dinov2/index.html#fine-tuning-for-downstream-tasks",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "DINOv2 can be fine-tuned for specific vision tasks:\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModel\n\n# Load pre-trained DINOv2 model\nbackbone = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n\n# Create a custom classification head\nclass ClassificationHead(nn.Module):\n    def __init__(self, backbone, num_classes=1000):\n        super().__init__()\n        self.backbone = backbone\n        self.classifier = nn.Linear(backbone.config.hidden_size, num_classes)\n        \n    def forward(self, x):\n        outputs = self.backbone(x)\n        cls_token = outputs.last_hidden_state[:, 0]\n        return self.classifier(cls_token)\n\n# Create the complete model\nmodel = ClassificationHead(backbone, num_classes=100)  # For 100 classes\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop example\ndef train_one_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    \n    for batch in dataloader:\n        images = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(dataloader)"
  },
  {
    "objectID": "posts/dinov2/index.html#image-classification-example",
    "href": "posts/dinov2/index.html#image-classification-example",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "Here’s a complete example for image classification using DINOv2:\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as transforms\nfrom transformers import AutoImageProcessor, AutoModel\n\n# Define the dataset and transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Load your dataset (adjust the path)\ntrain_dataset = ImageFolder(root=\"path/to/train\", transform=transform)\nval_dataset = ImageFolder(root=\"path/to/val\", transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n\n# Create the model\nclass DINOv2Classifier(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.dinov2 = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n        self.classifier = nn.Linear(768, num_classes)  # 768 is the hidden size for base model\n        \n    def forward(self, x):\n        # Extract features\n        with torch.set_grad_enabled(self.training):\n            features = self.dinov2(x).last_hidden_state[:, 0]  # Get CLS token\n        \n        # Classify\n        logits = self.classifier(features)\n        return logits\n\n# Initialize model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DINOv2Classifier(num_classes=len(train_dataset.classes))\nmodel = model.to(device)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW([\n    {'params': model.classifier.parameters(), 'lr': 1e-3},\n    {'params': model.dinov2.parameters(), 'lr': 1e-5}\n])\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    # Training\n    model.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    \n    for inputs, targets in train_loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n    \n    train_accuracy = 100 * correct / total\n    \n    # Validation\n    model.eval()\n    val_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            \n            val_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    \n    val_accuracy = 100 * correct / total\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    print(f\"Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_accuracy:.2f}%\")\n    print(f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_accuracy:.2f}%\")"
  },
  {
    "objectID": "posts/dinov2/index.html#semantic-segmentation-example",
    "href": "posts/dinov2/index.html#semantic-segmentation-example",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "DINOv2 is particularly powerful for segmentation tasks:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModel\n\nclass DINOv2Segmenter(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        # Load DINOv2 backbone\n        self.backbone = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n        \n        # Define segmentation head\n        hidden_dim = self.backbone.config.hidden_size\n        self.segmentation_head = nn.Sequential(\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(hidden_dim, num_classes, kernel_size=1)\n        )\n        \n        # Image size and patch size for reshaping\n        self.image_size = 224\n        self.patch_size = 14  # For ViT-Base\n        self.num_patches = (self.image_size // self.patch_size) ** 2\n        \n    def forward(self, x):\n        # Get patch features\n        outputs = self.backbone(x)\n        patch_features = outputs.last_hidden_state[:, 1:]  # Remove CLS token\n        \n        # Reshape to 2D spatial layout\n        B = x.shape[0]\n        H = W = self.image_size // self.patch_size\n        patch_features = patch_features.reshape(B, H, W, -1).permute(0, 3, 1, 2)\n        \n        # Apply segmentation head\n        segmentation_logits = self.segmentation_head(patch_features)\n        \n        # Upsample to original image size\n        segmentation_logits = F.interpolate(\n            segmentation_logits, \n            size=(self.image_size, self.image_size), \n            mode='bilinear', \n            align_corners=False\n        )\n        \n        return segmentation_logits\n\n# Create model and move to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DINOv2Segmenter(num_classes=21)  # 21 classes for Pascal VOC\nmodel = model.to(device)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW([\n    {'params': model.segmentation_head.parameters(), 'lr': 1e-3},\n    {'params': model.backbone.parameters(), 'lr': 1e-5}\n])\ncriterion = nn.CrossEntropyLoss(ignore_index=255)  # 255 is typically the ignore index\n\n# Rest of the training code would be similar to the classification example"
  },
  {
    "objectID": "posts/dinov2/index.html#object-detection-example",
    "href": "posts/dinov2/index.html#object-detection-example",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "Here’s how to use DINOv2 features for object detection with a simple detection head:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModel\n\nclass DINOv2Detector(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        # Load DINOv2 backbone\n        self.backbone = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n        hidden_dim = self.backbone.config.hidden_size\n        \n        # Detection heads\n        self.box_predictor = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, 4)  # (x1, y1, x2, y2)\n        )\n        \n        self.class_predictor = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, num_classes + 1)  # +1 for background\n        )\n        \n        # Image size and patch size for feature map creation\n        self.image_size = 224\n        self.patch_size = 14  # For ViT-Base\n        \n    def forward(self, x):\n        # Get features\n        outputs = self.backbone(x)\n        features = outputs.last_hidden_state[:, 1:]  # Remove CLS token\n        \n        # Reshape to 2D spatial layout\n        B = x.shape[0]\n        H = W = self.image_size // self.patch_size\n        features = features.reshape(B, H, W, -1)\n        \n        # Flatten for prediction heads\n        features_flat = features.reshape(B, -1, features.shape[-1])\n        \n        # Predict boxes and classes\n        boxes = self.box_predictor(features_flat)\n        classes = self.class_predictor(features_flat)\n        \n        return {'boxes': boxes, 'classes': classes, 'features_map': features}\n\n# Create model and move to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DINOv2Detector(num_classes=80)  # 80 classes for COCO\nmodel = model.to(device)\n\n# Training would require a more complex detection pipeline with NMS, etc."
  },
  {
    "objectID": "posts/dinov2/index.html#advanced-usage-and-customization",
    "href": "posts/dinov2/index.html#advanced-usage-and-customization",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "You can customize the DINOv2 model architecture:\nfrom dinov2.configs import get_config\nfrom dinov2.models import build_model_from_cfg\n\n# Get default configuration and modify it\ncfg = get_config(\"dinov2_base\")\n\n# Modify configuration\ncfg.student.drop_path_rate = 0.2  # Change stochastic depth rate\ncfg.student.num_registers = 16    # Change the number of registers\n\n# Build model from modified config\nmodel = build_model_from_cfg(cfg)\n\n\n\nFor some applications, you might want to extract features from intermediate layers:\nimport torch\nfrom transformers import AutoModel\nfrom torch.utils.hooks import RemovableHandle\n\nclass FeatureExtractor:\n    def __init__(self, model, layers=None):\n        self.model = model\n        self.features = {}\n        self.hooks = []\n        \n        # Default to extracting from the last block if no layers specified\n        self.layers = layers if layers is not None else [11]  # Base model has 12 blocks (0-11)\n        \n        # Register hooks\n        for idx in self.layers:\n            hook = self.model.encoder.layer[idx].register_forward_hook(\n                lambda module, input, output, idx=idx: self.features.update({f\"layer_{idx}\": output})\n            )\n            self.hooks.append(hook)\n    \n    def __call__(self, x):\n        self.features.clear()\n        with torch.no_grad():\n            outputs = self.model(x)\n        return self.features\n    \n    def remove_hooks(self):\n        for hook in self.hooks:\n            hook.remove()\n\n# Usage\nmodel = AutoModel.from_pretrained(\"facebook/dinov2-base\")\nextractor = FeatureExtractor(model, layers=[3, 7, 11])\n\n# Extract features\nfeatures = extractor(input_image)\nlayer_3_features = features[\"layer_3\"]\nlayer_7_features = features[\"layer_7\"]\nlayer_11_features = features[\"layer_11\"]\n\n# Clean up\nextractor.remove_hooks()"
  },
  {
    "objectID": "posts/dinov2/index.html#performance-benchmarks",
    "href": "posts/dinov2/index.html#performance-benchmarks",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "DINOv2 achieves excellent results across various vision tasks. Here are typical performance metrics:\n\nImageNet-1K Classification (top-1 accuracy):\n\nDINOv2-Small: ~80.0%\nDINOv2-Base: ~84.5%\nDINOv2-Large: ~86.3%\nDINOv2-Giant: ~87.0%\n\nSemantic Segmentation (ADE20K) (mIoU):\n\nDINOv2-Small: ~47.5%\nDINOv2-Base: ~50.2%\nDINOv2-Large: ~52.5%\nDINOv2-Giant: ~53.8%\n\nObject Detection (COCO) (AP):\n\nDINOv2-Small: ~48.5%\nDINOv2-Base: ~51.3%\nDINOv2-Large: ~53.2%\nDINOv2-Giant: ~54.5%"
  },
  {
    "objectID": "posts/dinov2/index.html#troubleshooting",
    "href": "posts/dinov2/index.html#troubleshooting",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "Out of Memory Errors\n\nReduce batch size\nUse gradient accumulation\nUse a smaller model variant (Small or Base)\nUse mixed precision training\n\n\n# Example of mixed precision training\nfrom torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nfor inputs, targets in train_loader:\n    inputs, targets = inputs.to(device), targets.to(device)\n    \n    optimizer.zero_grad()\n    \n    # Use autocast for mixed precision\n    with autocast():\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n    \n    # Scale loss and backprop\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n\nSlow Inference\n\nUse batch processing\nUse model.eval() and torch.no_grad()\nConsider model distillation or quantization\n\nPoor Performance on Downstream Tasks\n\nEnsure proper data preprocessing\nAdjust learning rates (lower for backbone, higher for heads)\nUse appropriate augmentations\nConsider using a larger variant of DINOv2\n\n\n\n\n\n\nVisualize model attention maps to understand what the model focuses on:\n\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom PIL import Image\nimport torchvision.transforms as T\n\ndef get_attention_map(model, img_tensor):\n    model.eval()\n    with torch.no_grad():\n        outputs = model(img_tensor.unsqueeze(0), output_attentions=True)\n    \n    # Get attention weights from the last layer\n    att_mat = outputs.attentions[-1]\n    \n    # Average attention across heads\n    att_mat = att_mat.mean(dim=1)\n    \n    # Extract attention for cls token to patch tokens\n    cls_att_map = att_mat[0, 0, 1:].reshape(14, 14)\n    \n    return cls_att_map.cpu().numpy()\n\n# Load and preprocess image\nimage = Image.open(\"path/to/image.jpg\").convert(\"RGB\")\ntransform = T.Compose([\n    T.Resize((224, 224)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\nimg_tensor = transform(image).to(device)\n\n# Get attention map\nfrom transformers import AutoModel\nmodel = AutoModel.from_pretrained(\"facebook/dinov2-base\", output_attentions=True)\nmodel.to(device)\nattention_map = get_attention_map(model, img_tensor)\n\n# Visualize\nplt.figure(figsize=(10, 10))\nplt.imshow(image.resize((224, 224)))\nplt.imshow(attention_map, alpha=0.5, cmap='jet')\nplt.axis('off')\nplt.colorbar()\nplt.savefig('attention_map.png')\nplt.close()\nThis guide should help you get started with DINOv2 and explore its capabilities for various computer vision tasks. As a self-supervised vision foundation model, DINOv2 provides a strong starting point for numerous applications with minimal labeled data requirements."
  },
  {
    "objectID": "posts/pytorch-to-pytorchlightning/index.html",
    "href": "posts/pytorch-to-pytorchlightning/index.html",
    "title": "PyTorch to PyTorch Lightning Migration Guide",
    "section": "",
    "text": "Introduction\nKey Concepts\nBasic Migration Steps\nCode Examples\nAdvanced Features\nBest Practices\nCommon Pitfalls\n\n\n\n\nPyTorch Lightning is a lightweight wrapper around PyTorch that eliminates boilerplate code while maintaining full control over your models. It provides a structured approach to organizing PyTorch code and includes built-in support for distributed training, logging, and experiment management.\n\n\n\nReduced Boilerplate: Lightning handles training loops, device management, and distributed training\nBetter Organization: Standardized code structure improves readability and maintenance\nBuilt-in Features: Automatic logging, checkpointing, early stopping, and more\nScalability: Easy multi-GPU and multi-node training\nReproducibility: Better experiment tracking and configuration management\n\n\n\n\n\n\n\nThe core abstraction that wraps your PyTorch model. It defines:\n\nModel architecture (__init__)\nForward pass (forward)\nTraining step (training_step)\nValidation step (validation_step)\nOptimizer configuration (configure_optimizers)\n\n\n\n\nHandles the training loop, device management, and various training configurations.\n\n\n\nEncapsulates data loading logic, including datasets and dataloaders.\n\n\n\n\n\n\nBefore (PyTorch):\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass SimpleModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\nAfter (Lightning):\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass LightningModel(pl.LightningModule):\n    def __init__(self, input_size, hidden_size, num_classes, learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        acc = (y_hat.argmax(dim=1) == y).float().mean()\n        self.log('val_loss', loss)\n        self.log('val_acc', acc)\n        \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n\n\n\nBefore (PyTorch):\n# Training loop\nmodel = SimpleModel(input_size=784, hidden_size=128, num_classes=10)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        \n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        \n        if batch_idx % 100 == 0:\n            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}')\n    \n    # Validation\n    model.eval()\n    val_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in val_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            val_loss += criterion(output, target).item()\n            pred = output.argmax(dim=1)\n            correct += pred.eq(target).sum().item()\n    \n    print(f'Validation Loss: {val_loss/len(val_loader)}, Accuracy: {correct/len(val_dataset)}')\nAfter (Lightning):\n# Training with Lightning\nmodel = LightningModel(input_size=784, hidden_size=128, num_classes=10)\ntrainer = pl.Trainer(max_epochs=10, accelerator='auto', devices='auto')\ntrainer.fit(model, train_loader, val_loader)\n\n\n\n\n\n\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\n\nclass MNISTDataModule(pl.LightningDataModule):\n    def __init__(self, data_dir: str = \"data/\", batch_size: int = 64):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n\n    def prepare_data(self):\n        # Download data\n        datasets.MNIST(self.data_dir, train=True, download=True)\n        datasets.MNIST(self.data_dir, train=False, download=True)\n\n    def setup(self, stage: str):\n        # Assign train/val datasets\n        if stage == 'fit':\n            mnist_full = datasets.MNIST(\n                self.data_dir, train=True, transform=self.transform\n            )\n            self.mnist_train, self.mnist_val = random_split(\n                mnist_full, [55000, 5000]\n            )\n\n        if stage == 'test':\n            self.mnist_test = datasets.MNIST(\n                self.data_dir, train=False, transform=self.transform\n            )\n\n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=self.batch_size, shuffle=True)\n\n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n\n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=self.batch_size)\n\nclass MNISTClassifier(pl.LightningModule):\n    def __init__(self, learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        self.log(\"train_loss\", loss, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = torch.sum(preds == y).float() / len(y)\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = torch.sum(preds == y).float() / len(y)\n        self.log(\"test_loss\", loss)\n        self.log(\"test_acc\", acc)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n\n# Usage\nif __name__ == \"__main__\":\n    # Initialize data module and model\n    dm = MNISTDataModule()\n    model = MNISTClassifier()\n    \n    # Initialize trainer\n    trainer = pl.Trainer(\n        max_epochs=5,\n        accelerator='auto',\n        devices='auto',\n        logger=pl.loggers.TensorBoardLogger('lightning_logs/'),\n        callbacks=[\n            pl.callbacks.EarlyStopping(monitor='val_loss', patience=3),\n            pl.callbacks.ModelCheckpoint(monitor='val_loss', save_top_k=1)\n        ]\n    )\n    \n    # Train the model\n    trainer.fit(model, dm)\n    \n    # Test the model\n    trainer.test(model, dm)\n\n\n\nimport torchmetrics\n\nclass AdvancedClassifier(pl.LightningModule):\n    def __init__(self, num_classes=10, learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Model layers\n        self.model = nn.Sequential(\n            nn.Linear(784, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, num_classes)\n        )\n        \n        # Metrics\n        self.train_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n        self.val_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n        self.val_f1 = torchmetrics.F1Score(task='multiclass', num_classes=num_classes)\n        \n    def forward(self, x):\n        return self.model(x.view(x.size(0), -1))\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        \n        # Log metrics\n        self.train_accuracy(y_hat, y)\n        self.log('train_loss', loss, on_step=True, on_epoch=True)\n        self.log('train_acc', self.train_accuracy, on_step=True, on_epoch=True)\n        \n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        \n        # Update metrics\n        self.val_accuracy(y_hat, y)\n        self.val_f1(y_hat, y)\n        \n        # Log metrics\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', self.val_accuracy, prog_bar=True)\n        self.log('val_f1', self.val_f1, prog_bar=True)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(\n            self.parameters(), \n            lr=self.hparams.learning_rate,\n            weight_decay=1e-4\n        )\n        \n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer, T_max=100\n        )\n        \n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"monitor\": \"val_loss\",\n                \"frequency\": 1\n            }\n        }\n\n\n\n\n\n\ndef configure_optimizers(self):\n    # Different learning rates for different parts\n    opt_g = torch.optim.Adam(self.generator.parameters(), lr=0.0002)\n    opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=0.0002)\n    \n    # Different schedulers\n    sch_g = torch.optim.lr_scheduler.StepLR(opt_g, step_size=50, gamma=0.5)\n    sch_d = torch.optim.lr_scheduler.StepLR(opt_d, step_size=50, gamma=0.5)\n    \n    return [opt_g, opt_d], [sch_g, sch_d]\n\n\n\nclass CustomCallback(pl.Callback):\n    def on_train_epoch_end(self, trainer, pl_module):\n        # Custom logic at end of each epoch\n        if trainer.current_epoch % 10 == 0:\n            print(f\"Completed {trainer.current_epoch} epochs\")\n    \n    def on_validation_end(self, trainer, pl_module):\n        # Custom validation logic\n        val_loss = trainer.callback_metrics.get('val_loss')\n        if val_loss and val_loss &lt; 0.1:\n            print(\"Excellent validation performance!\")\n\n# Usage\ntrainer = pl.Trainer(\n    callbacks=[CustomCallback(), pl.callbacks.EarlyStopping(monitor='val_loss')]\n)\n\n\n\nclass ManualOptimizationModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.automatic_optimization = False\n        # ... model definition\n        \n    def training_step(self, batch, batch_idx):\n        opt = self.optimizers()\n        \n        # Manual optimization\n        opt.zero_grad()\n        loss = self.compute_loss(batch)\n        self.manual_backward(loss)\n        opt.step()\n        \n        self.log('train_loss', loss)\n        return loss\n\n\n\n\n\n\nclass ConfigurableModel(pl.LightningModule):\n    def __init__(self, **kwargs):\n        super().__init__()\n        # Save all hyperparameters\n        self.save_hyperparameters()\n        \n        # Access with self.hparams\n        self.model = self._build_model()\n        \n    def _build_model(self):\n        return nn.Sequential(\n            nn.Linear(self.hparams.input_size, self.hparams.hidden_size),\n            nn.ReLU(),\n            nn.Linear(self.hparams.hidden_size, self.hparams.num_classes)\n        )\n\n\n\ndef training_step(self, batch, batch_idx):\n    loss = self.compute_loss(batch)\n    \n    # Log to both step and epoch\n    self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n    \n    # Log learning rate\n    self.log('lr', self.trainer.optimizers[0].param_groups[0]['lr'])\n    \n    return loss\n\n\n\n# Separate model definition from Lightning logic\nclass ResNetBackbone(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        # Model architecture here\n        \nclass ResNetLightning(pl.LightningModule):\n    def __init__(self, num_classes, learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Use separate model class\n        self.model = ResNetBackbone(num_classes)\n        \n    def forward(self, x):\n        return self.model(x)\n    \n    # Training logic here...\n\n\n\ndef validation_step(self, batch, batch_idx):\n    # Always include validation metrics\n    loss, acc = self._shared_eval_step(batch, batch_idx)\n    self.log_dict({'val_loss': loss, 'val_acc': acc}, prog_bar=True)\n    \ndef test_step(self, batch, batch_idx):\n    # Comprehensive test metrics\n    loss, acc = self._shared_eval_step(batch, batch_idx)\n    self.log_dict({'test_loss': loss, 'test_acc': acc})\n    \ndef _shared_eval_step(self, batch, batch_idx):\n    x, y = batch\n    y_hat = self(x)\n    loss = F.cross_entropy(y_hat, y)\n    acc = (y_hat.argmax(dim=1) == y).float().mean()\n    return loss, acc\n\n\n\n\n\n\nWrong:\ndef training_step(self, batch, batch_idx):\n    x, y = batch\n    x = x.cuda()  # Don't manually move to device\n    y = y.cuda()\n    # ...\nCorrect:\ndef training_step(self, batch, batch_idx):\n    x, y = batch  # Lightning handles device placement\n    # ...\n\n\n\nWrong:\ndef training_step(self, batch, batch_idx):\n    loss = self.compute_loss(batch)\n    loss.backward()  # Don't call backward manually\n    return loss\nCorrect:\ndef training_step(self, batch, batch_idx):\n    loss = self.compute_loss(batch)\n    return loss  # Lightning handles backward pass\n\n\n\nWrong:\ndef validation_step(self, batch, batch_idx):\n    # Computing metrics inside step leads to incorrect averages\n    acc = compute_accuracy(batch)\n    self.log('val_acc', acc.mean())\nCorrect:\ndef __init__(self):\n    super().__init__()\n    self.val_acc = torchmetrics.Accuracy()\n\ndef validation_step(self, batch, batch_idx):\n    # Let torchmetrics handle the averaging\n    y_hat = self(x)\n    self.val_acc(y_hat, y)\n    self.log('val_acc', self.val_acc)\n\n\n\nWrong:\nclass Model(pl.LightningModule):\n    def train_dataloader(self):\n        # Don't put data loading in model\n        return DataLoader(...)\nCorrect:\n# Use separate DataModule\nclass DataModule(pl.LightningDataModule):\n    def train_dataloader(self):\n        return DataLoader(...)\n\n# Or pass dataloaders to trainer.fit()\ntrainer.fit(model, train_dataloader, val_dataloader)\n\n\n\n\n\nConvert model class to inherit from pl.LightningModule\nImplement required methods: training_step, configure_optimizers\nAdd validation_step if you have validation data\nReplace manual training loop with pl.Trainer\nMove data loading to pl.LightningDataModule or separate functions\nAdd proper logging with self.log()\nUse self.save_hyperparameters() for configuration\nAdd callbacks for checkpointing, early stopping, etc.\nRemove manual device management (CUDA calls)\nTest with different accelerators (CPU, GPU, multi-GPU)\nUpdate any custom metrics to use torchmetrics\nVerify logging and experiment tracking works\nAdd proper test methods if needed\n\nBy following this guide, you should be able to successfully migrate your PyTorch code to PyTorch Lightning while maintaining all functionality and gaining the benefits of Lightning’s structured approach."
  },
  {
    "objectID": "posts/pytorch-to-pytorchlightning/index.html#table-of-contents",
    "href": "posts/pytorch-to-pytorchlightning/index.html#table-of-contents",
    "title": "PyTorch to PyTorch Lightning Migration Guide",
    "section": "",
    "text": "Introduction\nKey Concepts\nBasic Migration Steps\nCode Examples\nAdvanced Features\nBest Practices\nCommon Pitfalls"
  },
  {
    "objectID": "posts/pytorch-to-pytorchlightning/index.html#introduction",
    "href": "posts/pytorch-to-pytorchlightning/index.html#introduction",
    "title": "PyTorch to PyTorch Lightning Migration Guide",
    "section": "",
    "text": "PyTorch Lightning is a lightweight wrapper around PyTorch that eliminates boilerplate code while maintaining full control over your models. It provides a structured approach to organizing PyTorch code and includes built-in support for distributed training, logging, and experiment management.\n\n\n\nReduced Boilerplate: Lightning handles training loops, device management, and distributed training\nBetter Organization: Standardized code structure improves readability and maintenance\nBuilt-in Features: Automatic logging, checkpointing, early stopping, and more\nScalability: Easy multi-GPU and multi-node training\nReproducibility: Better experiment tracking and configuration management"
  },
  {
    "objectID": "posts/pytorch-to-pytorchlightning/index.html#key-concepts",
    "href": "posts/pytorch-to-pytorchlightning/index.html#key-concepts",
    "title": "PyTorch to PyTorch Lightning Migration Guide",
    "section": "",
    "text": "The core abstraction that wraps your PyTorch model. It defines:\n\nModel architecture (__init__)\nForward pass (forward)\nTraining step (training_step)\nValidation step (validation_step)\nOptimizer configuration (configure_optimizers)\n\n\n\n\nHandles the training loop, device management, and various training configurations.\n\n\n\nEncapsulates data loading logic, including datasets and dataloaders."
  },
  {
    "objectID": "posts/pytorch-to-pytorchlightning/index.html#basic-migration-steps",
    "href": "posts/pytorch-to-pytorchlightning/index.html#basic-migration-steps",
    "title": "PyTorch to PyTorch Lightning Migration Guide",
    "section": "",
    "text": "Before (PyTorch):\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass SimpleModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\nAfter (Lightning):\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass LightningModel(pl.LightningModule):\n    def __init__(self, input_size, hidden_size, num_classes, learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        acc = (y_hat.argmax(dim=1) == y).float().mean()\n        self.log('val_loss', loss)\n        self.log('val_acc', acc)\n        \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n\n\n\nBefore (PyTorch):\n# Training loop\nmodel = SimpleModel(input_size=784, hidden_size=128, num_classes=10)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        \n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        \n        if batch_idx % 100 == 0:\n            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}')\n    \n    # Validation\n    model.eval()\n    val_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in val_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            val_loss += criterion(output, target).item()\n            pred = output.argmax(dim=1)\n            correct += pred.eq(target).sum().item()\n    \n    print(f'Validation Loss: {val_loss/len(val_loader)}, Accuracy: {correct/len(val_dataset)}')\nAfter (Lightning):\n# Training with Lightning\nmodel = LightningModel(input_size=784, hidden_size=128, num_classes=10)\ntrainer = pl.Trainer(max_epochs=10, accelerator='auto', devices='auto')\ntrainer.fit(model, train_loader, val_loader)"
  },
  {
    "objectID": "posts/pytorch-to-pytorchlightning/index.html#code-examples",
    "href": "posts/pytorch-to-pytorchlightning/index.html#code-examples",
    "title": "PyTorch to PyTorch Lightning Migration Guide",
    "section": "",
    "text": "import pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\n\nclass MNISTDataModule(pl.LightningDataModule):\n    def __init__(self, data_dir: str = \"data/\", batch_size: int = 64):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n\n    def prepare_data(self):\n        # Download data\n        datasets.MNIST(self.data_dir, train=True, download=True)\n        datasets.MNIST(self.data_dir, train=False, download=True)\n\n    def setup(self, stage: str):\n        # Assign train/val datasets\n        if stage == 'fit':\n            mnist_full = datasets.MNIST(\n                self.data_dir, train=True, transform=self.transform\n            )\n            self.mnist_train, self.mnist_val = random_split(\n                mnist_full, [55000, 5000]\n            )\n\n        if stage == 'test':\n            self.mnist_test = datasets.MNIST(\n                self.data_dir, train=False, transform=self.transform\n            )\n\n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=self.batch_size, shuffle=True)\n\n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n\n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=self.batch_size)\n\nclass MNISTClassifier(pl.LightningModule):\n    def __init__(self, learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        self.log(\"train_loss\", loss, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = torch.sum(preds == y).float() / len(y)\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = torch.sum(preds == y).float() / len(y)\n        self.log(\"test_loss\", loss)\n        self.log(\"test_acc\", acc)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n\n# Usage\nif __name__ == \"__main__\":\n    # Initialize data module and model\n    dm = MNISTDataModule()\n    model = MNISTClassifier()\n    \n    # Initialize trainer\n    trainer = pl.Trainer(\n        max_epochs=5,\n        accelerator='auto',\n        devices='auto',\n        logger=pl.loggers.TensorBoardLogger('lightning_logs/'),\n        callbacks=[\n            pl.callbacks.EarlyStopping(monitor='val_loss', patience=3),\n            pl.callbacks.ModelCheckpoint(monitor='val_loss', save_top_k=1)\n        ]\n    )\n    \n    # Train the model\n    trainer.fit(model, dm)\n    \n    # Test the model\n    trainer.test(model, dm)\n\n\n\nimport torchmetrics\n\nclass AdvancedClassifier(pl.LightningModule):\n    def __init__(self, num_classes=10, learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Model layers\n        self.model = nn.Sequential(\n            nn.Linear(784, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, num_classes)\n        )\n        \n        # Metrics\n        self.train_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n        self.val_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n        self.val_f1 = torchmetrics.F1Score(task='multiclass', num_classes=num_classes)\n        \n    def forward(self, x):\n        return self.model(x.view(x.size(0), -1))\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        \n        # Log metrics\n        self.train_accuracy(y_hat, y)\n        self.log('train_loss', loss, on_step=True, on_epoch=True)\n        self.log('train_acc', self.train_accuracy, on_step=True, on_epoch=True)\n        \n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        \n        # Update metrics\n        self.val_accuracy(y_hat, y)\n        self.val_f1(y_hat, y)\n        \n        # Log metrics\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', self.val_accuracy, prog_bar=True)\n        self.log('val_f1', self.val_f1, prog_bar=True)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(\n            self.parameters(), \n            lr=self.hparams.learning_rate,\n            weight_decay=1e-4\n        )\n        \n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer, T_max=100\n        )\n        \n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"monitor\": \"val_loss\",\n                \"frequency\": 1\n            }\n        }"
  },
  {
    "objectID": "posts/pytorch-to-pytorchlightning/index.html#advanced-features",
    "href": "posts/pytorch-to-pytorchlightning/index.html#advanced-features",
    "title": "PyTorch to PyTorch Lightning Migration Guide",
    "section": "",
    "text": "def configure_optimizers(self):\n    # Different learning rates for different parts\n    opt_g = torch.optim.Adam(self.generator.parameters(), lr=0.0002)\n    opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=0.0002)\n    \n    # Different schedulers\n    sch_g = torch.optim.lr_scheduler.StepLR(opt_g, step_size=50, gamma=0.5)\n    sch_d = torch.optim.lr_scheduler.StepLR(opt_d, step_size=50, gamma=0.5)\n    \n    return [opt_g, opt_d], [sch_g, sch_d]\n\n\n\nclass CustomCallback(pl.Callback):\n    def on_train_epoch_end(self, trainer, pl_module):\n        # Custom logic at end of each epoch\n        if trainer.current_epoch % 10 == 0:\n            print(f\"Completed {trainer.current_epoch} epochs\")\n    \n    def on_validation_end(self, trainer, pl_module):\n        # Custom validation logic\n        val_loss = trainer.callback_metrics.get('val_loss')\n        if val_loss and val_loss &lt; 0.1:\n            print(\"Excellent validation performance!\")\n\n# Usage\ntrainer = pl.Trainer(\n    callbacks=[CustomCallback(), pl.callbacks.EarlyStopping(monitor='val_loss')]\n)\n\n\n\nclass ManualOptimizationModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.automatic_optimization = False\n        # ... model definition\n        \n    def training_step(self, batch, batch_idx):\n        opt = self.optimizers()\n        \n        # Manual optimization\n        opt.zero_grad()\n        loss = self.compute_loss(batch)\n        self.manual_backward(loss)\n        opt.step()\n        \n        self.log('train_loss', loss)\n        return loss"
  },
  {
    "objectID": "posts/pytorch-to-pytorchlightning/index.html#best-practices",
    "href": "posts/pytorch-to-pytorchlightning/index.html#best-practices",
    "title": "PyTorch to PyTorch Lightning Migration Guide",
    "section": "",
    "text": "class ConfigurableModel(pl.LightningModule):\n    def __init__(self, **kwargs):\n        super().__init__()\n        # Save all hyperparameters\n        self.save_hyperparameters()\n        \n        # Access with self.hparams\n        self.model = self._build_model()\n        \n    def _build_model(self):\n        return nn.Sequential(\n            nn.Linear(self.hparams.input_size, self.hparams.hidden_size),\n            nn.ReLU(),\n            nn.Linear(self.hparams.hidden_size, self.hparams.num_classes)\n        )\n\n\n\ndef training_step(self, batch, batch_idx):\n    loss = self.compute_loss(batch)\n    \n    # Log to both step and epoch\n    self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n    \n    # Log learning rate\n    self.log('lr', self.trainer.optimizers[0].param_groups[0]['lr'])\n    \n    return loss\n\n\n\n# Separate model definition from Lightning logic\nclass ResNetBackbone(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        # Model architecture here\n        \nclass ResNetLightning(pl.LightningModule):\n    def __init__(self, num_classes, learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Use separate model class\n        self.model = ResNetBackbone(num_classes)\n        \n    def forward(self, x):\n        return self.model(x)\n    \n    # Training logic here...\n\n\n\ndef validation_step(self, batch, batch_idx):\n    # Always include validation metrics\n    loss, acc = self._shared_eval_step(batch, batch_idx)\n    self.log_dict({'val_loss': loss, 'val_acc': acc}, prog_bar=True)\n    \ndef test_step(self, batch, batch_idx):\n    # Comprehensive test metrics\n    loss, acc = self._shared_eval_step(batch, batch_idx)\n    self.log_dict({'test_loss': loss, 'test_acc': acc})\n    \ndef _shared_eval_step(self, batch, batch_idx):\n    x, y = batch\n    y_hat = self(x)\n    loss = F.cross_entropy(y_hat, y)\n    acc = (y_hat.argmax(dim=1) == y).float().mean()\n    return loss, acc"
  },
  {
    "objectID": "posts/pytorch-to-pytorchlightning/index.html#common-pitfalls",
    "href": "posts/pytorch-to-pytorchlightning/index.html#common-pitfalls",
    "title": "PyTorch to PyTorch Lightning Migration Guide",
    "section": "",
    "text": "Wrong:\ndef training_step(self, batch, batch_idx):\n    x, y = batch\n    x = x.cuda()  # Don't manually move to device\n    y = y.cuda()\n    # ...\nCorrect:\ndef training_step(self, batch, batch_idx):\n    x, y = batch  # Lightning handles device placement\n    # ...\n\n\n\nWrong:\ndef training_step(self, batch, batch_idx):\n    loss = self.compute_loss(batch)\n    loss.backward()  # Don't call backward manually\n    return loss\nCorrect:\ndef training_step(self, batch, batch_idx):\n    loss = self.compute_loss(batch)\n    return loss  # Lightning handles backward pass\n\n\n\nWrong:\ndef validation_step(self, batch, batch_idx):\n    # Computing metrics inside step leads to incorrect averages\n    acc = compute_accuracy(batch)\n    self.log('val_acc', acc.mean())\nCorrect:\ndef __init__(self):\n    super().__init__()\n    self.val_acc = torchmetrics.Accuracy()\n\ndef validation_step(self, batch, batch_idx):\n    # Let torchmetrics handle the averaging\n    y_hat = self(x)\n    self.val_acc(y_hat, y)\n    self.log('val_acc', self.val_acc)\n\n\n\nWrong:\nclass Model(pl.LightningModule):\n    def train_dataloader(self):\n        # Don't put data loading in model\n        return DataLoader(...)\nCorrect:\n# Use separate DataModule\nclass DataModule(pl.LightningDataModule):\n    def train_dataloader(self):\n        return DataLoader(...)\n\n# Or pass dataloaders to trainer.fit()\ntrainer.fit(model, train_dataloader, val_dataloader)"
  },
  {
    "objectID": "posts/pytorch-to-pytorchlightning/index.html#migration-checklist",
    "href": "posts/pytorch-to-pytorchlightning/index.html#migration-checklist",
    "title": "PyTorch to PyTorch Lightning Migration Guide",
    "section": "",
    "text": "Convert model class to inherit from pl.LightningModule\nImplement required methods: training_step, configure_optimizers\nAdd validation_step if you have validation data\nReplace manual training loop with pl.Trainer\nMove data loading to pl.LightningDataModule or separate functions\nAdd proper logging with self.log()\nUse self.save_hyperparameters() for configuration\nAdd callbacks for checkpointing, early stopping, etc.\nRemove manual device management (CUDA calls)\nTest with different accelerators (CPU, GPU, multi-GPU)\nUpdate any custom metrics to use torchmetrics\nVerify logging and experiment tracking works\nAdd proper test methods if needed\n\nBy following this guide, you should be able to successfully migrate your PyTorch code to PyTorch Lightning while maintaining all functionality and gaining the benefits of Lightning’s structured approach."
  },
  {
    "objectID": "posts/dino-v2-explained/index.html",
    "href": "posts/dino-v2-explained/index.html",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "In 2023, Meta AI Research unveiled DINOv2 (Self-Distillation with No Labels v2), a breakthrough in self-supervised visual learning that produces remarkably versatile and robust visual features. This article provides a detailed exploration of DINOv2’s architecture and training methodology, explaining how it achieves state-of-the-art performance across diverse visual tasks without task-specific supervision.\n\n\n\nAt the heart of DINOv2 is the Vision Transformer (ViT) architecture, which has proven highly effective for computer vision tasks. DINOv2 specifically uses:\n\n\n\nViT-S/14: Small model (22M parameters)\nViT-B/14: Base model (87M parameters)\n\nViT-L/14: Large model (304M parameters)\nViT-g/14: Giant model (1.1B parameters)\n\nThe “/14” indicates a patch size of 14×14 pixels. These patches are how images are tokenized before being processed by the transformer.\n\n\n\nDINOv2 incorporates several architectural improvements over the original DINO:\n\nImproved Layer Normalization: Uses a modified version of layer normalization that enhances stability during training at scale.\nSwiGLU Activation: Replaces standard ReLU or GELU activations with SwiGLU, which improves representation quality.\nRegister Tokens: Additional learnable tokens (alongside the [CLS] token) that capture different aspects of image information.\nAttention Bias: Incorporates relative position embeddings through attention biases instead of absolute positional encodings.\nPost-Normalization: Places the layer normalization after the multi-head attention and feed-forward blocks rather than before them.\n\n\n\n\n\nDINOv2’s training methodology centers around self-distillation, where a model essentially teaches itself. This is implemented through a student-teacher framework:\n\n\n\nStudent Network: The network being trained, updated via backpropagation\nTeacher Network: An exponential moving average (EMA) of the student’s parameters\nBoth networks share the same architecture but different parameters\n\nThis approach creates a moving target that continuously evolves as training progresses, preventing trivial solutions where the network collapses to outputting the same representation for all inputs.\n\n\n\nA key component of DINOv2’s training is its sophisticated multi-crop approach:\n\nGlobal Views: Two large crops covering significant portions of the image (224×224 pixels)\nLocal Views: Multiple smaller crops capturing image details (96×96 pixels)\n\nThe student network processes both global and local views, while the teacher network only processes global views. This forces the model to learn both global context and local details.\n\n\n\nThe training objective is a cross-entropy loss that encourages the student’s output distribution for local views to match the teacher’s output distribution for global views of the same image. Mathematically:\nL = H(Pt(g), Ps(l))\nWhere:\n\nH is the cross-entropy\nPt(g) is the teacher’s prediction on global views\nPs(l) is the student’s prediction on local views\n\nThe teacher’s outputs are sharpened using a temperature parameter that gradually decreases throughout training, making the targets increasingly focused on specific features.\n\n\n\n\nDINOv2’s impressive performance comes not just from architecture but from meticulous data preparation:\n\n\nThe researchers curated a high-quality dataset of 142 million images from publicly available sources, with careful filtering to remove:\n\nDuplicate images\nLow-quality content\nInappropriate material\nText-heavy images\nHuman faces\n\n\n\n\nDuring training, DINOv2 employs a robust augmentation strategy:\n\nRandom resized cropping: Different sized views of the same image\nRandom horizontal flips: Mirroring images horizontally\nColor jittering: Altering brightness, contrast, saturation, and hue\nGaussian blur: Adding controlled blur to some views\nSolarization: Inverting pixels above a threshold (applied selectively)\n\nThese augmentations create diverse views while preserving the semantic content, forcing the model to learn invariance to these transformations.\n\n\n\n\nTraining a model of DINOv2’s scale requires sophisticated distributed computing approaches:\n\n\n\nOptimizer: AdamW with a cosine learning rate schedule\nGradient Accumulation: Used to handle effectively larger batch sizes\nMixed Precision: FP16 calculations to speed up training\nSharding: Model parameters distributed across multiple GPUs\n\n\n\n\nDINOv2 uses enormous effective batch sizes (up to 65,536 images) by leveraging distributed training across hundreds of GPUs. This large batch size is crucial for learning high-quality representations.\n\n\n\n\nTo prevent representation collapse and ensure diverse, meaningful features, DINOv2 employs:\n\nCentering: Ensuring the average output across the batch remains close to zero\nSharpening: Gradually decreasing the temperature parameter of the teacher’s softmax\nDALL-E VAE Integration: Using a pre-trained DALL-E VAE to improve representation quality\nWeight Decay: Applied differently to various components of the model\n\n\n\n\nAfter training, DINOv2 can be used in different ways:\n\n\n\n[CLS] Token: The class token representation serves as a global image descriptor\nRegister Tokens: Multiple specialized tokens that capture different aspects of the image\nPatch Tokens: Local features corresponding to specific regions of the image\n\n\n\n\nThe researchers also created smaller, distilled versions of DINOv2 that maintain much of the performance while requiring significantly fewer computational resources for deployment.\n\n\n\n\nDINOv2 represents a remarkable achievement in self-supervised visual learning. Its sophisticated architecture and training methodology enable it to learn general-purpose visual features that transfer exceptionally well across diverse tasks. The careful balance of architectural innovations, data curation, and training techniques creates a visual representation system that approaches the versatility and power that we’ve seen in large language models.\nThe success of DINOv2 highlights how self-supervised learning can leverage vast amounts of unlabeled data to create foundation models for computer vision that may eventually eliminate the need for task-specific supervised training in many applications."
  },
  {
    "objectID": "posts/dino-v2-explained/index.html#architectural-foundation-vision-transformers",
    "href": "posts/dino-v2-explained/index.html#architectural-foundation-vision-transformers",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "At the heart of DINOv2 is the Vision Transformer (ViT) architecture, which has proven highly effective for computer vision tasks. DINOv2 specifically uses:\n\n\n\nViT-S/14: Small model (22M parameters)\nViT-B/14: Base model (87M parameters)\n\nViT-L/14: Large model (304M parameters)\nViT-g/14: Giant model (1.1B parameters)\n\nThe “/14” indicates a patch size of 14×14 pixels. These patches are how images are tokenized before being processed by the transformer.\n\n\n\nDINOv2 incorporates several architectural improvements over the original DINO:\n\nImproved Layer Normalization: Uses a modified version of layer normalization that enhances stability during training at scale.\nSwiGLU Activation: Replaces standard ReLU or GELU activations with SwiGLU, which improves representation quality.\nRegister Tokens: Additional learnable tokens (alongside the [CLS] token) that capture different aspects of image information.\nAttention Bias: Incorporates relative position embeddings through attention biases instead of absolute positional encodings.\nPost-Normalization: Places the layer normalization after the multi-head attention and feed-forward blocks rather than before them."
  },
  {
    "objectID": "posts/dino-v2-explained/index.html#training-methodology-self-distillation-framework",
    "href": "posts/dino-v2-explained/index.html#training-methodology-self-distillation-framework",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "DINOv2’s training methodology centers around self-distillation, where a model essentially teaches itself. This is implemented through a student-teacher framework:\n\n\n\nStudent Network: The network being trained, updated via backpropagation\nTeacher Network: An exponential moving average (EMA) of the student’s parameters\nBoth networks share the same architecture but different parameters\n\nThis approach creates a moving target that continuously evolves as training progresses, preventing trivial solutions where the network collapses to outputting the same representation for all inputs.\n\n\n\nA key component of DINOv2’s training is its sophisticated multi-crop approach:\n\nGlobal Views: Two large crops covering significant portions of the image (224×224 pixels)\nLocal Views: Multiple smaller crops capturing image details (96×96 pixels)\n\nThe student network processes both global and local views, while the teacher network only processes global views. This forces the model to learn both global context and local details.\n\n\n\nThe training objective is a cross-entropy loss that encourages the student’s output distribution for local views to match the teacher’s output distribution for global views of the same image. Mathematically:\nL = H(Pt(g), Ps(l))\nWhere:\n\nH is the cross-entropy\nPt(g) is the teacher’s prediction on global views\nPs(l) is the student’s prediction on local views\n\nThe teacher’s outputs are sharpened using a temperature parameter that gradually decreases throughout training, making the targets increasingly focused on specific features."
  },
  {
    "objectID": "posts/dino-v2-explained/index.html#data-curation-and-processing",
    "href": "posts/dino-v2-explained/index.html#data-curation-and-processing",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "DINOv2’s impressive performance comes not just from architecture but from meticulous data preparation:\n\n\nThe researchers curated a high-quality dataset of 142 million images from publicly available sources, with careful filtering to remove:\n\nDuplicate images\nLow-quality content\nInappropriate material\nText-heavy images\nHuman faces\n\n\n\n\nDuring training, DINOv2 employs a robust augmentation strategy:\n\nRandom resized cropping: Different sized views of the same image\nRandom horizontal flips: Mirroring images horizontally\nColor jittering: Altering brightness, contrast, saturation, and hue\nGaussian blur: Adding controlled blur to some views\nSolarization: Inverting pixels above a threshold (applied selectively)\n\nThese augmentations create diverse views while preserving the semantic content, forcing the model to learn invariance to these transformations."
  },
  {
    "objectID": "posts/dino-v2-explained/index.html#distributed-training-strategy",
    "href": "posts/dino-v2-explained/index.html#distributed-training-strategy",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "Training a model of DINOv2’s scale requires sophisticated distributed computing approaches:\n\n\n\nOptimizer: AdamW with a cosine learning rate schedule\nGradient Accumulation: Used to handle effectively larger batch sizes\nMixed Precision: FP16 calculations to speed up training\nSharding: Model parameters distributed across multiple GPUs\n\n\n\n\nDINOv2 uses enormous effective batch sizes (up to 65,536 images) by leveraging distributed training across hundreds of GPUs. This large batch size is crucial for learning high-quality representations."
  },
  {
    "objectID": "posts/dino-v2-explained/index.html#regularization-techniques",
    "href": "posts/dino-v2-explained/index.html#regularization-techniques",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "To prevent representation collapse and ensure diverse, meaningful features, DINOv2 employs:\n\nCentering: Ensuring the average output across the batch remains close to zero\nSharpening: Gradually decreasing the temperature parameter of the teacher’s softmax\nDALL-E VAE Integration: Using a pre-trained DALL-E VAE to improve representation quality\nWeight Decay: Applied differently to various components of the model"
  },
  {
    "objectID": "posts/dino-v2-explained/index.html#feature-extraction-and-deployment",
    "href": "posts/dino-v2-explained/index.html#feature-extraction-and-deployment",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "After training, DINOv2 can be used in different ways:\n\n\n\n[CLS] Token: The class token representation serves as a global image descriptor\nRegister Tokens: Multiple specialized tokens that capture different aspects of the image\nPatch Tokens: Local features corresponding to specific regions of the image\n\n\n\n\nThe researchers also created smaller, distilled versions of DINOv2 that maintain much of the performance while requiring significantly fewer computational resources for deployment."
  },
  {
    "objectID": "posts/dino-v2-explained/index.html#conclusion",
    "href": "posts/dino-v2-explained/index.html#conclusion",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "DINOv2 represents a remarkable achievement in self-supervised visual learning. Its sophisticated architecture and training methodology enable it to learn general-purpose visual features that transfer exceptionally well across diverse tasks. The careful balance of architectural innovations, data curation, and training techniques creates a visual representation system that approaches the versatility and power that we’ve seen in large language models.\nThe success of DINOv2 highlights how self-supervised learning can leverage vast amounts of unlabeled data to create foundation models for computer vision that may eventually eliminate the need for task-specific supervised training in many applications."
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html",
    "href": "posts/data-visualization-tutorial/index.html",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "This guide compares three popular Python data visualization libraries: Matplotlib, Seaborn, and Altair (Vega-Altair). Each library has its own strengths, weaknesses, and ideal use cases. This comparison will help you choose the right tool for your specific visualization needs.\n\n\n\n\n\n\n\n\n\n\n\nFeature\nMatplotlib\nSeaborn\nAltair\n\n\n\n\nRelease Year\n2003\n2013\n2016\n\n\nFoundation\nStandalone\nBuilt on Matplotlib\nBased on Vega-Lite\n\n\nPhilosophy\nImperative\nStatistical\nDeclarative\n\n\nAbstraction Level\nLow\nMedium\nHigh\n\n\nLearning Curve\nSteep\nModerate\nGentle\n\n\nCode Verbosity\nHigh\nMedium\nLow\n\n\nCustomization\nExtensive\nGood\nLimited\n\n\nStatistical Integration\nManual\nBuilt-in\nGood\n\n\nInteractive Features\nLimited\nLimited\nExcellent\n\n\nPerformance with Large Data\nGood\nModerate\nLimited\n\n\nCommunity & Resources\nExtensive\nGood\nGrowing\n\n\n\n\n\n\nMatplotlib is the foundational plotting library in Python’s data visualization ecosystem.\n\n\n\nFine-grained control: Almost every aspect of a visualization can be customized\nVersatility: Can create virtually any type of static plot\nMaturity: Extensive documentation and community support\nEcosystem integration: Many libraries integrate with or build upon Matplotlib\nPerformance: Handles large datasets well\n\n\n\n\n\nVerbose syntax: Requires many lines of code for complex visualizations\nSteep learning curve: Many functions and parameters to learn\nDefault aesthetics: Basic default styling (though this has improved)\nLimited interactivity: Primarily designed for static plots\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\n# Create figure and axis\nfig, ax = plt.subplots(figsize=(8, 4))\n\n# Plot data\nax.plot(x, y, label='Sine Wave')\n\n# Add grid, legend, title and labels\nax.grid(True)\nax.set_xlabel('X-axis')\nax.set_ylabel('Y-axis')\nax.set_title('Simple Sine Wave Plot')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nYou need complete control over every aspect of your visualization\nYou’re creating complex, publication-quality figures\nYou’re working with specialized plot types not available in higher-level libraries\nYou need to integrate with many other Python libraries\nYou’re working with large datasets\n\n\n\n\n\nSeaborn is a statistical visualization library built on top of Matplotlib.\n\n\n\nAesthetic defaults: Beautiful out-of-the-box styling\nStatistical integration: Built-in support for statistical visualizations\nDataset awareness: Works well with pandas DataFrames\nSimplicity: Fewer lines of code than Matplotlib for common plots\nHigh-level functions: Specialized plots like lmplot, catplot, etc.\n\n\n\n\n\nLimited customization: Some advanced customizations require falling back to Matplotlib\nPerformance: Can be slower with very large datasets\nRestricted scope: Focused on statistical visualization, not general-purpose plotting\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Create sample data\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.2, size=len(x))\ndata = pd.DataFrame({'x': x, 'y': y})\n\n# Set the aesthetic style\nsns.set_theme(style=\"whitegrid\")\n\n# Create the plot\nplt.figure(figsize=(8, 4))\nsns.lineplot(data=data, x='x', y='y', label='Noisy Sine Wave')\nsns.regplot(data=data, x='x', y='y', scatter=False, label='Regression Line')\n\n# Add title and labels\nplt.title('Seaborn Line Plot with Regression')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nYou want attractive visualizations with minimal code\nYou’re performing statistical analysis\nYou’re working with pandas DataFrames\nYou’re creating common statistical plots (distributions, relationships, categorical plots)\nYou want the power of Matplotlib with a simpler interface\n\n\n\n\n\nAltair is a declarative statistical visualization library based on Vega-Lite.\n\n\n\nDeclarative approach: Focus on what to visualize, not how to draw it\nConcise syntax: Very readable, clear code\nLayered grammar of graphics: Intuitive composition of plots\nInteractive visualizations: Built-in support for interactive features\nJSON output: Visualizations can be saved as JSON specifications\n\n\n\n\n\nPerformance limitations: Not ideal for very large datasets (&gt;5000 points)\nLimited customization: Less fine-grained control than Matplotlib\nLearning curve: Different paradigm from traditional plotting libraries\nBrowser dependency: Uses JavaScript rendering for advanced features\n\n\n\n\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\n# Create sample data\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.2, size=len(x))\ndata = pd.DataFrame({'x': x, 'y': y})\n\n# Create a simple scatter plot with interactive tooltips\nchart = alt.Chart(data).mark_circle().encode(\n    x='x',\n    y='y',\n    tooltip=['x', 'y']\n).properties(\n    width=600,\n    height=300,\n    title='Interactive Altair Scatter Plot'\n).interactive()\n\n# Add a regression line\nregression = alt.Chart(data).transform_regression(\n    'x', 'y'\n).mark_line(color='red').encode(\n    x='x',\n    y='y'\n)\n\n# Combine the plots\nfinal_chart = chart + regression\n\n# Display the chart\nfinal_chart\n\n\n\n\n\n\n\n\n\n\n\nYou want interactive visualizations\nYou prefer a declarative approach to visualization\nYou’re working with small to medium-sized datasets\nYou want to publish visualizations on the web\nYou appreciate a consistent grammar of graphics\n\n\n\n\n\n\n\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.random.randn(100)\ny = np.random.randn(100)\n\nplt.figure(figsize=(8, 6))\nplt.scatter(x, y, alpha=0.7)\nplt.title('Matplotlib Scatter Plot')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndata = pd.DataFrame({\n    'x': np.random.randn(100),\n    'y': np.random.randn(100)\n})\n\nsns.set_theme(style=\"whitegrid\")\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=data, x='x', y='y', alpha=0.7)\nplt.title('Seaborn Scatter Plot')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame({\n    'x': np.random.randn(100),\n    'y': np.random.randn(100)\n})\n\nalt.Chart(data).mark_circle(opacity=0.7).encode(\n    x='x',\n    y='y'\n).properties(\n    width=500,\n    height=400,\n    title='Altair Scatter Plot'\n)\n\n\n\n\n\n\n\n\n\n\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.randn(1000)\n\nplt.figure(figsize=(8, 6))\nplt.hist(data, bins=30, alpha=0.7, edgecolor='black')\nplt.title('Matplotlib Histogram')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.randn(1000)\n\nsns.set_theme(style=\"whitegrid\")\nplt.figure(figsize=(8, 6))\nsns.histplot(data=data, bins=30, kde=True)\nplt.title('Seaborn Histogram with KDE')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame({'value': np.random.randn(1000)})\n\nalt.Chart(data).mark_bar().encode(\n    alt.X('value', bin=alt.Bin(maxbins=30)),\n    y='count()'\n).properties(\n    width=500,\n    height=400,\n    title='Altair Histogram'\n)\n\n\n\n\n\n\n\n\n\n\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny1 = np.sin(x)\ny2 = np.cos(x)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y1, label='Sine')\nplt.plot(x, y2, label='Cosine')\nplt.title('Matplotlib Line Plot')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nx = np.linspace(0, 10, 100)\ndata = pd.DataFrame({\n    'x': np.concatenate([x, x]),\n    'y': np.concatenate([np.sin(x), np.cos(x)]),\n    'function': ['Sine']*100 + ['Cosine']*100\n})\n\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=data, x='x', y='y', hue='function')\nplt.title('Seaborn Line Plot')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ndata = pd.DataFrame({\n    'x': np.concatenate([x, x]),\n    'y': np.concatenate([np.sin(x), np.cos(x)]),\n    'function': ['Sine']*100 + ['Cosine']*100\n})\n\nalt.Chart(data).mark_line().encode(\n    x='x',\n    y='y',\n    color='function'\n).properties(\n    width=600,\n    height=400,\n    title='Altair Line Plot'\n)\n\n\n\n\n\n\n\n\n\n\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.rand(10, 12)\n\nplt.figure(figsize=(10, 8))\nplt.imshow(data, cmap='viridis')\nplt.colorbar(label='Value')\nplt.title('Matplotlib Heatmap')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.rand(10, 12)\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(data, annot=True, cmap='viridis', fmt='.2f')\nplt.title('Seaborn Heatmap')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\n# Create sample data\ndata = np.random.rand(10, 12)\ndf = pd.DataFrame(data)\n\n# Reshape for Altair\ndf_long = df.reset_index().melt(id_vars='index')\ndf_long.columns = ['y', 'x', 'value']\n\nalt.Chart(df_long).mark_rect().encode(\n    x='x:O',\n    y='y:O',\n    color='value:Q'\n).properties(\n    width=500,\n    height=400,\n    title='Altair Heatmap'\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou need complete control over every detail of your visualization\nYou’re creating complex, custom plots\nYour visualizations will be included in scientific publications\nYou’re working with very large datasets\nYou need to create animations or specialized chart types\n\n\n\n\n\nYou want attractive plots with minimal code\nYou’re performing statistical analysis\nYou want to create common statistical plots quickly\nYou need to visualize relationships between variables\nYou want good-looking defaults but still need some customization\n\n\n\n\n\nYou want interactive visualizations\nYou prefer a declarative approach to visualization\nYou want concise, readable code\nYou’re creating dashboards or web-based visualizations\nYou’re working with small to medium-sized datasets\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\n# Create sample data\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'x': np.random.normal(0, 1, 100),\n    'y': np.random.normal(0, 1, 100),\n    'category': np.random.choice(['A', 'B', 'C'], 100)\n})\n\n# Create a figure with Matplotlib\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Use Seaborn for the main plot\nsns.scatterplot(data=data, x='x', y='y', hue='category', ax=ax)\n\n# Add Matplotlib customizations\nax.set_title('Combining Matplotlib and Seaborn', fontsize=16)\nax.grid(True, linestyle='--', alpha=0.7)\nax.set_xlabel('X Variable', fontsize=12)\nax.set_ylabel('Y Variable', fontsize=12)\n\n# Add annotations using Matplotlib\nax.annotate('Interesting Point', xy=(-1, 1), xytext=(-2, 1.5),\n            arrowprops=dict(facecolor='black', shrink=0.05))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\n# Create sample data with pandas\nnp.random.seed(42)\ndf = pd.DataFrame({\n    'date': pd.date_range('2023-01-01', periods=100),\n    'value': np.cumsum(np.random.randn(100)),\n    'category': np.random.choice(['Group A', 'Group B'], 100)\n})\n\n# Use pandas to prepare the data\ndf['month'] = df['date'].dt.month\nmonthly_avg = df.groupby(['month', 'category'])['value'].mean().reset_index()\n\n# Create the Altair visualization\nchart = alt.Chart(monthly_avg).mark_line(point=True).encode(\n    x='month:O',\n    y='value:Q',\n    color='category:N',\n    tooltip=['month', 'value', 'category']\n).properties(\n    width=600,\n    height=400,\n    title='Monthly Averages by Category'\n).interactive()\n\nchart\n\n\n\n\n\n\n\n\n\n\n\nFor libraries like Matplotlib, Seaborn, and Altair, performance can vary widely depending on the size of your dataset and the complexity of your visualization. Here’s a general overview:\n\n\n\nAll three libraries perform well\nAltair might have slightly more overhead due to its JSON specification generation\n\n\n\n\n\nMatplotlib and Seaborn continue to perform well\nAltair starts to slow down but remains usable\n\n\n\n\n\nMatplotlib performs best for large static visualizations\nSeaborn becomes slower as it adds statistical computations\nAltair significantly slows down and may require data aggregation\n\n\n\n\n\nMatplotlib: Use plot() instead of scatter() for line plots, or try hexbin() for density plots\nSeaborn: Use sample() or aggregation methods before plotting\nAltair: Use transform_sample() or pre-aggregate your data\n\n\n\n\n\nThe Python visualization ecosystem offers tools for every need, from low-level control to high-level abstraction:\n\nMatplotlib provides ultimate flexibility and control but requires more code and knowledge\nSeaborn offers a perfect middle ground with statistical integration and clean defaults\nAltair delivers a concise, declarative approach with built-in interactivity\n\nRather than picking just one library, consider becoming familiar with all three and selecting the right tool for each visualization task. Many data scientists use a combination of these libraries, leveraging the strengths of each one as needed.\nFor those just starting, Seaborn provides a gentle entry point with attractive results for common visualization needs. As your skills advance, you can incorporate Matplotlib for customization and Altair for interactive visualizations."
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#quick-reference-comparison",
    "href": "posts/data-visualization-tutorial/index.html#quick-reference-comparison",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "Feature\nMatplotlib\nSeaborn\nAltair\n\n\n\n\nRelease Year\n2003\n2013\n2016\n\n\nFoundation\nStandalone\nBuilt on Matplotlib\nBased on Vega-Lite\n\n\nPhilosophy\nImperative\nStatistical\nDeclarative\n\n\nAbstraction Level\nLow\nMedium\nHigh\n\n\nLearning Curve\nSteep\nModerate\nGentle\n\n\nCode Verbosity\nHigh\nMedium\nLow\n\n\nCustomization\nExtensive\nGood\nLimited\n\n\nStatistical Integration\nManual\nBuilt-in\nGood\n\n\nInteractive Features\nLimited\nLimited\nExcellent\n\n\nPerformance with Large Data\nGood\nModerate\nLimited\n\n\nCommunity & Resources\nExtensive\nGood\nGrowing"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#matplotlib",
    "href": "posts/data-visualization-tutorial/index.html#matplotlib",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "Matplotlib is the foundational plotting library in Python’s data visualization ecosystem.\n\n\n\nFine-grained control: Almost every aspect of a visualization can be customized\nVersatility: Can create virtually any type of static plot\nMaturity: Extensive documentation and community support\nEcosystem integration: Many libraries integrate with or build upon Matplotlib\nPerformance: Handles large datasets well\n\n\n\n\n\nVerbose syntax: Requires many lines of code for complex visualizations\nSteep learning curve: Many functions and parameters to learn\nDefault aesthetics: Basic default styling (though this has improved)\nLimited interactivity: Primarily designed for static plots\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\n# Create figure and axis\nfig, ax = plt.subplots(figsize=(8, 4))\n\n# Plot data\nax.plot(x, y, label='Sine Wave')\n\n# Add grid, legend, title and labels\nax.grid(True)\nax.set_xlabel('X-axis')\nax.set_ylabel('Y-axis')\nax.set_title('Simple Sine Wave Plot')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nYou need complete control over every aspect of your visualization\nYou’re creating complex, publication-quality figures\nYou’re working with specialized plot types not available in higher-level libraries\nYou need to integrate with many other Python libraries\nYou’re working with large datasets"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#seaborn",
    "href": "posts/data-visualization-tutorial/index.html#seaborn",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "Seaborn is a statistical visualization library built on top of Matplotlib.\n\n\n\nAesthetic defaults: Beautiful out-of-the-box styling\nStatistical integration: Built-in support for statistical visualizations\nDataset awareness: Works well with pandas DataFrames\nSimplicity: Fewer lines of code than Matplotlib for common plots\nHigh-level functions: Specialized plots like lmplot, catplot, etc.\n\n\n\n\n\nLimited customization: Some advanced customizations require falling back to Matplotlib\nPerformance: Can be slower with very large datasets\nRestricted scope: Focused on statistical visualization, not general-purpose plotting\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Create sample data\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.2, size=len(x))\ndata = pd.DataFrame({'x': x, 'y': y})\n\n# Set the aesthetic style\nsns.set_theme(style=\"whitegrid\")\n\n# Create the plot\nplt.figure(figsize=(8, 4))\nsns.lineplot(data=data, x='x', y='y', label='Noisy Sine Wave')\nsns.regplot(data=data, x='x', y='y', scatter=False, label='Regression Line')\n\n# Add title and labels\nplt.title('Seaborn Line Plot with Regression')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nYou want attractive visualizations with minimal code\nYou’re performing statistical analysis\nYou’re working with pandas DataFrames\nYou’re creating common statistical plots (distributions, relationships, categorical plots)\nYou want the power of Matplotlib with a simpler interface"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#altair-vega-altair",
    "href": "posts/data-visualization-tutorial/index.html#altair-vega-altair",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "Altair is a declarative statistical visualization library based on Vega-Lite.\n\n\n\nDeclarative approach: Focus on what to visualize, not how to draw it\nConcise syntax: Very readable, clear code\nLayered grammar of graphics: Intuitive composition of plots\nInteractive visualizations: Built-in support for interactive features\nJSON output: Visualizations can be saved as JSON specifications\n\n\n\n\n\nPerformance limitations: Not ideal for very large datasets (&gt;5000 points)\nLimited customization: Less fine-grained control than Matplotlib\nLearning curve: Different paradigm from traditional plotting libraries\nBrowser dependency: Uses JavaScript rendering for advanced features\n\n\n\n\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\n# Create sample data\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.2, size=len(x))\ndata = pd.DataFrame({'x': x, 'y': y})\n\n# Create a simple scatter plot with interactive tooltips\nchart = alt.Chart(data).mark_circle().encode(\n    x='x',\n    y='y',\n    tooltip=['x', 'y']\n).properties(\n    width=600,\n    height=300,\n    title='Interactive Altair Scatter Plot'\n).interactive()\n\n# Add a regression line\nregression = alt.Chart(data).transform_regression(\n    'x', 'y'\n).mark_line(color='red').encode(\n    x='x',\n    y='y'\n)\n\n# Combine the plots\nfinal_chart = chart + regression\n\n# Display the chart\nfinal_chart\n\n\n\n\n\n\n\n\n\n\n\nYou want interactive visualizations\nYou prefer a declarative approach to visualization\nYou’re working with small to medium-sized datasets\nYou want to publish visualizations on the web\nYou appreciate a consistent grammar of graphics"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#common-visualization-types-comparison",
    "href": "posts/data-visualization-tutorial/index.html#common-visualization-types-comparison",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "Matplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.random.randn(100)\ny = np.random.randn(100)\n\nplt.figure(figsize=(8, 6))\nplt.scatter(x, y, alpha=0.7)\nplt.title('Matplotlib Scatter Plot')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndata = pd.DataFrame({\n    'x': np.random.randn(100),\n    'y': np.random.randn(100)\n})\n\nsns.set_theme(style=\"whitegrid\")\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=data, x='x', y='y', alpha=0.7)\nplt.title('Seaborn Scatter Plot')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame({\n    'x': np.random.randn(100),\n    'y': np.random.randn(100)\n})\n\nalt.Chart(data).mark_circle(opacity=0.7).encode(\n    x='x',\n    y='y'\n).properties(\n    width=500,\n    height=400,\n    title='Altair Scatter Plot'\n)\n\n\n\n\n\n\n\n\n\n\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.randn(1000)\n\nplt.figure(figsize=(8, 6))\nplt.hist(data, bins=30, alpha=0.7, edgecolor='black')\nplt.title('Matplotlib Histogram')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.randn(1000)\n\nsns.set_theme(style=\"whitegrid\")\nplt.figure(figsize=(8, 6))\nsns.histplot(data=data, bins=30, kde=True)\nplt.title('Seaborn Histogram with KDE')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame({'value': np.random.randn(1000)})\n\nalt.Chart(data).mark_bar().encode(\n    alt.X('value', bin=alt.Bin(maxbins=30)),\n    y='count()'\n).properties(\n    width=500,\n    height=400,\n    title='Altair Histogram'\n)\n\n\n\n\n\n\n\n\n\n\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny1 = np.sin(x)\ny2 = np.cos(x)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y1, label='Sine')\nplt.plot(x, y2, label='Cosine')\nplt.title('Matplotlib Line Plot')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nx = np.linspace(0, 10, 100)\ndata = pd.DataFrame({\n    'x': np.concatenate([x, x]),\n    'y': np.concatenate([np.sin(x), np.cos(x)]),\n    'function': ['Sine']*100 + ['Cosine']*100\n})\n\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=data, x='x', y='y', hue='function')\nplt.title('Seaborn Line Plot')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ndata = pd.DataFrame({\n    'x': np.concatenate([x, x]),\n    'y': np.concatenate([np.sin(x), np.cos(x)]),\n    'function': ['Sine']*100 + ['Cosine']*100\n})\n\nalt.Chart(data).mark_line().encode(\n    x='x',\n    y='y',\n    color='function'\n).properties(\n    width=600,\n    height=400,\n    title='Altair Line Plot'\n)\n\n\n\n\n\n\n\n\n\n\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.rand(10, 12)\n\nplt.figure(figsize=(10, 8))\nplt.imshow(data, cmap='viridis')\nplt.colorbar(label='Value')\nplt.title('Matplotlib Heatmap')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.rand(10, 12)\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(data, annot=True, cmap='viridis', fmt='.2f')\nplt.title('Seaborn Heatmap')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\n# Create sample data\ndata = np.random.rand(10, 12)\ndf = pd.DataFrame(data)\n\n# Reshape for Altair\ndf_long = df.reset_index().melt(id_vars='index')\ndf_long.columns = ['y', 'x', 'value']\n\nalt.Chart(df_long).mark_rect().encode(\n    x='x:O',\n    y='y:O',\n    color='value:Q'\n).properties(\n    width=500,\n    height=400,\n    title='Altair Heatmap'\n)"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#decision-framework-for-choosing-a-library",
    "href": "posts/data-visualization-tutorial/index.html#decision-framework-for-choosing-a-library",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "You need complete control over every detail of your visualization\nYou’re creating complex, custom plots\nYour visualizations will be included in scientific publications\nYou’re working with very large datasets\nYou need to create animations or specialized chart types\n\n\n\n\n\nYou want attractive plots with minimal code\nYou’re performing statistical analysis\nYou want to create common statistical plots quickly\nYou need to visualize relationships between variables\nYou want good-looking defaults but still need some customization\n\n\n\n\n\nYou want interactive visualizations\nYou prefer a declarative approach to visualization\nYou want concise, readable code\nYou’re creating dashboards or web-based visualizations\nYou’re working with small to medium-sized datasets"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#integration-examples",
    "href": "posts/data-visualization-tutorial/index.html#integration-examples",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\n# Create sample data\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'x': np.random.normal(0, 1, 100),\n    'y': np.random.normal(0, 1, 100),\n    'category': np.random.choice(['A', 'B', 'C'], 100)\n})\n\n# Create a figure with Matplotlib\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Use Seaborn for the main plot\nsns.scatterplot(data=data, x='x', y='y', hue='category', ax=ax)\n\n# Add Matplotlib customizations\nax.set_title('Combining Matplotlib and Seaborn', fontsize=16)\nax.grid(True, linestyle='--', alpha=0.7)\nax.set_xlabel('X Variable', fontsize=12)\nax.set_ylabel('Y Variable', fontsize=12)\n\n# Add annotations using Matplotlib\nax.annotate('Interesting Point', xy=(-1, 1), xytext=(-2, 1.5),\n            arrowprops=dict(facecolor='black', shrink=0.05))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\n# Create sample data with pandas\nnp.random.seed(42)\ndf = pd.DataFrame({\n    'date': pd.date_range('2023-01-01', periods=100),\n    'value': np.cumsum(np.random.randn(100)),\n    'category': np.random.choice(['Group A', 'Group B'], 100)\n})\n\n# Use pandas to prepare the data\ndf['month'] = df['date'].dt.month\nmonthly_avg = df.groupby(['month', 'category'])['value'].mean().reset_index()\n\n# Create the Altair visualization\nchart = alt.Chart(monthly_avg).mark_line(point=True).encode(\n    x='month:O',\n    y='value:Q',\n    color='category:N',\n    tooltip=['month', 'value', 'category']\n).properties(\n    width=600,\n    height=400,\n    title='Monthly Averages by Category'\n).interactive()\n\nchart"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#performance-comparison",
    "href": "posts/data-visualization-tutorial/index.html#performance-comparison",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "For libraries like Matplotlib, Seaborn, and Altair, performance can vary widely depending on the size of your dataset and the complexity of your visualization. Here’s a general overview:\n\n\n\nAll three libraries perform well\nAltair might have slightly more overhead due to its JSON specification generation\n\n\n\n\n\nMatplotlib and Seaborn continue to perform well\nAltair starts to slow down but remains usable\n\n\n\n\n\nMatplotlib performs best for large static visualizations\nSeaborn becomes slower as it adds statistical computations\nAltair significantly slows down and may require data aggregation\n\n\n\n\n\nMatplotlib: Use plot() instead of scatter() for line plots, or try hexbin() for density plots\nSeaborn: Use sample() or aggregation methods before plotting\nAltair: Use transform_sample() or pre-aggregate your data"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#conclusion",
    "href": "posts/data-visualization-tutorial/index.html#conclusion",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "The Python visualization ecosystem offers tools for every need, from low-level control to high-level abstraction:\n\nMatplotlib provides ultimate flexibility and control but requires more code and knowledge\nSeaborn offers a perfect middle ground with statistical integration and clean defaults\nAltair delivers a concise, declarative approach with built-in interactivity\n\nRather than picking just one library, consider becoming familiar with all three and selecting the right tool for each visualization task. Many data scientists use a combination of these libraries, leveraging the strengths of each one as needed.\nFor those just starting, Seaborn provides a gentle entry point with attractive results for common visualization needs. As your skills advance, you can incorporate Matplotlib for customization and Altair for interactive visualizations."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "👁️ Welcome to My Computer Vision Blog!\n\nThis is the first post in this blog.\nHello and welcome!\nI’m thrilled to kick off this blog dedicated to exploring the fascinating world of computer vision — a field where machines learn to see, interpret, and understand the visual world around us. Whether you’re a seasoned AI researcher, an aspiring developer, or simply curious about how technology can “see,” you’ll find something valuable here.\nFrom image processing techniques to deep learning breakthroughs, from real-world applications to hands-on tutorials — this blog will cover it all. My goal is to make computer vision approachable, insightful, and exciting for everyone.\nSo, whether you’re here to learn, build, or stay on top of the latest innovations, I’m glad to have you along for the journey. Let’s dive into the visual future together!\nStay tuned, and let’s make machines see the world like never before. 🚀\nCheers,  Krishna"
  },
  {
    "objectID": "posts/python-pi/index.html",
    "href": "posts/python-pi/index.html",
    "title": "Python 3.14: The Next Evolution in Python Development",
    "section": "",
    "text": "Python continues its steady march forward with the anticipated release of Python 3.14, marking another significant milestone in the language’s evolution. As the Python Software Foundation maintains its annual release cycle, Python 3.14 represents the ongoing commitment to improving performance, developer experience, and language capabilities.\n\n\nFollowing Python’s established release schedule, Python 3.14 continues the pattern of annual major releases that began with Python 3.9. The development process follows the standard Python Enhancement Proposal (PEP) system, where community members propose, discuss, and refine new features before implementation.\nThe release represents months of collaborative work from core developers, contributors, and the broader Python community, focusing on backward compatibility while introducing meaningful improvements to the language.\n\n\n\nPython 3.14 builds upon the performance improvements introduced in recent versions. The development team has continued optimizing the interpreter, with particular attention to:\n\nMemory Management: Further refinements to Python’s garbage collection system, reducing memory overhead and improving allocation efficiency\nBytecode Optimization: Enhanced compilation processes that generate more efficient bytecode\nStandard Library Performance: Optimizations to frequently-used modules and functions\n\nThese improvements contribute to faster execution times and reduced resource consumption, particularly beneficial for long-running applications and data-intensive workloads.\n\n\n\nWhile maintaining Python’s philosophy of readability and simplicity, Python 3.14 introduces carefully considered language enhancements:\n\n\nThe static typing ecosystem continues to mature, with enhancements to type hints and better integration between runtime and static analysis tools. These improvements make it easier for developers to write type-safe code while maintaining Python’s dynamic nature.\n\n\n\nSeveral quality-of-life improvements have been introduced to make Python development more efficient and enjoyable. Error messages have been further refined to provide clearer guidance, and debugging capabilities have been enhanced.\n\n\n\n\nPython’s “batteries included” philosophy remains strong in 3.14, with updates across the standard library:\n\nNew Modules: Introduction of modules addressing modern development needs\nDeprecated Module Updates: Continued modernization of older modules while maintaining backward compatibility\nSecurity Enhancements: Strengthened cryptographic modules and security-related functionality\n\n\n\n\nPython 3.14 maintains the project’s commitment to stability. Any breaking changes are minimal and well-documented, with clear migration paths provided. The development team continues to balance innovation with the needs of existing codebases.\nMost Python 3.13 code should run without modification on Python 3.14, though developers are encouraged to review the official migration guide for any project-specific considerations.\n\n\n\nThe release reflects the vibrant Python ecosystem, with contributions from developers worldwide. The Python Software Foundation’s governance model ensures that changes serve the broad community while maintaining the language’s core principles.\n\n\n\nPython 3.14 sets the foundation for future developments while addressing current needs. The development team continues to explore areas such as:\n\nPerformance optimization strategies\nImproved tooling integration\nEnhanced support for modern development practices\nContinued evolution of the type system\n\n\n\n\nDevelopers interested in trying Python 3.14 can download it from the official Python website. The comprehensive documentation includes migration notes, feature explanations, and examples to help developers transition smoothly.\nFor production environments, thorough testing is recommended before upgrading, though the Python team’s commitment to stability makes the transition process straightforward for most applications.\n\n\n\nPython 3.14 represents another solid step forward for the language, balancing innovation with stability. The release demonstrates the Python community’s continued dedication to creating a language that remains accessible to beginners while powerful enough for the most demanding applications.\nAs Python approaches its fourth decade, releases like 3.14 show that the language continues to evolve thoughtfully, maintaining its position as one of the world’s most popular and versatile programming languages.\n\nFor the most current information about Python 3.14, including detailed release notes and migration guides, visit the official Python documentation at python.org."
  },
  {
    "objectID": "posts/python-pi/index.html#release-timeline-and-development",
    "href": "posts/python-pi/index.html#release-timeline-and-development",
    "title": "Python 3.14: The Next Evolution in Python Development",
    "section": "",
    "text": "Following Python’s established release schedule, Python 3.14 continues the pattern of annual major releases that began with Python 3.9. The development process follows the standard Python Enhancement Proposal (PEP) system, where community members propose, discuss, and refine new features before implementation.\nThe release represents months of collaborative work from core developers, contributors, and the broader Python community, focusing on backward compatibility while introducing meaningful improvements to the language."
  },
  {
    "objectID": "posts/python-pi/index.html#performance-enhancements",
    "href": "posts/python-pi/index.html#performance-enhancements",
    "title": "Python 3.14: The Next Evolution in Python Development",
    "section": "",
    "text": "Python 3.14 builds upon the performance improvements introduced in recent versions. The development team has continued optimizing the interpreter, with particular attention to:\n\nMemory Management: Further refinements to Python’s garbage collection system, reducing memory overhead and improving allocation efficiency\nBytecode Optimization: Enhanced compilation processes that generate more efficient bytecode\nStandard Library Performance: Optimizations to frequently-used modules and functions\n\nThese improvements contribute to faster execution times and reduced resource consumption, particularly beneficial for long-running applications and data-intensive workloads."
  },
  {
    "objectID": "posts/python-pi/index.html#language-features-and-syntax",
    "href": "posts/python-pi/index.html#language-features-and-syntax",
    "title": "Python 3.14: The Next Evolution in Python Development",
    "section": "",
    "text": "While maintaining Python’s philosophy of readability and simplicity, Python 3.14 introduces carefully considered language enhancements:\n\n\nThe static typing ecosystem continues to mature, with enhancements to type hints and better integration between runtime and static analysis tools. These improvements make it easier for developers to write type-safe code while maintaining Python’s dynamic nature.\n\n\n\nSeveral quality-of-life improvements have been introduced to make Python development more efficient and enjoyable. Error messages have been further refined to provide clearer guidance, and debugging capabilities have been enhanced."
  },
  {
    "objectID": "posts/python-pi/index.html#standard-library-updates",
    "href": "posts/python-pi/index.html#standard-library-updates",
    "title": "Python 3.14: The Next Evolution in Python Development",
    "section": "",
    "text": "Python’s “batteries included” philosophy remains strong in 3.14, with updates across the standard library:\n\nNew Modules: Introduction of modules addressing modern development needs\nDeprecated Module Updates: Continued modernization of older modules while maintaining backward compatibility\nSecurity Enhancements: Strengthened cryptographic modules and security-related functionality"
  },
  {
    "objectID": "posts/python-pi/index.html#breaking-changes-and-migration",
    "href": "posts/python-pi/index.html#breaking-changes-and-migration",
    "title": "Python 3.14: The Next Evolution in Python Development",
    "section": "",
    "text": "Python 3.14 maintains the project’s commitment to stability. Any breaking changes are minimal and well-documented, with clear migration paths provided. The development team continues to balance innovation with the needs of existing codebases.\nMost Python 3.13 code should run without modification on Python 3.14, though developers are encouraged to review the official migration guide for any project-specific considerations."
  },
  {
    "objectID": "posts/python-pi/index.html#community-impact",
    "href": "posts/python-pi/index.html#community-impact",
    "title": "Python 3.14: The Next Evolution in Python Development",
    "section": "",
    "text": "The release reflects the vibrant Python ecosystem, with contributions from developers worldwide. The Python Software Foundation’s governance model ensures that changes serve the broad community while maintaining the language’s core principles."
  },
  {
    "objectID": "posts/python-pi/index.html#looking-forward",
    "href": "posts/python-pi/index.html#looking-forward",
    "title": "Python 3.14: The Next Evolution in Python Development",
    "section": "",
    "text": "Python 3.14 sets the foundation for future developments while addressing current needs. The development team continues to explore areas such as:\n\nPerformance optimization strategies\nImproved tooling integration\nEnhanced support for modern development practices\nContinued evolution of the type system"
  },
  {
    "objectID": "posts/python-pi/index.html#getting-started-with-python-3.14",
    "href": "posts/python-pi/index.html#getting-started-with-python-3.14",
    "title": "Python 3.14: The Next Evolution in Python Development",
    "section": "",
    "text": "Developers interested in trying Python 3.14 can download it from the official Python website. The comprehensive documentation includes migration notes, feature explanations, and examples to help developers transition smoothly.\nFor production environments, thorough testing is recommended before upgrading, though the Python team’s commitment to stability makes the transition process straightforward for most applications."
  },
  {
    "objectID": "posts/python-pi/index.html#conclusion",
    "href": "posts/python-pi/index.html#conclusion",
    "title": "Python 3.14: The Next Evolution in Python Development",
    "section": "",
    "text": "Python 3.14 represents another solid step forward for the language, balancing innovation with stability. The release demonstrates the Python community’s continued dedication to creating a language that remains accessible to beginners while powerful enough for the most demanding applications.\nAs Python approaches its fourth decade, releases like 3.14 show that the language continues to evolve thoughtfully, maintaining its position as one of the world’s most popular and versatile programming languages.\n\nFor the most current information about Python 3.14, including detailed release notes and migration guides, visit the official Python documentation at python.org."
  },
  {
    "objectID": "posts/vision-transformers/index.html",
    "href": "posts/vision-transformers/index.html",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Vision Transformers (ViT) represent a significant paradigm shift in computer vision, applying the transformer architecture initially developed for NLP to image processing tasks. This guide walks through implementing a Vision Transformer from scratch using PyTorch.\n\n\n\nIntroduction to Vision Transformers\nUnderstanding the Architecture\nImplementation\n\nImage Patching\nPatch Embedding\nPosition Embedding\nTransformer Encoder\nMLP Head\n\nTraining the Model\nInference and Usage\nOptimization Techniques\nAdvanced Variants\n\n\n\n\nVision Transformers (ViT) were introduced in the paper “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale” by Dosovitskiy et al. in 2020. The core idea is to treat an image as a sequence of patches, similar to how words are treated in NLP, and process them using a transformer encoder.\nKey advantages of ViTs include:\n\nGlobal receptive field from the start\nAbility to capture long-range dependencies\nScalability to large datasets\nNo inductive bias towards local processing (unlike CNNs)\n\n\n\n\nThe ViT architecture consists of the following components:\n\nImage Patching: Dividing the input image into fixed-size patches\nPatch Embedding: Linear projection of flattened patches\nPosition Embedding: Adding positional information\nTransformer Encoder: Self-attention and feed-forward layers\nMLP Head: Final classification layer\n\n\n\n\n\nLet’s implement each component of the Vision Transformer step by step.\n\n\nFirst, we need to divide the input image into fixed-size patches. For a typical ViT, these are 16×16 pixel patches.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n        super().__init__()\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.num_patches = (image_size // patch_size) ** 2\n        \n        # Convert image into patches and embed them\n        # Instead of using einops, we'll use standard PyTorch operations\n        self.projection = nn.Conv2d(\n            in_channels, embed_dim, \n            kernel_size=patch_size, stride=patch_size\n        )\n        \n    def forward(self, x):\n        # x: (batch_size, channels, height, width)\n        # Convert image into patches using convolution\n        x = self.projection(x)  # (batch_size, embed_dim, grid_height, grid_width)\n        \n        # Flatten spatial dimensions and transpose to get \n        # (batch_size, num_patches, embed_dim)\n        batch_size = x.shape[0]\n        x = x.flatten(2)  # (batch_size, embed_dim, num_patches)\n        x = x.transpose(1, 2)  # (batch_size, num_patches, embed_dim)\n        \n        return x\n\n\n\nAfter patching, we need to add a learnable class token and position embeddings.\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, num_classes, \n                 embed_dim, depth, num_heads, mlp_ratio=4, \n                 dropout_rate=0.1):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(\n            image_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embedding.num_patches\n        \n        # Class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # Position embedding for patches + class token\n        self.pos_embedding = nn.Parameter(\n            torch.zeros(1, self.num_patches + 1, embed_dim))\n        \n        self.dropout = nn.Dropout(dropout_rate)\n\n\n\nThe position embeddings are added to provide spatial information:\n    def forward(self, x):\n        # Get patch embeddings\n        x = self.patch_embedding(x)  # (B, num_patches, embed_dim)\n        \n        # Add class token\n        batch_size = x.shape[0]\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (B, 1, embed_dim)\n        x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches + 1, embed_dim)\n        \n        # Add position embedding\n        x = x + self.pos_embedding\n        x = self.dropout(x)\n\n\n\nNext, let’s implement the transformer encoder blocks:\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout_rate=0.1):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        \n        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout_rate)\n        \n    def forward(self, x):\n        batch_size, seq_len, embed_dim = x.shape\n        \n        # Get query, key, and value projections\n        qkv = self.qkv(x)\n        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, H, N, D)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # Each is (B, H, N, D)\n        \n        # Attention\n        attn = (q @ k.transpose(-2, -1)) * self.scale  # (B, H, N, N)\n        attn = attn.softmax(dim=-1)\n        attn = self.dropout(attn)\n        \n        # Apply attention to values\n        out = (attn @ v).transpose(1, 2)  # (B, N, H, D)\n        out = out.reshape(batch_size, seq_len, embed_dim)  # (B, N, E)\n        out = self.proj(out)\n        \n        return out\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, embed_dim, num_heads, mlp_ratio=4, dropout_rate=0.1):\n        super().__init__()\n        \n        # Layer normalization\n        self.norm1 = nn.LayerNorm(embed_dim)\n        \n        # Multi-head self-attention\n        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout_rate)\n        self.dropout1 = nn.Dropout(dropout_rate)\n        \n        # Layer normalization\n        self.norm2 = nn.LayerNorm(embed_dim)\n        \n        # MLP block\n        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, mlp_hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(mlp_hidden_dim, embed_dim),\n            nn.Dropout(dropout_rate)\n        )\n        \n    def forward(self, x):\n        # Apply layer normalization and self-attention\n        attn_output = self.attn(self.norm1(x))\n        x = x + self.dropout1(attn_output)\n        \n        # Apply MLP block with residual connection\n        x = x + self.mlp(self.norm2(x))\n        return x\nNow, let’s update our main ViT class to include the transformer encoder blocks:\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, num_classes, \n                 embed_dim, depth, num_heads, mlp_ratio=4, \n                 dropout_rate=0.1):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(\n            image_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embedding.num_patches\n        \n        # Class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # Position embedding for patches + class token\n        self.pos_embedding = nn.Parameter(\n            torch.zeros(1, self.num_patches + 1, embed_dim))\n        \n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # Transformer encoder blocks\n        self.transformer_blocks = nn.ModuleList([\n            TransformerEncoder(embed_dim, num_heads, mlp_ratio, dropout_rate)\n            for _ in range(depth)\n        ])\n        \n        # Layer normalization\n        self.norm = nn.LayerNorm(embed_dim)\n\n\n\nFinally, let’s add the classification head and complete the forward pass:\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, num_classes, \n                 embed_dim, depth, num_heads, mlp_ratio=4, \n                 dropout_rate=0.1):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(\n            image_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embedding.num_patches\n        \n        # Class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # Position embedding for patches + class token\n        self.pos_embedding = nn.Parameter(\n            torch.zeros(1, self.num_patches + 1, embed_dim))\n        \n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # Transformer encoder blocks\n        self.transformer_blocks = nn.ModuleList([\n            TransformerEncoder(embed_dim, num_heads, mlp_ratio, dropout_rate)\n            for _ in range(depth)\n        ])\n        \n        # Layer normalization\n        self.norm = nn.LayerNorm(embed_dim)\n        \n        # Classification head\n        self.mlp_head = nn.Linear(embed_dim, num_classes)\n        \n        # Initialize weights\n        self._init_weights()\n        \n    def _init_weights(self):\n        # Initialize patch embedding and MLP heads\n        nn.init.normal_(self.cls_token, std=0.02)\n        nn.init.normal_(self.pos_embedding, std=0.02)\n        \n    def forward(self, x):\n        # Get patch embeddings\n        x = self.patch_embedding(x)  # (B, num_patches, embed_dim)\n        \n        # Add class token\n        batch_size = x.shape[0]\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (B, 1, embed_dim)\n        x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches + 1, embed_dim)\n        \n        # Add position embedding\n        x = x + self.pos_embedding\n        x = self.dropout(x)\n        \n        # Apply transformer blocks\n        for block in self.transformer_blocks:\n            x = block(x)\n        \n        # Apply final layer normalization\n        x = self.norm(x)\n        \n        # Take class token for classification\n        cls_token_final = x[:, 0]\n        \n        # Classification\n        return self.mlp_head(cls_token_final)\n\n\n\n\nLet’s implement a training function for our Vision Transformer:\ndef train_vit(model, train_loader, optimizer, criterion, device, epochs=10):\n    model.train()\n    for epoch in range(epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            # Zero the gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            \n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n            \n            # Statistics\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n            \n            # Print statistics every 100 batches\n            if batch_idx % 100 == 99:\n                print(f'Epoch: {epoch+1}, Batch: {batch_idx+1}, '\n                      f'Loss: {running_loss/100:.3f}, '\n                      f'Accuracy: {100.*correct/total:.2f}%')\n                running_loss = 0.0\n                \n        # Epoch statistics\n        print(f'Epoch {epoch+1} completed. '\n              f'Accuracy: {100.*correct/total:.2f}%')\nExample usage:\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Create ViT model\nmodel = VisionTransformer(\n    image_size=224,\n    patch_size=16,\n    in_channels=3,\n    num_classes=10,\n    embed_dim=768,\n    depth=12,\n    num_heads=12,\n    mlp_ratio=4,\n    dropout_rate=0.1\n).to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# Load data\ntransform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = datasets.FakeData(\n    transform=transforms.ToTensor()\n)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n\n# Train model\ntrain_vit(model, train_loader, optimizer, criterion, device, epochs=10)\n\n\n\nHere’s how to use the model for inference:\ndef inference(model, image_tensor, device):\n    model.eval()\n    with torch.no_grad():\n        image_tensor = image_tensor.unsqueeze(0).to(device)  # Add batch dimension\n        output = model(image_tensor)\n        probabilities = F.softmax(output, dim=1)\n        predicted_class = torch.argmax(probabilities, dim=1)\n    return predicted_class.item(), probabilities[0]\n\n# Example usage\nimage = transform(image).to(device)\npredicted_class, probabilities = inference(model, image, device)\nprint(f\"Predicted class: {predicted_class}\")\nprint(f\"Confidence: {probabilities[predicted_class]:.4f}\")\n\n\n\nTo improve the training and performance of ViT models, consider these optimization techniques:\n\n\nThe standard attention implementation can be memory-intensive. You can use a more efficient implementation:\ndef efficient_attention(q, k, v, mask=None):\n    # q, k, v: [batch_size, num_heads, seq_len, head_dim]\n    \n    # Scaled dot-product attention\n    scale = q.size(-1) ** -0.5\n    attention = torch.matmul(q, k.transpose(-2, -1)) * scale  # [B, H, L, L]\n    \n    if mask is not None:\n        attention = attention.masked_fill(mask == 0, -1e9)\n    \n    attention = F.softmax(attention, dim=-1)\n    output = torch.matmul(attention, v)  # [B, H, L, D]\n    \n    return output\n\n\n\nUse mixed precision training to reduce memory usage and increase training speed:\nfrom torch.cuda.amp import autocast, GradScaler\n\ndef train_with_mixed_precision(model, train_loader, optimizer, criterion, device, epochs=10):\n    scaler = GradScaler()\n    model.train()\n    \n    for epoch in range(epochs):\n        running_loss = 0.0\n        \n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Use autocast for mixed precision\n            with autocast():\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n            \n            # Scale gradients and optimize\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            running_loss += loss.item()\n            # Rest of the training loop...\n\n\n\nImplement regularization techniques such as stochastic depth to prevent overfitting:\nclass StochasticDepth(nn.Module):\n    def __init__(self, drop_prob=0.1):\n        super().__init__()\n        self.drop_prob = drop_prob\n        \n    def forward(self, x):\n        if not self.training or self.drop_prob == 0.:\n            return x\n        \n        keep_prob = 1 - self.drop_prob\n        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n        random_tensor.floor_()  # binarize\n        output = x.div(keep_prob) * random_tensor\n        return output\n\n\n\n\nSeveral advanced variants of Vision Transformers have been developed:\n\n\nDeiT introduces a distillation token and a teacher-student strategy:\nclass DeiT(VisionTransformer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        \n        # Distillation token\n        self.dist_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n        \n        # Update position embeddings to include distillation token\n        # Original position embeddings are for [class_token, patches]\n        # New position embeddings are for [class_token, dist_token, patches]\n        num_patches = self.patch_embedding.num_patches\n        new_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 2, self.embed_dim))\n        \n        # Initialize new position embeddings with the original ones\n        # Copy class token position embedding\n        new_pos_embed.data[:, 0:1, :] = self.pos_embedding.data[:, 0:1, :]\n        # Add a new position embedding for distillation token\n        new_pos_embed.data[:, 1:2, :] = self.pos_embedding.data[:, 0:1, :]\n        # Copy patch position embeddings\n        new_pos_embed.data[:, 2:, :] = self.pos_embedding.data[:, 1:, :]\n        \n        self.pos_embedding = new_pos_embed\n        \n        # Additional classification head for distillation\n        self.head_dist = nn.Linear(self.embed_dim, kwargs.get('num_classes', 1000))\n        \n    def forward(self, x):\n        # Get patch embeddings\n        x = self.patch_embedding(x)\n        \n        # Add class and distillation tokens\n        batch_size = x.shape[0]\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n        dist_tokens = self.dist_token.expand(batch_size, -1, -1)\n        x = torch.cat((cls_tokens, dist_tokens, x), dim=1)\n        \n        # Add position embedding and apply dropout\n        x = x + self.pos_embedding\n        x = self.dropout(x)\n        \n        # Apply transformer blocks\n        for block in self.transformer_blocks:\n            x = block(x)\n        \n        # Apply final layer normalization\n        x = self.norm(x)\n        \n        # Get class and distillation tokens\n        cls_token_final = x[:, 0]\n        dist_token_final = x[:, 1]\n        \n        # Apply classification heads\n        x_cls = self.mlp_head(cls_token_final)\n        x_dist = self.head_dist(dist_token_final)\n        \n        if self.training:\n            # During training, return both outputs\n            return x_cls, x_dist\n        else:\n            # During inference, return average\n            return (x_cls + x_dist) / 2\n\n\n\nSwin Transformer introduces hierarchical feature maps and shifted windows:\n# This is a simplified conceptual implementation of the Swin Transformer block\nclass WindowAttention(nn.Module):\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, dropout=0.):\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # (height, width)\n        self.num_heads = num_heads\n        self.scale = (dim // num_heads) ** -0.5\n        \n        # Linear projections\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n        \n        # Define relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))\n            \n        # Generate pair-wise relative position index for each token in the window\n        coords_h = torch.arange(self.window_size[0])\n        coords_w = torch.arange(self.window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        \n        # Calculate relative positions\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        \n        self.register_buffer(\"relative_position_index\", relative_position_index)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, mask=None):\n        B_, N, C = x.shape\n        \n        # Generate QKV matrices\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # 3, B_, num_heads, N, C//num_heads\n        q, k, v = qkv[0], qkv[1], qkv[2]  # each has shape [B_, num_heads, N, C//num_heads]\n        \n        # Scaled dot-product attention\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))  # B_, num_heads, N, N\n        \n        # Add relative position bias\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)]\n        relative_position_bias = relative_position_bias.view(\n            self.window_size[0] * self.window_size[1], \n            self.window_size[0] * self.window_size[1], \n            -1)  # Wh*Ww, Wh*Ww, num_heads\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # num_heads, Wh*Ww, Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0)\n        \n        # Apply mask if needed\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n        \n        # Apply softmax\n        attn = F.softmax(attn, dim=-1)\n        attn = self.dropout(attn)\n        \n        # Apply attention to values\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        \n        return x\n\n\n\n\nVision Transformers represent a significant advancement in computer vision, offering a different approach from traditional CNNs. This guide has covered the essential components for implementing a Vision Transformer from scratch, including image patching, position embeddings, transformer encoders, and classification heads.\nBy understanding these fundamentals, you can implement your own ViT models and experiment with various modifications to improve performance for specific tasks.\n\n\n\n\nDosovitskiy, A., et al. (2020). “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” arXiv:2010.11929.\nTouvron, H., et al. (2021). “Training data-efficient image transformers & distillation through attention.” arXiv:2012.12877.\nLiu, Z., et al. (2021). “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” arXiv:2103.14030."
  },
  {
    "objectID": "posts/vision-transformers/index.html#table-of-contents",
    "href": "posts/vision-transformers/index.html#table-of-contents",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Introduction to Vision Transformers\nUnderstanding the Architecture\nImplementation\n\nImage Patching\nPatch Embedding\nPosition Embedding\nTransformer Encoder\nMLP Head\n\nTraining the Model\nInference and Usage\nOptimization Techniques\nAdvanced Variants"
  },
  {
    "objectID": "posts/vision-transformers/index.html#introduction-to-vision-transformers",
    "href": "posts/vision-transformers/index.html#introduction-to-vision-transformers",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Vision Transformers (ViT) were introduced in the paper “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale” by Dosovitskiy et al. in 2020. The core idea is to treat an image as a sequence of patches, similar to how words are treated in NLP, and process them using a transformer encoder.\nKey advantages of ViTs include:\n\nGlobal receptive field from the start\nAbility to capture long-range dependencies\nScalability to large datasets\nNo inductive bias towards local processing (unlike CNNs)"
  },
  {
    "objectID": "posts/vision-transformers/index.html#understanding-the-architecture",
    "href": "posts/vision-transformers/index.html#understanding-the-architecture",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "The ViT architecture consists of the following components:\n\nImage Patching: Dividing the input image into fixed-size patches\nPatch Embedding: Linear projection of flattened patches\nPosition Embedding: Adding positional information\nTransformer Encoder: Self-attention and feed-forward layers\nMLP Head: Final classification layer"
  },
  {
    "objectID": "posts/vision-transformers/index.html#implementation",
    "href": "posts/vision-transformers/index.html#implementation",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Let’s implement each component of the Vision Transformer step by step.\n\n\nFirst, we need to divide the input image into fixed-size patches. For a typical ViT, these are 16×16 pixel patches.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n        super().__init__()\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.num_patches = (image_size // patch_size) ** 2\n        \n        # Convert image into patches and embed them\n        # Instead of using einops, we'll use standard PyTorch operations\n        self.projection = nn.Conv2d(\n            in_channels, embed_dim, \n            kernel_size=patch_size, stride=patch_size\n        )\n        \n    def forward(self, x):\n        # x: (batch_size, channels, height, width)\n        # Convert image into patches using convolution\n        x = self.projection(x)  # (batch_size, embed_dim, grid_height, grid_width)\n        \n        # Flatten spatial dimensions and transpose to get \n        # (batch_size, num_patches, embed_dim)\n        batch_size = x.shape[0]\n        x = x.flatten(2)  # (batch_size, embed_dim, num_patches)\n        x = x.transpose(1, 2)  # (batch_size, num_patches, embed_dim)\n        \n        return x\n\n\n\nAfter patching, we need to add a learnable class token and position embeddings.\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, num_classes, \n                 embed_dim, depth, num_heads, mlp_ratio=4, \n                 dropout_rate=0.1):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(\n            image_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embedding.num_patches\n        \n        # Class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # Position embedding for patches + class token\n        self.pos_embedding = nn.Parameter(\n            torch.zeros(1, self.num_patches + 1, embed_dim))\n        \n        self.dropout = nn.Dropout(dropout_rate)\n\n\n\nThe position embeddings are added to provide spatial information:\n    def forward(self, x):\n        # Get patch embeddings\n        x = self.patch_embedding(x)  # (B, num_patches, embed_dim)\n        \n        # Add class token\n        batch_size = x.shape[0]\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (B, 1, embed_dim)\n        x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches + 1, embed_dim)\n        \n        # Add position embedding\n        x = x + self.pos_embedding\n        x = self.dropout(x)\n\n\n\nNext, let’s implement the transformer encoder blocks:\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout_rate=0.1):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        \n        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout_rate)\n        \n    def forward(self, x):\n        batch_size, seq_len, embed_dim = x.shape\n        \n        # Get query, key, and value projections\n        qkv = self.qkv(x)\n        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, H, N, D)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # Each is (B, H, N, D)\n        \n        # Attention\n        attn = (q @ k.transpose(-2, -1)) * self.scale  # (B, H, N, N)\n        attn = attn.softmax(dim=-1)\n        attn = self.dropout(attn)\n        \n        # Apply attention to values\n        out = (attn @ v).transpose(1, 2)  # (B, N, H, D)\n        out = out.reshape(batch_size, seq_len, embed_dim)  # (B, N, E)\n        out = self.proj(out)\n        \n        return out\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, embed_dim, num_heads, mlp_ratio=4, dropout_rate=0.1):\n        super().__init__()\n        \n        # Layer normalization\n        self.norm1 = nn.LayerNorm(embed_dim)\n        \n        # Multi-head self-attention\n        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout_rate)\n        self.dropout1 = nn.Dropout(dropout_rate)\n        \n        # Layer normalization\n        self.norm2 = nn.LayerNorm(embed_dim)\n        \n        # MLP block\n        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, mlp_hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(mlp_hidden_dim, embed_dim),\n            nn.Dropout(dropout_rate)\n        )\n        \n    def forward(self, x):\n        # Apply layer normalization and self-attention\n        attn_output = self.attn(self.norm1(x))\n        x = x + self.dropout1(attn_output)\n        \n        # Apply MLP block with residual connection\n        x = x + self.mlp(self.norm2(x))\n        return x\nNow, let’s update our main ViT class to include the transformer encoder blocks:\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, num_classes, \n                 embed_dim, depth, num_heads, mlp_ratio=4, \n                 dropout_rate=0.1):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(\n            image_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embedding.num_patches\n        \n        # Class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # Position embedding for patches + class token\n        self.pos_embedding = nn.Parameter(\n            torch.zeros(1, self.num_patches + 1, embed_dim))\n        \n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # Transformer encoder blocks\n        self.transformer_blocks = nn.ModuleList([\n            TransformerEncoder(embed_dim, num_heads, mlp_ratio, dropout_rate)\n            for _ in range(depth)\n        ])\n        \n        # Layer normalization\n        self.norm = nn.LayerNorm(embed_dim)\n\n\n\nFinally, let’s add the classification head and complete the forward pass:\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, num_classes, \n                 embed_dim, depth, num_heads, mlp_ratio=4, \n                 dropout_rate=0.1):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(\n            image_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embedding.num_patches\n        \n        # Class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # Position embedding for patches + class token\n        self.pos_embedding = nn.Parameter(\n            torch.zeros(1, self.num_patches + 1, embed_dim))\n        \n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # Transformer encoder blocks\n        self.transformer_blocks = nn.ModuleList([\n            TransformerEncoder(embed_dim, num_heads, mlp_ratio, dropout_rate)\n            for _ in range(depth)\n        ])\n        \n        # Layer normalization\n        self.norm = nn.LayerNorm(embed_dim)\n        \n        # Classification head\n        self.mlp_head = nn.Linear(embed_dim, num_classes)\n        \n        # Initialize weights\n        self._init_weights()\n        \n    def _init_weights(self):\n        # Initialize patch embedding and MLP heads\n        nn.init.normal_(self.cls_token, std=0.02)\n        nn.init.normal_(self.pos_embedding, std=0.02)\n        \n    def forward(self, x):\n        # Get patch embeddings\n        x = self.patch_embedding(x)  # (B, num_patches, embed_dim)\n        \n        # Add class token\n        batch_size = x.shape[0]\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (B, 1, embed_dim)\n        x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches + 1, embed_dim)\n        \n        # Add position embedding\n        x = x + self.pos_embedding\n        x = self.dropout(x)\n        \n        # Apply transformer blocks\n        for block in self.transformer_blocks:\n            x = block(x)\n        \n        # Apply final layer normalization\n        x = self.norm(x)\n        \n        # Take class token for classification\n        cls_token_final = x[:, 0]\n        \n        # Classification\n        return self.mlp_head(cls_token_final)"
  },
  {
    "objectID": "posts/vision-transformers/index.html#training-the-model",
    "href": "posts/vision-transformers/index.html#training-the-model",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Let’s implement a training function for our Vision Transformer:\ndef train_vit(model, train_loader, optimizer, criterion, device, epochs=10):\n    model.train()\n    for epoch in range(epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            # Zero the gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            \n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n            \n            # Statistics\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n            \n            # Print statistics every 100 batches\n            if batch_idx % 100 == 99:\n                print(f'Epoch: {epoch+1}, Batch: {batch_idx+1}, '\n                      f'Loss: {running_loss/100:.3f}, '\n                      f'Accuracy: {100.*correct/total:.2f}%')\n                running_loss = 0.0\n                \n        # Epoch statistics\n        print(f'Epoch {epoch+1} completed. '\n              f'Accuracy: {100.*correct/total:.2f}%')\nExample usage:\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Create ViT model\nmodel = VisionTransformer(\n    image_size=224,\n    patch_size=16,\n    in_channels=3,\n    num_classes=10,\n    embed_dim=768,\n    depth=12,\n    num_heads=12,\n    mlp_ratio=4,\n    dropout_rate=0.1\n).to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# Load data\ntransform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = datasets.FakeData(\n    transform=transforms.ToTensor()\n)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n\n# Train model\ntrain_vit(model, train_loader, optimizer, criterion, device, epochs=10)"
  },
  {
    "objectID": "posts/vision-transformers/index.html#inference-and-usage",
    "href": "posts/vision-transformers/index.html#inference-and-usage",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Here’s how to use the model for inference:\ndef inference(model, image_tensor, device):\n    model.eval()\n    with torch.no_grad():\n        image_tensor = image_tensor.unsqueeze(0).to(device)  # Add batch dimension\n        output = model(image_tensor)\n        probabilities = F.softmax(output, dim=1)\n        predicted_class = torch.argmax(probabilities, dim=1)\n    return predicted_class.item(), probabilities[0]\n\n# Example usage\nimage = transform(image).to(device)\npredicted_class, probabilities = inference(model, image, device)\nprint(f\"Predicted class: {predicted_class}\")\nprint(f\"Confidence: {probabilities[predicted_class]:.4f}\")"
  },
  {
    "objectID": "posts/vision-transformers/index.html#optimization-techniques",
    "href": "posts/vision-transformers/index.html#optimization-techniques",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "To improve the training and performance of ViT models, consider these optimization techniques:\n\n\nThe standard attention implementation can be memory-intensive. You can use a more efficient implementation:\ndef efficient_attention(q, k, v, mask=None):\n    # q, k, v: [batch_size, num_heads, seq_len, head_dim]\n    \n    # Scaled dot-product attention\n    scale = q.size(-1) ** -0.5\n    attention = torch.matmul(q, k.transpose(-2, -1)) * scale  # [B, H, L, L]\n    \n    if mask is not None:\n        attention = attention.masked_fill(mask == 0, -1e9)\n    \n    attention = F.softmax(attention, dim=-1)\n    output = torch.matmul(attention, v)  # [B, H, L, D]\n    \n    return output\n\n\n\nUse mixed precision training to reduce memory usage and increase training speed:\nfrom torch.cuda.amp import autocast, GradScaler\n\ndef train_with_mixed_precision(model, train_loader, optimizer, criterion, device, epochs=10):\n    scaler = GradScaler()\n    model.train()\n    \n    for epoch in range(epochs):\n        running_loss = 0.0\n        \n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Use autocast for mixed precision\n            with autocast():\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n            \n            # Scale gradients and optimize\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            running_loss += loss.item()\n            # Rest of the training loop...\n\n\n\nImplement regularization techniques such as stochastic depth to prevent overfitting:\nclass StochasticDepth(nn.Module):\n    def __init__(self, drop_prob=0.1):\n        super().__init__()\n        self.drop_prob = drop_prob\n        \n    def forward(self, x):\n        if not self.training or self.drop_prob == 0.:\n            return x\n        \n        keep_prob = 1 - self.drop_prob\n        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n        random_tensor.floor_()  # binarize\n        output = x.div(keep_prob) * random_tensor\n        return output"
  },
  {
    "objectID": "posts/vision-transformers/index.html#advanced-variants",
    "href": "posts/vision-transformers/index.html#advanced-variants",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Several advanced variants of Vision Transformers have been developed:\n\n\nDeiT introduces a distillation token and a teacher-student strategy:\nclass DeiT(VisionTransformer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        \n        # Distillation token\n        self.dist_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n        \n        # Update position embeddings to include distillation token\n        # Original position embeddings are for [class_token, patches]\n        # New position embeddings are for [class_token, dist_token, patches]\n        num_patches = self.patch_embedding.num_patches\n        new_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 2, self.embed_dim))\n        \n        # Initialize new position embeddings with the original ones\n        # Copy class token position embedding\n        new_pos_embed.data[:, 0:1, :] = self.pos_embedding.data[:, 0:1, :]\n        # Add a new position embedding for distillation token\n        new_pos_embed.data[:, 1:2, :] = self.pos_embedding.data[:, 0:1, :]\n        # Copy patch position embeddings\n        new_pos_embed.data[:, 2:, :] = self.pos_embedding.data[:, 1:, :]\n        \n        self.pos_embedding = new_pos_embed\n        \n        # Additional classification head for distillation\n        self.head_dist = nn.Linear(self.embed_dim, kwargs.get('num_classes', 1000))\n        \n    def forward(self, x):\n        # Get patch embeddings\n        x = self.patch_embedding(x)\n        \n        # Add class and distillation tokens\n        batch_size = x.shape[0]\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n        dist_tokens = self.dist_token.expand(batch_size, -1, -1)\n        x = torch.cat((cls_tokens, dist_tokens, x), dim=1)\n        \n        # Add position embedding and apply dropout\n        x = x + self.pos_embedding\n        x = self.dropout(x)\n        \n        # Apply transformer blocks\n        for block in self.transformer_blocks:\n            x = block(x)\n        \n        # Apply final layer normalization\n        x = self.norm(x)\n        \n        # Get class and distillation tokens\n        cls_token_final = x[:, 0]\n        dist_token_final = x[:, 1]\n        \n        # Apply classification heads\n        x_cls = self.mlp_head(cls_token_final)\n        x_dist = self.head_dist(dist_token_final)\n        \n        if self.training:\n            # During training, return both outputs\n            return x_cls, x_dist\n        else:\n            # During inference, return average\n            return (x_cls + x_dist) / 2\n\n\n\nSwin Transformer introduces hierarchical feature maps and shifted windows:\n# This is a simplified conceptual implementation of the Swin Transformer block\nclass WindowAttention(nn.Module):\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, dropout=0.):\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # (height, width)\n        self.num_heads = num_heads\n        self.scale = (dim // num_heads) ** -0.5\n        \n        # Linear projections\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n        \n        # Define relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))\n            \n        # Generate pair-wise relative position index for each token in the window\n        coords_h = torch.arange(self.window_size[0])\n        coords_w = torch.arange(self.window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        \n        # Calculate relative positions\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        \n        self.register_buffer(\"relative_position_index\", relative_position_index)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, mask=None):\n        B_, N, C = x.shape\n        \n        # Generate QKV matrices\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # 3, B_, num_heads, N, C//num_heads\n        q, k, v = qkv[0], qkv[1], qkv[2]  # each has shape [B_, num_heads, N, C//num_heads]\n        \n        # Scaled dot-product attention\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))  # B_, num_heads, N, N\n        \n        # Add relative position bias\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)]\n        relative_position_bias = relative_position_bias.view(\n            self.window_size[0] * self.window_size[1], \n            self.window_size[0] * self.window_size[1], \n            -1)  # Wh*Ww, Wh*Ww, num_heads\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # num_heads, Wh*Ww, Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0)\n        \n        # Apply mask if needed\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n        \n        # Apply softmax\n        attn = F.softmax(attn, dim=-1)\n        attn = self.dropout(attn)\n        \n        # Apply attention to values\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        \n        return x"
  },
  {
    "objectID": "posts/vision-transformers/index.html#conclusion",
    "href": "posts/vision-transformers/index.html#conclusion",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Vision Transformers represent a significant advancement in computer vision, offering a different approach from traditional CNNs. This guide has covered the essential components for implementing a Vision Transformer from scratch, including image patching, position embeddings, transformer encoders, and classification heads.\nBy understanding these fundamentals, you can implement your own ViT models and experiment with various modifications to improve performance for specific tasks."
  },
  {
    "objectID": "posts/vision-transformers/index.html#references",
    "href": "posts/vision-transformers/index.html#references",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Dosovitskiy, A., et al. (2020). “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” arXiv:2010.11929.\nTouvron, H., et al. (2021). “Training data-efficient image transformers & distillation through attention.” arXiv:2012.12877.\nLiu, Z., et al. (2021). “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” arXiv:2103.14030."
  },
  {
    "objectID": "posts/mobilenet-deployment/index.html",
    "href": "posts/mobilenet-deployment/index.html",
    "title": "MobileNetV2 PyTorch Docker Deployment Guide",
    "section": "",
    "text": "This guide walks you through deploying a pre-trained MobileNetV2 model using PyTorch and Docker, creating a REST API for image classification.\n\n\nmobilenetv2-pytorch-docker/\n├── app/\n│   ├── __init__.py\n│   ├── main.py\n│   ├── model_handler.py\n│   └── utils.py\n├── requirements.txt\n├── Dockerfile\n├── docker-compose.yml\n├── .dockerignore\n└── README.md\n\n\n\n\n\nfrom fastapi import FastAPI, File, UploadFile, HTTPException\nfrom fastapi.responses import JSONResponse\nimport uvicorn\nimport io\nfrom PIL import Image\nimport numpy as np\nfrom .model_handler import MobileNetV2Handler\nfrom .utils import preprocess_image, decode_predictions\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\napp = FastAPI(\n    title=\"MobileNetV2 PyTorch Image Classification API\",\n    description=\"Deploy MobileNetV2 using PyTorch for image classification\",\n    version=\"1.0.0\"\n)\n\n# Initialize model handler\nmodel_handler = MobileNetV2Handler()\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    \"\"\"Load model on startup\"\"\"\n    try:\n        model_handler.load_model()\n        logger.info(\"Model loaded successfully\")\n    except Exception as e:\n        logger.error(f\"Failed to load model: {e}\")\n        raise\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"MobileNetV2 PyTorch Classification API\", \"status\": \"running\"}\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\", \"model_loaded\": model_handler.is_loaded()}\n\n@app.post(\"/predict\")\nasync def predict(file: UploadFile = File(...)):\n    \"\"\"\n    Predict image class using MobileNetV2\n    \"\"\"\n    if not file.content_type.startswith(\"image/\"):\n        raise HTTPException(status_code=400, detail=\"File must be an image\")\n    \n    try:\n        # Read and preprocess image\n        image_data = await file.read()\n        image = Image.open(io.BytesIO(image_data))\n        \n        if image.mode != 'RGB':\n            image = image.convert('RGB')\n        \n        # Preprocess for MobileNetV2\n        processed_image = preprocess_image(image)\n        \n        # Make prediction\n        predictions = model_handler.predict(processed_image)\n        \n        # Decode predictions\n        decoded_predictions = decode_predictions(predictions, top=5)\n        \n        return JSONResponse(content={\n            \"predictions\": decoded_predictions,\n            \"success\": True\n        })\n        \n    except Exception as e:\n        logger.error(f\"Prediction error: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Prediction failed: {str(e)}\")\n\n@app.post(\"/batch_predict\")\nasync def batch_predict(files: list[UploadFile] = File(...)):\n    \"\"\"\n    Batch prediction for multiple images\n    \"\"\"\n    if len(files) &gt; 10:  # Limit batch size\n        raise HTTPException(status_code=400, detail=\"Maximum 10 images allowed per batch\")\n    \n    results = []\n    \n    for file in files:\n        if not file.content_type.startswith(\"image/\"):\n            results.append({\n                \"filename\": file.filename,\n                \"error\": \"File must be an image\"\n            })\n            continue\n        \n        try:\n            image_data = await file.read()\n            image = Image.open(io.BytesIO(image_data))\n            \n            if image.mode != 'RGB':\n                image = image.convert('RGB')\n            \n            processed_image = preprocess_image(image)\n            predictions = model_handler.predict(processed_image)\n            decoded_predictions = decode_predictions(predictions, top=3)\n            \n            results.append({\n                \"filename\": file.filename,\n                \"predictions\": decoded_predictions,\n                \"success\": True\n            })\n            \n        except Exception as e:\n            results.append({\n                \"filename\": file.filename,\n                \"error\": str(e),\n                \"success\": False\n            })\n    \n    return JSONResponse(content={\"results\": results})\n\n@app.get(\"/model_info\")\nasync def model_info():\n    \"\"\"Get model information\"\"\"\n    return {\n        \"model_name\": \"MobileNetV2\",\n        \"framework\": \"PyTorch\",\n        \"input_size\": [224, 224],\n        \"num_classes\": 1000,\n        \"pretrained\": True\n    }\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\n\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models\nimport numpy as np\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass MobileNetV2Handler:\n    def __init__(self):\n        self.model = None\n        self.device = None\n        self._loaded = False\n        \n    def load_model(self):\n        \"\"\"Load pre-trained MobileNetV2 model\"\"\"\n        try:\n            logger.info(\"Loading MobileNetV2 PyTorch model...\")\n            \n            # Determine device (CPU/GPU)\n            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n            logger.info(f\"Using device: {self.device}\")\n            \n            # Load pre-trained MobileNetV2\n            self.model = models.mobilenet_v2(pretrained=True)\n            self.model.eval()  # Set to evaluation mode\n            self.model.to(self.device)\n            \n            # Warm up the model with a dummy prediction\n            dummy_input = torch.randn(1, 3, 224, 224).to(self.device)\n            with torch.no_grad():\n                _ = self.model(dummy_input)\n            \n            self._loaded = True\n            logger.info(\"Model loaded and warmed up successfully\")\n            \n        except Exception as e:\n            logger.error(f\"Failed to load model: {e}\")\n            raise\n    \n    def predict(self, image_tensor):\n        \"\"\"Make prediction on preprocessed image tensor\"\"\"\n        if not self._loaded:\n            raise RuntimeError(\"Model not loaded\")\n        \n        try:\n            # Ensure tensor is on correct device\n            if isinstance(image_tensor, np.ndarray):\n                image_tensor = torch.from_numpy(image_tensor)\n            \n            image_tensor = image_tensor.to(self.device)\n            \n            # Ensure batch dimension\n            if len(image_tensor.shape) == 3:\n                image_tensor = image_tensor.unsqueeze(0)\n            \n            # Make prediction\n            with torch.no_grad():\n                outputs = self.model(image_tensor)\n                # Apply softmax to get probabilities\n                probabilities = torch.nn.functional.softmax(outputs, dim=1)\n            \n            return probabilities.cpu().numpy()\n            \n        except Exception as e:\n            logger.error(f\"Prediction failed: {e}\")\n            raise\n    \n    def predict_batch(self, image_tensors):\n        \"\"\"Make batch predictions\"\"\"\n        if not self._loaded:\n            raise RuntimeError(\"Model not loaded\")\n        \n        try:\n            # Convert to tensor if numpy array\n            if isinstance(image_tensors, np.ndarray):\n                image_tensors = torch.from_numpy(image_tensors)\n            \n            image_tensors = image_tensors.to(self.device)\n            \n            # Make batch prediction\n            with torch.no_grad():\n                outputs = self.model(image_tensors)\n                probabilities = torch.nn.functional.softmax(outputs, dim=1)\n            \n            return probabilities.cpu().numpy()\n            \n        except Exception as e:\n            logger.error(f\"Batch prediction failed: {e}\")\n            raise\n    \n    def is_loaded(self):\n        \"\"\"Check if model is loaded\"\"\"\n        return self._loaded\n    \n    def get_device(self):\n        \"\"\"Get current device\"\"\"\n        return str(self.device) if self.device else \"not initialized\"\n\n\n\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom torchvision import transforms\nimport json\nimport os\nimport requests\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# ImageNet class labels\nIMAGENET_CLASSES_URL = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n\ndef get_imagenet_classes():\n    \"\"\"Download and cache ImageNet class labels\"\"\"\n    try:\n        if os.path.exists(\"imagenet_classes.txt\"):\n            with open(\"imagenet_classes.txt\", \"r\") as f:\n                classes = [line.strip() for line in f.readlines()]\n        else:\n            logger.info(\"Downloading ImageNet class labels...\")\n            response = requests.get(IMAGENET_CLASSES_URL)\n            classes = response.text.strip().split('\\n')\n            \n            # Cache the classes\n            with open(\"imagenet_classes.txt\", \"w\") as f:\n                for class_name in classes:\n                    f.write(f\"{class_name}\\n\")\n        \n        return classes\n    except Exception as e:\n        logger.warning(f\"Could not load ImageNet classes: {e}\")\n        return [f\"class_{i}\" for i in range(1000)]\n\n# Load ImageNet classes\nIMAGENET_CLASSES = get_imagenet_classes()\n\ndef preprocess_image(image: Image.Image, target_size=(224, 224)):\n    \"\"\"\n    Preprocess image for MobileNetV2 PyTorch model\n    \"\"\"\n    try:\n        # Define transforms\n        transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(target_size),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],  # ImageNet normalization\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n        \n        # Apply transforms\n        image_tensor = transform(image)\n        \n        return image_tensor\n        \n    except Exception as e:\n        raise ValueError(f\"Image preprocessing failed: {e}\")\n\ndef preprocess_batch(images: list, target_size=(224, 224)):\n    \"\"\"\n    Preprocess batch of images\n    \"\"\"\n    try:\n        transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(target_size),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n        \n        batch_tensors = []\n        for image in images:\n            if isinstance(image, str):  # If path\n                image = Image.open(image).convert('RGB')\n            elif not isinstance(image, Image.Image):\n                raise ValueError(\"Invalid image type\")\n            \n            tensor = transform(image)\n            batch_tensors.append(tensor)\n        \n        return torch.stack(batch_tensors)\n        \n    except Exception as e:\n        raise ValueError(f\"Batch preprocessing failed: {e}\")\n\ndef decode_predictions(predictions, top=5):\n    \"\"\"\n    Decode model predictions to human-readable labels\n    \"\"\"\n    try:\n        # Get top predictions\n        if isinstance(predictions, torch.Tensor):\n            predictions = predictions.numpy()\n        \n        # Handle batch predictions (take first sample)\n        if len(predictions.shape) &gt; 1:\n            predictions = predictions[0]\n        \n        # Get top k indices\n        top_indices = np.argsort(predictions)[-top:][::-1]\n        \n        # Format results\n        results = []\n        for idx in top_indices:\n            confidence = float(predictions[idx])\n            class_name = IMAGENET_CLASSES[idx] if idx &lt; len(IMAGENET_CLASSES) else f\"class_{idx}\"\n            \n            results.append({\n                \"class_id\": int(idx),\n                \"class_name\": class_name,\n                \"confidence\": confidence\n            })\n        \n        return results\n        \n    except Exception as e:\n        raise ValueError(f\"Prediction decoding failed: {e}\")\n\ndef validate_image(image_data):\n    \"\"\"\n    Validate image data\n    \"\"\"\n    try:\n        image = Image.open(image_data)\n        return image.format in ['JPEG', 'PNG', 'BMP', 'TIFF', 'WEBP']\n    except:\n        return False\n\ndef tensor_to_numpy(tensor):\n    \"\"\"Convert PyTorch tensor to numpy array\"\"\"\n    if isinstance(tensor, torch.Tensor):\n        return tensor.detach().cpu().numpy()\n    return tensor\n\ndef numpy_to_tensor(array, device='cpu'):\n    \"\"\"Convert numpy array to PyTorch tensor\"\"\"\n    if isinstance(array, np.ndarray):\n        return torch.from_numpy(array).to(device)\n    return array\n\nclass ModelProfiler:\n    \"\"\"Simple profiler for model performance\"\"\"\n    \n    def __init__(self):\n        self.inference_times = []\n        self.preprocessing_times = []\n    \n    def record_inference_time(self, time_ms):\n        self.inference_times.append(time_ms)\n    \n    def record_preprocessing_time(self, time_ms):\n        self.preprocessing_times.append(time_ms)\n    \n    def get_stats(self):\n        if not self.inference_times:\n            return {\"message\": \"No inference data recorded\"}\n        \n        return {\n            \"avg_inference_time_ms\": np.mean(self.inference_times),\n            \"avg_preprocessing_time_ms\": np.mean(self.preprocessing_times) if self.preprocessing_times else 0,\n            \"total_inferences\": len(self.inference_times),\n            \"min_inference_time_ms\": np.min(self.inference_times),\n            \"max_inference_time_ms\": np.max(self.inference_times)\n        }\n\n# Global profiler instance\nprofiler = ModelProfiler()\n\n\n\n# Empty file to make app a Python package\n\n\n\n\n\n\nfastapi==0.104.1\nuvicorn[standard]==0.24.0\ntorch==2.1.0\ntorchvision==0.16.0\nPillow==10.1.0\npython-multipart==0.0.6\nnumpy==1.24.3\nrequests==2.31.0\n\n\n\n# Use official Python runtime as base image\nFROM python:3.11-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    libgl1-mesa-glx \\\n    libglib2.0-0 \\\n    libsm6 \\\n    libxext6 \\\n    libxrender-dev \\\n    libgomp1 \\\n    wget \\\n    curl \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements first for better caching\nCOPY requirements.txt .\n\n# Install PyTorch CPU version (smaller image)\nRUN pip install --no-cache-dir --upgrade pip && \\\n    pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu && \\\n    pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY app/ ./app/\n\n# Create non-root user for security\nRUN useradd --create-home --shell /bin/bash app && \\\n    chown -R app:app /app\nUSER app\n\n# Pre-download ImageNet classes\nRUN python -c \"from app.utils import get_imagenet_classes; get_imagenet_classes()\"\n\n# Expose port\nEXPOSE 8000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=30s --start-period=60s --retries=3 \\\n    CMD curl -f http://localhost:8000/health || exit 1\n\n# Command to run the application\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\n\n\n# Use NVIDIA PyTorch base image\nFROM pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime\n\n# Set working directory\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    curl \\\n    wget \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements first for better caching\nCOPY requirements.txt .\n\n# Install additional dependencies\nRUN pip install --no-cache-dir --upgrade pip && \\\n    pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY app/ ./app/\n\n# Create non-root user for security\nRUN useradd --create-home --shell /bin/bash app && \\\n    chown -R app:app /app\nUSER app\n\n# Pre-download ImageNet classes\nRUN python -c \"from app.utils import get_imagenet_classes; get_imagenet_classes()\"\n\n# Expose port\nEXPOSE 8000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=30s --start-period=60s --retries=3 \\\n    CMD curl -f http://localhost:8000/health || exit 1\n\n# Command to run the application\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\n\n\nversion: '3.8'\n\nservices:\n  mobilenetv2-pytorch-api:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - PYTHONPATH=/app\n      - TORCH_HOME=/app/.torch\n    volumes:\n      - ./logs:/app/logs\n      - torch_cache:/app/.torch\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 60s\n    deploy:\n      resources:\n        limits:\n          memory: 2G\n        reservations:\n          memory: 1G\n\n  # GPU version (uncomment and modify as needed)\n  # mobilenetv2-pytorch-gpu:\n  #   build:\n  #     context: .\n  #     dockerfile: Dockerfile.gpu\n  #   ports:\n  #     - \"8000:8000\"\n  #   environment:\n  #     - PYTHONPATH=/app\n  #     - TORCH_HOME=/app/.torch\n  #     - NVIDIA_VISIBLE_DEVICES=all\n  #   volumes:\n  #     - ./logs:/app/logs\n  #     - torch_cache:/app/.torch\n  #   restart: unless-stopped\n  #   deploy:\n  #     resources:\n  #       reservations:\n  #         devices:\n  #           - driver: nvidia\n  #             count: 1\n  #             capabilities: [gpu]\n\n  # Optional: Add nginx for production\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n    depends_on:\n      - mobilenetv2-pytorch-api\n    restart: unless-stopped\n\nvolumes:\n  torch_cache:\n\n\n\n__pycache__\n*.pyc\n*.pyo\n*.pyd\n.Python\nenv/\npip-log.txt\npip-delete-this-directory.txt\n.git\n.gitignore\nREADME.md\n.pytest_cache\n.coverage\n.nyc_output\nnode_modules\n.DS_Store\n*.log\nlogs/\n*.pth\n*.pt\n.torch/\n\n\n\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    upstream api {\n        server mobilenetv2-pytorch-api:8000;\n    }\n\n    server {\n        listen 80;\n        client_max_body_size 10M;\n\n        location / {\n            proxy_pass http://api;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n            proxy_read_timeout 300;\n            proxy_connect_timeout 300;\n            proxy_send_timeout 300;\n        }\n    }\n}\n\n\n\n\n\n\n# Build the CPU image\ndocker build -t mobilenetv2-pytorch-api .\n\n# Build the GPU image (if you have NVIDIA GPU)\ndocker build -f Dockerfile.gpu -t mobilenetv2-pytorch-gpu .\n\n# Run CPU version\ndocker run -p 8000:8000 mobilenetv2-pytorch-api\n\n# Run GPU version\ndocker run --gpus all -p 8000:8000 mobilenetv2-pytorch-gpu\n\n# Run with environment variables\ndocker run -p 8000:8000 -e TORCH_HOME=/tmp/.torch mobilenetv2-pytorch-api\n\n\n\n# Build and start services\ndocker-compose up --build\n\n# Run in background\ndocker-compose up -d\n\n# View logs\ndocker-compose logs -f mobilenetv2-pytorch-api\n\n# Stop services\ndocker-compose down\n\n\n\n\n\n\n# Health check\ncurl http://localhost:8000/health\n\n# Model info\ncurl http://localhost:8000/model_info\n\n# Single image prediction\ncurl -X POST \"http://localhost:8000/predict\" \\\n     -H \"accept: application/json\" \\\n     -H \"Content-Type: multipart/form-data\" \\\n     -F \"file=@path/to/your/image.jpg\"\n\n# Batch prediction\ncurl -X POST \"http://localhost:8000/batch_predict\" \\\n     -H \"accept: application/json\" \\\n     -H \"Content-Type: multipart/form-data\" \\\n     -F \"files=@image1.jpg\" \\\n     -F \"files=@image2.jpg\"\n\n\n\nimport requests\nimport json\n\n# Single prediction\ndef predict_image(image_path, api_url=\"http://localhost:8000\"):\n    url = f\"{api_url}/predict\"\n    files = {\"file\": open(image_path, \"rb\")}\n    response = requests.post(url, files=files)\n    return response.json()\n\n# Batch prediction\ndef predict_batch(image_paths, api_url=\"http://localhost:8000\"):\n    url = f\"{api_url}/batch_predict\"\n    files = [(\"files\", open(path, \"rb\")) for path in image_paths]\n    response = requests.post(url, files=files)\n    return response.json()\n\n# Usage\nresult = predict_image(\"cat.jpg\")\nprint(json.dumps(result, indent=2))\n\nbatch_result = predict_batch([\"cat.jpg\", \"dog.jpg\"])\nprint(json.dumps(batch_result, indent=2))\n\n\n\n{\n  \"predictions\": [\n    {\n      \"class_id\": 281,\n      \"class_name\": \"tabby\",\n      \"confidence\": 0.8234567\n    },\n    {\n      \"class_id\": 282,\n      \"class_name\": \"tiger_cat\",\n      \"confidence\": 0.1234567\n    }\n  ],\n  \"success\": true\n}\n\n\n\n\n\n\n# Add to model_handler.py for optimization\nimport torch.jit\n\nclass OptimizedMobileNetV2Handler(MobileNetV2Handler):\n    def __init__(self, use_jit=True, use_half_precision=False):\n        super().__init__()\n        self.use_jit = use_jit\n        self.use_half_precision = use_half_precision\n    \n    def load_model(self):\n        super().load_model()\n        \n        if self.use_jit:\n            # TorchScript compilation for faster inference\n            self.model = torch.jit.script(self.model)\n            logger.info(\"Model compiled with TorchScript\")\n        \n        if self.use_half_precision and self.device.type == 'cuda':\n            # Half precision for GPU\n            self.model = self.model.half()\n            logger.info(\"Model converted to half precision\")\n\n\n\n# Multi-stage build for smaller image\nFROM python:3.11-slim as builder\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --user --no-cache-dir -r requirements.txt\n\nFROM python:3.11-slim\n\nWORKDIR /app\nCOPY --from=builder /root/.local /root/.local\nCOPY app/ ./app/\n\n# Make sure scripts in .local are usable\nENV PATH=/root/.local/bin:$PATH\n\nEXPOSE 8000\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\n\n\n\n\n\n# Add to main.py\nimport time\nfrom app.utils import profiler\n\n@app.middleware(\"http\")\nasync def log_requests(request, call_next):\n    start_time = time.time()\n    response = await call_next(request)\n    process_time = (time.time() - start_time) * 1000\n    \n    logger.info(f\"{request.method} {request.url.path} - {process_time:.2f}ms\")\n    \n    if request.url.path == \"/predict\":\n        profiler.record_inference_time(process_time)\n    \n    return response\n\n@app.get(\"/stats\")\nasync def get_stats():\n    \"\"\"Get performance statistics\"\"\"\n    return profiler.get_stats()\n\n\n\n\n\n\n{\n  \"family\": \"mobilenetv2-pytorch-task\",\n  \"networkMode\": \"awsvpc\",\n  \"requiresCompatibilities\": [\"FARGATE\"],\n  \"cpu\": \"1024\",\n  \"memory\": \"2048\",\n  \"containerDefinitions\": [\n    {\n      \"name\": \"mobilenetv2-pytorch-api\",\n      \"image\": \"your-registry/mobilenetv2-pytorch-api:latest\",\n      \"portMappings\": [\n        {\n          \"containerPort\": 8000,\n          \"protocol\": \"tcp\"\n        }\n      ],\n      \"environment\": [\n        {\n          \"name\": \"TORCH_HOME\",\n          \"value\": \"/tmp/.torch\"\n        }\n      ],\n      \"essential\": true,\n      \"logConfiguration\": {\n        \"logDriver\": \"awslogs\",\n        \"options\": {\n          \"awslogs-group\": \"/ecs/mobilenetv2-pytorch\",\n          \"awslogs-region\": \"us-east-1\",\n          \"awslogs-stream-prefix\": \"ecs\"\n        }\n      }\n    }\n  ]\n}\n\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mobilenetv2-pytorch-api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mobilenetv2-pytorch-api\n  template:\n    metadata:\n      labels:\n        app: mobilenetv2-pytorch-api\n    spec:\n      containers:\n      - name: mobilenetv2-pytorch-api\n        image: mobilenetv2-pytorch-api:latest\n        ports:\n        - containerPort: 8000\n        env:\n        - name: TORCH_HOME\n          value: /tmp/.torch\n        resources:\n          requests:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"2Gi\"\n            cpu: \"1000m\"\n        volumeMounts:\n        - name: torch-cache\n          mountPath: /tmp/.torch\n      volumes:\n      - name: torch-cache\n        emptyDir: {}\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mobilenetv2-pytorch-service\nspec:\n  selector:\n    app: mobilenetv2-pytorch-api\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8000\n  type: LoadBalancer\nThis PyTorch-based guide provides the same functionality as the TensorFlow version but uses PyTorch’s ecosystem, including torchvision for pre-trained models, PyTorch transformations for preprocessing, and proper tensor handling throughout the application."
  },
  {
    "objectID": "posts/mobilenet-deployment/index.html#project-structure",
    "href": "posts/mobilenet-deployment/index.html#project-structure",
    "title": "MobileNetV2 PyTorch Docker Deployment Guide",
    "section": "",
    "text": "mobilenetv2-pytorch-docker/\n├── app/\n│   ├── __init__.py\n│   ├── main.py\n│   ├── model_handler.py\n│   └── utils.py\n├── requirements.txt\n├── Dockerfile\n├── docker-compose.yml\n├── .dockerignore\n└── README.md"
  },
  {
    "objectID": "posts/mobilenet-deployment/index.html#application-code",
    "href": "posts/mobilenet-deployment/index.html#application-code",
    "title": "MobileNetV2 PyTorch Docker Deployment Guide",
    "section": "",
    "text": "from fastapi import FastAPI, File, UploadFile, HTTPException\nfrom fastapi.responses import JSONResponse\nimport uvicorn\nimport io\nfrom PIL import Image\nimport numpy as np\nfrom .model_handler import MobileNetV2Handler\nfrom .utils import preprocess_image, decode_predictions\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\napp = FastAPI(\n    title=\"MobileNetV2 PyTorch Image Classification API\",\n    description=\"Deploy MobileNetV2 using PyTorch for image classification\",\n    version=\"1.0.0\"\n)\n\n# Initialize model handler\nmodel_handler = MobileNetV2Handler()\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    \"\"\"Load model on startup\"\"\"\n    try:\n        model_handler.load_model()\n        logger.info(\"Model loaded successfully\")\n    except Exception as e:\n        logger.error(f\"Failed to load model: {e}\")\n        raise\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"MobileNetV2 PyTorch Classification API\", \"status\": \"running\"}\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\", \"model_loaded\": model_handler.is_loaded()}\n\n@app.post(\"/predict\")\nasync def predict(file: UploadFile = File(...)):\n    \"\"\"\n    Predict image class using MobileNetV2\n    \"\"\"\n    if not file.content_type.startswith(\"image/\"):\n        raise HTTPException(status_code=400, detail=\"File must be an image\")\n    \n    try:\n        # Read and preprocess image\n        image_data = await file.read()\n        image = Image.open(io.BytesIO(image_data))\n        \n        if image.mode != 'RGB':\n            image = image.convert('RGB')\n        \n        # Preprocess for MobileNetV2\n        processed_image = preprocess_image(image)\n        \n        # Make prediction\n        predictions = model_handler.predict(processed_image)\n        \n        # Decode predictions\n        decoded_predictions = decode_predictions(predictions, top=5)\n        \n        return JSONResponse(content={\n            \"predictions\": decoded_predictions,\n            \"success\": True\n        })\n        \n    except Exception as e:\n        logger.error(f\"Prediction error: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Prediction failed: {str(e)}\")\n\n@app.post(\"/batch_predict\")\nasync def batch_predict(files: list[UploadFile] = File(...)):\n    \"\"\"\n    Batch prediction for multiple images\n    \"\"\"\n    if len(files) &gt; 10:  # Limit batch size\n        raise HTTPException(status_code=400, detail=\"Maximum 10 images allowed per batch\")\n    \n    results = []\n    \n    for file in files:\n        if not file.content_type.startswith(\"image/\"):\n            results.append({\n                \"filename\": file.filename,\n                \"error\": \"File must be an image\"\n            })\n            continue\n        \n        try:\n            image_data = await file.read()\n            image = Image.open(io.BytesIO(image_data))\n            \n            if image.mode != 'RGB':\n                image = image.convert('RGB')\n            \n            processed_image = preprocess_image(image)\n            predictions = model_handler.predict(processed_image)\n            decoded_predictions = decode_predictions(predictions, top=3)\n            \n            results.append({\n                \"filename\": file.filename,\n                \"predictions\": decoded_predictions,\n                \"success\": True\n            })\n            \n        except Exception as e:\n            results.append({\n                \"filename\": file.filename,\n                \"error\": str(e),\n                \"success\": False\n            })\n    \n    return JSONResponse(content={\"results\": results})\n\n@app.get(\"/model_info\")\nasync def model_info():\n    \"\"\"Get model information\"\"\"\n    return {\n        \"model_name\": \"MobileNetV2\",\n        \"framework\": \"PyTorch\",\n        \"input_size\": [224, 224],\n        \"num_classes\": 1000,\n        \"pretrained\": True\n    }\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\n\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models\nimport numpy as np\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass MobileNetV2Handler:\n    def __init__(self):\n        self.model = None\n        self.device = None\n        self._loaded = False\n        \n    def load_model(self):\n        \"\"\"Load pre-trained MobileNetV2 model\"\"\"\n        try:\n            logger.info(\"Loading MobileNetV2 PyTorch model...\")\n            \n            # Determine device (CPU/GPU)\n            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n            logger.info(f\"Using device: {self.device}\")\n            \n            # Load pre-trained MobileNetV2\n            self.model = models.mobilenet_v2(pretrained=True)\n            self.model.eval()  # Set to evaluation mode\n            self.model.to(self.device)\n            \n            # Warm up the model with a dummy prediction\n            dummy_input = torch.randn(1, 3, 224, 224).to(self.device)\n            with torch.no_grad():\n                _ = self.model(dummy_input)\n            \n            self._loaded = True\n            logger.info(\"Model loaded and warmed up successfully\")\n            \n        except Exception as e:\n            logger.error(f\"Failed to load model: {e}\")\n            raise\n    \n    def predict(self, image_tensor):\n        \"\"\"Make prediction on preprocessed image tensor\"\"\"\n        if not self._loaded:\n            raise RuntimeError(\"Model not loaded\")\n        \n        try:\n            # Ensure tensor is on correct device\n            if isinstance(image_tensor, np.ndarray):\n                image_tensor = torch.from_numpy(image_tensor)\n            \n            image_tensor = image_tensor.to(self.device)\n            \n            # Ensure batch dimension\n            if len(image_tensor.shape) == 3:\n                image_tensor = image_tensor.unsqueeze(0)\n            \n            # Make prediction\n            with torch.no_grad():\n                outputs = self.model(image_tensor)\n                # Apply softmax to get probabilities\n                probabilities = torch.nn.functional.softmax(outputs, dim=1)\n            \n            return probabilities.cpu().numpy()\n            \n        except Exception as e:\n            logger.error(f\"Prediction failed: {e}\")\n            raise\n    \n    def predict_batch(self, image_tensors):\n        \"\"\"Make batch predictions\"\"\"\n        if not self._loaded:\n            raise RuntimeError(\"Model not loaded\")\n        \n        try:\n            # Convert to tensor if numpy array\n            if isinstance(image_tensors, np.ndarray):\n                image_tensors = torch.from_numpy(image_tensors)\n            \n            image_tensors = image_tensors.to(self.device)\n            \n            # Make batch prediction\n            with torch.no_grad():\n                outputs = self.model(image_tensors)\n                probabilities = torch.nn.functional.softmax(outputs, dim=1)\n            \n            return probabilities.cpu().numpy()\n            \n        except Exception as e:\n            logger.error(f\"Batch prediction failed: {e}\")\n            raise\n    \n    def is_loaded(self):\n        \"\"\"Check if model is loaded\"\"\"\n        return self._loaded\n    \n    def get_device(self):\n        \"\"\"Get current device\"\"\"\n        return str(self.device) if self.device else \"not initialized\"\n\n\n\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom torchvision import transforms\nimport json\nimport os\nimport requests\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# ImageNet class labels\nIMAGENET_CLASSES_URL = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n\ndef get_imagenet_classes():\n    \"\"\"Download and cache ImageNet class labels\"\"\"\n    try:\n        if os.path.exists(\"imagenet_classes.txt\"):\n            with open(\"imagenet_classes.txt\", \"r\") as f:\n                classes = [line.strip() for line in f.readlines()]\n        else:\n            logger.info(\"Downloading ImageNet class labels...\")\n            response = requests.get(IMAGENET_CLASSES_URL)\n            classes = response.text.strip().split('\\n')\n            \n            # Cache the classes\n            with open(\"imagenet_classes.txt\", \"w\") as f:\n                for class_name in classes:\n                    f.write(f\"{class_name}\\n\")\n        \n        return classes\n    except Exception as e:\n        logger.warning(f\"Could not load ImageNet classes: {e}\")\n        return [f\"class_{i}\" for i in range(1000)]\n\n# Load ImageNet classes\nIMAGENET_CLASSES = get_imagenet_classes()\n\ndef preprocess_image(image: Image.Image, target_size=(224, 224)):\n    \"\"\"\n    Preprocess image for MobileNetV2 PyTorch model\n    \"\"\"\n    try:\n        # Define transforms\n        transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(target_size),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],  # ImageNet normalization\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n        \n        # Apply transforms\n        image_tensor = transform(image)\n        \n        return image_tensor\n        \n    except Exception as e:\n        raise ValueError(f\"Image preprocessing failed: {e}\")\n\ndef preprocess_batch(images: list, target_size=(224, 224)):\n    \"\"\"\n    Preprocess batch of images\n    \"\"\"\n    try:\n        transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(target_size),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n        \n        batch_tensors = []\n        for image in images:\n            if isinstance(image, str):  # If path\n                image = Image.open(image).convert('RGB')\n            elif not isinstance(image, Image.Image):\n                raise ValueError(\"Invalid image type\")\n            \n            tensor = transform(image)\n            batch_tensors.append(tensor)\n        \n        return torch.stack(batch_tensors)\n        \n    except Exception as e:\n        raise ValueError(f\"Batch preprocessing failed: {e}\")\n\ndef decode_predictions(predictions, top=5):\n    \"\"\"\n    Decode model predictions to human-readable labels\n    \"\"\"\n    try:\n        # Get top predictions\n        if isinstance(predictions, torch.Tensor):\n            predictions = predictions.numpy()\n        \n        # Handle batch predictions (take first sample)\n        if len(predictions.shape) &gt; 1:\n            predictions = predictions[0]\n        \n        # Get top k indices\n        top_indices = np.argsort(predictions)[-top:][::-1]\n        \n        # Format results\n        results = []\n        for idx in top_indices:\n            confidence = float(predictions[idx])\n            class_name = IMAGENET_CLASSES[idx] if idx &lt; len(IMAGENET_CLASSES) else f\"class_{idx}\"\n            \n            results.append({\n                \"class_id\": int(idx),\n                \"class_name\": class_name,\n                \"confidence\": confidence\n            })\n        \n        return results\n        \n    except Exception as e:\n        raise ValueError(f\"Prediction decoding failed: {e}\")\n\ndef validate_image(image_data):\n    \"\"\"\n    Validate image data\n    \"\"\"\n    try:\n        image = Image.open(image_data)\n        return image.format in ['JPEG', 'PNG', 'BMP', 'TIFF', 'WEBP']\n    except:\n        return False\n\ndef tensor_to_numpy(tensor):\n    \"\"\"Convert PyTorch tensor to numpy array\"\"\"\n    if isinstance(tensor, torch.Tensor):\n        return tensor.detach().cpu().numpy()\n    return tensor\n\ndef numpy_to_tensor(array, device='cpu'):\n    \"\"\"Convert numpy array to PyTorch tensor\"\"\"\n    if isinstance(array, np.ndarray):\n        return torch.from_numpy(array).to(device)\n    return array\n\nclass ModelProfiler:\n    \"\"\"Simple profiler for model performance\"\"\"\n    \n    def __init__(self):\n        self.inference_times = []\n        self.preprocessing_times = []\n    \n    def record_inference_time(self, time_ms):\n        self.inference_times.append(time_ms)\n    \n    def record_preprocessing_time(self, time_ms):\n        self.preprocessing_times.append(time_ms)\n    \n    def get_stats(self):\n        if not self.inference_times:\n            return {\"message\": \"No inference data recorded\"}\n        \n        return {\n            \"avg_inference_time_ms\": np.mean(self.inference_times),\n            \"avg_preprocessing_time_ms\": np.mean(self.preprocessing_times) if self.preprocessing_times else 0,\n            \"total_inferences\": len(self.inference_times),\n            \"min_inference_time_ms\": np.min(self.inference_times),\n            \"max_inference_time_ms\": np.max(self.inference_times)\n        }\n\n# Global profiler instance\nprofiler = ModelProfiler()\n\n\n\n# Empty file to make app a Python package"
  },
  {
    "objectID": "posts/mobilenet-deployment/index.html#configuration-files",
    "href": "posts/mobilenet-deployment/index.html#configuration-files",
    "title": "MobileNetV2 PyTorch Docker Deployment Guide",
    "section": "",
    "text": "fastapi==0.104.1\nuvicorn[standard]==0.24.0\ntorch==2.1.0\ntorchvision==0.16.0\nPillow==10.1.0\npython-multipart==0.0.6\nnumpy==1.24.3\nrequests==2.31.0\n\n\n\n# Use official Python runtime as base image\nFROM python:3.11-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    libgl1-mesa-glx \\\n    libglib2.0-0 \\\n    libsm6 \\\n    libxext6 \\\n    libxrender-dev \\\n    libgomp1 \\\n    wget \\\n    curl \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements first for better caching\nCOPY requirements.txt .\n\n# Install PyTorch CPU version (smaller image)\nRUN pip install --no-cache-dir --upgrade pip && \\\n    pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu && \\\n    pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY app/ ./app/\n\n# Create non-root user for security\nRUN useradd --create-home --shell /bin/bash app && \\\n    chown -R app:app /app\nUSER app\n\n# Pre-download ImageNet classes\nRUN python -c \"from app.utils import get_imagenet_classes; get_imagenet_classes()\"\n\n# Expose port\nEXPOSE 8000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=30s --start-period=60s --retries=3 \\\n    CMD curl -f http://localhost:8000/health || exit 1\n\n# Command to run the application\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\n\n\n# Use NVIDIA PyTorch base image\nFROM pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime\n\n# Set working directory\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    curl \\\n    wget \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements first for better caching\nCOPY requirements.txt .\n\n# Install additional dependencies\nRUN pip install --no-cache-dir --upgrade pip && \\\n    pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY app/ ./app/\n\n# Create non-root user for security\nRUN useradd --create-home --shell /bin/bash app && \\\n    chown -R app:app /app\nUSER app\n\n# Pre-download ImageNet classes\nRUN python -c \"from app.utils import get_imagenet_classes; get_imagenet_classes()\"\n\n# Expose port\nEXPOSE 8000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=30s --start-period=60s --retries=3 \\\n    CMD curl -f http://localhost:8000/health || exit 1\n\n# Command to run the application\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\n\n\nversion: '3.8'\n\nservices:\n  mobilenetv2-pytorch-api:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - PYTHONPATH=/app\n      - TORCH_HOME=/app/.torch\n    volumes:\n      - ./logs:/app/logs\n      - torch_cache:/app/.torch\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 60s\n    deploy:\n      resources:\n        limits:\n          memory: 2G\n        reservations:\n          memory: 1G\n\n  # GPU version (uncomment and modify as needed)\n  # mobilenetv2-pytorch-gpu:\n  #   build:\n  #     context: .\n  #     dockerfile: Dockerfile.gpu\n  #   ports:\n  #     - \"8000:8000\"\n  #   environment:\n  #     - PYTHONPATH=/app\n  #     - TORCH_HOME=/app/.torch\n  #     - NVIDIA_VISIBLE_DEVICES=all\n  #   volumes:\n  #     - ./logs:/app/logs\n  #     - torch_cache:/app/.torch\n  #   restart: unless-stopped\n  #   deploy:\n  #     resources:\n  #       reservations:\n  #         devices:\n  #           - driver: nvidia\n  #             count: 1\n  #             capabilities: [gpu]\n\n  # Optional: Add nginx for production\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n    depends_on:\n      - mobilenetv2-pytorch-api\n    restart: unless-stopped\n\nvolumes:\n  torch_cache:\n\n\n\n__pycache__\n*.pyc\n*.pyo\n*.pyd\n.Python\nenv/\npip-log.txt\npip-delete-this-directory.txt\n.git\n.gitignore\nREADME.md\n.pytest_cache\n.coverage\n.nyc_output\nnode_modules\n.DS_Store\n*.log\nlogs/\n*.pth\n*.pt\n.torch/\n\n\n\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    upstream api {\n        server mobilenetv2-pytorch-api:8000;\n    }\n\n    server {\n        listen 80;\n        client_max_body_size 10M;\n\n        location / {\n            proxy_pass http://api;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n            proxy_read_timeout 300;\n            proxy_connect_timeout 300;\n            proxy_send_timeout 300;\n        }\n    }\n}"
  },
  {
    "objectID": "posts/mobilenet-deployment/index.html#deployment-commands",
    "href": "posts/mobilenet-deployment/index.html#deployment-commands",
    "title": "MobileNetV2 PyTorch Docker Deployment Guide",
    "section": "",
    "text": "# Build the CPU image\ndocker build -t mobilenetv2-pytorch-api .\n\n# Build the GPU image (if you have NVIDIA GPU)\ndocker build -f Dockerfile.gpu -t mobilenetv2-pytorch-gpu .\n\n# Run CPU version\ndocker run -p 8000:8000 mobilenetv2-pytorch-api\n\n# Run GPU version\ndocker run --gpus all -p 8000:8000 mobilenetv2-pytorch-gpu\n\n# Run with environment variables\ndocker run -p 8000:8000 -e TORCH_HOME=/tmp/.torch mobilenetv2-pytorch-api\n\n\n\n# Build and start services\ndocker-compose up --build\n\n# Run in background\ndocker-compose up -d\n\n# View logs\ndocker-compose logs -f mobilenetv2-pytorch-api\n\n# Stop services\ndocker-compose down"
  },
  {
    "objectID": "posts/mobilenet-deployment/index.html#usage-examples",
    "href": "posts/mobilenet-deployment/index.html#usage-examples",
    "title": "MobileNetV2 PyTorch Docker Deployment Guide",
    "section": "",
    "text": "# Health check\ncurl http://localhost:8000/health\n\n# Model info\ncurl http://localhost:8000/model_info\n\n# Single image prediction\ncurl -X POST \"http://localhost:8000/predict\" \\\n     -H \"accept: application/json\" \\\n     -H \"Content-Type: multipart/form-data\" \\\n     -F \"file=@path/to/your/image.jpg\"\n\n# Batch prediction\ncurl -X POST \"http://localhost:8000/batch_predict\" \\\n     -H \"accept: application/json\" \\\n     -H \"Content-Type: multipart/form-data\" \\\n     -F \"files=@image1.jpg\" \\\n     -F \"files=@image2.jpg\"\n\n\n\nimport requests\nimport json\n\n# Single prediction\ndef predict_image(image_path, api_url=\"http://localhost:8000\"):\n    url = f\"{api_url}/predict\"\n    files = {\"file\": open(image_path, \"rb\")}\n    response = requests.post(url, files=files)\n    return response.json()\n\n# Batch prediction\ndef predict_batch(image_paths, api_url=\"http://localhost:8000\"):\n    url = f\"{api_url}/batch_predict\"\n    files = [(\"files\", open(path, \"rb\")) for path in image_paths]\n    response = requests.post(url, files=files)\n    return response.json()\n\n# Usage\nresult = predict_image(\"cat.jpg\")\nprint(json.dumps(result, indent=2))\n\nbatch_result = predict_batch([\"cat.jpg\", \"dog.jpg\"])\nprint(json.dumps(batch_result, indent=2))\n\n\n\n{\n  \"predictions\": [\n    {\n      \"class_id\": 281,\n      \"class_name\": \"tabby\",\n      \"confidence\": 0.8234567\n    },\n    {\n      \"class_id\": 282,\n      \"class_name\": \"tiger_cat\",\n      \"confidence\": 0.1234567\n    }\n  ],\n  \"success\": true\n}"
  },
  {
    "objectID": "posts/mobilenet-deployment/index.html#performance-optimization",
    "href": "posts/mobilenet-deployment/index.html#performance-optimization",
    "title": "MobileNetV2 PyTorch Docker Deployment Guide",
    "section": "",
    "text": "# Add to model_handler.py for optimization\nimport torch.jit\n\nclass OptimizedMobileNetV2Handler(MobileNetV2Handler):\n    def __init__(self, use_jit=True, use_half_precision=False):\n        super().__init__()\n        self.use_jit = use_jit\n        self.use_half_precision = use_half_precision\n    \n    def load_model(self):\n        super().load_model()\n        \n        if self.use_jit:\n            # TorchScript compilation for faster inference\n            self.model = torch.jit.script(self.model)\n            logger.info(\"Model compiled with TorchScript\")\n        \n        if self.use_half_precision and self.device.type == 'cuda':\n            # Half precision for GPU\n            self.model = self.model.half()\n            logger.info(\"Model converted to half precision\")\n\n\n\n# Multi-stage build for smaller image\nFROM python:3.11-slim as builder\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --user --no-cache-dir -r requirements.txt\n\nFROM python:3.11-slim\n\nWORKDIR /app\nCOPY --from=builder /root/.local /root/.local\nCOPY app/ ./app/\n\n# Make sure scripts in .local are usable\nENV PATH=/root/.local/bin:$PATH\n\nEXPOSE 8000\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]"
  },
  {
    "objectID": "posts/mobilenet-deployment/index.html#monitoring-and-logging",
    "href": "posts/mobilenet-deployment/index.html#monitoring-and-logging",
    "title": "MobileNetV2 PyTorch Docker Deployment Guide",
    "section": "",
    "text": "# Add to main.py\nimport time\nfrom app.utils import profiler\n\n@app.middleware(\"http\")\nasync def log_requests(request, call_next):\n    start_time = time.time()\n    response = await call_next(request)\n    process_time = (time.time() - start_time) * 1000\n    \n    logger.info(f\"{request.method} {request.url.path} - {process_time:.2f}ms\")\n    \n    if request.url.path == \"/predict\":\n        profiler.record_inference_time(process_time)\n    \n    return response\n\n@app.get(\"/stats\")\nasync def get_stats():\n    \"\"\"Get performance statistics\"\"\"\n    return profiler.get_stats()"
  },
  {
    "objectID": "posts/mobilenet-deployment/index.html#cloud-deployment",
    "href": "posts/mobilenet-deployment/index.html#cloud-deployment",
    "title": "MobileNetV2 PyTorch Docker Deployment Guide",
    "section": "",
    "text": "{\n  \"family\": \"mobilenetv2-pytorch-task\",\n  \"networkMode\": \"awsvpc\",\n  \"requiresCompatibilities\": [\"FARGATE\"],\n  \"cpu\": \"1024\",\n  \"memory\": \"2048\",\n  \"containerDefinitions\": [\n    {\n      \"name\": \"mobilenetv2-pytorch-api\",\n      \"image\": \"your-registry/mobilenetv2-pytorch-api:latest\",\n      \"portMappings\": [\n        {\n          \"containerPort\": 8000,\n          \"protocol\": \"tcp\"\n        }\n      ],\n      \"environment\": [\n        {\n          \"name\": \"TORCH_HOME\",\n          \"value\": \"/tmp/.torch\"\n        }\n      ],\n      \"essential\": true,\n      \"logConfiguration\": {\n        \"logDriver\": \"awslogs\",\n        \"options\": {\n          \"awslogs-group\": \"/ecs/mobilenetv2-pytorch\",\n          \"awslogs-region\": \"us-east-1\",\n          \"awslogs-stream-prefix\": \"ecs\"\n        }\n      }\n    }\n  ]\n}\n\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mobilenetv2-pytorch-api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mobilenetv2-pytorch-api\n  template:\n    metadata:\n      labels:\n        app: mobilenetv2-pytorch-api\n    spec:\n      containers:\n      - name: mobilenetv2-pytorch-api\n        image: mobilenetv2-pytorch-api:latest\n        ports:\n        - containerPort: 8000\n        env:\n        - name: TORCH_HOME\n          value: /tmp/.torch\n        resources:\n          requests:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"2Gi\"\n            cpu: \"1000m\"\n        volumeMounts:\n        - name: torch-cache\n          mountPath: /tmp/.torch\n      volumes:\n      - name: torch-cache\n        emptyDir: {}\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mobilenetv2-pytorch-service\nspec:\n  selector:\n    app: mobilenetv2-pytorch-api\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8000\n  type: LoadBalancer\nThis PyTorch-based guide provides the same functionality as the TensorFlow version but uses PyTorch’s ecosystem, including torchvision for pre-trained models, PyTorch transformations for preprocessing, and proper tensor handling throughout the application."
  },
  {
    "objectID": "posts/pandas-to-polars/index.html",
    "href": "posts/pandas-to-polars/index.html",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "As datasets grow in size and complexity, performance and efficiency become critical in data processing. While Pandas has long been the go-to library for data manipulation in Python, it can struggle with speed and memory usage, especially on large datasets. Polars, a newer DataFrame library written in Rust, offers a faster, more memory-efficient alternative with support for lazy evaluation and multi-threading.\nThis guide explores how to convert Pandas DataFrames to Polars, and highlights key differences in syntax, performance, and functionality. Whether you’re looking to speed up your data workflows or just exploring modern tools, understanding the transition from Pandas to Polars is a valuable step.\n\n\n\nInstallation and Setup\nCreating DataFrames\nBasic Operations\nFiltering Data\nGrouping and Aggregation\nJoining/Merging DataFrames\nHandling Missing Values\nString Operations\nTime Series Operations\nPerformance Comparison\nAPI Philosophy Differences\nMigration Guide\n\n\n\n\n\n\n\n# Import pandas\nimport pandas as pd\n\n\n\n\n\n# Import polars\nimport polars as pl\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\n\n# Create DataFrame from dictionary\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'age': [25, 30, 35, 40],\n    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n}\ndf_pd = pd.DataFrame(data)\nprint(df_pd)\n\n      name  age         city\n0    Alice   25     New York\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n3    David   40      Houston\n\n\n\n\n\n\nimport polars as pl\n\n# Create DataFrame from dictionary\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'age': [25, 30, 35, 40],\n    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n}\ndf_pl = pl.DataFrame(data)\nprint(df_pl)\n\nshape: (4, 3)\n┌─────────┬─────┬─────────────┐\n│ name    ┆ age ┆ city        │\n│ ---     ┆ --- ┆ ---         │\n│ str     ┆ i64 ┆ str         │\n╞═════════╪═════╪═════════════╡\n│ Alice   ┆ 25  ┆ New York    │\n│ Bob     ┆ 30  ┆ Los Angeles │\n│ Charlie ┆ 35  ┆ Chicago     │\n│ David   ┆ 40  ┆ Houston     │\n└─────────┴─────┴─────────────┘\n\n\n\n\n\n\n\n\n\n\n\n\n# Select a single column (returns Series)\nseries = df_pd['name']\n\n# Select multiple columns\ndf_subset = df_pd[['name', 'age']]\n\n\n\n\n\n# Select a single column (returns Series)\nseries = df_pl['name']\n# Alternative method\nseries = df_pl.select(pl.col('name')).to_series()\n\n# Select multiple columns\ndf_subset = df_pl.select(['name', 'age'])\n# Alternative method\ndf_subset = df_pl.select(pl.col(['name', 'age']))\n\n\n\n\n\n\n\n\n# Add a new column\ndf_pd['is_adult'] = df_pd['age'] &gt;= 18\n\n# Using assign (creates a new DataFrame)\ndf_pd = df_pd.assign(age_squared=df_pd['age'] ** 2)\n\n\n\n\n\n# Add a new column\ndf_pl = df_pl.with_columns(\n    pl.when(pl.col('age') &gt;= 18).then(True).otherwise(False).alias('is_adult')\n)\n\n# Creating derived columns \ndf_pl = df_pl.with_columns(\n    (pl.col('age') ** 2).alias('age_squared')\n)\n\n# Multiple columns at once\ndf_pl = df_pl.with_columns([\n    pl.col('age').is_null().alias('age_is_null'),\n    (pl.col('age') * 2).alias('age_doubled')\n])\n\n\n\n\n\n\n\n\n# Get summary statistics\nsummary = df_pd.describe()\n\n# Individual statistics\nmean_age = df_pd['age'].mean()\nmedian_age = df_pd['age'].median()\nmin_age = df_pd['age'].min()\nmax_age = df_pd['age'].max()\n\n\n\n\n\n# Get summary statistics\nsummary = df_pl.describe()\n\n# Individual statistics\nmean_age = df_pl.select(pl.col('age').mean()).item()\nmedian_age = df_pl.select(pl.col('age').median()).item()\nmin_age = df_pl.select(pl.col('age').min()).item()\nmax_age = df_pl.select(pl.col('age').max()).item()\n\n\n\n\n\n\n\n\n\n\n\n# Filter rows\nadults = df_pd[df_pd['age'] &gt;= 18]\n\n# Multiple conditions\nfiltered = df_pd[(df_pd['age'] &gt; 30) & (df_pd['city'] == 'Chicago')]\n\n\n\n\n\n# Filter rows\nadults = df_pl.filter(pl.col('age') &gt;= 18)\n\n# Multiple conditions\nfiltered = df_pl.filter((pl.col('age') &gt; 30) & (pl.col('city') == 'Chicago'))\n\n\n\n\n\n\n\n\n# Filter with OR conditions\ndf_filtered = df_pd[(df_pd['city'] == 'New York') | (df_pd['city'] == 'Chicago')]\n\n# Using isin\ncities = ['New York', 'Chicago']\ndf_filtered = df_pd[df_pd['city'].isin(cities)]\n\n# String contains\ndf_filtered = df_pd[df_pd['name'].str.contains('li')]\n\n\n\n\n\n# Filter with OR conditions\ndf_filtered = df_pl.filter((pl.col('city') == 'New York') | (pl.col('city') == 'Chicago'))\n\n# Using is_in\ncities = ['New York', 'Chicago']\ndf_filtered = df_pl.filter(pl.col('city').is_in(cities))\n\n# String contains\ndf_filtered = df_pl.filter(pl.col('name').str.contains('li'))\n\n\n\n\n\n\n\n\n\n\n\n# Group by one column and aggregate\ncity_stats = df_pd.groupby('city').agg({\n    'age': ['mean', 'min', 'max', 'count']\n})\n\n# Reset index for flat DataFrame\ncity_stats = city_stats.reset_index()\n\n\n\n\n\n# Group by one column and aggregate\ncity_stats = df_pl.group_by('city').agg([\n    pl.col('age').mean().alias('age_mean'),\n    pl.col('age').min().alias('age_min'),\n    pl.col('age').max().alias('age_max'),\n    pl.col('age').count().alias('age_count')\n])\n\n\n\n\n\n\n\n\n\n\n\n# Create another DataFrame\nemployee_data = {\n    'emp_id': [1, 2, 3, 4],\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'dept': ['HR', 'IT', 'Finance', 'IT']\n}\nemployee_df_pd = pd.DataFrame(employee_data)\n\nsalary_data = {\n    'emp_id': [1, 2, 3, 5],\n    'salary': [50000, 60000, 70000, 80000]\n}\nsalary_df_pd = pd.DataFrame(salary_data)\n\n# Inner join\nmerged_df = employee_df_pd.merge(\n    salary_df_pd,\n    on='emp_id',\n    how='inner'\n)\n\n\n\n\n\n# Create another DataFrame\nemployee_data = {\n    'emp_id': [1, 2, 3, 4],\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'dept': ['HR', 'IT', 'Finance', 'IT']\n}\nemployee_df_pl = pl.DataFrame(employee_data)\n\nsalary_data = {\n    'emp_id': [1, 2, 3, 5],\n    'salary': [50000, 60000, 70000, 80000]\n}\nsalary_df_pl = pl.DataFrame(salary_data)\n\n# Inner join\nmerged_df = employee_df_pl.join(\n    salary_df_pl,\n    on='emp_id',\n    how='inner'\n)\n\n\n\n\n\n\n\n\n# Left join\nleft_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='left')\n\n# Right join\nright_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='right')\n\n# Outer join\nouter_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='outer')\n\n\n\n\n\n# Left join\nleft_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='left')\n\n# Right join\nright_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='right')\n\n# Outer join\nouter_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='full')\n\n\n\n\n\n\n\n\n\n\n\n# Check for missing values\nmissing_count = df_pd.isnull().sum()\n\n# Check if any column has missing values\nhas_missing = df_pd.isnull().any().any()\n\n\n\n\n\n# Check for missing values\nmissing_count = df_pl.null_count()\n\n# Check if specific column has missing values\nhas_missing = df_pl.select(pl.col('age').is_null().any()).item()\n\n\n\n\n\n\n\n\n# Drop rows with any missing values\ndf_pd_clean = df_pd.dropna()\n\n# Fill missing values\ndf_pd_filled = df_pd.fillna({\n    'age': 0,\n    'city': 'Unknown'\n})\n\n# Forward fill\ndf_pd_ffill = df_pd.ffill()\n\n\n\n\n\n# Drop rows with any missing values\ndf_pl_clean = df_pl.drop_nulls()\n\n# Fill missing values\ndf_pl_filled = df_pl.with_columns([\n    pl.col('age').fill_null(0),\n    pl.col('city').fill_null('Unknown')\n])\n\n# Forward fill\ndf_pl_ffill = df_pl.with_columns([\n    pl.col('age').fill_null(strategy='forward'),\n    pl.col('city').fill_null(strategy='forward')\n])\n\n\n\n\n\n\n\n\n\n\n\n# Convert to uppercase\ndf_pd['name_upper'] = df_pd['name'].str.upper()\n\n# Get string length\ndf_pd['name_length'] = df_pd['name'].str.len()\n\n# Extract substring\ndf_pd['name_first_char'] = df_pd['name'].str[0]\n\n# Replace substrings\ndf_pd['city_replaced'] = df_pd['city'].str.replace('New', 'Old')\n\n\n\n\n\n# Convert to uppercase\ndf_pl = df_pl.with_columns(pl.col('name').str.to_uppercase().alias('name_upper'))\n\n# Get string length\ndf_pl = df_pl.with_columns(pl.col('name').str.len_chars().alias('name_length'))\n\n# Extract substring \ndf_pl = df_pl.with_columns(pl.col('name').str.slice(0, 1).alias('name_first_char'))\n\n# Replace substrings\ndf_pl = df_pl.with_columns(pl.col('city').str.replace('New', 'Old').alias('city_replaced'))\n\n\n\n\n\n\n\n\n# Split string\ndf_pd['first_word'] = df_pd['city'].str.split(' ').str[0]\n\n# Pattern matching\nhas_new = df_pd['city'].str.contains('New')\n\n# Extract with regex\ndf_pd['extracted'] = df_pd['city'].str.extract(r'(\\w+)\\s')\n\n\n\n\n\n# Split string\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.split(' ').list.get(0).alias('first_word')\n)\n\n# Pattern matching\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.contains('New').alias('has_new')\n)\n\n# Extract with regex\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.extract(r'(\\w+)\\s').alias('extracted')\n)\n\n\n\n\n\n\n\n\n\n\n\n# Create DataFrame with dates\ndates_pd = pd.DataFrame({\n    'date_str': ['2023-01-01', '2023-02-15', '2023-03-30']\n})\n\n# Parse dates\ndates_pd['date'] = pd.to_datetime(dates_pd['date_str'])\n\n# Extract components\ndates_pd['year'] = dates_pd['date'].dt.year\ndates_pd['month'] = dates_pd['date'].dt.month\ndates_pd['day'] = dates_pd['date'].dt.day\ndates_pd['weekday'] = dates_pd['date'].dt.day_name()\n\n\n\n\n\n# Create DataFrame with dates\ndates_pl = pl.DataFrame({\n    'date_str': ['2023-01-01', '2023-02-15', '2023-03-30']\n})\n\n# Parse dates\ndates_pl = dates_pl.with_columns(\n    pl.col('date_str').str.strptime(pl.Datetime, '%Y-%m-%d').alias('date')\n)\n\n# Extract components\ndates_pl = dates_pl.with_columns([\n    pl.col('date').dt.year().alias('year'),\n    pl.col('date').dt.month().alias('month'),\n    pl.col('date').dt.day().alias('day'),\n    pl.col('date').dt.weekday().replace_strict({\n        0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', \n        3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'\n    }, default=\"unknown\").alias('weekday')\n])\n\n\n\n\n\n\n\n\n# Add days\ndates_pd['next_week'] = dates_pd['date'] + pd.Timedelta(days=7)\n\n# Date difference\ndate_range = pd.date_range(start='2023-01-01', end='2023-01-10')\ndf_dates = pd.DataFrame({'date': date_range})\ndf_dates['days_since_start'] = (df_dates['date'] - df_dates['date'].min()).dt.days\n\n\n\n\n\n# Add days\ndates_pl = dates_pl.with_columns(\n    (pl.col('date') + pl.duration(days=7)).alias('next_week')\n)\n\n# Date difference\ndate_range = pd.date_range(start='2023-01-01', end='2023-01-10')  # Using pandas to generate range\ndf_dates = pl.DataFrame({'date': date_range})\ndf_dates = df_dates.with_columns(\n    (pl.col('date') - pl.col('date').min()).dt.total_days().alias('days_since_start')\n)\n\n\n\n\n\n\nThis section demonstrates performance differences between pandas and polars for a large dataset operation.\n\nimport pandas as pd\nimport polars as pl\nimport time\nimport numpy as np\n\n# Generate a large dataset (10 million rows)\nn = 10_000_000\ndata = {\n    'id': np.arange(n),\n    'value': np.random.randn(n),\n    'group': np.random.choice(['A', 'B', 'C', 'D'], n)\n}\n\n# Convert to pandas DataFrame\ndf_pd = pd.DataFrame(data)\n\n# Convert to polars DataFrame\ndf_pl = pl.DataFrame(data)\n\n# Benchmark: Group by and calculate mean, min, max\nprint(\"Running pandas groupby...\")\nstart = time.time()\nresult_pd = df_pd.groupby('group').agg({\n    'value': ['mean', 'min', 'max', 'count']\n})\npd_time = time.time() - start\nprint(f\"Pandas time: {pd_time:.4f} seconds\")\n\nprint(\"Running polars groupby...\")\nstart = time.time()\nresult_pl = df_pl.group_by('group').agg([\n    pl.col('value').mean().alias('value_mean'),\n    pl.col('value').min().alias('value_min'),\n    pl.col('value').max().alias('value_max'),\n    pl.col('value').count().alias('value_count')\n])\npl_time = time.time() - start\nprint(f\"Polars time: {pl_time:.4f} seconds\")\n\nprint(f\"Polars is {pd_time / pl_time:.2f}x faster\")\n\nRunning pandas groupby...\nPandas time: 0.2320 seconds\nRunning polars groupby...\nPolars time: 0.0545 seconds\nPolars is 4.26x faster\n\n\nTypically, for operations like this, Polars will be 3-10x faster than pandas, especially as data sizes increase. The performance gap widens further with more complex operations that can benefit from query optimization.\n\n\n\nPandas and Polars differ in several fundamental aspects:\n\n\nPandas uses eager execution by default:\nPolars supports both eager and lazy execution:\n\n\n\nPandas often uses assignment operations:\n\n# Many pandas operations use in-place assignment\npd_df['new_col'] = pd_df['new_col'] * 2\npd_df['new_col'] = pd_df['new_col'].fillna(0)\n\n# Some operations return new DataFrames\npd_df = pd_df.sort_values('new_col')\n\nPolars consistently uses method chaining:\n\n# All operations return new DataFrames and can be chained\npl_df = (pl_df\n    .with_columns((pl.col('new_col') * 2).alias('new_col'))\n    .with_columns(pl.col('new_col').fill_null(0))\n    .sort('new_col')\n)\n\n\n\n\nPandas directly references columns:\n\npd_df['result'] = pd_df['age'] + pd_df['new_col']\nfiltered = pd_df[pd_df['age'] &gt; pd_df['age'].mean()]\n\nPolars uses an expression API:\n\npl_df = pl_df.with_columns(\n    (pl.col('age') + pl.col('new_col')).alias('result')\n)\nfiltered = pl_df.filter(pl.col('age') &gt; pl.col('age').mean())\n\n\n\n\n\nIf you’re transitioning from pandas to polars, here are key mappings between common operations:\n\n\n\n\n\n\n\n\nOperation\nPandas\nPolars\n\n\n\n\nRead CSV\npd.read_csv('file.csv')\npl.read_csv('file.csv')\n\n\nSelect columns\ndf[['col1', 'col2']]\ndf.select(['col1', 'col2'])\n\n\nAdd column\ndf['new'] = df['col1'] * 2\ndf.with_columns((pl.col('col1') * 2).alias('new'))\n\n\nFilter rows\ndf[df['col'] &gt; 5]\ndf.filter(pl.col('col') &gt; 5)\n\n\nSort\ndf.sort_values('col')\ndf.sort('col')\n\n\nGroup by\ndf.groupby('col').agg({'val': 'sum'})\ndf.group_by('col').agg(pl.col('val').sum())\n\n\nJoin\ndf1.merge(df2, on='key')\ndf1.join(df2, on='key')\n\n\nFill NA\ndf.fillna(0)\ndf.fill_null(0)\n\n\nDrop NA\ndf.dropna()\ndf.drop_nulls()\n\n\nRename\ndf.rename(columns={'a': 'b'})\ndf.rename({'a': 'b'})\n\n\nUnique values\ndf['col'].unique()\ndf.select(pl.col('col').unique())\n\n\nValue counts\ndf['col'].value_counts()\ndf.group_by('col').count()\n\n\n\n\n\n\nThink in expressions: Use pl.col() to reference columns in operations\nEmbrace method chaining: String operations together instead of intermediate variables\nTry lazy execution: For complex operations, use pl.scan_csv() and lazy operations\nUse with_columns(): Instead of direct assignment, use with_columns for adding/modifying columns\nLearn the expression functions: Many operations like string manipulation use different syntax\n\n\n\n\nDespite Polars’ advantages, pandas might still be preferred when:\n\nWorking with existing codebases heavily dependent on pandas\nUsing specialized libraries that only support pandas (some visualization tools)\nDealing with very small datasets where performance isn’t critical\nUsing pandas-specific features without polars equivalents\nWorking with time series data that benefits from pandas’ specialized functionality\n\n\n\n\n\nPolars offers significant performance improvements and a more consistent API compared to pandas, particularly for large datasets and complex operations. While the syntax differences require some adjustment, the benefits in speed and memory efficiency make it a compelling choice for modern data analysis workflows.\nBoth libraries have their place in the Python data ecosystem. Pandas remains the more mature option with broader ecosystem compatibility, while Polars represents the future of high-performance data processing. For new projects dealing with large datasets, Polars is increasingly becoming the recommended choice."
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#table-of-contents",
    "href": "posts/pandas-to-polars/index.html#table-of-contents",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "Installation and Setup\nCreating DataFrames\nBasic Operations\nFiltering Data\nGrouping and Aggregation\nJoining/Merging DataFrames\nHandling Missing Values\nString Operations\nTime Series Operations\nPerformance Comparison\nAPI Philosophy Differences\nMigration Guide"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#installation-and-setup",
    "href": "posts/pandas-to-polars/index.html#installation-and-setup",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Import pandas\nimport pandas as pd\n\n\n\n\n\n# Import polars\nimport polars as pl"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#creating-dataframes",
    "href": "posts/pandas-to-polars/index.html#creating-dataframes",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "import pandas as pd\n\n# Create DataFrame from dictionary\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'age': [25, 30, 35, 40],\n    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n}\ndf_pd = pd.DataFrame(data)\nprint(df_pd)\n\n      name  age         city\n0    Alice   25     New York\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n3    David   40      Houston\n\n\n\n\n\n\nimport polars as pl\n\n# Create DataFrame from dictionary\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'age': [25, 30, 35, 40],\n    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n}\ndf_pl = pl.DataFrame(data)\nprint(df_pl)\n\nshape: (4, 3)\n┌─────────┬─────┬─────────────┐\n│ name    ┆ age ┆ city        │\n│ ---     ┆ --- ┆ ---         │\n│ str     ┆ i64 ┆ str         │\n╞═════════╪═════╪═════════════╡\n│ Alice   ┆ 25  ┆ New York    │\n│ Bob     ┆ 30  ┆ Los Angeles │\n│ Charlie ┆ 35  ┆ Chicago     │\n│ David   ┆ 40  ┆ Houston     │\n└─────────┴─────┴─────────────┘"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#basic-operations",
    "href": "posts/pandas-to-polars/index.html#basic-operations",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Select a single column (returns Series)\nseries = df_pd['name']\n\n# Select multiple columns\ndf_subset = df_pd[['name', 'age']]\n\n\n\n\n\n# Select a single column (returns Series)\nseries = df_pl['name']\n# Alternative method\nseries = df_pl.select(pl.col('name')).to_series()\n\n# Select multiple columns\ndf_subset = df_pl.select(['name', 'age'])\n# Alternative method\ndf_subset = df_pl.select(pl.col(['name', 'age']))\n\n\n\n\n\n\n\n\n# Add a new column\ndf_pd['is_adult'] = df_pd['age'] &gt;= 18\n\n# Using assign (creates a new DataFrame)\ndf_pd = df_pd.assign(age_squared=df_pd['age'] ** 2)\n\n\n\n\n\n# Add a new column\ndf_pl = df_pl.with_columns(\n    pl.when(pl.col('age') &gt;= 18).then(True).otherwise(False).alias('is_adult')\n)\n\n# Creating derived columns \ndf_pl = df_pl.with_columns(\n    (pl.col('age') ** 2).alias('age_squared')\n)\n\n# Multiple columns at once\ndf_pl = df_pl.with_columns([\n    pl.col('age').is_null().alias('age_is_null'),\n    (pl.col('age') * 2).alias('age_doubled')\n])\n\n\n\n\n\n\n\n\n# Get summary statistics\nsummary = df_pd.describe()\n\n# Individual statistics\nmean_age = df_pd['age'].mean()\nmedian_age = df_pd['age'].median()\nmin_age = df_pd['age'].min()\nmax_age = df_pd['age'].max()\n\n\n\n\n\n# Get summary statistics\nsummary = df_pl.describe()\n\n# Individual statistics\nmean_age = df_pl.select(pl.col('age').mean()).item()\nmedian_age = df_pl.select(pl.col('age').median()).item()\nmin_age = df_pl.select(pl.col('age').min()).item()\nmax_age = df_pl.select(pl.col('age').max()).item()"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#filtering-data",
    "href": "posts/pandas-to-polars/index.html#filtering-data",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Filter rows\nadults = df_pd[df_pd['age'] &gt;= 18]\n\n# Multiple conditions\nfiltered = df_pd[(df_pd['age'] &gt; 30) & (df_pd['city'] == 'Chicago')]\n\n\n\n\n\n# Filter rows\nadults = df_pl.filter(pl.col('age') &gt;= 18)\n\n# Multiple conditions\nfiltered = df_pl.filter((pl.col('age') &gt; 30) & (pl.col('city') == 'Chicago'))\n\n\n\n\n\n\n\n\n# Filter with OR conditions\ndf_filtered = df_pd[(df_pd['city'] == 'New York') | (df_pd['city'] == 'Chicago')]\n\n# Using isin\ncities = ['New York', 'Chicago']\ndf_filtered = df_pd[df_pd['city'].isin(cities)]\n\n# String contains\ndf_filtered = df_pd[df_pd['name'].str.contains('li')]\n\n\n\n\n\n# Filter with OR conditions\ndf_filtered = df_pl.filter((pl.col('city') == 'New York') | (pl.col('city') == 'Chicago'))\n\n# Using is_in\ncities = ['New York', 'Chicago']\ndf_filtered = df_pl.filter(pl.col('city').is_in(cities))\n\n# String contains\ndf_filtered = df_pl.filter(pl.col('name').str.contains('li'))"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#grouping-and-aggregation",
    "href": "posts/pandas-to-polars/index.html#grouping-and-aggregation",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Group by one column and aggregate\ncity_stats = df_pd.groupby('city').agg({\n    'age': ['mean', 'min', 'max', 'count']\n})\n\n# Reset index for flat DataFrame\ncity_stats = city_stats.reset_index()\n\n\n\n\n\n# Group by one column and aggregate\ncity_stats = df_pl.group_by('city').agg([\n    pl.col('age').mean().alias('age_mean'),\n    pl.col('age').min().alias('age_min'),\n    pl.col('age').max().alias('age_max'),\n    pl.col('age').count().alias('age_count')\n])"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#joiningmerging-dataframes",
    "href": "posts/pandas-to-polars/index.html#joiningmerging-dataframes",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Create another DataFrame\nemployee_data = {\n    'emp_id': [1, 2, 3, 4],\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'dept': ['HR', 'IT', 'Finance', 'IT']\n}\nemployee_df_pd = pd.DataFrame(employee_data)\n\nsalary_data = {\n    'emp_id': [1, 2, 3, 5],\n    'salary': [50000, 60000, 70000, 80000]\n}\nsalary_df_pd = pd.DataFrame(salary_data)\n\n# Inner join\nmerged_df = employee_df_pd.merge(\n    salary_df_pd,\n    on='emp_id',\n    how='inner'\n)\n\n\n\n\n\n# Create another DataFrame\nemployee_data = {\n    'emp_id': [1, 2, 3, 4],\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'dept': ['HR', 'IT', 'Finance', 'IT']\n}\nemployee_df_pl = pl.DataFrame(employee_data)\n\nsalary_data = {\n    'emp_id': [1, 2, 3, 5],\n    'salary': [50000, 60000, 70000, 80000]\n}\nsalary_df_pl = pl.DataFrame(salary_data)\n\n# Inner join\nmerged_df = employee_df_pl.join(\n    salary_df_pl,\n    on='emp_id',\n    how='inner'\n)\n\n\n\n\n\n\n\n\n# Left join\nleft_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='left')\n\n# Right join\nright_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='right')\n\n# Outer join\nouter_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='outer')\n\n\n\n\n\n# Left join\nleft_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='left')\n\n# Right join\nright_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='right')\n\n# Outer join\nouter_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='full')"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#handling-missing-values",
    "href": "posts/pandas-to-polars/index.html#handling-missing-values",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Check for missing values\nmissing_count = df_pd.isnull().sum()\n\n# Check if any column has missing values\nhas_missing = df_pd.isnull().any().any()\n\n\n\n\n\n# Check for missing values\nmissing_count = df_pl.null_count()\n\n# Check if specific column has missing values\nhas_missing = df_pl.select(pl.col('age').is_null().any()).item()\n\n\n\n\n\n\n\n\n# Drop rows with any missing values\ndf_pd_clean = df_pd.dropna()\n\n# Fill missing values\ndf_pd_filled = df_pd.fillna({\n    'age': 0,\n    'city': 'Unknown'\n})\n\n# Forward fill\ndf_pd_ffill = df_pd.ffill()\n\n\n\n\n\n# Drop rows with any missing values\ndf_pl_clean = df_pl.drop_nulls()\n\n# Fill missing values\ndf_pl_filled = df_pl.with_columns([\n    pl.col('age').fill_null(0),\n    pl.col('city').fill_null('Unknown')\n])\n\n# Forward fill\ndf_pl_ffill = df_pl.with_columns([\n    pl.col('age').fill_null(strategy='forward'),\n    pl.col('city').fill_null(strategy='forward')\n])"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#string-operations",
    "href": "posts/pandas-to-polars/index.html#string-operations",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Convert to uppercase\ndf_pd['name_upper'] = df_pd['name'].str.upper()\n\n# Get string length\ndf_pd['name_length'] = df_pd['name'].str.len()\n\n# Extract substring\ndf_pd['name_first_char'] = df_pd['name'].str[0]\n\n# Replace substrings\ndf_pd['city_replaced'] = df_pd['city'].str.replace('New', 'Old')\n\n\n\n\n\n# Convert to uppercase\ndf_pl = df_pl.with_columns(pl.col('name').str.to_uppercase().alias('name_upper'))\n\n# Get string length\ndf_pl = df_pl.with_columns(pl.col('name').str.len_chars().alias('name_length'))\n\n# Extract substring \ndf_pl = df_pl.with_columns(pl.col('name').str.slice(0, 1).alias('name_first_char'))\n\n# Replace substrings\ndf_pl = df_pl.with_columns(pl.col('city').str.replace('New', 'Old').alias('city_replaced'))\n\n\n\n\n\n\n\n\n# Split string\ndf_pd['first_word'] = df_pd['city'].str.split(' ').str[0]\n\n# Pattern matching\nhas_new = df_pd['city'].str.contains('New')\n\n# Extract with regex\ndf_pd['extracted'] = df_pd['city'].str.extract(r'(\\w+)\\s')\n\n\n\n\n\n# Split string\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.split(' ').list.get(0).alias('first_word')\n)\n\n# Pattern matching\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.contains('New').alias('has_new')\n)\n\n# Extract with regex\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.extract(r'(\\w+)\\s').alias('extracted')\n)"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#time-series-operations",
    "href": "posts/pandas-to-polars/index.html#time-series-operations",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Create DataFrame with dates\ndates_pd = pd.DataFrame({\n    'date_str': ['2023-01-01', '2023-02-15', '2023-03-30']\n})\n\n# Parse dates\ndates_pd['date'] = pd.to_datetime(dates_pd['date_str'])\n\n# Extract components\ndates_pd['year'] = dates_pd['date'].dt.year\ndates_pd['month'] = dates_pd['date'].dt.month\ndates_pd['day'] = dates_pd['date'].dt.day\ndates_pd['weekday'] = dates_pd['date'].dt.day_name()\n\n\n\n\n\n# Create DataFrame with dates\ndates_pl = pl.DataFrame({\n    'date_str': ['2023-01-01', '2023-02-15', '2023-03-30']\n})\n\n# Parse dates\ndates_pl = dates_pl.with_columns(\n    pl.col('date_str').str.strptime(pl.Datetime, '%Y-%m-%d').alias('date')\n)\n\n# Extract components\ndates_pl = dates_pl.with_columns([\n    pl.col('date').dt.year().alias('year'),\n    pl.col('date').dt.month().alias('month'),\n    pl.col('date').dt.day().alias('day'),\n    pl.col('date').dt.weekday().replace_strict({\n        0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', \n        3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'\n    }, default=\"unknown\").alias('weekday')\n])\n\n\n\n\n\n\n\n\n# Add days\ndates_pd['next_week'] = dates_pd['date'] + pd.Timedelta(days=7)\n\n# Date difference\ndate_range = pd.date_range(start='2023-01-01', end='2023-01-10')\ndf_dates = pd.DataFrame({'date': date_range})\ndf_dates['days_since_start'] = (df_dates['date'] - df_dates['date'].min()).dt.days\n\n\n\n\n\n# Add days\ndates_pl = dates_pl.with_columns(\n    (pl.col('date') + pl.duration(days=7)).alias('next_week')\n)\n\n# Date difference\ndate_range = pd.date_range(start='2023-01-01', end='2023-01-10')  # Using pandas to generate range\ndf_dates = pl.DataFrame({'date': date_range})\ndf_dates = df_dates.with_columns(\n    (pl.col('date') - pl.col('date').min()).dt.total_days().alias('days_since_start')\n)"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#performance-comparison",
    "href": "posts/pandas-to-polars/index.html#performance-comparison",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "This section demonstrates performance differences between pandas and polars for a large dataset operation.\n\nimport pandas as pd\nimport polars as pl\nimport time\nimport numpy as np\n\n# Generate a large dataset (10 million rows)\nn = 10_000_000\ndata = {\n    'id': np.arange(n),\n    'value': np.random.randn(n),\n    'group': np.random.choice(['A', 'B', 'C', 'D'], n)\n}\n\n# Convert to pandas DataFrame\ndf_pd = pd.DataFrame(data)\n\n# Convert to polars DataFrame\ndf_pl = pl.DataFrame(data)\n\n# Benchmark: Group by and calculate mean, min, max\nprint(\"Running pandas groupby...\")\nstart = time.time()\nresult_pd = df_pd.groupby('group').agg({\n    'value': ['mean', 'min', 'max', 'count']\n})\npd_time = time.time() - start\nprint(f\"Pandas time: {pd_time:.4f} seconds\")\n\nprint(\"Running polars groupby...\")\nstart = time.time()\nresult_pl = df_pl.group_by('group').agg([\n    pl.col('value').mean().alias('value_mean'),\n    pl.col('value').min().alias('value_min'),\n    pl.col('value').max().alias('value_max'),\n    pl.col('value').count().alias('value_count')\n])\npl_time = time.time() - start\nprint(f\"Polars time: {pl_time:.4f} seconds\")\n\nprint(f\"Polars is {pd_time / pl_time:.2f}x faster\")\n\nRunning pandas groupby...\nPandas time: 0.2320 seconds\nRunning polars groupby...\nPolars time: 0.0545 seconds\nPolars is 4.26x faster\n\n\nTypically, for operations like this, Polars will be 3-10x faster than pandas, especially as data sizes increase. The performance gap widens further with more complex operations that can benefit from query optimization."
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#api-philosophy-differences",
    "href": "posts/pandas-to-polars/index.html#api-philosophy-differences",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "Pandas and Polars differ in several fundamental aspects:\n\n\nPandas uses eager execution by default:\nPolars supports both eager and lazy execution:\n\n\n\nPandas often uses assignment operations:\n\n# Many pandas operations use in-place assignment\npd_df['new_col'] = pd_df['new_col'] * 2\npd_df['new_col'] = pd_df['new_col'].fillna(0)\n\n# Some operations return new DataFrames\npd_df = pd_df.sort_values('new_col')\n\nPolars consistently uses method chaining:\n\n# All operations return new DataFrames and can be chained\npl_df = (pl_df\n    .with_columns((pl.col('new_col') * 2).alias('new_col'))\n    .with_columns(pl.col('new_col').fill_null(0))\n    .sort('new_col')\n)\n\n\n\n\nPandas directly references columns:\n\npd_df['result'] = pd_df['age'] + pd_df['new_col']\nfiltered = pd_df[pd_df['age'] &gt; pd_df['age'].mean()]\n\nPolars uses an expression API:\n\npl_df = pl_df.with_columns(\n    (pl.col('age') + pl.col('new_col')).alias('result')\n)\nfiltered = pl_df.filter(pl.col('age') &gt; pl.col('age').mean())"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#migration-guide",
    "href": "posts/pandas-to-polars/index.html#migration-guide",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "If you’re transitioning from pandas to polars, here are key mappings between common operations:\n\n\n\n\n\n\n\n\nOperation\nPandas\nPolars\n\n\n\n\nRead CSV\npd.read_csv('file.csv')\npl.read_csv('file.csv')\n\n\nSelect columns\ndf[['col1', 'col2']]\ndf.select(['col1', 'col2'])\n\n\nAdd column\ndf['new'] = df['col1'] * 2\ndf.with_columns((pl.col('col1') * 2).alias('new'))\n\n\nFilter rows\ndf[df['col'] &gt; 5]\ndf.filter(pl.col('col') &gt; 5)\n\n\nSort\ndf.sort_values('col')\ndf.sort('col')\n\n\nGroup by\ndf.groupby('col').agg({'val': 'sum'})\ndf.group_by('col').agg(pl.col('val').sum())\n\n\nJoin\ndf1.merge(df2, on='key')\ndf1.join(df2, on='key')\n\n\nFill NA\ndf.fillna(0)\ndf.fill_null(0)\n\n\nDrop NA\ndf.dropna()\ndf.drop_nulls()\n\n\nRename\ndf.rename(columns={'a': 'b'})\ndf.rename({'a': 'b'})\n\n\nUnique values\ndf['col'].unique()\ndf.select(pl.col('col').unique())\n\n\nValue counts\ndf['col'].value_counts()\ndf.group_by('col').count()\n\n\n\n\n\n\nThink in expressions: Use pl.col() to reference columns in operations\nEmbrace method chaining: String operations together instead of intermediate variables\nTry lazy execution: For complex operations, use pl.scan_csv() and lazy operations\nUse with_columns(): Instead of direct assignment, use with_columns for adding/modifying columns\nLearn the expression functions: Many operations like string manipulation use different syntax\n\n\n\n\nDespite Polars’ advantages, pandas might still be preferred when:\n\nWorking with existing codebases heavily dependent on pandas\nUsing specialized libraries that only support pandas (some visualization tools)\nDealing with very small datasets where performance isn’t critical\nUsing pandas-specific features without polars equivalents\nWorking with time series data that benefits from pandas’ specialized functionality"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#conclusion",
    "href": "posts/pandas-to-polars/index.html#conclusion",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "Polars offers significant performance improvements and a more consistent API compared to pandas, particularly for large datasets and complex operations. While the syntax differences require some adjustment, the benefits in speed and memory efficiency make it a compelling choice for modern data analysis workflows.\nBoth libraries have their place in the Python data ecosystem. Pandas remains the more mature option with broader ecosystem compatibility, while Polars represents the future of high-performance data processing. For new projects dealing with large datasets, Polars is increasingly becoming the recommended choice."
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html",
    "href": "posts/albumentations-vs-torchvision/index.html",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "This guide compares two popular image augmentation libraries for PyTorch:\n\nTorchVision Transforms: Built-in PyTorch library for basic image transformations\nAlbumentations: Fast, flexible library with advanced augmentation techniques\n\n\n\n\n# TorchVision (comes with PyTorch)\npip install torch torchvision\n\n# Albumentations\npip install albumentations\n\n\n\n\nimport torch\nimport torchvision.transforms as T\nfrom torchvision.transforms import functional as TF\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nTorchVision\nAlbumentations\n\n\n\n\nInput Format\nPIL Image, Tensor\nNumPy array (OpenCV format)\n\n\nPerformance\nModerate\nFast (optimized)\n\n\nAugmentation Variety\nBasic to intermediate\nExtensive advanced options\n\n\nBounding Box Support\nLimited\nExcellent\n\n\nSegmentation Masks\nBasic\nAdvanced\n\n\nKeypoint Support\nNo\nYes\n\n\nProbability Control\nLimited\nFine-grained\n\n\n\n\n\n\n\n\n\n# TorchVision approach\ndef load_image_torchvision(path):\n    return Image.open(path).convert('RGB')\n\n# Albumentations approach  \ndef load_image_albumentations(path):\n    image = cv2.imread(path)\n    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# Example usage\nimg_path = \"cat.jpg\"\ntorch_img = load_image_torchvision(img_path)\nalbu_img = load_image_albumentations(img_path)\n\n\n\n\n\n\n# TorchVision transforms\ntorchvision_transform = T.Compose([\n    T.Resize((224, 224)),\n    T.RandomHorizontalFlip(p=0.5),\n    T.RandomRotation(degrees=15),\n    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Albumentations equivalent\nalbumentations_transform = A.Compose([\n    A.Resize(224, 224),\n    A.HorizontalFlip(p=0.5),\n    A.Rotate(limit=15, p=1.0),\n    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=1.0),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2()\n])\n\n# Apply transforms\ntorch_result = torchvision_transform(torch_img)\nalbu_result = albumentations_transform(image=albu_img)['image']\n\n\n\n\n\n\n\n# Advanced geometric transformations\nadvanced_geometric = A.Compose([\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30, p=0.8),\n    A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.3),\n    A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3),\n    A.OpticalDistortion(distort_limit=0.5, shift_limit=0.5, p=0.3)\n])\n\n# Weather and lighting effects\nweather_effects = A.Compose([\n    A.RandomRain(slant_lower=-10, slant_upper=10, drop_length=20, p=0.3),\n    A.RandomSnow(snow_point_lower=0.1, snow_point_upper=0.3, p=0.3),\n    A.RandomFog(fog_coef_lower=0.3, fog_coef_upper=1, p=0.3),\n    A.RandomSunFlare(flare_roi=(0, 0, 1, 0.5), angle_lower=0, p=0.3)\n])\n\n# Noise and blur effects\nnoise_blur = A.Compose([\n    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n    A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=0.3),\n    A.MotionBlur(blur_limit=7, p=0.3),\n    A.MedianBlur(blur_limit=7, p=0.3),\n    A.Blur(blur_limit=7, p=0.3)\n])\n\n\n\nimport torchvision.transforms.v2 as T2\n\n# TorchVision v2 with better functionality\ntorchvision_v2_transform = T2.Compose([\n    T2.Resize((224, 224)),\n    T2.RandomHorizontalFlip(p=0.5),\n    T2.RandomChoice([\n        T2.ColorJitter(brightness=0.3),\n        T2.ColorJitter(contrast=0.3),\n        T2.ColorJitter(saturation=0.3)\n    ]),\n    T2.RandomApply([T2.GaussianBlur(kernel_size=3)], p=0.3),\n    T2.ToTensor(),\n    T2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n\n\n\n\n\n# Define bounding boxes in Pascal VOC format (x_min, y_min, x_max, y_max)\nbboxes = [[50, 50, 150, 150, 'person'], [200, 100, 300, 200, 'car']]\n\nbbox_transform = A.Compose([\n    A.Resize(416, 416),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.3),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),\n], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n\n# Apply transform\ntransformed = bbox_transform(\n    image=image, \n    bboxes=[[50, 50, 150, 150], [200, 100, 300, 200]], \n    class_labels=['person', 'car']\n)\n\ntransformed_image = transformed['image']\ntransformed_bboxes = transformed['bboxes']\ntransformed_labels = transformed['class_labels']\n\n\n\n# TorchVision v2 has some bbox support\nimport torchvision.transforms.v2 as T2\n\nbbox_torchvision = T2.Compose([\n    T2.Resize((416, 416)),\n    T2.RandomHorizontalFlip(p=0.5),\n    T2.ToTensor()\n])\n\n# Requires manual handling of bounding boxes\n# Less intuitive than Albumentations\n\n\n\n\n\n\n# Segmentation mask handling\nsegmentation_transform = A.Compose([\n    A.Resize(512, 512),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.3),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),\n    A.Normalize(),\n    ToTensorV2()\n])\n\n# Apply to image and mask simultaneously\nresult = segmentation_transform(image=image, mask=mask)\ntransformed_image = result['image']\ntransformed_mask = result['mask']\n\n\n\n# TorchVision requires separate handling\ndef apply_transform_to_mask(transform, image, mask):\n    # Manual synchronization needed\n    seed = torch.randint(0, 2**32, size=(1,)).item()\n    \n    torch.manual_seed(seed)\n    transformed_image = transform(image)\n    \n    torch.manual_seed(seed)\n    # Apply only geometric transforms to mask\n    mask_transform = T.Compose([\n        T.Resize((512, 512)),\n        T.RandomHorizontalFlip(p=0.5),\n        T.ToTensor()\n    ])\n    transformed_mask = mask_transform(mask)\n    \n    return transformed_image, transformed_mask\n\n\n\n\n\nimport time\n\ndef benchmark_transforms(image, iterations=1000):\n    # TorchVision timing\n    start_time = time.time()\n    for _ in range(iterations):\n        _ = torchvision_transform(image.copy())\n    torch_time = time.time() - start_time\n    \n    # Convert to numpy for Albumentations\n    np_image = np.array(image)\n    \n    # Albumentations timing\n    start_time = time.time()\n    for _ in range(iterations):\n        _ = albumentations_transform(image=np_image.copy())\n    albu_time = time.time() - start_time\n    \n    print(f\"TorchVision: {torch_time:.3f}s ({iterations} iterations)\")\n    print(f\"Albumentations: {albu_time:.3f}s ({iterations} iterations)\")\n    print(f\"Speedup: {torch_time/albu_time:.2f}x\")\n\n# Run benchmark\nbenchmark_transforms(torch_img)\n\nTorchVision: 38.434s (1000 iterations)\nAlbumentations: 3.152s (1000 iterations)\nSpeedup: 12.19x\n\n\n\n\n\n\n\ndef create_training_pipeline():\n    return A.Compose([\n        # Geometric transformations\n        A.OneOf([\n            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30),\n            A.ElasticTransform(alpha=1, sigma=50),\n            A.GridDistortion(num_steps=5, distort_limit=0.3),\n        ], p=0.5),\n        \n        # Color augmentations\n        A.OneOf([\n            A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20),\n            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n        ], p=0.8),\n        \n        # Noise and blur\n        A.OneOf([\n            A.GaussNoise(var_limit=(10, 50)),\n            A.ISONoise(),\n            A.MultiplicativeNoise(),\n        ], p=0.3),\n        \n        A.OneOf([\n            A.MotionBlur(blur_limit=5),\n            A.MedianBlur(blur_limit=5),\n            A.GaussianBlur(blur_limit=5),\n        ], p=0.3),\n        \n        # Final processing\n        A.Resize(224, 224),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n\ndef create_validation_pipeline():\n    return A.Compose([\n        A.Resize(224, 224),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n\n\n\ndef create_simple_training_pipeline():\n    return T.Compose([\n        T.Resize((224, 224)),\n        T.RandomHorizontalFlip(p=0.5),\n        T.RandomRotation(degrees=15),\n        T.RandomApply([T.ColorJitter(0.3, 0.3, 0.3, 0.1)], p=0.8),\n        T.RandomApply([T.GaussianBlur(kernel_size=3)], p=0.3),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\ndef create_simple_validation_pipeline():\n    return T.Compose([\n        T.Resize((224, 224)),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n\n\n\n\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CustomDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Load image for Albumentations (OpenCV format)\n        image = cv2.imread(self.image_paths[idx])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.transform:\n            transformed = self.transform(image=image)\n            image = transformed['image']\n        \n        return image, self.labels[idx]\n\n# Usage\ntrain_dataset = CustomDataset(\n    image_paths=train_paths,\n    labels=train_labels,\n    transform=create_training_pipeline()\n)\n\n\n\nclass TorchVisionDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Load image for TorchVision (PIL format)\n        image = Image.open(self.image_paths[idx]).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, self.labels[idx]\n\n# Usage\ntrain_dataset = TorchVisionDataset(\n    image_paths=train_paths,\n    labels=train_labels,\n    transform=create_simple_training_pipeline()\n)\n\n\n\n\n\n\n\nWorking with object detection or segmentation tasks\nNeed advanced augmentation techniques (weather effects, distortions)\nPerformance is critical (processing large datasets)\nWorking with bounding boxes or keypoints\nNeed fine-grained control over augmentation probabilities\nDealing with medical or satellite imagery\n\n\n\n\n\nBuilding simple image classification models\nWorking within pure PyTorch ecosystem\nNeed basic augmentations only\nPrototyping quickly\nFollowing PyTorch tutorials or established workflows\nWorking with pre-trained models that expect specific preprocessing\n\n\n\n\n\n\n\n# Use ReplayCompose for debugging\nreplay_transform = A.ReplayCompose([\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.3),\n])\n\nresult = replay_transform(image=image)\ntransformed_image = result['image']\nreplay_data = result['replay']\n\n# Apply same transforms to another image\nresult2 = A.ReplayCompose.replay(replay_data, image=another_image)\n\n# Efficient bbox handling\nbbox_params = A.BboxParams(\n    format='pascal_voc',\n    min_area=1024,  # Filter out small boxes\n    min_visibility=0.3,  # Filter out mostly occluded boxes\n    label_fields=['class_labels']\n)\n\n\n\n# Use functional API for custom control\ndef custom_transform(image):\n    if torch.rand(1) &lt; 0.5:\n        image = TF.hflip(image)\n    \n    # Apply rotation with custom logic\n    angle = torch.randint(-30, 30, (1,)).item()\n    image = TF.rotate(image, angle)\n    \n    return image\n\n# Combine with standard transforms\ncombined_transform = T.Compose([\n    T.Lambda(custom_transform),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n\n\n\nBoth libraries have their strengths:\nAlbumentations excels in:\n\nAdvanced augmentation techniques\nPerformance optimization\nComputer vision tasks beyond classification\nProfessional production environments\n\nTorchVision is ideal for:\n\nSimple classification tasks\nLearning and prototyping\nTight PyTorch integration\nBasic augmentation needs\n\nChoose based on your specific requirements, with Albumentations being the go-to choice for advanced computer vision projects and TorchVision for simpler classification tasks."
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#overview",
    "href": "posts/albumentations-vs-torchvision/index.html#overview",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "This guide compares two popular image augmentation libraries for PyTorch:\n\nTorchVision Transforms: Built-in PyTorch library for basic image transformations\nAlbumentations: Fast, flexible library with advanced augmentation techniques"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#installation",
    "href": "posts/albumentations-vs-torchvision/index.html#installation",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "# TorchVision (comes with PyTorch)\npip install torch torchvision\n\n# Albumentations\npip install albumentations"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#basic-setup-and-imports",
    "href": "posts/albumentations-vs-torchvision/index.html#basic-setup-and-imports",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "import torch\nimport torchvision.transforms as T\nfrom torchvision.transforms import functional as TF\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#key-differences-at-a-glance",
    "href": "posts/albumentations-vs-torchvision/index.html#key-differences-at-a-glance",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "Feature\nTorchVision\nAlbumentations\n\n\n\n\nInput Format\nPIL Image, Tensor\nNumPy array (OpenCV format)\n\n\nPerformance\nModerate\nFast (optimized)\n\n\nAugmentation Variety\nBasic to intermediate\nExtensive advanced options\n\n\nBounding Box Support\nLimited\nExcellent\n\n\nSegmentation Masks\nBasic\nAdvanced\n\n\nKeypoint Support\nNo\nYes\n\n\nProbability Control\nLimited\nFine-grained"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#basic-transformations-comparison",
    "href": "posts/albumentations-vs-torchvision/index.html#basic-transformations-comparison",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "# TorchVision approach\ndef load_image_torchvision(path):\n    return Image.open(path).convert('RGB')\n\n# Albumentations approach  \ndef load_image_albumentations(path):\n    image = cv2.imread(path)\n    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# Example usage\nimg_path = \"cat.jpg\"\ntorch_img = load_image_torchvision(img_path)\nalbu_img = load_image_albumentations(img_path)\n\n\n\n\n\n\n# TorchVision transforms\ntorchvision_transform = T.Compose([\n    T.Resize((224, 224)),\n    T.RandomHorizontalFlip(p=0.5),\n    T.RandomRotation(degrees=15),\n    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Albumentations equivalent\nalbumentations_transform = A.Compose([\n    A.Resize(224, 224),\n    A.HorizontalFlip(p=0.5),\n    A.Rotate(limit=15, p=1.0),\n    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=1.0),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2()\n])\n\n# Apply transforms\ntorch_result = torchvision_transform(torch_img)\nalbu_result = albumentations_transform(image=albu_img)['image']"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#advanced-augmentations",
    "href": "posts/albumentations-vs-torchvision/index.html#advanced-augmentations",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "# Advanced geometric transformations\nadvanced_geometric = A.Compose([\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30, p=0.8),\n    A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.3),\n    A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3),\n    A.OpticalDistortion(distort_limit=0.5, shift_limit=0.5, p=0.3)\n])\n\n# Weather and lighting effects\nweather_effects = A.Compose([\n    A.RandomRain(slant_lower=-10, slant_upper=10, drop_length=20, p=0.3),\n    A.RandomSnow(snow_point_lower=0.1, snow_point_upper=0.3, p=0.3),\n    A.RandomFog(fog_coef_lower=0.3, fog_coef_upper=1, p=0.3),\n    A.RandomSunFlare(flare_roi=(0, 0, 1, 0.5), angle_lower=0, p=0.3)\n])\n\n# Noise and blur effects\nnoise_blur = A.Compose([\n    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n    A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=0.3),\n    A.MotionBlur(blur_limit=7, p=0.3),\n    A.MedianBlur(blur_limit=7, p=0.3),\n    A.Blur(blur_limit=7, p=0.3)\n])\n\n\n\nimport torchvision.transforms.v2 as T2\n\n# TorchVision v2 with better functionality\ntorchvision_v2_transform = T2.Compose([\n    T2.Resize((224, 224)),\n    T2.RandomHorizontalFlip(p=0.5),\n    T2.RandomChoice([\n        T2.ColorJitter(brightness=0.3),\n        T2.ColorJitter(contrast=0.3),\n        T2.ColorJitter(saturation=0.3)\n    ]),\n    T2.RandomApply([T2.GaussianBlur(kernel_size=3)], p=0.3),\n    T2.ToTensor(),\n    T2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#working-with-bounding-boxes",
    "href": "posts/albumentations-vs-torchvision/index.html#working-with-bounding-boxes",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "# Define bounding boxes in Pascal VOC format (x_min, y_min, x_max, y_max)\nbboxes = [[50, 50, 150, 150, 'person'], [200, 100, 300, 200, 'car']]\n\nbbox_transform = A.Compose([\n    A.Resize(416, 416),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.3),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),\n], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n\n# Apply transform\ntransformed = bbox_transform(\n    image=image, \n    bboxes=[[50, 50, 150, 150], [200, 100, 300, 200]], \n    class_labels=['person', 'car']\n)\n\ntransformed_image = transformed['image']\ntransformed_bboxes = transformed['bboxes']\ntransformed_labels = transformed['class_labels']\n\n\n\n# TorchVision v2 has some bbox support\nimport torchvision.transforms.v2 as T2\n\nbbox_torchvision = T2.Compose([\n    T2.Resize((416, 416)),\n    T2.RandomHorizontalFlip(p=0.5),\n    T2.ToTensor()\n])\n\n# Requires manual handling of bounding boxes\n# Less intuitive than Albumentations"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#working-with-segmentation-masks",
    "href": "posts/albumentations-vs-torchvision/index.html#working-with-segmentation-masks",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "# Segmentation mask handling\nsegmentation_transform = A.Compose([\n    A.Resize(512, 512),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.3),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),\n    A.Normalize(),\n    ToTensorV2()\n])\n\n# Apply to image and mask simultaneously\nresult = segmentation_transform(image=image, mask=mask)\ntransformed_image = result['image']\ntransformed_mask = result['mask']\n\n\n\n# TorchVision requires separate handling\ndef apply_transform_to_mask(transform, image, mask):\n    # Manual synchronization needed\n    seed = torch.randint(0, 2**32, size=(1,)).item()\n    \n    torch.manual_seed(seed)\n    transformed_image = transform(image)\n    \n    torch.manual_seed(seed)\n    # Apply only geometric transforms to mask\n    mask_transform = T.Compose([\n        T.Resize((512, 512)),\n        T.RandomHorizontalFlip(p=0.5),\n        T.ToTensor()\n    ])\n    transformed_mask = mask_transform(mask)\n    \n    return transformed_image, transformed_mask"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#performance-comparison",
    "href": "posts/albumentations-vs-torchvision/index.html#performance-comparison",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "import time\n\ndef benchmark_transforms(image, iterations=1000):\n    # TorchVision timing\n    start_time = time.time()\n    for _ in range(iterations):\n        _ = torchvision_transform(image.copy())\n    torch_time = time.time() - start_time\n    \n    # Convert to numpy for Albumentations\n    np_image = np.array(image)\n    \n    # Albumentations timing\n    start_time = time.time()\n    for _ in range(iterations):\n        _ = albumentations_transform(image=np_image.copy())\n    albu_time = time.time() - start_time\n    \n    print(f\"TorchVision: {torch_time:.3f}s ({iterations} iterations)\")\n    print(f\"Albumentations: {albu_time:.3f}s ({iterations} iterations)\")\n    print(f\"Speedup: {torch_time/albu_time:.2f}x\")\n\n# Run benchmark\nbenchmark_transforms(torch_img)\n\nTorchVision: 38.434s (1000 iterations)\nAlbumentations: 3.152s (1000 iterations)\nSpeedup: 12.19x"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#custom-pipeline-examples",
    "href": "posts/albumentations-vs-torchvision/index.html#custom-pipeline-examples",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "def create_training_pipeline():\n    return A.Compose([\n        # Geometric transformations\n        A.OneOf([\n            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30),\n            A.ElasticTransform(alpha=1, sigma=50),\n            A.GridDistortion(num_steps=5, distort_limit=0.3),\n        ], p=0.5),\n        \n        # Color augmentations\n        A.OneOf([\n            A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20),\n            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n        ], p=0.8),\n        \n        # Noise and blur\n        A.OneOf([\n            A.GaussNoise(var_limit=(10, 50)),\n            A.ISONoise(),\n            A.MultiplicativeNoise(),\n        ], p=0.3),\n        \n        A.OneOf([\n            A.MotionBlur(blur_limit=5),\n            A.MedianBlur(blur_limit=5),\n            A.GaussianBlur(blur_limit=5),\n        ], p=0.3),\n        \n        # Final processing\n        A.Resize(224, 224),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n\ndef create_validation_pipeline():\n    return A.Compose([\n        A.Resize(224, 224),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n\n\n\ndef create_simple_training_pipeline():\n    return T.Compose([\n        T.Resize((224, 224)),\n        T.RandomHorizontalFlip(p=0.5),\n        T.RandomRotation(degrees=15),\n        T.RandomApply([T.ColorJitter(0.3, 0.3, 0.3, 0.1)], p=0.8),\n        T.RandomApply([T.GaussianBlur(kernel_size=3)], p=0.3),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\ndef create_simple_validation_pipeline():\n    return T.Compose([\n        T.Resize((224, 224)),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#dataset-integration",
    "href": "posts/albumentations-vs-torchvision/index.html#dataset-integration",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "from torch.utils.data import Dataset, DataLoader\n\nclass CustomDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Load image for Albumentations (OpenCV format)\n        image = cv2.imread(self.image_paths[idx])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.transform:\n            transformed = self.transform(image=image)\n            image = transformed['image']\n        \n        return image, self.labels[idx]\n\n# Usage\ntrain_dataset = CustomDataset(\n    image_paths=train_paths,\n    labels=train_labels,\n    transform=create_training_pipeline()\n)\n\n\n\nclass TorchVisionDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Load image for TorchVision (PIL format)\n        image = Image.open(self.image_paths[idx]).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, self.labels[idx]\n\n# Usage\ntrain_dataset = TorchVisionDataset(\n    image_paths=train_paths,\n    labels=train_labels,\n    transform=create_simple_training_pipeline()\n)"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#when-to-use-which-library",
    "href": "posts/albumentations-vs-torchvision/index.html#when-to-use-which-library",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "Working with object detection or segmentation tasks\nNeed advanced augmentation techniques (weather effects, distortions)\nPerformance is critical (processing large datasets)\nWorking with bounding boxes or keypoints\nNeed fine-grained control over augmentation probabilities\nDealing with medical or satellite imagery\n\n\n\n\n\nBuilding simple image classification models\nWorking within pure PyTorch ecosystem\nNeed basic augmentations only\nPrototyping quickly\nFollowing PyTorch tutorials or established workflows\nWorking with pre-trained models that expect specific preprocessing"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#best-practices",
    "href": "posts/albumentations-vs-torchvision/index.html#best-practices",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "# Use ReplayCompose for debugging\nreplay_transform = A.ReplayCompose([\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.3),\n])\n\nresult = replay_transform(image=image)\ntransformed_image = result['image']\nreplay_data = result['replay']\n\n# Apply same transforms to another image\nresult2 = A.ReplayCompose.replay(replay_data, image=another_image)\n\n# Efficient bbox handling\nbbox_params = A.BboxParams(\n    format='pascal_voc',\n    min_area=1024,  # Filter out small boxes\n    min_visibility=0.3,  # Filter out mostly occluded boxes\n    label_fields=['class_labels']\n)\n\n\n\n# Use functional API for custom control\ndef custom_transform(image):\n    if torch.rand(1) &lt; 0.5:\n        image = TF.hflip(image)\n    \n    # Apply rotation with custom logic\n    angle = torch.randint(-30, 30, (1,)).item()\n    image = TF.rotate(image, angle)\n    \n    return image\n\n# Combine with standard transforms\ncombined_transform = T.Compose([\n    T.Lambda(custom_transform),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#conclusion",
    "href": "posts/albumentations-vs-torchvision/index.html#conclusion",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "Both libraries have their strengths:\nAlbumentations excels in:\n\nAdvanced augmentation techniques\nPerformance optimization\nComputer vision tasks beyond classification\nProfessional production environments\n\nTorchVision is ideal for:\n\nSimple classification tasks\nLearning and prototyping\nTight PyTorch integration\nBasic augmentation needs\n\nChoose based on your specific requirements, with Albumentations being the go-to choice for advanced computer vision projects and TorchVision for simpler classification tasks."
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html",
    "href": "posts/vision-transformers-explained/index.html",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "Vision Transformers (ViTs) represent a paradigm shift in computer vision, adapting the transformer architecture that revolutionized natural language processing for image classification and other visual tasks. Instead of relying on convolutional neural networks (CNNs), ViTs treat images as sequences of patches, applying the self-attention mechanism to understand spatial relationships and visual features.\n\n\n\nTraditional computer vision relied heavily on Convolutional Neural Networks (CNNs), which process images through layers of convolutions that detect local features like edges, textures, and patterns. While effective, CNNs have limitations in capturing long-range dependencies across an image due to their local receptive fields.\nTransformers, originally designed for language tasks, excel at modeling long-range dependencies through self-attention mechanisms. The key insight behind Vision Transformers was asking: “What if we could apply this powerful attention mechanism to images?”\n\n\n\nThe fundamental innovation of ViTs lies in treating images as sequences of patches rather than pixel grids. Here’s how this transformation works:\n\n\n\nPatch Division: An input image (typically 224×224 pixels) is divided into fixed-size patches (commonly 16×16 pixels), resulting in a sequence of patches\nLinear Projection: Each patch is flattened into a vector and linearly projected to create patch embeddings\nPosition Encoding: Since transformers don’t inherently understand spatial relationships, positional encodings are added to maintain spatial information\nClassification Token: A special learnable [CLS] token is prepended to the sequence, similar to BERT’s approach\n\n\n\n\nFor an image of size H×W×C divided into patches of size P×P:\n\nNumber of patches: N = (H×W)/P²\nEach patch becomes a vector of size P²×C\nAfter linear projection: embedding dimension D\n\n\n\n\n\n\n\nThe patch embedding layer converts image patches into token embeddings that the transformer can process. This involves:\n\nReshaping patches into vectors\nLinear transformation to desired embedding dimension\nAdding positional encodings\n\n\n\n\nThe core of ViT consists of standard transformer encoder blocks, each containing:\n\nMulti-Head Self-Attention (MSA): Allows patches to attend to all other patches\nLayer Normalization: Applied before both attention and MLP layers\nMulti-Layer Perceptron (MLP): Two-layer feedforward network with GELU activation\nResidual Connections: Skip connections around both attention and MLP blocks\n\n\n\n\nThe final component extracts the [CLS] token’s representation and passes it through:\n\nLayer normalization\nLinear classifier to produce class predictions\n\n\n\n\n\nThe self-attention mechanism in ViTs operates differently from CNNs:\n\n\n\nEach patch can attend to every other patch in the image\nAttention weights reveal which parts of the image are most relevant for classification\nThis enables modeling of long-range spatial dependencies\n\n\n\n\nUnlike CNNs that build up receptive fields gradually, ViTs have global receptive fields from the first layer, allowing immediate access to information across the entire image.\n\n\n\n\n\n\nVision Transformers typically require large amounts of training data to perform well:\n\nPre-training: Often trained on large datasets like ImageNet-21k or JFT-300M\nFine-tuning: Then adapted to specific tasks with smaller datasets\nData Efficiency: ViTs can be less data-efficient than CNNs when training from scratch\n\n\n\n\n\nInitialization: Careful weight initialization is crucial\nLearning Rate: Often requires different learning rates for different components\nRegularization: Techniques like dropout and weight decay are important\nWarmup: Learning rate warmup is commonly used\n\n\n\n\n\n\n\n\nViT-B/16, ViT-L/16, ViT-H/14: Different model sizes with varying patch sizes\nDeiT (Data-efficient ViT): Improved training strategies for smaller datasets\nSwin Transformer: Hierarchical vision transformer with shifted windows\nCaiT: Class-Attention in Image Transformers with separate class attention\n\n\n\n\n\nHierarchical Processing: Multi-scale feature extraction\nLocal Attention: Restricting attention to local neighborhoods\nHybrid Models: Combining CNN features with transformer processing\n\n\n\n\n\n\n\n\nLong-range Dependencies: Natural ability to model global relationships\nInterpretability: Attention maps provide insights into model decisions\nScalability: Performance improves with larger models and datasets\nTransfer Learning: Excellent pre-trained representations\nArchitectural Simplicity: Unified architecture for various vision tasks\n\n\n\n\n\nState-of-the-art results on image classification\nStrong performance on object detection and segmentation when adapted\nExcellent transfer learning capabilities across domains\n\n\n\n\n\n\n\n\nData Hunger: Requires large datasets for optimal performance\nComputational Cost: High memory and compute requirements\nInductive Bias: Lacks CNN’s built-in spatial inductive biases\nSmall Dataset Performance: Can underperform CNNs on limited data\n\n\n\n\n\nImproving data efficiency\nReducing computational requirements\nBetter integration of spatial inductive biases\nHybrid CNN-Transformer architectures\n\n\n\n\n\n\n\n\nObject Detection: DETR (Detection Transformer) applies transformers to detection\nSemantic Segmentation: Segmentation transformers for pixel-level predictions\nImage Generation: Vision transformers in generative models\nVideo Analysis: Extending to temporal sequences\n\n\n\n\n\nVision-Language Models: CLIP and similar models combining vision and text\nVisual Question Answering: Integrating visual and textual understanding\nImage Captioning: Generating descriptions from visual content\n\n\n\n\n\n\n\nChoose ViT variants based on:\n\nAvailable computational resources\nDataset size and characteristics\nRequired inference speed\nTarget accuracy requirements\n\n\n\n\n\nUse pre-trained models when possible\nApply appropriate data augmentation\nConsider knowledge distillation for smaller models\nMonitor for overfitting, especially on smaller datasets\n\n\n\n\n\nUse mixed precision training to reduce memory usage\nImplement gradient checkpointing for large models\nConsider model parallelism for very large architectures\nApply appropriate regularization techniques\n\n\n\n\n\n\n\n\nEfficiency Improvements: Making ViTs more computationally efficient\nArchitecture Search: Automated design of vision transformer architectures\nSelf-Supervised Learning: Reducing dependence on labeled data\nUnified Architectures: Single models handling multiple vision tasks\n\n\n\n\n\nReal-time vision applications\nMobile and edge deployment\nScientific imaging and medical applications\nAutonomous systems and robotics\n\n\n\n\n\nVision Transformers represent a fundamental shift in computer vision, demonstrating that the transformer architecture’s success in NLP can extend to visual tasks. While they present challenges in terms of data requirements and computational cost, their ability to model long-range dependencies and achieve state-of-the-art performance makes them a crucial tool in modern computer vision.\nThe field continues to evolve rapidly, with ongoing research addressing current limitations while exploring new applications. As the technology matures, we can expect ViTs to become increasingly practical for a wider range of real-world applications, potentially reshaping how we approach visual understanding tasks.\nUnderstanding Vision Transformers is essential for anyone working in modern computer vision, as they represent not just a new model architecture, but a new way of thinking about how machines can understand and process visual information."
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#introduction",
    "href": "posts/vision-transformers-explained/index.html#introduction",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "Vision Transformers (ViTs) represent a paradigm shift in computer vision, adapting the transformer architecture that revolutionized natural language processing for image classification and other visual tasks. Instead of relying on convolutional neural networks (CNNs), ViTs treat images as sequences of patches, applying the self-attention mechanism to understand spatial relationships and visual features."
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#background-from-cnns-to-transformers",
    "href": "posts/vision-transformers-explained/index.html#background-from-cnns-to-transformers",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "Traditional computer vision relied heavily on Convolutional Neural Networks (CNNs), which process images through layers of convolutions that detect local features like edges, textures, and patterns. While effective, CNNs have limitations in capturing long-range dependencies across an image due to their local receptive fields.\nTransformers, originally designed for language tasks, excel at modeling long-range dependencies through self-attention mechanisms. The key insight behind Vision Transformers was asking: “What if we could apply this powerful attention mechanism to images?”"
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#core-concept-images-as-sequences",
    "href": "posts/vision-transformers-explained/index.html#core-concept-images-as-sequences",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "The fundamental innovation of ViTs lies in treating images as sequences of patches rather than pixel grids. Here’s how this transformation works:\n\n\n\nPatch Division: An input image (typically 224×224 pixels) is divided into fixed-size patches (commonly 16×16 pixels), resulting in a sequence of patches\nLinear Projection: Each patch is flattened into a vector and linearly projected to create patch embeddings\nPosition Encoding: Since transformers don’t inherently understand spatial relationships, positional encodings are added to maintain spatial information\nClassification Token: A special learnable [CLS] token is prepended to the sequence, similar to BERT’s approach\n\n\n\n\nFor an image of size H×W×C divided into patches of size P×P:\n\nNumber of patches: N = (H×W)/P²\nEach patch becomes a vector of size P²×C\nAfter linear projection: embedding dimension D"
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#architecture-components",
    "href": "posts/vision-transformers-explained/index.html#architecture-components",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "The patch embedding layer converts image patches into token embeddings that the transformer can process. This involves:\n\nReshaping patches into vectors\nLinear transformation to desired embedding dimension\nAdding positional encodings\n\n\n\n\nThe core of ViT consists of standard transformer encoder blocks, each containing:\n\nMulti-Head Self-Attention (MSA): Allows patches to attend to all other patches\nLayer Normalization: Applied before both attention and MLP layers\nMulti-Layer Perceptron (MLP): Two-layer feedforward network with GELU activation\nResidual Connections: Skip connections around both attention and MLP blocks\n\n\n\n\nThe final component extracts the [CLS] token’s representation and passes it through:\n\nLayer normalization\nLinear classifier to produce class predictions"
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#self-attention-in-vision",
    "href": "posts/vision-transformers-explained/index.html#self-attention-in-vision",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "The self-attention mechanism in ViTs operates differently from CNNs:\n\n\n\nEach patch can attend to every other patch in the image\nAttention weights reveal which parts of the image are most relevant for classification\nThis enables modeling of long-range spatial dependencies\n\n\n\n\nUnlike CNNs that build up receptive fields gradually, ViTs have global receptive fields from the first layer, allowing immediate access to information across the entire image."
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#training-considerations",
    "href": "posts/vision-transformers-explained/index.html#training-considerations",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "Vision Transformers typically require large amounts of training data to perform well:\n\nPre-training: Often trained on large datasets like ImageNet-21k or JFT-300M\nFine-tuning: Then adapted to specific tasks with smaller datasets\nData Efficiency: ViTs can be less data-efficient than CNNs when training from scratch\n\n\n\n\n\nInitialization: Careful weight initialization is crucial\nLearning Rate: Often requires different learning rates for different components\nRegularization: Techniques like dropout and weight decay are important\nWarmup: Learning rate warmup is commonly used"
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#variants-and-improvements",
    "href": "posts/vision-transformers-explained/index.html#variants-and-improvements",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "ViT-B/16, ViT-L/16, ViT-H/14: Different model sizes with varying patch sizes\nDeiT (Data-efficient ViT): Improved training strategies for smaller datasets\nSwin Transformer: Hierarchical vision transformer with shifted windows\nCaiT: Class-Attention in Image Transformers with separate class attention\n\n\n\n\n\nHierarchical Processing: Multi-scale feature extraction\nLocal Attention: Restricting attention to local neighborhoods\nHybrid Models: Combining CNN features with transformer processing"
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#advantages-of-vision-transformers",
    "href": "posts/vision-transformers-explained/index.html#advantages-of-vision-transformers",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "Long-range Dependencies: Natural ability to model global relationships\nInterpretability: Attention maps provide insights into model decisions\nScalability: Performance improves with larger models and datasets\nTransfer Learning: Excellent pre-trained representations\nArchitectural Simplicity: Unified architecture for various vision tasks\n\n\n\n\n\nState-of-the-art results on image classification\nStrong performance on object detection and segmentation when adapted\nExcellent transfer learning capabilities across domains"
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#limitations-and-challenges",
    "href": "posts/vision-transformers-explained/index.html#limitations-and-challenges",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "Data Hunger: Requires large datasets for optimal performance\nComputational Cost: High memory and compute requirements\nInductive Bias: Lacks CNN’s built-in spatial inductive biases\nSmall Dataset Performance: Can underperform CNNs on limited data\n\n\n\n\n\nImproving data efficiency\nReducing computational requirements\nBetter integration of spatial inductive biases\nHybrid CNN-Transformer architectures"
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#applications-beyond-classification",
    "href": "posts/vision-transformers-explained/index.html#applications-beyond-classification",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "Object Detection: DETR (Detection Transformer) applies transformers to detection\nSemantic Segmentation: Segmentation transformers for pixel-level predictions\nImage Generation: Vision transformers in generative models\nVideo Analysis: Extending to temporal sequences\n\n\n\n\n\nVision-Language Models: CLIP and similar models combining vision and text\nVisual Question Answering: Integrating visual and textual understanding\nImage Captioning: Generating descriptions from visual content"
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#implementation-considerations",
    "href": "posts/vision-transformers-explained/index.html#implementation-considerations",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "Choose ViT variants based on:\n\nAvailable computational resources\nDataset size and characteristics\nRequired inference speed\nTarget accuracy requirements\n\n\n\n\n\nUse pre-trained models when possible\nApply appropriate data augmentation\nConsider knowledge distillation for smaller models\nMonitor for overfitting, especially on smaller datasets\n\n\n\n\n\nUse mixed precision training to reduce memory usage\nImplement gradient checkpointing for large models\nConsider model parallelism for very large architectures\nApply appropriate regularization techniques"
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#future-directions",
    "href": "posts/vision-transformers-explained/index.html#future-directions",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "Efficiency Improvements: Making ViTs more computationally efficient\nArchitecture Search: Automated design of vision transformer architectures\nSelf-Supervised Learning: Reducing dependence on labeled data\nUnified Architectures: Single models handling multiple vision tasks\n\n\n\n\n\nReal-time vision applications\nMobile and edge deployment\nScientific imaging and medical applications\nAutonomous systems and robotics"
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#conclusion",
    "href": "posts/vision-transformers-explained/index.html#conclusion",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "Vision Transformers represent a fundamental shift in computer vision, demonstrating that the transformer architecture’s success in NLP can extend to visual tasks. While they present challenges in terms of data requirements and computational cost, their ability to model long-range dependencies and achieve state-of-the-art performance makes them a crucial tool in modern computer vision.\nThe field continues to evolve rapidly, with ongoing research addressing current limitations while exploring new applications. As the technology matures, we can expect ViTs to become increasingly practical for a wider range of real-world applications, potentially reshaping how we approach visual understanding tasks.\nUnderstanding Vision Transformers is essential for anyone working in modern computer vision, as they represent not just a new model architecture, but a new way of thinking about how machines can understand and process visual information."
  },
  {
    "objectID": "posts/influence-selection/index.html",
    "href": "posts/influence-selection/index.html",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Active learning is a machine learning paradigm where the algorithm can interactively query an oracle (typically a human annotator) to label new data points. The key idea is to select the most informative samples to be labeled, reducing the overall labeling effort while maintaining or improving model performance. This guide focuses on influence selection methods used in active learning strategies.\n\n\n\n\nFundamentals of Active Learning\nInfluence Selection Strategies\nUncertainty-Based Methods\nDiversity-Based Methods\nExpected Model Change\nExpected Error Reduction\nInfluence Functions\nQuery-by-Committee\nImplementation Considerations\nEvaluation Metrics\nPractical Examples\nAdvanced Topics\n\n\n\n\n\n\nThe typical active learning process follows these steps:\n\nStart with a small labeled dataset and a large unlabeled pool\nTrain an initial model on the labeled data\nApply an influence selection strategy to choose informative samples from the unlabeled pool\nGet annotations for the selected samples\nAdd the newly labeled samples to the training set\nRetrain the model and repeat steps 3-6 until a stopping condition is met\n\n\n\n\n\nPool-based: The learner has access to a pool of unlabeled data and selects the most informative samples\nStream-based: Samples arrive sequentially, and the learner must decide on-the-fly whether to request labels\n\n\n\n\n\nInfluence selection is about identifying which unlabeled samples would be most beneficial to label next. Here are the main strategies:\n\n\n\nThese methods select samples that the model is most uncertain about.\n\n\n\ndef least_confidence(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    \n    # Get the confidence values for the most probable class\n    confidences = np.max(probabilities, axis=1)\n    \n    # Select the k samples with the lowest confidence\n    least_confident_indices = np.argsort(confidences)[:k]\n    \n    return unlabeled_pool[least_confident_indices]\n\n\n\n\nSelects samples with the smallest margin between the two most likely classes:\n\ndef margin_sampling(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    \n    # Sort the probabilities in descending order\n    sorted_probs = np.sort(probabilities, axis=1)[:, ::-1]\n    \n    # Calculate the margin between the first and second most probable classes\n    margins = sorted_probs[:, 0] - sorted_probs[:, 1]\n    \n    # Select the k samples with the smallest margins\n    smallest_margin_indices = np.argsort(margins)[:k]\n    \n    return unlabeled_pool[smallest_margin_indices]\n\n\n\n\nSelects samples with the highest predictive entropy:\n\ndef entropy_sampling(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    \n    # Calculate entropy for each sample\n    entropies = -np.sum(probabilities * np.log(probabilities + 1e-10), axis=1)\n    \n    # Select the k samples with the highest entropy\n    highest_entropy_indices = np.argsort(entropies)[::-1][:k]\n    \n    return unlabeled_pool[highest_entropy_indices]\n\n\n\n\nFor Bayesian models, BALD selects samples that maximize the mutual information between predictions and model parameters:\n\ndef bald_sampling(bayesian_model, unlabeled_pool, k, n_samples=100):\n    # Get multiple predictions by sampling from the model's posterior\n    probs_samples = []\n    for _ in range(n_samples):\n        probs = bayesian_model.predict_proba(unlabeled_pool)\n        probs_samples.append(probs)\n    \n    # Stack into a 3D array: (samples, data points, classes)\n    probs_samples = np.stack(probs_samples)\n    \n    # Calculate the average probability across all samples\n    mean_probs = np.mean(probs_samples, axis=0)\n    \n    # Calculate the entropy of the average prediction\n    entropy_mean = -np.sum(mean_probs * np.log(mean_probs + 1e-10), axis=1)\n    \n    # Calculate the average entropy across all samples\n    entropy_samples = -np.sum(probs_samples * np.log(probs_samples + 1e-10), axis=2)\n    mean_entropy = np.mean(entropy_samples, axis=0)\n    \n    # Mutual information = entropy of the mean - mean of entropies\n    bald_scores = entropy_mean - mean_entropy\n    \n    # Select the k samples with the highest BALD scores\n    highest_bald_indices = np.argsort(bald_scores)[::-1][:k]\n    \n    return unlabeled_pool[highest_bald_indices]\n\n\n\n\n\nThese methods aim to select a diverse set of examples to ensure broad coverage of the input space.\n\n\n\ndef clustering_based_sampling(unlabeled_pool, k, n_clusters=None):\n    if n_clusters is None:\n        n_clusters = k\n    \n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(unlabeled_pool)\n    \n    # Get the cluster centers and distances to each point\n    centers = kmeans.cluster_centers_\n    distances = kmeans.transform(unlabeled_pool)  # Distance to each cluster center\n    \n    # Select one sample from each cluster (closest to the center)\n    selected_indices = []\n    for i in range(n_clusters):\n        # Get the samples in this cluster\n        cluster_samples = np.where(kmeans.labels_ == i)[0]\n        \n        # Find the sample closest to the center\n        closest_sample = cluster_samples[np.argmin(distances[cluster_samples, i])]\n        selected_indices.append(closest_sample)\n    \n    # If we need more samples than clusters, fill with the most uncertain samples\n    if k &gt; n_clusters:\n        # Implementation depends on uncertainty measure\n        pass\n    \n    return unlabeled_pool[selected_indices[:k]]\n\n\n\n\nThe core-set approach aims to select a subset of data that best represents the whole dataset:\n\ndef core_set_sampling(labeled_pool, unlabeled_pool, k):\n    # Combine labeled and unlabeled data for distance calculations\n    all_data = np.vstack((labeled_pool, unlabeled_pool))\n    \n    # Compute pairwise distances\n    distances = pairwise_distances(all_data)\n    \n    # Split distances into labeled-unlabeled and unlabeled-unlabeled\n    n_labeled = labeled_pool.shape[0]\n    dist_labeled_unlabeled = distances[:n_labeled, n_labeled:]\n    \n    # For each unlabeled sample, find the minimum distance to any labeled sample\n    min_distances = np.min(dist_labeled_unlabeled, axis=0)\n    \n    # Select the k samples with the largest minimum distances\n    farthest_indices = np.argsort(min_distances)[::-1][:k]\n    \n    return unlabeled_pool[farthest_indices]\n\n\n\n\n\nThe Expected Model Change (EMC) method selects samples that would cause the greatest change in the model if they were labeled:\n\ndef expected_model_change(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    n_classes = probabilities.shape[1]\n    \n    # Calculate expected gradient length for each sample\n    expected_changes = []\n    for i, x in enumerate(unlabeled_pool):\n        # Calculate expected gradient length across all possible labels\n        change = 0\n        for c in range(n_classes):\n            # For each possible class, calculate the gradient if this was the true label\n            x_expanded = x.reshape(1, -1)\n            # Here we would compute the gradient of the model with respect to the sample\n            # For simplicity, we use a placeholder\n            gradient = compute_gradient(model, x_expanded, c)\n            norm_gradient = np.linalg.norm(gradient)\n            \n            # Weight by the probability of this class\n            change += probabilities[i, c] * norm_gradient\n        \n        expected_changes.append(change)\n    \n    # Select the k samples with the highest expected change\n    highest_change_indices = np.argsort(expected_changes)[::-1][:k]\n    \n    return unlabeled_pool[highest_change_indices]\n\nNote: The compute_gradient function would need to be implemented based on the specific model being used.\n\n\n\nThe Expected Error Reduction method selects samples that, when labeled, would minimally reduce the model’s expected error:\n\ndef expected_error_reduction(model, unlabeled_pool, unlabeled_pool_remaining, k):\n    # Predict probabilities for all remaining unlabeled data\n    current_probs = model.predict_proba(unlabeled_pool_remaining)\n    current_entropy = -np.sum(current_probs * np.log(current_probs + 1e-10), axis=1)\n    \n    expected_error_reductions = []\n    \n    # For each sample in the unlabeled pool we're considering\n    for i, x in enumerate(unlabeled_pool):\n        # Predict probabilities for this sample\n        probs = model.predict_proba(x.reshape(1, -1))[0]\n        \n        # Calculate the expected error reduction for each possible label\n        error_reduction = 0\n        for c in range(len(probs)):\n            # Create a hypothetical new model with this labeled sample\n            # For simplicity, we use a placeholder function\n            hypothetical_model = train_with_additional_sample(model, x, c)\n            \n            # Get new probabilities with this model\n            new_probs = hypothetical_model.predict_proba(unlabeled_pool_remaining)\n            new_entropy = -np.sum(new_probs * np.log(new_probs + 1e-10), axis=1)\n            \n            # Expected entropy reduction\n            reduction = np.sum(current_entropy - new_entropy)\n            \n            # Weight by the probability of this class\n            error_reduction += probs[c] * reduction\n        \n        expected_error_reductions.append(error_reduction)\n    \n    # Select the k samples with the highest expected error reduction\n    highest_reduction_indices = np.argsort(expected_error_reductions)[::-1][:k]\n    \n    return unlabeled_pool[highest_reduction_indices]\n\nNote: The train_with_additional_sample function would need to be implemented based on the specific model being used.\n\n\n\nInfluence functions approximate the effect of adding or removing a training example without retraining the model:\n\ndef influence_function_sampling(model, unlabeled_pool, labeled_pool, k, labels):\n    influences = []\n    \n    # For each unlabeled sample\n    for x_u in unlabeled_pool:\n        # Calculate the influence of adding this sample to the training set\n        influence = calculate_influence(model, x_u, labeled_pool, labels)\n        influences.append(influence)\n    \n    # Select the k samples with the highest influence\n    highest_influence_indices = np.argsort(influences)[::-1][:k]\n    \n    return unlabeled_pool[highest_influence_indices]\n\nNote: The calculate_influence function would need to be implemented based on the specific model and influence metric being used.\n\n\n\nQuery-by-Committee (QBC) methods train multiple models (a committee) and select samples where they disagree:\n\ndef query_by_committee(committee_models, unlabeled_pool, k):\n    # Get predictions from all committee members\n    all_predictions = []\n    for model in committee_models:\n        preds = model.predict(unlabeled_pool)\n        all_predictions.append(preds)\n    \n    # Stack predictions into a 2D array (committee members, data points)\n    all_predictions = np.stack(all_predictions)\n    \n    # Calculate disagreement (e.g., using vote entropy)\n    disagreements = []\n    for i in range(unlabeled_pool.shape[0]):\n        # Count votes for each class\n        votes = np.bincount(all_predictions[:, i])\n        # Normalize to get probabilities\n        vote_probs = votes / len(committee_models)\n        # Calculate entropy\n        entropy = -np.sum(vote_probs * np.log2(vote_probs + 1e-10))\n        disagreements.append(entropy)\n    \n    # Select the k samples with the highest disagreement\n    highest_disagreement_indices = np.argsort(disagreements)[::-1][:k]\n    \n    return unlabeled_pool[highest_disagreement_indices]\n\n\n\n\n\n\nIn practice, it’s often more efficient to select multiple samples at once. However, simply selecting the top-k samples may lead to redundancy. Consider using:\n\nGreedy Selection with Diversity: Select one sample at a time, then update the diversity metrics to avoid selecting similar samples.\n\n\ndef batch_selection_with_diversity(model, unlabeled_pool, k, lambda_diversity=0.5):\n    selected_indices = []\n    remaining_indices = list(range(len(unlabeled_pool)))\n    \n    # Calculate uncertainty scores for all samples\n    probabilities = model.predict_proba(unlabeled_pool)\n    entropies = -np.sum(probabilities * np.log(probabilities + 1e-10), axis=1)\n    \n    # Calculate distance matrix for diversity\n    distance_matrix = pairwise_distances(unlabeled_pool)\n    \n    for _ in range(k):\n        if not remaining_indices:\n            break\n        \n        scores = np.zeros(len(remaining_indices))\n        \n        # Calculate uncertainty scores\n        uncertainty_scores = entropies[remaining_indices]\n        \n        # Calculate diversity scores (if we have already selected some samples)\n        if selected_indices:\n            # For each remaining sample, calculate the minimum distance to any selected sample\n            diversity_scores = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n        else:\n            diversity_scores = np.zeros(len(remaining_indices))\n        \n        # Normalize scores\n        uncertainty_scores = (uncertainty_scores - np.min(uncertainty_scores)) / (np.max(uncertainty_scores) - np.min(uncertainty_scores) + 1e-10)\n        if selected_indices:\n            diversity_scores = (diversity_scores - np.min(diversity_scores)) / (np.max(diversity_scores) - np.min(diversity_scores) + 1e-10)\n        \n        # Combine scores\n        scores = (1 - lambda_diversity) * uncertainty_scores + lambda_diversity * diversity_scores\n        \n        # Select the sample with the highest score\n        best_idx = np.argmax(scores)\n        selected_idx = remaining_indices[best_idx]\n        \n        # Add to selected and remove from remaining\n        selected_indices.append(selected_idx)\n        remaining_indices.remove(selected_idx)\n    \n    return unlabeled_pool[selected_indices]\n\n\nSubmodular Function Maximization: Use a submodular function to ensure diversity in the selected batch.\n\n\n\n\nActive learning can inadvertently reinforce class imbalance. Consider:\n\nStratified Sampling: Ensure representation from all classes.\nHybrid Approaches: Combine uncertainty-based and density-based methods.\nDiversity Constraints: Explicitly enforce diversity in feature space.\n\n\n\n\nSome methods (like expected error reduction) can be computationally expensive. Consider:\n\nSubsample the Unlabeled Pool: Only consider a random subset for selection.\nPre-compute Embeddings: Use a fixed feature extractor to pre-compute embeddings.\nApproximate Methods: Use approximations for expensive operations.\n\n\n\n\n\n\n\nPlot model performance vs. number of labeled samples:\n\ndef plot_learning_curve(model_factory, X_train, y_train, X_test, y_test, \n                        active_learning_strategy, initial_size=10, \n                        batch_size=10, n_iterations=20):\n    # Initialize with a small labeled set\n    labeled_indices = np.random.choice(len(X_train), initial_size, replace=False)\n    unlabeled_indices = np.setdiff1d(np.arange(len(X_train)), labeled_indices)\n    \n    performance = []\n    \n    for i in range(n_iterations):\n        # Create a fresh model\n        model = model_factory()\n        \n        # Train on the currently labeled data\n        model.fit(X_train[labeled_indices], y_train[labeled_indices])\n        \n        # Evaluate on the test set\n        score = model.score(X_test, y_test)\n        performance.append((len(labeled_indices), score))\n        \n        # Select the next batch of samples\n        if len(unlabeled_indices) &gt; 0:\n            # Use the specified active learning strategy\n            selected_indices = active_learning_strategy(\n                model, X_train[unlabeled_indices], batch_size\n            )\n            \n            # Map back to original indices\n            selected_original_indices = unlabeled_indices[selected_indices]\n            \n            # Update labeled and unlabeled indices\n            labeled_indices = np.append(labeled_indices, selected_original_indices)\n            unlabeled_indices = np.setdiff1d(unlabeled_indices, selected_original_indices)\n    \n    # Plot the learning curve\n    counts, scores = zip(*performance)\n    plt.figure(figsize=(10, 6))\n    plt.plot(counts, scores, 'o-')\n    plt.xlabel('Number of labeled samples')\n    plt.ylabel('Model accuracy')\n    plt.title('Active Learning Performance')\n    plt.grid(True)\n    \n    return performance\n\n\n\n\nAlways compare your active learning strategy with random sampling as a baseline.\n\n\n\nCalculate how many annotations you saved compared to using the entire dataset.\n\n\n\n\n\n\n\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n# Load data\nmnist = fetch_openml('mnist_784', version=1, cache=True)\nX, y = mnist['data'], mnist['target']\n\n# Split into train and test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Initially, only a small portion is labeled\ninitial_size = 100\nlabeled_indices = np.random.choice(len(X_train), initial_size, replace=False)\nunlabeled_indices = np.setdiff1d(np.arange(len(X_train)), labeled_indices)\n\n# Tracking performance\nactive_learning_performance = []\nrandom_sampling_performance = []\n\n# Active learning loop\nfor i in range(10):  # 10 iterations\n    # Train a model on the currently labeled data\n    model = RandomForestClassifier(n_estimators=50, random_state=42)\n    model.fit(X_train.iloc[labeled_indices], y_train.iloc[labeled_indices])\n    \n    # Evaluate on the test set\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    active_learning_performance.append((len(labeled_indices), accuracy))\n    \n    print(f\"Iteration {i+1}: {len(labeled_indices)} labeled samples, \"\n          f\"accuracy: {accuracy:.4f}\")\n    \n    # Select 100 new samples using entropy sampling\n    if len(unlabeled_indices) &gt; 0:\n        # Predict probabilities for each unlabeled sample\n        probs = model.predict_proba(X_train.iloc[unlabeled_indices])\n        \n        # Calculate entropy\n        entropies = -np.sum(probs * np.log(probs + 1e-10), axis=1)\n        \n        # Select samples with the highest entropy\n        top_indices = np.argsort(entropies)[::-1][:100]\n        \n        # Update labeled and unlabeled indices\n        selected_indices = unlabeled_indices[top_indices]\n        labeled_indices = np.append(labeled_indices, selected_indices)\n        unlabeled_indices = np.setdiff1d(unlabeled_indices, selected_indices)\n\n# Plot learning curve\ncounts, scores = zip(*active_learning_performance)\nplt.figure(figsize=(10, 6))\nplt.plot(counts, scores, 'o-', label='Active Learning')\nplt.xlabel('Number of labeled samples')\nplt.ylabel('Model accuracy')\nplt.title('Active Learning Performance')\nplt.grid(True)\nplt.legend()\nplt.show()\n\nIteration 1: 100 labeled samples, accuracy: 0.6917\nIteration 2: 200 labeled samples, accuracy: 0.7290\nIteration 3: 300 labeled samples, accuracy: 0.7716\nIteration 4: 400 labeled samples, accuracy: 0.7817\nIteration 5: 500 labeled samples, accuracy: 0.8239\nIteration 6: 600 labeled samples, accuracy: 0.8227\nIteration 7: 700 labeled samples, accuracy: 0.8282\nIteration 8: 800 labeled samples, accuracy: 0.8435\nIteration 9: 900 labeled samples, accuracy: 0.8454\nIteration 10: 1000 labeled samples, accuracy: 0.8549\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.datasets import fetch_20newsgroups\n\n# Load data\ncategories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\ntwenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\ntwenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n\n# Feature extraction\nvectorizer = TfidfVectorizer(stop_words='english')\nX_train = vectorizer.fit_transform(twenty_train.data)\nX_test = vectorizer.transform(twenty_test.data)\ny_train = twenty_train.target\ny_test = twenty_test.target\n\n# Initially, only a small portion is labeled\ninitial_size = 20\nlabeled_indices = np.random.choice(len(X_train.toarray()), initial_size, replace=False)\nunlabeled_indices = np.setdiff1d(np.arange(len(X_train.toarray())), labeled_indices)\n\n# Create a committee of models\nmodels = [\n    ('nb', MultinomialNB()),\n    ('svm', SVC(kernel='linear', probability=True)),\n    ('svm2', SVC(kernel='rbf', probability=True))\n]\n\n# Active learning loop\nfor i in range(10):  # 10 iterations\n    # Train each model on the currently labeled data\n    committee_models = []\n    for name, model in models:\n        model.fit(X_train[labeled_indices], y_train[labeled_indices])\n        committee_models.append(model)\n    \n    # Evaluate using the VotingClassifier\n    voting_clf = VotingClassifier(estimators=models, voting='soft')\n    voting_clf.fit(X_train[labeled_indices], y_train[labeled_indices])\n    \n    accuracy = voting_clf.score(X_test, y_test)\n    print(f\"Iteration {i+1}: {len(labeled_indices)} labeled samples, \"\n          f\"accuracy: {accuracy:.4f}\")\n    \n    # Select 10 new samples using Query-by-Committee\n    if len(unlabeled_indices) &gt; 0:\n        # Get predictions from all committee members\n        all_predictions = []\n        for model in committee_models:\n            preds = model.predict(X_train[unlabeled_indices])\n            all_predictions.append(preds)\n        \n        # Calculate vote entropy\n        vote_entropies = []\n        all_predictions = np.array(all_predictions)\n        for i in range(len(unlabeled_indices)):\n            # Count votes for each class\n            votes = np.bincount(all_predictions[:, i], minlength=len(categories))\n            # Normalize to get probabilities\n            vote_probs = votes / len(committee_models)\n            # Calculate entropy\n            entropy = -np.sum(vote_probs * np.log2(vote_probs + 1e-10))\n            vote_entropies.append(entropy)\n        \n        # Select samples with the highest vote entropy\n        top_indices = np.argsort(vote_entropies)[::-1][:10]\n        \n        # Update labeled and unlabeled indices\n        selected_indices = unlabeled_indices[top_indices]\n        labeled_indices = np.append(labeled_indices, selected_indices)\n        unlabeled_indices = np.setdiff1d(unlabeled_indices, selected_indices)\n\nIteration 1: 20 labeled samples, accuracy: 0.2636\nIteration 2: 30 labeled samples, accuracy: 0.3442\nIteration 3: 40 labeled samples, accuracy: 0.3435\nIteration 4: 50 labeled samples, accuracy: 0.4634\nIteration 5: 60 labeled samples, accuracy: 0.5386\nIteration 6: 70 labeled samples, accuracy: 0.5499\nIteration 7: 80 labeled samples, accuracy: 0.6119\nIteration 8: 90 labeled samples, accuracy: 0.6784\nIteration 9: 100 labeled samples, accuracy: 0.7583\nIteration 10: 110 labeled samples, accuracy: 0.7730\n\n\n\n\n\n\n\n\nCombining transfer learning with active learning can be powerful:\n\nUse pre-trained models as feature extractors.\nApply active learning on the feature space.\nFine-tune the model on the selected samples.\n\n\n\n\nSpecial considerations for deep learning models:\n\nUncertainty Estimation: Use dropout or ensemble methods for better uncertainty estimation.\nBatch Normalization: Be careful with batch normalization layers when retraining.\nData Augmentation: Apply data augmentation to increase the effective size of the labeled pool.\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Subset\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\n# Define a simple CNN\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x, dropout=True):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        if dropout:\n            x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        if dropout:\n            x = self.dropout2(x)\n        x = self.fc2(x)\n        return x\n\n# MC Dropout for uncertainty estimation\ndef mc_dropout_uncertainty(model, data_loader, n_samples=10):\n    model.eval()\n    all_probs = []\n    \n    with torch.no_grad():\n        for _ in range(n_samples):\n            batch_probs = []\n            for data, _ in data_loader:\n                output = model(data, dropout=True)\n                probs = F.softmax(output, dim=1)\n                batch_probs.append(probs)\n            \n            # Concatenate batch probabilities\n            all_probs.append(torch.cat(batch_probs))\n    \n    # Stack along a new dimension\n    all_probs = torch.stack(all_probs)\n    \n    # Calculate the mean probabilities\n    mean_probs = torch.mean(all_probs, dim=0)\n    \n    # Calculate entropy of the mean prediction\n    entropy = -torch.sum(mean_probs * torch.log(mean_probs + 1e-10), dim=1)\n    \n    return entropy.numpy()\n\n\n\n\nLeverage both labeled and unlabeled data during training:\n\nSelf-Training: Use model predictions on unlabeled data as pseudo-labels.\nCo-Training: Train multiple models and use their predictions to teach each other.\nConsistency Regularization: Enforce consistent predictions across different perturbations.\n\n\ndef semi_supervised_active_learning(labeled_X, labeled_y, unlabeled_X, model, confidence_threshold=0.95):\n    # Train model on labeled data\n    model.fit(labeled_X, labeled_y)\n    \n    # Predict on unlabeled data\n    probabilities = model.predict_proba(unlabeled_X)\n    max_probs = np.max(probabilities, axis=1)\n    \n    # Get high confidence predictions\n    confident_indices = np.where(max_probs &gt;= confidence_threshold)[0]\n    \n    # Get pseudo-labels for confident predictions\n    pseudo_labels = model.predict(unlabeled_X[confident_indices])\n    \n    # Train on combined dataset\n    combined_X = np.vstack([labeled_X, unlabeled_X[confident_indices]])\n    combined_y = np.concatenate([labeled_y, pseudo_labels])\n    \n    model.fit(combined_X, combined_y)\n    \n    return model, confident_indices\n\n\n\n\nWhen labeled data from the target domain is scarce, active learning can help select the most informative samples:\n\nDomain Discrepancy Measures: Select samples that minimize domain discrepancy.\nAdversarial Selection: Select samples that the domain discriminator is most uncertain about.\nFeature Space Alignment: Select samples that help align feature spaces between domains.\n\n\n\n\n\nAnnotation Interface Design: Make the annotation process intuitive and efficient.\nCognitive Load Management: Group similar samples to reduce cognitive switching.\nExplanations: Provide model explanations to help annotators understand the current model’s decisions.\nQuality Control: Incorporate mechanisms to detect and correct annotation errors.\n\n\n\n\n\nActive learning provides a powerful framework for efficiently building machine learning models with limited labeled data. By selecting the most informative samples for annotation, active learning can significantly reduce the labeling effort while maintaining high model performance.\nThe key to successful active learning is choosing the right influence selection strategy for your specific problem and data characteristics. Consider the following when designing your active learning pipeline:\n\nData Characteristics: Dense vs. sparse data, balanced vs. imbalanced classes, feature distribution.\nModel Type: Linear models, tree-based models, deep learning models.\nComputational Resources: Available memory and processing power.\nAnnotation Budget: Number of samples that can be labeled.\nTask Complexity: Classification vs. regression, number of classes, difficulty of the task.\n\nBy carefully considering these factors and implementing the appropriate influence selection methods, you can build high-performance models with minimal annotation effort."
  },
  {
    "objectID": "posts/influence-selection/index.html#introduction",
    "href": "posts/influence-selection/index.html#introduction",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Active learning is a machine learning paradigm where the algorithm can interactively query an oracle (typically a human annotator) to label new data points. The key idea is to select the most informative samples to be labeled, reducing the overall labeling effort while maintaining or improving model performance. This guide focuses on influence selection methods used in active learning strategies."
  },
  {
    "objectID": "posts/influence-selection/index.html#table-of-contents",
    "href": "posts/influence-selection/index.html#table-of-contents",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Fundamentals of Active Learning\nInfluence Selection Strategies\nUncertainty-Based Methods\nDiversity-Based Methods\nExpected Model Change\nExpected Error Reduction\nInfluence Functions\nQuery-by-Committee\nImplementation Considerations\nEvaluation Metrics\nPractical Examples\nAdvanced Topics"
  },
  {
    "objectID": "posts/influence-selection/index.html#fundamentals-of-active-learning",
    "href": "posts/influence-selection/index.html#fundamentals-of-active-learning",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "The typical active learning process follows these steps:\n\nStart with a small labeled dataset and a large unlabeled pool\nTrain an initial model on the labeled data\nApply an influence selection strategy to choose informative samples from the unlabeled pool\nGet annotations for the selected samples\nAdd the newly labeled samples to the training set\nRetrain the model and repeat steps 3-6 until a stopping condition is met\n\n\n\n\n\nPool-based: The learner has access to a pool of unlabeled data and selects the most informative samples\nStream-based: Samples arrive sequentially, and the learner must decide on-the-fly whether to request labels"
  },
  {
    "objectID": "posts/influence-selection/index.html#influence-selection-strategies",
    "href": "posts/influence-selection/index.html#influence-selection-strategies",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Influence selection is about identifying which unlabeled samples would be most beneficial to label next. Here are the main strategies:"
  },
  {
    "objectID": "posts/influence-selection/index.html#uncertainty-based-methods",
    "href": "posts/influence-selection/index.html#uncertainty-based-methods",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "These methods select samples that the model is most uncertain about.\n\n\n\ndef least_confidence(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    \n    # Get the confidence values for the most probable class\n    confidences = np.max(probabilities, axis=1)\n    \n    # Select the k samples with the lowest confidence\n    least_confident_indices = np.argsort(confidences)[:k]\n    \n    return unlabeled_pool[least_confident_indices]\n\n\n\n\nSelects samples with the smallest margin between the two most likely classes:\n\ndef margin_sampling(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    \n    # Sort the probabilities in descending order\n    sorted_probs = np.sort(probabilities, axis=1)[:, ::-1]\n    \n    # Calculate the margin between the first and second most probable classes\n    margins = sorted_probs[:, 0] - sorted_probs[:, 1]\n    \n    # Select the k samples with the smallest margins\n    smallest_margin_indices = np.argsort(margins)[:k]\n    \n    return unlabeled_pool[smallest_margin_indices]\n\n\n\n\nSelects samples with the highest predictive entropy:\n\ndef entropy_sampling(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    \n    # Calculate entropy for each sample\n    entropies = -np.sum(probabilities * np.log(probabilities + 1e-10), axis=1)\n    \n    # Select the k samples with the highest entropy\n    highest_entropy_indices = np.argsort(entropies)[::-1][:k]\n    \n    return unlabeled_pool[highest_entropy_indices]\n\n\n\n\nFor Bayesian models, BALD selects samples that maximize the mutual information between predictions and model parameters:\n\ndef bald_sampling(bayesian_model, unlabeled_pool, k, n_samples=100):\n    # Get multiple predictions by sampling from the model's posterior\n    probs_samples = []\n    for _ in range(n_samples):\n        probs = bayesian_model.predict_proba(unlabeled_pool)\n        probs_samples.append(probs)\n    \n    # Stack into a 3D array: (samples, data points, classes)\n    probs_samples = np.stack(probs_samples)\n    \n    # Calculate the average probability across all samples\n    mean_probs = np.mean(probs_samples, axis=0)\n    \n    # Calculate the entropy of the average prediction\n    entropy_mean = -np.sum(mean_probs * np.log(mean_probs + 1e-10), axis=1)\n    \n    # Calculate the average entropy across all samples\n    entropy_samples = -np.sum(probs_samples * np.log(probs_samples + 1e-10), axis=2)\n    mean_entropy = np.mean(entropy_samples, axis=0)\n    \n    # Mutual information = entropy of the mean - mean of entropies\n    bald_scores = entropy_mean - mean_entropy\n    \n    # Select the k samples with the highest BALD scores\n    highest_bald_indices = np.argsort(bald_scores)[::-1][:k]\n    \n    return unlabeled_pool[highest_bald_indices]"
  },
  {
    "objectID": "posts/influence-selection/index.html#diversity-based-methods",
    "href": "posts/influence-selection/index.html#diversity-based-methods",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "These methods aim to select a diverse set of examples to ensure broad coverage of the input space.\n\n\n\ndef clustering_based_sampling(unlabeled_pool, k, n_clusters=None):\n    if n_clusters is None:\n        n_clusters = k\n    \n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(unlabeled_pool)\n    \n    # Get the cluster centers and distances to each point\n    centers = kmeans.cluster_centers_\n    distances = kmeans.transform(unlabeled_pool)  # Distance to each cluster center\n    \n    # Select one sample from each cluster (closest to the center)\n    selected_indices = []\n    for i in range(n_clusters):\n        # Get the samples in this cluster\n        cluster_samples = np.where(kmeans.labels_ == i)[0]\n        \n        # Find the sample closest to the center\n        closest_sample = cluster_samples[np.argmin(distances[cluster_samples, i])]\n        selected_indices.append(closest_sample)\n    \n    # If we need more samples than clusters, fill with the most uncertain samples\n    if k &gt; n_clusters:\n        # Implementation depends on uncertainty measure\n        pass\n    \n    return unlabeled_pool[selected_indices[:k]]\n\n\n\n\nThe core-set approach aims to select a subset of data that best represents the whole dataset:\n\ndef core_set_sampling(labeled_pool, unlabeled_pool, k):\n    # Combine labeled and unlabeled data for distance calculations\n    all_data = np.vstack((labeled_pool, unlabeled_pool))\n    \n    # Compute pairwise distances\n    distances = pairwise_distances(all_data)\n    \n    # Split distances into labeled-unlabeled and unlabeled-unlabeled\n    n_labeled = labeled_pool.shape[0]\n    dist_labeled_unlabeled = distances[:n_labeled, n_labeled:]\n    \n    # For each unlabeled sample, find the minimum distance to any labeled sample\n    min_distances = np.min(dist_labeled_unlabeled, axis=0)\n    \n    # Select the k samples with the largest minimum distances\n    farthest_indices = np.argsort(min_distances)[::-1][:k]\n    \n    return unlabeled_pool[farthest_indices]"
  },
  {
    "objectID": "posts/influence-selection/index.html#expected-model-change",
    "href": "posts/influence-selection/index.html#expected-model-change",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "The Expected Model Change (EMC) method selects samples that would cause the greatest change in the model if they were labeled:\n\ndef expected_model_change(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    n_classes = probabilities.shape[1]\n    \n    # Calculate expected gradient length for each sample\n    expected_changes = []\n    for i, x in enumerate(unlabeled_pool):\n        # Calculate expected gradient length across all possible labels\n        change = 0\n        for c in range(n_classes):\n            # For each possible class, calculate the gradient if this was the true label\n            x_expanded = x.reshape(1, -1)\n            # Here we would compute the gradient of the model with respect to the sample\n            # For simplicity, we use a placeholder\n            gradient = compute_gradient(model, x_expanded, c)\n            norm_gradient = np.linalg.norm(gradient)\n            \n            # Weight by the probability of this class\n            change += probabilities[i, c] * norm_gradient\n        \n        expected_changes.append(change)\n    \n    # Select the k samples with the highest expected change\n    highest_change_indices = np.argsort(expected_changes)[::-1][:k]\n    \n    return unlabeled_pool[highest_change_indices]\n\nNote: The compute_gradient function would need to be implemented based on the specific model being used."
  },
  {
    "objectID": "posts/influence-selection/index.html#expected-error-reduction",
    "href": "posts/influence-selection/index.html#expected-error-reduction",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "The Expected Error Reduction method selects samples that, when labeled, would minimally reduce the model’s expected error:\n\ndef expected_error_reduction(model, unlabeled_pool, unlabeled_pool_remaining, k):\n    # Predict probabilities for all remaining unlabeled data\n    current_probs = model.predict_proba(unlabeled_pool_remaining)\n    current_entropy = -np.sum(current_probs * np.log(current_probs + 1e-10), axis=1)\n    \n    expected_error_reductions = []\n    \n    # For each sample in the unlabeled pool we're considering\n    for i, x in enumerate(unlabeled_pool):\n        # Predict probabilities for this sample\n        probs = model.predict_proba(x.reshape(1, -1))[0]\n        \n        # Calculate the expected error reduction for each possible label\n        error_reduction = 0\n        for c in range(len(probs)):\n            # Create a hypothetical new model with this labeled sample\n            # For simplicity, we use a placeholder function\n            hypothetical_model = train_with_additional_sample(model, x, c)\n            \n            # Get new probabilities with this model\n            new_probs = hypothetical_model.predict_proba(unlabeled_pool_remaining)\n            new_entropy = -np.sum(new_probs * np.log(new_probs + 1e-10), axis=1)\n            \n            # Expected entropy reduction\n            reduction = np.sum(current_entropy - new_entropy)\n            \n            # Weight by the probability of this class\n            error_reduction += probs[c] * reduction\n        \n        expected_error_reductions.append(error_reduction)\n    \n    # Select the k samples with the highest expected error reduction\n    highest_reduction_indices = np.argsort(expected_error_reductions)[::-1][:k]\n    \n    return unlabeled_pool[highest_reduction_indices]\n\nNote: The train_with_additional_sample function would need to be implemented based on the specific model being used."
  },
  {
    "objectID": "posts/influence-selection/index.html#influence-functions",
    "href": "posts/influence-selection/index.html#influence-functions",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Influence functions approximate the effect of adding or removing a training example without retraining the model:\n\ndef influence_function_sampling(model, unlabeled_pool, labeled_pool, k, labels):\n    influences = []\n    \n    # For each unlabeled sample\n    for x_u in unlabeled_pool:\n        # Calculate the influence of adding this sample to the training set\n        influence = calculate_influence(model, x_u, labeled_pool, labels)\n        influences.append(influence)\n    \n    # Select the k samples with the highest influence\n    highest_influence_indices = np.argsort(influences)[::-1][:k]\n    \n    return unlabeled_pool[highest_influence_indices]\n\nNote: The calculate_influence function would need to be implemented based on the specific model and influence metric being used."
  },
  {
    "objectID": "posts/influence-selection/index.html#query-by-committee",
    "href": "posts/influence-selection/index.html#query-by-committee",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Query-by-Committee (QBC) methods train multiple models (a committee) and select samples where they disagree:\n\ndef query_by_committee(committee_models, unlabeled_pool, k):\n    # Get predictions from all committee members\n    all_predictions = []\n    for model in committee_models:\n        preds = model.predict(unlabeled_pool)\n        all_predictions.append(preds)\n    \n    # Stack predictions into a 2D array (committee members, data points)\n    all_predictions = np.stack(all_predictions)\n    \n    # Calculate disagreement (e.g., using vote entropy)\n    disagreements = []\n    for i in range(unlabeled_pool.shape[0]):\n        # Count votes for each class\n        votes = np.bincount(all_predictions[:, i])\n        # Normalize to get probabilities\n        vote_probs = votes / len(committee_models)\n        # Calculate entropy\n        entropy = -np.sum(vote_probs * np.log2(vote_probs + 1e-10))\n        disagreements.append(entropy)\n    \n    # Select the k samples with the highest disagreement\n    highest_disagreement_indices = np.argsort(disagreements)[::-1][:k]\n    \n    return unlabeled_pool[highest_disagreement_indices]"
  },
  {
    "objectID": "posts/influence-selection/index.html#implementation-considerations",
    "href": "posts/influence-selection/index.html#implementation-considerations",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "In practice, it’s often more efficient to select multiple samples at once. However, simply selecting the top-k samples may lead to redundancy. Consider using:\n\nGreedy Selection with Diversity: Select one sample at a time, then update the diversity metrics to avoid selecting similar samples.\n\n\ndef batch_selection_with_diversity(model, unlabeled_pool, k, lambda_diversity=0.5):\n    selected_indices = []\n    remaining_indices = list(range(len(unlabeled_pool)))\n    \n    # Calculate uncertainty scores for all samples\n    probabilities = model.predict_proba(unlabeled_pool)\n    entropies = -np.sum(probabilities * np.log(probabilities + 1e-10), axis=1)\n    \n    # Calculate distance matrix for diversity\n    distance_matrix = pairwise_distances(unlabeled_pool)\n    \n    for _ in range(k):\n        if not remaining_indices:\n            break\n        \n        scores = np.zeros(len(remaining_indices))\n        \n        # Calculate uncertainty scores\n        uncertainty_scores = entropies[remaining_indices]\n        \n        # Calculate diversity scores (if we have already selected some samples)\n        if selected_indices:\n            # For each remaining sample, calculate the minimum distance to any selected sample\n            diversity_scores = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n        else:\n            diversity_scores = np.zeros(len(remaining_indices))\n        \n        # Normalize scores\n        uncertainty_scores = (uncertainty_scores - np.min(uncertainty_scores)) / (np.max(uncertainty_scores) - np.min(uncertainty_scores) + 1e-10)\n        if selected_indices:\n            diversity_scores = (diversity_scores - np.min(diversity_scores)) / (np.max(diversity_scores) - np.min(diversity_scores) + 1e-10)\n        \n        # Combine scores\n        scores = (1 - lambda_diversity) * uncertainty_scores + lambda_diversity * diversity_scores\n        \n        # Select the sample with the highest score\n        best_idx = np.argmax(scores)\n        selected_idx = remaining_indices[best_idx]\n        \n        # Add to selected and remove from remaining\n        selected_indices.append(selected_idx)\n        remaining_indices.remove(selected_idx)\n    \n    return unlabeled_pool[selected_indices]\n\n\nSubmodular Function Maximization: Use a submodular function to ensure diversity in the selected batch.\n\n\n\n\nActive learning can inadvertently reinforce class imbalance. Consider:\n\nStratified Sampling: Ensure representation from all classes.\nHybrid Approaches: Combine uncertainty-based and density-based methods.\nDiversity Constraints: Explicitly enforce diversity in feature space.\n\n\n\n\nSome methods (like expected error reduction) can be computationally expensive. Consider:\n\nSubsample the Unlabeled Pool: Only consider a random subset for selection.\nPre-compute Embeddings: Use a fixed feature extractor to pre-compute embeddings.\nApproximate Methods: Use approximations for expensive operations."
  },
  {
    "objectID": "posts/influence-selection/index.html#evaluation-metrics",
    "href": "posts/influence-selection/index.html#evaluation-metrics",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Plot model performance vs. number of labeled samples:\n\ndef plot_learning_curve(model_factory, X_train, y_train, X_test, y_test, \n                        active_learning_strategy, initial_size=10, \n                        batch_size=10, n_iterations=20):\n    # Initialize with a small labeled set\n    labeled_indices = np.random.choice(len(X_train), initial_size, replace=False)\n    unlabeled_indices = np.setdiff1d(np.arange(len(X_train)), labeled_indices)\n    \n    performance = []\n    \n    for i in range(n_iterations):\n        # Create a fresh model\n        model = model_factory()\n        \n        # Train on the currently labeled data\n        model.fit(X_train[labeled_indices], y_train[labeled_indices])\n        \n        # Evaluate on the test set\n        score = model.score(X_test, y_test)\n        performance.append((len(labeled_indices), score))\n        \n        # Select the next batch of samples\n        if len(unlabeled_indices) &gt; 0:\n            # Use the specified active learning strategy\n            selected_indices = active_learning_strategy(\n                model, X_train[unlabeled_indices], batch_size\n            )\n            \n            # Map back to original indices\n            selected_original_indices = unlabeled_indices[selected_indices]\n            \n            # Update labeled and unlabeled indices\n            labeled_indices = np.append(labeled_indices, selected_original_indices)\n            unlabeled_indices = np.setdiff1d(unlabeled_indices, selected_original_indices)\n    \n    # Plot the learning curve\n    counts, scores = zip(*performance)\n    plt.figure(figsize=(10, 6))\n    plt.plot(counts, scores, 'o-')\n    plt.xlabel('Number of labeled samples')\n    plt.ylabel('Model accuracy')\n    plt.title('Active Learning Performance')\n    plt.grid(True)\n    \n    return performance\n\n\n\n\nAlways compare your active learning strategy with random sampling as a baseline.\n\n\n\nCalculate how many annotations you saved compared to using the entire dataset."
  },
  {
    "objectID": "posts/influence-selection/index.html#practical-examples",
    "href": "posts/influence-selection/index.html#practical-examples",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "import numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n# Load data\nmnist = fetch_openml('mnist_784', version=1, cache=True)\nX, y = mnist['data'], mnist['target']\n\n# Split into train and test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Initially, only a small portion is labeled\ninitial_size = 100\nlabeled_indices = np.random.choice(len(X_train), initial_size, replace=False)\nunlabeled_indices = np.setdiff1d(np.arange(len(X_train)), labeled_indices)\n\n# Tracking performance\nactive_learning_performance = []\nrandom_sampling_performance = []\n\n# Active learning loop\nfor i in range(10):  # 10 iterations\n    # Train a model on the currently labeled data\n    model = RandomForestClassifier(n_estimators=50, random_state=42)\n    model.fit(X_train.iloc[labeled_indices], y_train.iloc[labeled_indices])\n    \n    # Evaluate on the test set\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    active_learning_performance.append((len(labeled_indices), accuracy))\n    \n    print(f\"Iteration {i+1}: {len(labeled_indices)} labeled samples, \"\n          f\"accuracy: {accuracy:.4f}\")\n    \n    # Select 100 new samples using entropy sampling\n    if len(unlabeled_indices) &gt; 0:\n        # Predict probabilities for each unlabeled sample\n        probs = model.predict_proba(X_train.iloc[unlabeled_indices])\n        \n        # Calculate entropy\n        entropies = -np.sum(probs * np.log(probs + 1e-10), axis=1)\n        \n        # Select samples with the highest entropy\n        top_indices = np.argsort(entropies)[::-1][:100]\n        \n        # Update labeled and unlabeled indices\n        selected_indices = unlabeled_indices[top_indices]\n        labeled_indices = np.append(labeled_indices, selected_indices)\n        unlabeled_indices = np.setdiff1d(unlabeled_indices, selected_indices)\n\n# Plot learning curve\ncounts, scores = zip(*active_learning_performance)\nplt.figure(figsize=(10, 6))\nplt.plot(counts, scores, 'o-', label='Active Learning')\nplt.xlabel('Number of labeled samples')\nplt.ylabel('Model accuracy')\nplt.title('Active Learning Performance')\nplt.grid(True)\nplt.legend()\nplt.show()\n\nIteration 1: 100 labeled samples, accuracy: 0.6917\nIteration 2: 200 labeled samples, accuracy: 0.7290\nIteration 3: 300 labeled samples, accuracy: 0.7716\nIteration 4: 400 labeled samples, accuracy: 0.7817\nIteration 5: 500 labeled samples, accuracy: 0.8239\nIteration 6: 600 labeled samples, accuracy: 0.8227\nIteration 7: 700 labeled samples, accuracy: 0.8282\nIteration 8: 800 labeled samples, accuracy: 0.8435\nIteration 9: 900 labeled samples, accuracy: 0.8454\nIteration 10: 1000 labeled samples, accuracy: 0.8549\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.datasets import fetch_20newsgroups\n\n# Load data\ncategories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\ntwenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\ntwenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n\n# Feature extraction\nvectorizer = TfidfVectorizer(stop_words='english')\nX_train = vectorizer.fit_transform(twenty_train.data)\nX_test = vectorizer.transform(twenty_test.data)\ny_train = twenty_train.target\ny_test = twenty_test.target\n\n# Initially, only a small portion is labeled\ninitial_size = 20\nlabeled_indices = np.random.choice(len(X_train.toarray()), initial_size, replace=False)\nunlabeled_indices = np.setdiff1d(np.arange(len(X_train.toarray())), labeled_indices)\n\n# Create a committee of models\nmodels = [\n    ('nb', MultinomialNB()),\n    ('svm', SVC(kernel='linear', probability=True)),\n    ('svm2', SVC(kernel='rbf', probability=True))\n]\n\n# Active learning loop\nfor i in range(10):  # 10 iterations\n    # Train each model on the currently labeled data\n    committee_models = []\n    for name, model in models:\n        model.fit(X_train[labeled_indices], y_train[labeled_indices])\n        committee_models.append(model)\n    \n    # Evaluate using the VotingClassifier\n    voting_clf = VotingClassifier(estimators=models, voting='soft')\n    voting_clf.fit(X_train[labeled_indices], y_train[labeled_indices])\n    \n    accuracy = voting_clf.score(X_test, y_test)\n    print(f\"Iteration {i+1}: {len(labeled_indices)} labeled samples, \"\n          f\"accuracy: {accuracy:.4f}\")\n    \n    # Select 10 new samples using Query-by-Committee\n    if len(unlabeled_indices) &gt; 0:\n        # Get predictions from all committee members\n        all_predictions = []\n        for model in committee_models:\n            preds = model.predict(X_train[unlabeled_indices])\n            all_predictions.append(preds)\n        \n        # Calculate vote entropy\n        vote_entropies = []\n        all_predictions = np.array(all_predictions)\n        for i in range(len(unlabeled_indices)):\n            # Count votes for each class\n            votes = np.bincount(all_predictions[:, i], minlength=len(categories))\n            # Normalize to get probabilities\n            vote_probs = votes / len(committee_models)\n            # Calculate entropy\n            entropy = -np.sum(vote_probs * np.log2(vote_probs + 1e-10))\n            vote_entropies.append(entropy)\n        \n        # Select samples with the highest vote entropy\n        top_indices = np.argsort(vote_entropies)[::-1][:10]\n        \n        # Update labeled and unlabeled indices\n        selected_indices = unlabeled_indices[top_indices]\n        labeled_indices = np.append(labeled_indices, selected_indices)\n        unlabeled_indices = np.setdiff1d(unlabeled_indices, selected_indices)\n\nIteration 1: 20 labeled samples, accuracy: 0.2636\nIteration 2: 30 labeled samples, accuracy: 0.3442\nIteration 3: 40 labeled samples, accuracy: 0.3435\nIteration 4: 50 labeled samples, accuracy: 0.4634\nIteration 5: 60 labeled samples, accuracy: 0.5386\nIteration 6: 70 labeled samples, accuracy: 0.5499\nIteration 7: 80 labeled samples, accuracy: 0.6119\nIteration 8: 90 labeled samples, accuracy: 0.6784\nIteration 9: 100 labeled samples, accuracy: 0.7583\nIteration 10: 110 labeled samples, accuracy: 0.7730"
  },
  {
    "objectID": "posts/influence-selection/index.html#advanced-topics",
    "href": "posts/influence-selection/index.html#advanced-topics",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Combining transfer learning with active learning can be powerful:\n\nUse pre-trained models as feature extractors.\nApply active learning on the feature space.\nFine-tune the model on the selected samples.\n\n\n\n\nSpecial considerations for deep learning models:\n\nUncertainty Estimation: Use dropout or ensemble methods for better uncertainty estimation.\nBatch Normalization: Be careful with batch normalization layers when retraining.\nData Augmentation: Apply data augmentation to increase the effective size of the labeled pool.\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Subset\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\n# Define a simple CNN\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x, dropout=True):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        if dropout:\n            x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        if dropout:\n            x = self.dropout2(x)\n        x = self.fc2(x)\n        return x\n\n# MC Dropout for uncertainty estimation\ndef mc_dropout_uncertainty(model, data_loader, n_samples=10):\n    model.eval()\n    all_probs = []\n    \n    with torch.no_grad():\n        for _ in range(n_samples):\n            batch_probs = []\n            for data, _ in data_loader:\n                output = model(data, dropout=True)\n                probs = F.softmax(output, dim=1)\n                batch_probs.append(probs)\n            \n            # Concatenate batch probabilities\n            all_probs.append(torch.cat(batch_probs))\n    \n    # Stack along a new dimension\n    all_probs = torch.stack(all_probs)\n    \n    # Calculate the mean probabilities\n    mean_probs = torch.mean(all_probs, dim=0)\n    \n    # Calculate entropy of the mean prediction\n    entropy = -torch.sum(mean_probs * torch.log(mean_probs + 1e-10), dim=1)\n    \n    return entropy.numpy()\n\n\n\n\nLeverage both labeled and unlabeled data during training:\n\nSelf-Training: Use model predictions on unlabeled data as pseudo-labels.\nCo-Training: Train multiple models and use their predictions to teach each other.\nConsistency Regularization: Enforce consistent predictions across different perturbations.\n\n\ndef semi_supervised_active_learning(labeled_X, labeled_y, unlabeled_X, model, confidence_threshold=0.95):\n    # Train model on labeled data\n    model.fit(labeled_X, labeled_y)\n    \n    # Predict on unlabeled data\n    probabilities = model.predict_proba(unlabeled_X)\n    max_probs = np.max(probabilities, axis=1)\n    \n    # Get high confidence predictions\n    confident_indices = np.where(max_probs &gt;= confidence_threshold)[0]\n    \n    # Get pseudo-labels for confident predictions\n    pseudo_labels = model.predict(unlabeled_X[confident_indices])\n    \n    # Train on combined dataset\n    combined_X = np.vstack([labeled_X, unlabeled_X[confident_indices]])\n    combined_y = np.concatenate([labeled_y, pseudo_labels])\n    \n    model.fit(combined_X, combined_y)\n    \n    return model, confident_indices\n\n\n\n\nWhen labeled data from the target domain is scarce, active learning can help select the most informative samples:\n\nDomain Discrepancy Measures: Select samples that minimize domain discrepancy.\nAdversarial Selection: Select samples that the domain discriminator is most uncertain about.\nFeature Space Alignment: Select samples that help align feature spaces between domains.\n\n\n\n\n\nAnnotation Interface Design: Make the annotation process intuitive and efficient.\nCognitive Load Management: Group similar samples to reduce cognitive switching.\nExplanations: Provide model explanations to help annotators understand the current model’s decisions.\nQuality Control: Incorporate mechanisms to detect and correct annotation errors."
  },
  {
    "objectID": "posts/influence-selection/index.html#conclusion",
    "href": "posts/influence-selection/index.html#conclusion",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Active learning provides a powerful framework for efficiently building machine learning models with limited labeled data. By selecting the most informative samples for annotation, active learning can significantly reduce the labeling effort while maintaining high model performance.\nThe key to successful active learning is choosing the right influence selection strategy for your specific problem and data characteristics. Consider the following when designing your active learning pipeline:\n\nData Characteristics: Dense vs. sparse data, balanced vs. imbalanced classes, feature distribution.\nModel Type: Linear models, tree-based models, deep learning models.\nComputational Resources: Available memory and processing power.\nAnnotation Budget: Number of samples that can be labeled.\nTask Complexity: Classification vs. regression, number of classes, difficulty of the task.\n\nBy carefully considering these factors and implementing the appropriate influence selection methods, you can build high-performance models with minimal annotation effort."
  },
  {
    "objectID": "posts/dino-explained/index.html",
    "href": "posts/dino-explained/index.html",
    "title": "DINO: Emerging Properties in Self-Supervised Vision Transformers",
    "section": "",
    "text": "In 2021, Facebook AI Research (now Meta AI) introduced DINO (Self-Distillation with No Labels), a groundbreaking approach to self-supervised learning in computer vision. Published in the paper “Emerging Properties in Self-Supervised Vision Transformers” by Mathilde Caron and colleagues, DINO represented a significant leap forward in learning visual representations without relying on labeled data. This article explores the key aspects of the original DINO research, its methodology, and its implications for computer vision.\n\n\n\nTraditionally, computer vision models have relied heavily on supervised learning using massive labeled datasets like ImageNet. However, creating such datasets requires enormous human effort for annotation. Self-supervised learning aims to overcome this limitation by teaching models to learn meaningful representations from unlabeled images, which are abundantly available.\nSeveral approaches to self-supervised learning had been proposed before DINO, including:\n\nContrastive learning (SimCLR, MoCo)\nClustering-based methods (SwAV, DeepCluster)\nPredictive methods (predicting rotations, solving jigsaw puzzles)\n\nDINO introduced a novel approach that combined elements of knowledge distillation and self-supervision to produce surprisingly effective visual representations.\n\n\n\nDINO’s key innovation was adapting the concept of knowledge distillation to a self-supervised setting. Traditional knowledge distillation involves a teacher model transferring knowledge to a student model, but DINO cleverly applies this concept without requiring separate pre-trained teacher models.\n\n\nIn DINO:\n\nTeacher and Student Networks: Both networks share the same architecture but have different parameters.\nParameter Updates:\n\nThe student network is updated through standard backpropagation\nThe teacher is updated as an exponential moving average (EMA) of the student’s parameters\n\n\nThis creates a bootstrapping effect where the teacher continually provides slightly better targets for the student to learn from.\n\n\n\nDINO employs a sophisticated data augmentation approach:\n\nGlobal Views: Two larger crops of an image (covering significant portions)\nLocal Views: Several smaller crops that focus on details\n\nThe student network processes all views (global and local), while the teacher only processes the global views. The student network is trained to predict the teacher’s output for the global views from the local views, forcing it to understand both global context and local details.\n\n\n\nThe training objective minimizes the cross-entropy between the teacher’s output distribution for global views and the student’s output distribution for all views (both global and local). This encourages consistency across different scales and regions of the image.\n\n\n\nA major challenge in self-supervised learning is representation collapse—where the model outputs the same representation regardless of input. DINO prevents this through:\n\nCentering: Subtracting a running average of the network’s output from the current output\nSharpening: Using a temperature parameter in the softmax that gradually decreases throughout training\n\nThese techniques ensure the model learns diverse and meaningful features.\n\n\n\n\nWhile DINO can be applied to various neural network architectures, the paper demonstrated particularly impressive results using Vision Transformers (ViT). The combination of DINO with ViT offered several advantages:\n\nPatch-based processing: ViT divides images into patches, which aligns well with DINO’s local-global view approach\nSelf-attention mechanism: Enables capturing long-range dependencies in images\nScalability: The architecture scales effectively with more data and parameters\n\nDINO was implemented with various sizes of ViT models: - ViT-S: Small (22M parameters) - ViT-B: Base (86M parameters)\n\n\n\nThe most surprising aspect of DINO was the emergence of properties that weren’t explicitly trained for:\n\n\nRemarkably, the self-attention maps from DINO-trained Vision Transformers naturally highlighted object boundaries in images. Without any segmentation supervision, the model learned to focus attention on semantically meaningful regions. This surprised the research community and suggested that the model had developed a deeper understanding of visual structures than previous self-supervised approaches.\n\n\n\nDINO produced local features (from patch tokens) that proved extremely effective for tasks requiring spatial understanding, like semantic segmentation. The features exhibited strong semantic coherence across spatial regions.\n\n\n\nUsing DINO features with simple k-nearest neighbor classifiers achieved impressive accuracy on ImageNet classification, demonstrating the quality of the learned representations.\n\n\n\n\nThe original DINO paper described several important implementation details:\n\n\nThe augmentation pipeline included: - Random resized cropping - Horizontal flipping - Color jittering - Gaussian blur - Solarization (for some views)\n\n\n\n\nOptimizer: AdamW with weight decay\nLearning rate: Cosine schedule with linear warmup\nBatch size: 1024 images\n\n\n\n\n\nProjection head: 3-layer MLP with bottleneck structure\nCLS token: Used as global image representation\nPositional embeddings: Standard learnable embeddings\n\n\n\n\n\nDINO achieved remarkable results on several benchmarks:\n\n\n\n80.1% top-1 accuracy with k-NN classification using ViT-B\nCompetitive with supervised methods and superior to previous self-supervised approaches\n\n\n\n\nDINO features transferred successfully to: - Object detection - Semantic segmentation - Video instance segmentation\n\n\n\nThe features showed strong robustness to distribution shifts and generalized well to out-of-distribution data.\n\n\n\n\nDINO differed from earlier self-supervised approaches in several key ways:\n\n\n\nNo need for large negative sample sets\nNo dependence on intricate data augmentation strategies\nMore stable training dynamics\n\n\n\n\n\nNo explicit clustering objective\nMore straightforward implementation\nBetter scaling properties with model size\n\n\n\n\n\nThe original DINO research represented a significant step forward in self-supervised visual representation learning. By combining knowledge distillation techniques with self-supervision and leveraging the Vision Transformer architecture, DINO produced features with remarkable properties for a wide range of computer vision tasks.\nThe emergence of semantic features and unsupervised segmentation abilities demonstrated that well-designed self-supervised methods could lead to models that understand visual concepts in ways previously thought to require explicit supervision. DINO laid the groundwork for subsequent advances in this field, including its successor DINOv2, and helped establish self-supervised learning as a powerful paradigm for computer vision.\nThe success of DINO highlighted the potential for self-supervised learning to reduce reliance on large labeled datasets and pointed toward a future where visual foundation models could be developed primarily through self-supervision – mirroring similar developments in natural language processing with large language models."
  },
  {
    "objectID": "posts/dino-explained/index.html#introduction",
    "href": "posts/dino-explained/index.html#introduction",
    "title": "DINO: Emerging Properties in Self-Supervised Vision Transformers",
    "section": "",
    "text": "In 2021, Facebook AI Research (now Meta AI) introduced DINO (Self-Distillation with No Labels), a groundbreaking approach to self-supervised learning in computer vision. Published in the paper “Emerging Properties in Self-Supervised Vision Transformers” by Mathilde Caron and colleagues, DINO represented a significant leap forward in learning visual representations without relying on labeled data. This article explores the key aspects of the original DINO research, its methodology, and its implications for computer vision."
  },
  {
    "objectID": "posts/dino-explained/index.html#the-challenge-of-self-supervised-learning",
    "href": "posts/dino-explained/index.html#the-challenge-of-self-supervised-learning",
    "title": "DINO: Emerging Properties in Self-Supervised Vision Transformers",
    "section": "",
    "text": "Traditionally, computer vision models have relied heavily on supervised learning using massive labeled datasets like ImageNet. However, creating such datasets requires enormous human effort for annotation. Self-supervised learning aims to overcome this limitation by teaching models to learn meaningful representations from unlabeled images, which are abundantly available.\nSeveral approaches to self-supervised learning had been proposed before DINO, including:\n\nContrastive learning (SimCLR, MoCo)\nClustering-based methods (SwAV, DeepCluster)\nPredictive methods (predicting rotations, solving jigsaw puzzles)\n\nDINO introduced a novel approach that combined elements of knowledge distillation and self-supervision to produce surprisingly effective visual representations."
  },
  {
    "objectID": "posts/dino-explained/index.html#dinos-core-methodology",
    "href": "posts/dino-explained/index.html#dinos-core-methodology",
    "title": "DINO: Emerging Properties in Self-Supervised Vision Transformers",
    "section": "",
    "text": "DINO’s key innovation was adapting the concept of knowledge distillation to a self-supervised setting. Traditional knowledge distillation involves a teacher model transferring knowledge to a student model, but DINO cleverly applies this concept without requiring separate pre-trained teacher models.\n\n\nIn DINO:\n\nTeacher and Student Networks: Both networks share the same architecture but have different parameters.\nParameter Updates:\n\nThe student network is updated through standard backpropagation\nThe teacher is updated as an exponential moving average (EMA) of the student’s parameters\n\n\nThis creates a bootstrapping effect where the teacher continually provides slightly better targets for the student to learn from.\n\n\n\nDINO employs a sophisticated data augmentation approach:\n\nGlobal Views: Two larger crops of an image (covering significant portions)\nLocal Views: Several smaller crops that focus on details\n\nThe student network processes all views (global and local), while the teacher only processes the global views. The student network is trained to predict the teacher’s output for the global views from the local views, forcing it to understand both global context and local details.\n\n\n\nThe training objective minimizes the cross-entropy between the teacher’s output distribution for global views and the student’s output distribution for all views (both global and local). This encourages consistency across different scales and regions of the image.\n\n\n\nA major challenge in self-supervised learning is representation collapse—where the model outputs the same representation regardless of input. DINO prevents this through:\n\nCentering: Subtracting a running average of the network’s output from the current output\nSharpening: Using a temperature parameter in the softmax that gradually decreases throughout training\n\nThese techniques ensure the model learns diverse and meaningful features."
  },
  {
    "objectID": "posts/dino-explained/index.html#vision-transformer-architecture",
    "href": "posts/dino-explained/index.html#vision-transformer-architecture",
    "title": "DINO: Emerging Properties in Self-Supervised Vision Transformers",
    "section": "",
    "text": "While DINO can be applied to various neural network architectures, the paper demonstrated particularly impressive results using Vision Transformers (ViT). The combination of DINO with ViT offered several advantages:\n\nPatch-based processing: ViT divides images into patches, which aligns well with DINO’s local-global view approach\nSelf-attention mechanism: Enables capturing long-range dependencies in images\nScalability: The architecture scales effectively with more data and parameters\n\nDINO was implemented with various sizes of ViT models: - ViT-S: Small (22M parameters) - ViT-B: Base (86M parameters)"
  },
  {
    "objectID": "posts/dino-explained/index.html#emergent-properties",
    "href": "posts/dino-explained/index.html#emergent-properties",
    "title": "DINO: Emerging Properties in Self-Supervised Vision Transformers",
    "section": "",
    "text": "The most surprising aspect of DINO was the emergence of properties that weren’t explicitly trained for:\n\n\nRemarkably, the self-attention maps from DINO-trained Vision Transformers naturally highlighted object boundaries in images. Without any segmentation supervision, the model learned to focus attention on semantically meaningful regions. This surprised the research community and suggested that the model had developed a deeper understanding of visual structures than previous self-supervised approaches.\n\n\n\nDINO produced local features (from patch tokens) that proved extremely effective for tasks requiring spatial understanding, like semantic segmentation. The features exhibited strong semantic coherence across spatial regions.\n\n\n\nUsing DINO features with simple k-nearest neighbor classifiers achieved impressive accuracy on ImageNet classification, demonstrating the quality of the learned representations."
  },
  {
    "objectID": "posts/dino-explained/index.html#training-details",
    "href": "posts/dino-explained/index.html#training-details",
    "title": "DINO: Emerging Properties in Self-Supervised Vision Transformers",
    "section": "",
    "text": "The original DINO paper described several important implementation details:\n\n\nThe augmentation pipeline included: - Random resized cropping - Horizontal flipping - Color jittering - Gaussian blur - Solarization (for some views)\n\n\n\n\nOptimizer: AdamW with weight decay\nLearning rate: Cosine schedule with linear warmup\nBatch size: 1024 images\n\n\n\n\n\nProjection head: 3-layer MLP with bottleneck structure\nCLS token: Used as global image representation\nPositional embeddings: Standard learnable embeddings"
  },
  {
    "objectID": "posts/dino-explained/index.html#results-and-impact",
    "href": "posts/dino-explained/index.html#results-and-impact",
    "title": "DINO: Emerging Properties in Self-Supervised Vision Transformers",
    "section": "",
    "text": "DINO achieved remarkable results on several benchmarks:\n\n\n\n80.1% top-1 accuracy with k-NN classification using ViT-B\nCompetitive with supervised methods and superior to previous self-supervised approaches\n\n\n\n\nDINO features transferred successfully to: - Object detection - Semantic segmentation - Video instance segmentation\n\n\n\nThe features showed strong robustness to distribution shifts and generalized well to out-of-distribution data."
  },
  {
    "objectID": "posts/dino-explained/index.html#comparison-with-previous-methods",
    "href": "posts/dino-explained/index.html#comparison-with-previous-methods",
    "title": "DINO: Emerging Properties in Self-Supervised Vision Transformers",
    "section": "",
    "text": "DINO differed from earlier self-supervised approaches in several key ways:\n\n\n\nNo need for large negative sample sets\nNo dependence on intricate data augmentation strategies\nMore stable training dynamics\n\n\n\n\n\nNo explicit clustering objective\nMore straightforward implementation\nBetter scaling properties with model size"
  },
  {
    "objectID": "posts/dino-explained/index.html#conclusion",
    "href": "posts/dino-explained/index.html#conclusion",
    "title": "DINO: Emerging Properties in Self-Supervised Vision Transformers",
    "section": "",
    "text": "The original DINO research represented a significant step forward in self-supervised visual representation learning. By combining knowledge distillation techniques with self-supervision and leveraging the Vision Transformer architecture, DINO produced features with remarkable properties for a wide range of computer vision tasks.\nThe emergence of semantic features and unsupervised segmentation abilities demonstrated that well-designed self-supervised methods could lead to models that understand visual concepts in ways previously thought to require explicit supervision. DINO laid the groundwork for subsequent advances in this field, including its successor DINOv2, and helped establish self-supervised learning as a powerful paradigm for computer vision.\nThe success of DINO highlighted the potential for self-supervised learning to reduce reliance on large labeled datasets and pointed toward a future where visual foundation models could be developed primarily through self-supervision – mirroring similar developments in natural language processing with large language models."
  },
  {
    "objectID": "posts/litserve-basics/index.html",
    "href": "posts/litserve-basics/index.html",
    "title": "LitServe Code Guide",
    "section": "",
    "text": "LitServe is a high-performance, flexible AI model serving framework designed to deploy machine learning models with minimal code. It provides automatic batching, GPU acceleration, and easy scaling capabilities.\n\n\n\nInstallation\nBasic Usage\nCore Concepts\nAdvanced Features\nConfiguration Options\nExamples\nBest Practices\nTroubleshooting\n\n\n\n\n# Install LitServe\npip install litserve\n\n# For GPU support\npip install litserve[gpu]\n\n# For development dependencies\npip install litserve[dev]\n\n\n\n\n\nimport litserve as ls\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nclass SimpleTextGenerator(ls.LitAPI):\n    def setup(self, device):\n        # Load model and tokenizer during server startup\n        self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n        self.model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n        self.model.to(device)\n        self.model.eval()\n    \n    def decode_request(self, request):\n        # Process incoming request\n        return request[\"prompt\"]\n    \n    def predict(self, x):\n        # Run inference\n        inputs = self.tokenizer.encode(x, return_tensors=\"pt\")\n        with torch.no_grad():\n            outputs = self.model.generate(\n                inputs, \n                max_length=100, \n                num_return_sequences=1,\n                temperature=0.7\n            )\n        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    def encode_response(self, output):\n        # Format response\n        return {\"generated_text\": output}\n\n# Create and start server\nif __name__ == \"__main__\":\n    api = SimpleTextGenerator()\n    server = ls.LitServer(api, accelerator=\"auto\", max_batch_size=4)\n    server.run(port=8000)\n\n\n\nimport requests\n\n# Test the API\nresponse = requests.post(\n    \"http://localhost:8000/predict\",\n    json={\"prompt\": \"The future of AI is\"}\n)\nprint(response.json())\n\n\n\n\n\n\nEvery LitServe API must inherit from ls.LitAPI and implement these core methods:\nclass MyAPI(ls.LitAPI):\n    def setup(self, device):\n        \"\"\"Initialize models, load weights, set up preprocessing\"\"\"\n        pass\n    \n    def decode_request(self, request):\n        \"\"\"Parse and validate incoming requests\"\"\"\n        pass\n    \n    def predict(self, x):\n        \"\"\"Run model inference\"\"\"\n        pass\n    \n    def encode_response(self, output):\n        \"\"\"Format model output for HTTP response\"\"\"\n        pass\n\n\n\nclass AdvancedAPI(ls.LitAPI):\n    def batch(self, inputs):\n        \"\"\"Custom batching logic (optional)\"\"\"\n        return inputs\n    \n    def unbatch(self, output):\n        \"\"\"Custom unbatching logic (optional)\"\"\"\n        return output\n    \n    def preprocess(self, input_data):\n        \"\"\"Additional preprocessing (optional)\"\"\"\n        return input_data\n    \n    def postprocess(self, output):\n        \"\"\"Additional postprocessing (optional)\"\"\"\n        return output\n\n\n\n\n\n\nclass BatchedImageClassifier(ls.LitAPI):\n    def setup(self, device):\n        from torchvision import models, transforms\n        self.model = models.resnet50(pretrained=True)\n        self.model.to(device)\n        self.model.eval()\n        \n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                               std=[0.229, 0.224, 0.225])\n        ])\n    \n    def decode_request(self, request):\n        from PIL import Image\n        import base64\n        import io\n        \n        # Decode base64 image\n        image_data = base64.b64decode(request[\"image\"])\n        image = Image.open(io.BytesIO(image_data))\n        return self.transform(image).unsqueeze(0)\n    \n    def batch(self, inputs):\n        # Custom batching for images\n        return torch.cat(inputs, dim=0)\n    \n    def predict(self, batch):\n        with torch.no_grad():\n            outputs = self.model(batch)\n            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n        return probabilities\n    \n    def unbatch(self, output):\n        # Split batch back to individual predictions\n        return [pred.unsqueeze(0) for pred in output]\n    \n    def encode_response(self, output):\n        # Get top prediction\n        confidence, predicted = torch.max(output, 1)\n        return {\n            \"class_id\": predicted.item(),\n            \"confidence\": confidence.item()\n        }\n\n\n\nclass StreamingChatAPI(ls.LitAPI):\n    def setup(self, device):\n        from transformers import AutoTokenizer, AutoModelForCausalLM\n        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n        self.model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n        self.model.to(device)\n    \n    def decode_request(self, request):\n        return request[\"message\"]\n    \n    def predict(self, x):\n        # Generator for streaming\n        inputs = self.tokenizer.encode(x, return_tensors=\"pt\")\n        \n        for i in range(50):  # Generate up to 50 tokens\n            with torch.no_grad():\n                outputs = self.model(inputs)\n                next_token_logits = outputs.logits[:, -1, :]\n                next_token = torch.multinomial(\n                    torch.softmax(next_token_logits, dim=-1), \n                    num_samples=1\n                )\n                inputs = torch.cat([inputs, next_token], dim=-1)\n                \n                # Yield each token\n                token_text = self.tokenizer.decode(next_token[0])\n                yield {\"token\": token_text}\n                \n                # Stop if end token\n                if next_token.item() == self.tokenizer.eos_token_id:\n                    break\n    \n    def encode_response(self, output):\n        return output\n\n# Enable streaming\nserver = ls.LitServer(api, accelerator=\"auto\", stream=True)\n\n\n\n# Automatic multi-GPU scaling\nserver = ls.LitServer(\n    api, \n    accelerator=\"auto\",\n    devices=\"auto\",  # Use all available GPUs\n    max_batch_size=8\n)\n\n# Specify specific GPUs\nserver = ls.LitServer(\n    api,\n    accelerator=\"gpu\",\n    devices=[0, 1, 2],  # Use GPUs 0, 1, and 2\n    max_batch_size=16\n)\n\n\n\nclass AuthenticatedAPI(ls.LitAPI):\n    def setup(self, device):\n        # Your model setup\n        pass\n    \n    def authenticate(self, request):\n        \"\"\"Custom authentication logic\"\"\"\n        api_key = request.headers.get(\"Authorization\")\n        if not api_key or not self.validate_api_key(api_key):\n            raise ls.AuthenticationError(\"Invalid API key\")\n        return True\n    \n    def validate_api_key(self, api_key):\n        # Your API key validation logic\n        valid_keys = [\"your-secret-key-1\", \"your-secret-key-2\"]\n        return api_key.replace(\"Bearer \", \"\") in valid_keys\n    \n    def decode_request(self, request):\n        return request[\"data\"]\n    \n    def predict(self, x):\n        # Your prediction logic\n        return f\"Processed: {x}\"\n    \n    def encode_response(self, output):\n        return {\"result\": output}\n\n\n\n\n\n\nserver = ls.LitServer(\n    api=api,\n    accelerator=\"auto\",           # \"auto\", \"cpu\", \"gpu\", \"mps\"\n    devices=\"auto\",               # Device selection\n    max_batch_size=4,            # Maximum batch size\n    batch_timeout=0.1,           # Batch timeout in seconds\n    workers_per_device=1,        # Workers per device\n    timeout=30,                  # Request timeout\n    stream=False,                # Enable streaming\n    spec=None,                   # Custom OpenAPI spec\n)\n\n\n\n# Set device preferences\nexport CUDA_VISIBLE_DEVICES=0,1,2\n\n# Set batch configuration\nexport LITSERVE_MAX_BATCH_SIZE=8\nexport LITSERVE_BATCH_TIMEOUT=0.05\n\n# Set worker configuration\nexport LITSERVE_WORKERS_PER_DEVICE=2\n\n\n\n# config.py\nclass Config:\n    MAX_BATCH_SIZE = 8\n    BATCH_TIMEOUT = 0.1\n    WORKERS_PER_DEVICE = 2\n    ACCELERATOR = \"auto\"\n    TIMEOUT = 60\n\n# Use in your API\nfrom config import Config\n\nserver = ls.LitServer(\n    api, \n    max_batch_size=Config.MAX_BATCH_SIZE,\n    batch_timeout=Config.BATCH_TIMEOUT,\n    workers_per_device=Config.WORKERS_PER_DEVICE\n)\n\n\n\n\n\n\nimport litserve as ls\nimport torch\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport base64\nimport io\n\nclass ImageClassificationAPI(ls.LitAPI):\n    def setup(self, device):\n        # Load pre-trained ResNet model\n        self.model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n        self.model.to(device)\n        self.model.eval()\n        \n        # Image preprocessing\n        self.preprocess = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                               std=[0.229, 0.224, 0.225]),\n        ])\n        \n        # Load ImageNet class labels\n        with open('imagenet_classes.txt') as f:\n            self.classes = [line.strip() for line in f.readlines()]\n    \n    def decode_request(self, request):\n        # Decode base64 image\n        encoded_image = request[\"image\"]\n        image_bytes = base64.b64decode(encoded_image)\n        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n        return self.preprocess(image).unsqueeze(0)\n    \n    def predict(self, x):\n        with torch.no_grad():\n            output = self.model(x)\n            probabilities = torch.nn.functional.softmax(output[0], dim=0)\n        return probabilities\n    \n    def encode_response(self, output):\n        # Get top 5 predictions\n        top5_prob, top5_catid = torch.topk(output, 5)\n        results = []\n        for i in range(top5_prob.size(0)):\n            results.append({\n                \"class\": self.classes[top5_catid[i]],\n                \"probability\": float(top5_prob[i])\n            })\n        return {\"predictions\": results}\n\nif __name__ == \"__main__\":\n    api = ImageClassificationAPI()\n    server = ls.LitServer(api, accelerator=\"auto\", max_batch_size=4)\n    server.run(port=8000)\n\n\n\nimport litserve as ls\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\n\nclass TextEmbeddingAPI(ls.LitAPI):\n    def setup(self, device):\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        self.model.to(device)\n    \n    def decode_request(self, request):\n        texts = request.get(\"texts\", [])\n        if isinstance(texts, str):\n            texts = [texts]\n        return texts\n    \n    def predict(self, texts):\n        embeddings = self.model.encode(texts)\n        return embeddings\n    \n    def encode_response(self, embeddings):\n        return {\n            \"embeddings\": embeddings.tolist(),\n            \"dimension\": embeddings.shape[1] if len(embeddings.shape) &gt; 1 else len(embeddings)\n        }\n\nif __name__ == \"__main__\":\n    api = TextEmbeddingAPI()\n    server = ls.LitServer(api, accelerator=\"auto\", max_batch_size=32)\n    server.run(port=8000)\n\n\n\nimport litserve as ls\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\nimport base64\nimport io\n\nclass ImageCaptioningAPI(ls.LitAPI):\n    def setup(self, device):\n        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n        self.model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n        self.model.to(device)\n    \n    def decode_request(self, request):\n        # Handle both image and optional text input\n        encoded_image = request[\"image\"]\n        text_prompt = request.get(\"text\", \"\")\n        \n        image_bytes = base64.b64decode(encoded_image)\n        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n        \n        return {\"image\": image, \"text\": text_prompt}\n    \n    def predict(self, inputs):\n        processed = self.processor(\n            images=inputs[\"image\"], \n            text=inputs[\"text\"], \n            return_tensors=\"pt\"\n        )\n        \n        with torch.no_grad():\n            outputs = self.model.generate(**processed, max_length=50)\n        \n        caption = self.processor.decode(outputs[0], skip_special_tokens=True)\n        return caption\n    \n    def encode_response(self, caption):\n        return {\"caption\": caption}\n\n\n\n\n\n\nclass OptimizedAPI(ls.LitAPI):\n    def setup(self, device):\n        # Use torch.jit.script for optimization\n        self.model = torch.jit.script(your_model)\n        \n        # Enable mixed precision if using GPU\n        if device.type == 'cuda':\n            self.scaler = torch.cuda.amp.GradScaler()\n        \n        # Pre-allocate tensors for common shapes\n        self.common_shapes = {}\n    \n    def predict(self, x):\n        # Use autocast for mixed precision\n        if hasattr(self, 'scaler'):\n            with torch.cuda.amp.autocast():\n                return self.model(x)\n        return self.model(x)\n\n\n\nclass RobustAPI(ls.LitAPI):\n    def decode_request(self, request):\n        try:\n            # Validate required fields\n            if \"input\" not in request:\n                raise ls.ValidationError(\"Missing required field: input\")\n            \n            data = request[\"input\"]\n            \n            # Type validation\n            if not isinstance(data, (str, list)):\n                raise ls.ValidationError(\"Input must be string or list\")\n            \n            return data\n        except Exception as e:\n            raise ls.ValidationError(f\"Request parsing failed: {str(e)}\")\n    \n    def predict(self, x):\n        try:\n            result = self.model(x)\n            return result\n        except torch.cuda.OutOfMemoryError:\n            raise ls.ServerError(\"GPU memory exhausted\")\n        except Exception as e:\n            raise ls.ServerError(f\"Prediction failed: {str(e)}\")\n\n\n\nimport logging\nimport time\n\nclass MonitoredAPI(ls.LitAPI):\n    def setup(self, device):\n        self.logger = logging.getLogger(__name__)\n        self.request_count = 0\n        # Your model setup\n    \n    def decode_request(self, request):\n        self.request_count += 1\n        self.logger.info(f\"Processing request #{self.request_count}\")\n        return request[\"data\"]\n    \n    def predict(self, x):\n        start_time = time.time()\n        result = self.model(x)\n        inference_time = time.time() - start_time\n        \n        self.logger.info(f\"Inference completed in {inference_time:.3f}s\")\n        return result\n\n\n\nclass VersionedAPI(ls.LitAPI):\n    def setup(self, device):\n        self.version = \"1.0.0\"\n        self.model_path = f\"models/model_v{self.version}\"\n        # Load versioned model\n    \n    def encode_response(self, output):\n        return {\n            \"result\": output,\n            \"model_version\": self.version,\n            \"timestamp\": time.time()\n        }\n\n\n\n\n\n\n\n\n# Solution: Reduce batch size or implement gradient checkpointing\nserver = ls.LitServer(api, max_batch_size=2)  # Reduce batch size\n\n# Or clear cache in your predict method\ndef predict(self, x):\n    torch.cuda.empty_cache()  # Clear unused memory\n    result = self.model(x)\n    return result\n\n\n\n# Enable model optimization\ndef setup(self, device):\n    self.model.eval()  # Set to evaluation mode\n    self.model = torch.jit.script(self.model)  # JIT compilation\n    \n    # Use half precision if supported\n    if device.type == 'cuda':\n        self.model.half()\n\n\n\n# Increase timeout settings\nserver = ls.LitServer(\n    api, \n    timeout=60,  # Increase request timeout\n    batch_timeout=1.0  # Increase batch timeout\n)\n\n\n\n# Check and kill existing processes\nimport subprocess\nsubprocess.run([\"lsof\", \"-ti:8000\", \"|\", \"xargs\", \"kill\", \"-9\"], shell=True)\n\n# Or use a different port\nserver.run(port=8001)\n\n\n\n\n\nUse appropriate batch sizes: Start with small batches and gradually increase\nEnable GPU acceleration: Use accelerator=\"auto\" for automatic GPU detection\nOptimize model loading: Load models once in setup(), not in predict()\nUse mixed precision: Enable autocast for GPU inference\nProfile your code: Use tools like torch.profiler to identify bottlenecks\nCache preprocessed data: Store frequently used transformations\n\n\n\n\n\nTest API locally with various input types\nValidate error handling for malformed requests\nCheck memory usage under load\nVerify GPU utilization (if using GPUs)\nTest with maximum expected batch size\nImplement proper logging and monitoring\nSet up health check endpoints\nConfigure appropriate timeouts\nTest authentication (if implemented)\nVerify response format consistency\n\nThis guide covers the essential aspects of using LitServe for deploying AI models. For the most up-to-date information, always refer to the official LitServe documentation."
  },
  {
    "objectID": "posts/litserve-basics/index.html#table-of-contents",
    "href": "posts/litserve-basics/index.html#table-of-contents",
    "title": "LitServe Code Guide",
    "section": "",
    "text": "Installation\nBasic Usage\nCore Concepts\nAdvanced Features\nConfiguration Options\nExamples\nBest Practices\nTroubleshooting"
  },
  {
    "objectID": "posts/litserve-basics/index.html#installation",
    "href": "posts/litserve-basics/index.html#installation",
    "title": "LitServe Code Guide",
    "section": "",
    "text": "# Install LitServe\npip install litserve\n\n# For GPU support\npip install litserve[gpu]\n\n# For development dependencies\npip install litserve[dev]"
  },
  {
    "objectID": "posts/litserve-basics/index.html#basic-usage",
    "href": "posts/litserve-basics/index.html#basic-usage",
    "title": "LitServe Code Guide",
    "section": "",
    "text": "import litserve as ls\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nclass SimpleTextGenerator(ls.LitAPI):\n    def setup(self, device):\n        # Load model and tokenizer during server startup\n        self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n        self.model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n        self.model.to(device)\n        self.model.eval()\n    \n    def decode_request(self, request):\n        # Process incoming request\n        return request[\"prompt\"]\n    \n    def predict(self, x):\n        # Run inference\n        inputs = self.tokenizer.encode(x, return_tensors=\"pt\")\n        with torch.no_grad():\n            outputs = self.model.generate(\n                inputs, \n                max_length=100, \n                num_return_sequences=1,\n                temperature=0.7\n            )\n        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    def encode_response(self, output):\n        # Format response\n        return {\"generated_text\": output}\n\n# Create and start server\nif __name__ == \"__main__\":\n    api = SimpleTextGenerator()\n    server = ls.LitServer(api, accelerator=\"auto\", max_batch_size=4)\n    server.run(port=8000)\n\n\n\nimport requests\n\n# Test the API\nresponse = requests.post(\n    \"http://localhost:8000/predict\",\n    json={\"prompt\": \"The future of AI is\"}\n)\nprint(response.json())"
  },
  {
    "objectID": "posts/litserve-basics/index.html#core-concepts",
    "href": "posts/litserve-basics/index.html#core-concepts",
    "title": "LitServe Code Guide",
    "section": "",
    "text": "Every LitServe API must inherit from ls.LitAPI and implement these core methods:\nclass MyAPI(ls.LitAPI):\n    def setup(self, device):\n        \"\"\"Initialize models, load weights, set up preprocessing\"\"\"\n        pass\n    \n    def decode_request(self, request):\n        \"\"\"Parse and validate incoming requests\"\"\"\n        pass\n    \n    def predict(self, x):\n        \"\"\"Run model inference\"\"\"\n        pass\n    \n    def encode_response(self, output):\n        \"\"\"Format model output for HTTP response\"\"\"\n        pass\n\n\n\nclass AdvancedAPI(ls.LitAPI):\n    def batch(self, inputs):\n        \"\"\"Custom batching logic (optional)\"\"\"\n        return inputs\n    \n    def unbatch(self, output):\n        \"\"\"Custom unbatching logic (optional)\"\"\"\n        return output\n    \n    def preprocess(self, input_data):\n        \"\"\"Additional preprocessing (optional)\"\"\"\n        return input_data\n    \n    def postprocess(self, output):\n        \"\"\"Additional postprocessing (optional)\"\"\"\n        return output"
  },
  {
    "objectID": "posts/litserve-basics/index.html#advanced-features",
    "href": "posts/litserve-basics/index.html#advanced-features",
    "title": "LitServe Code Guide",
    "section": "",
    "text": "class BatchedImageClassifier(ls.LitAPI):\n    def setup(self, device):\n        from torchvision import models, transforms\n        self.model = models.resnet50(pretrained=True)\n        self.model.to(device)\n        self.model.eval()\n        \n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                               std=[0.229, 0.224, 0.225])\n        ])\n    \n    def decode_request(self, request):\n        from PIL import Image\n        import base64\n        import io\n        \n        # Decode base64 image\n        image_data = base64.b64decode(request[\"image\"])\n        image = Image.open(io.BytesIO(image_data))\n        return self.transform(image).unsqueeze(0)\n    \n    def batch(self, inputs):\n        # Custom batching for images\n        return torch.cat(inputs, dim=0)\n    \n    def predict(self, batch):\n        with torch.no_grad():\n            outputs = self.model(batch)\n            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n        return probabilities\n    \n    def unbatch(self, output):\n        # Split batch back to individual predictions\n        return [pred.unsqueeze(0) for pred in output]\n    \n    def encode_response(self, output):\n        # Get top prediction\n        confidence, predicted = torch.max(output, 1)\n        return {\n            \"class_id\": predicted.item(),\n            \"confidence\": confidence.item()\n        }\n\n\n\nclass StreamingChatAPI(ls.LitAPI):\n    def setup(self, device):\n        from transformers import AutoTokenizer, AutoModelForCausalLM\n        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n        self.model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n        self.model.to(device)\n    \n    def decode_request(self, request):\n        return request[\"message\"]\n    \n    def predict(self, x):\n        # Generator for streaming\n        inputs = self.tokenizer.encode(x, return_tensors=\"pt\")\n        \n        for i in range(50):  # Generate up to 50 tokens\n            with torch.no_grad():\n                outputs = self.model(inputs)\n                next_token_logits = outputs.logits[:, -1, :]\n                next_token = torch.multinomial(\n                    torch.softmax(next_token_logits, dim=-1), \n                    num_samples=1\n                )\n                inputs = torch.cat([inputs, next_token], dim=-1)\n                \n                # Yield each token\n                token_text = self.tokenizer.decode(next_token[0])\n                yield {\"token\": token_text}\n                \n                # Stop if end token\n                if next_token.item() == self.tokenizer.eos_token_id:\n                    break\n    \n    def encode_response(self, output):\n        return output\n\n# Enable streaming\nserver = ls.LitServer(api, accelerator=\"auto\", stream=True)\n\n\n\n# Automatic multi-GPU scaling\nserver = ls.LitServer(\n    api, \n    accelerator=\"auto\",\n    devices=\"auto\",  # Use all available GPUs\n    max_batch_size=8\n)\n\n# Specify specific GPUs\nserver = ls.LitServer(\n    api,\n    accelerator=\"gpu\",\n    devices=[0, 1, 2],  # Use GPUs 0, 1, and 2\n    max_batch_size=16\n)\n\n\n\nclass AuthenticatedAPI(ls.LitAPI):\n    def setup(self, device):\n        # Your model setup\n        pass\n    \n    def authenticate(self, request):\n        \"\"\"Custom authentication logic\"\"\"\n        api_key = request.headers.get(\"Authorization\")\n        if not api_key or not self.validate_api_key(api_key):\n            raise ls.AuthenticationError(\"Invalid API key\")\n        return True\n    \n    def validate_api_key(self, api_key):\n        # Your API key validation logic\n        valid_keys = [\"your-secret-key-1\", \"your-secret-key-2\"]\n        return api_key.replace(\"Bearer \", \"\") in valid_keys\n    \n    def decode_request(self, request):\n        return request[\"data\"]\n    \n    def predict(self, x):\n        # Your prediction logic\n        return f\"Processed: {x}\"\n    \n    def encode_response(self, output):\n        return {\"result\": output}"
  },
  {
    "objectID": "posts/litserve-basics/index.html#configuration-options",
    "href": "posts/litserve-basics/index.html#configuration-options",
    "title": "LitServe Code Guide",
    "section": "",
    "text": "server = ls.LitServer(\n    api=api,\n    accelerator=\"auto\",           # \"auto\", \"cpu\", \"gpu\", \"mps\"\n    devices=\"auto\",               # Device selection\n    max_batch_size=4,            # Maximum batch size\n    batch_timeout=0.1,           # Batch timeout in seconds\n    workers_per_device=1,        # Workers per device\n    timeout=30,                  # Request timeout\n    stream=False,                # Enable streaming\n    spec=None,                   # Custom OpenAPI spec\n)\n\n\n\n# Set device preferences\nexport CUDA_VISIBLE_DEVICES=0,1,2\n\n# Set batch configuration\nexport LITSERVE_MAX_BATCH_SIZE=8\nexport LITSERVE_BATCH_TIMEOUT=0.05\n\n# Set worker configuration\nexport LITSERVE_WORKERS_PER_DEVICE=2\n\n\n\n# config.py\nclass Config:\n    MAX_BATCH_SIZE = 8\n    BATCH_TIMEOUT = 0.1\n    WORKERS_PER_DEVICE = 2\n    ACCELERATOR = \"auto\"\n    TIMEOUT = 60\n\n# Use in your API\nfrom config import Config\n\nserver = ls.LitServer(\n    api, \n    max_batch_size=Config.MAX_BATCH_SIZE,\n    batch_timeout=Config.BATCH_TIMEOUT,\n    workers_per_device=Config.WORKERS_PER_DEVICE\n)"
  },
  {
    "objectID": "posts/litserve-basics/index.html#examples",
    "href": "posts/litserve-basics/index.html#examples",
    "title": "LitServe Code Guide",
    "section": "",
    "text": "import litserve as ls\nimport torch\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport base64\nimport io\n\nclass ImageClassificationAPI(ls.LitAPI):\n    def setup(self, device):\n        # Load pre-trained ResNet model\n        self.model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n        self.model.to(device)\n        self.model.eval()\n        \n        # Image preprocessing\n        self.preprocess = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                               std=[0.229, 0.224, 0.225]),\n        ])\n        \n        # Load ImageNet class labels\n        with open('imagenet_classes.txt') as f:\n            self.classes = [line.strip() for line in f.readlines()]\n    \n    def decode_request(self, request):\n        # Decode base64 image\n        encoded_image = request[\"image\"]\n        image_bytes = base64.b64decode(encoded_image)\n        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n        return self.preprocess(image).unsqueeze(0)\n    \n    def predict(self, x):\n        with torch.no_grad():\n            output = self.model(x)\n            probabilities = torch.nn.functional.softmax(output[0], dim=0)\n        return probabilities\n    \n    def encode_response(self, output):\n        # Get top 5 predictions\n        top5_prob, top5_catid = torch.topk(output, 5)\n        results = []\n        for i in range(top5_prob.size(0)):\n            results.append({\n                \"class\": self.classes[top5_catid[i]],\n                \"probability\": float(top5_prob[i])\n            })\n        return {\"predictions\": results}\n\nif __name__ == \"__main__\":\n    api = ImageClassificationAPI()\n    server = ls.LitServer(api, accelerator=\"auto\", max_batch_size=4)\n    server.run(port=8000)\n\n\n\nimport litserve as ls\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\n\nclass TextEmbeddingAPI(ls.LitAPI):\n    def setup(self, device):\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        self.model.to(device)\n    \n    def decode_request(self, request):\n        texts = request.get(\"texts\", [])\n        if isinstance(texts, str):\n            texts = [texts]\n        return texts\n    \n    def predict(self, texts):\n        embeddings = self.model.encode(texts)\n        return embeddings\n    \n    def encode_response(self, embeddings):\n        return {\n            \"embeddings\": embeddings.tolist(),\n            \"dimension\": embeddings.shape[1] if len(embeddings.shape) &gt; 1 else len(embeddings)\n        }\n\nif __name__ == \"__main__\":\n    api = TextEmbeddingAPI()\n    server = ls.LitServer(api, accelerator=\"auto\", max_batch_size=32)\n    server.run(port=8000)\n\n\n\nimport litserve as ls\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\nimport base64\nimport io\n\nclass ImageCaptioningAPI(ls.LitAPI):\n    def setup(self, device):\n        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n        self.model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n        self.model.to(device)\n    \n    def decode_request(self, request):\n        # Handle both image and optional text input\n        encoded_image = request[\"image\"]\n        text_prompt = request.get(\"text\", \"\")\n        \n        image_bytes = base64.b64decode(encoded_image)\n        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n        \n        return {\"image\": image, \"text\": text_prompt}\n    \n    def predict(self, inputs):\n        processed = self.processor(\n            images=inputs[\"image\"], \n            text=inputs[\"text\"], \n            return_tensors=\"pt\"\n        )\n        \n        with torch.no_grad():\n            outputs = self.model.generate(**processed, max_length=50)\n        \n        caption = self.processor.decode(outputs[0], skip_special_tokens=True)\n        return caption\n    \n    def encode_response(self, caption):\n        return {\"caption\": caption}"
  },
  {
    "objectID": "posts/litserve-basics/index.html#best-practices",
    "href": "posts/litserve-basics/index.html#best-practices",
    "title": "LitServe Code Guide",
    "section": "",
    "text": "class OptimizedAPI(ls.LitAPI):\n    def setup(self, device):\n        # Use torch.jit.script for optimization\n        self.model = torch.jit.script(your_model)\n        \n        # Enable mixed precision if using GPU\n        if device.type == 'cuda':\n            self.scaler = torch.cuda.amp.GradScaler()\n        \n        # Pre-allocate tensors for common shapes\n        self.common_shapes = {}\n    \n    def predict(self, x):\n        # Use autocast for mixed precision\n        if hasattr(self, 'scaler'):\n            with torch.cuda.amp.autocast():\n                return self.model(x)\n        return self.model(x)\n\n\n\nclass RobustAPI(ls.LitAPI):\n    def decode_request(self, request):\n        try:\n            # Validate required fields\n            if \"input\" not in request:\n                raise ls.ValidationError(\"Missing required field: input\")\n            \n            data = request[\"input\"]\n            \n            # Type validation\n            if not isinstance(data, (str, list)):\n                raise ls.ValidationError(\"Input must be string or list\")\n            \n            return data\n        except Exception as e:\n            raise ls.ValidationError(f\"Request parsing failed: {str(e)}\")\n    \n    def predict(self, x):\n        try:\n            result = self.model(x)\n            return result\n        except torch.cuda.OutOfMemoryError:\n            raise ls.ServerError(\"GPU memory exhausted\")\n        except Exception as e:\n            raise ls.ServerError(f\"Prediction failed: {str(e)}\")\n\n\n\nimport logging\nimport time\n\nclass MonitoredAPI(ls.LitAPI):\n    def setup(self, device):\n        self.logger = logging.getLogger(__name__)\n        self.request_count = 0\n        # Your model setup\n    \n    def decode_request(self, request):\n        self.request_count += 1\n        self.logger.info(f\"Processing request #{self.request_count}\")\n        return request[\"data\"]\n    \n    def predict(self, x):\n        start_time = time.time()\n        result = self.model(x)\n        inference_time = time.time() - start_time\n        \n        self.logger.info(f\"Inference completed in {inference_time:.3f}s\")\n        return result\n\n\n\nclass VersionedAPI(ls.LitAPI):\n    def setup(self, device):\n        self.version = \"1.0.0\"\n        self.model_path = f\"models/model_v{self.version}\"\n        # Load versioned model\n    \n    def encode_response(self, output):\n        return {\n            \"result\": output,\n            \"model_version\": self.version,\n            \"timestamp\": time.time()\n        }"
  },
  {
    "objectID": "posts/litserve-basics/index.html#troubleshooting",
    "href": "posts/litserve-basics/index.html#troubleshooting",
    "title": "LitServe Code Guide",
    "section": "",
    "text": "# Solution: Reduce batch size or implement gradient checkpointing\nserver = ls.LitServer(api, max_batch_size=2)  # Reduce batch size\n\n# Or clear cache in your predict method\ndef predict(self, x):\n    torch.cuda.empty_cache()  # Clear unused memory\n    result = self.model(x)\n    return result\n\n\n\n# Enable model optimization\ndef setup(self, device):\n    self.model.eval()  # Set to evaluation mode\n    self.model = torch.jit.script(self.model)  # JIT compilation\n    \n    # Use half precision if supported\n    if device.type == 'cuda':\n        self.model.half()\n\n\n\n# Increase timeout settings\nserver = ls.LitServer(\n    api, \n    timeout=60,  # Increase request timeout\n    batch_timeout=1.0  # Increase batch timeout\n)\n\n\n\n# Check and kill existing processes\nimport subprocess\nsubprocess.run([\"lsof\", \"-ti:8000\", \"|\", \"xargs\", \"kill\", \"-9\"], shell=True)\n\n# Or use a different port\nserver.run(port=8001)\n\n\n\n\n\nUse appropriate batch sizes: Start with small batches and gradually increase\nEnable GPU acceleration: Use accelerator=\"auto\" for automatic GPU detection\nOptimize model loading: Load models once in setup(), not in predict()\nUse mixed precision: Enable autocast for GPU inference\nProfile your code: Use tools like torch.profiler to identify bottlenecks\nCache preprocessed data: Store frequently used transformations\n\n\n\n\n\nTest API locally with various input types\nValidate error handling for malformed requests\nCheck memory usage under load\nVerify GPU utilization (if using GPUs)\nTest with maximum expected batch size\nImplement proper logging and monitoring\nSet up health check endpoints\nConfigure appropriate timeouts\nTest authentication (if implemented)\nVerify response format consistency\n\nThis guide covers the essential aspects of using LitServe for deploying AI models. For the most up-to-date information, always refer to the official LitServe documentation."
  },
  {
    "objectID": "posts/python-pi-code/index.html",
    "href": "posts/python-pi-code/index.html",
    "title": "Python 3.14: Key Improvements and New Features",
    "section": "",
    "text": "Python 3.14 introduces several significant improvements focused on performance, developer experience, and language capabilities. This guide covers the most important changes that will impact your code and development workflow.\n\n\n\n\nPython 3.14 continues the experimental free-threading support introduced in 3.13, with significant improvements:\n# Enable free-threading with --disable-gil flag\n# Better performance for CPU-bound multi-threaded applications\nimport threading\nimport time\n\ndef cpu_intensive_task(n):\n    total = 0\n    for i in range(n):\n        total += i ** 2\n    return total\n\n# Now benefits more from true parallelism\nthreads = []\nfor i in range(4):\n    t = threading.Thread(target=cpu_intensive_task, args=(1000000,))\n    threads.append(t)\n    t.start()\n\nfor t in threads:\n    t.join()\n\n\n\nEnhanced Just-In-Time compilation for better runtime performance:\n# Functions with hot loops see significant speedups\ndef fibonacci(n):\n    if n &lt;= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n\n# JIT compiler optimizes recursive calls more effectively\nresult = fibonacci(35)  # Noticeably faster than previous versions\n\n\n\n\n\n\nEnhanced support for generic types and better inference:\nfrom typing import Generic, TypeVar\n\nT = TypeVar('T')\n\nclass Stack(Generic[T]):\n    def __init__(self) -&gt; None:\n        self._items: list[T] = []\n    \n    def push(self, item: T) -&gt; None:\n        self._items.append(item)\n    \n    def pop(self) -&gt; T:\n        if not self._items:\n            raise IndexError(\"Stack is empty\")\n        return self._items.pop()\n\n# Better type inference\nstack = Stack[int]()\nstack.push(42)\nvalue = stack.pop()  # Type checker knows this is int\n\n\n\nImproved pattern matching with new syntax options:\ndef process_data(data):\n    match data:\n        case {\"type\": \"user\", \"id\": int(user_id), \"active\": True}:\n            return f\"Active user: {user_id}\"\n        case {\"type\": \"user\", \"id\": int(user_id), \"active\": False}:\n            return f\"Inactive user: {user_id}\"\n        case {\"type\": \"admin\", \"permissions\": list(perms)} if len(perms) &gt; 0:\n            return f\"Admin with {len(perms)} permissions\"\n        case _:\n            return \"Unknown data format\"\n\n# Enhanced guard conditions and destructuring\nresult = process_data({\"type\": \"user\", \"id\": 123, \"active\": True})\n\n\n\nAdditional string manipulation methods for common operations:\ntext = \"Hello, World! How are you today?\"\n\n# New methods for better string handling\nwords = text.split_keep_separator(\" \")  # Keeps separators\n# Result: [\"Hello,\", \" \", \"World!\", \" \", \"How\", \" \", \"are\", \" \", \"you\", \" \", \"today?\"]\n\n# Improved case conversion\ntitle_text = \"hello-world_example\"\nresult = title_text.to_title_case()  # \"Hello World Example\"\n\n# Better whitespace handling\nmessy_text = \"  \\t\\n  Hello  World  \\t\\n  \"\nclean_text = messy_text.normalize_whitespace()  # \"Hello World\"\n\n\n\n\n\n\nBetter support for handling multiple exceptions:\nimport asyncio\n\nasync def fetch_data(url):\n    if \"invalid\" in url:\n        raise ValueError(f\"Invalid URL: {url}\")\n    if \"timeout\" in url:\n        raise TimeoutError(f\"Timeout for: {url}\")\n    return f\"Data from {url}\"\n\nasync def fetch_multiple():\n    urls = [\"http://valid.com\", \"http://invalid.com\", \"http://timeout.com\"]\n    \n    try:\n        results = await asyncio.gather(\n            *[fetch_data(url) for url in urls],\n            return_exceptions=True\n        )\n    except* ValueError as eg:\n        print(f\"Value errors: {len(eg.exceptions)}\")\n        for exc in eg.exceptions:\n            print(f\"  - {exc}\")\n    except* TimeoutError as eg:\n        print(f\"Timeout errors: {len(eg.exceptions)}\")\n        for exc in eg.exceptions:\n            print(f\"  - {exc}\")\n\n\n\nMore detailed and helpful error messages:\ndef process_nested_data(data):\n    return data[\"users\"][0][\"profile\"][\"email\"].upper()\n\n# Better error messages show the exact path that failed\ntry:\n    result = process_nested_data({\"users\": []})\nexcept (KeyError, IndexError) as e:\n    # Error message now includes: \"Failed accessing: data['users'][0]\"\n    print(f\"Access error: {e}\")\n\n\n\n\n\n\nNew methods for better file system operations:\nfrom pathlib import Path\n\npath = Path(\"./my_project\")\n\n# New methods for common operations\nif path.is_empty_dir():\n    print(\"Directory is empty\")\n\n# Better glob patterns\npython_files = path.rglob(\"*.py\", follow_symlinks=True)\n\n# Atomic operations\nconfig_file = path / \"config.json\"\nconfig_file.write_text_atomic('{\"version\": \"1.0\"}')  # Atomic write operation\n\n\n\nBetter async/await support and performance:\nimport asyncio\n\n# New context manager for better resource cleanup\nasync def main():\n    async with asyncio.TaskGroup() as tg:\n        task1 = tg.create_task(fetch_data(\"url1\"))\n        task2 = tg.create_task(fetch_data(\"url2\"))\n        task3 = tg.create_task(fetch_data(\"url3\"))\n    \n    # All tasks complete or all cancelled if one fails\n    print(\"All tasks completed successfully\")\n\n# Improved timeout handling\nasync def with_timeout():\n    try:\n        async with asyncio.timeout(5.0):\n            result = await slow_operation()\n            return result\n    except asyncio.TimeoutError:\n        print(\"Operation timed out\")\n        return None\n\n\n\nAdditional utilities for working with iterators:\nimport itertools\n\n# New batching function\ndata = range(15)\nbatches = list(itertools.batched(data, 4))\n# Result: [(0, 1, 2, 3), (4, 5, 6, 7), (8, 9, 10, 11), (12, 13, 14)]\n\n# Improved pairwise iteration\npoints = [(0, 0), (1, 1), (2, 4), (3, 9)]\nsegments = list(itertools.pairwise(points))\n# Result: [((0, 0), (1, 1)), ((1, 1), (2, 4)), ((2, 4), (3, 9))]\n\n\n\n\n\n\nEnhanced interactive Python shell:\n# Improved auto-completion and syntax highlighting\n# Better error recovery - continue working after syntax errors\n# Enhanced help system with examples\n\n# New REPL commands\n# %time &lt;expression&gt;  - Time execution\n# %edit &lt;function&gt;    - Edit function in external editor\n# %history           - Show command history\n\n\n\nBetter debugging capabilities:\nimport pdb\n\ndef complex_function(data):\n    # New breakpoint() enhancements\n    breakpoint()  # Now supports conditional breakpoints\n    \n    processed = []\n    for item in data:\n        # Enhanced step-through debugging\n        result = item * 2\n        processed.append(result)\n    \n    return processed\n\n# Better integration with IDE debuggers\n# Improved variable inspection\n# Enhanced stack trace navigation\n\n\n\n\n\n\nFeatures removed or deprecated in 3.14:\n# Deprecated: Old-style string formatting (still works but discouraged)\n# old_way = \"Hello %s\" % name\n# Better: Use f-strings or .format()\nnew_way = f\"Hello {name}\"\n\n# Removed: Some legacy asyncio APIs\n# Use modern async/await syntax consistently\n\n# Deprecated: Certain distutils modules\n# Use setuptools or build system alternatives\n\n\n\nImportant changes that might affect existing code:\n# Stricter type checking in some standard library functions\n# May need to update type annotations\n\n# Changed behavior in some edge cases for consistency\n# Review code that relies on previous undefined behavior\n\n# Updated default parameters for some functions\n# Check documentation for functions you use extensively\n\n\n\n\nTypical performance improvements you can expect:\n\nGeneral Python code: 10-15% faster execution\nMulti-threaded applications: Up to 40% improvement with free-threading\nString operations: 20-25% faster for common operations\nImport time: 15-20% faster module loading\nMemory usage: 5-10% reduction in typical applications\n\n\n\n\n\n\n# Install Python 3.14\npython3.14 -m pip install --upgrade pip\n\n# Check version\npython3.14 --version\n\n# Enable experimental features\npython3.14 --disable-gil script.py  # For free-threading\n\n\n\n\nTest your existing code with Python 3.14\nUpdate type annotations to use new features\nReview deprecated warnings in your codebase\nConsider enabling free-threading for CPU-bound applications\nUpdate development tools and IDE configurations\nBenchmark performance improvements in your applications\n\n\n\n\n\n\n\n# Use enhanced pattern matching for complex data processing\ndef process_api_response(response):\n    match response:\n        case {\"status\": \"success\", \"data\": list(items)} if len(items) &gt; 0:\n            return [process_item(item) for item in items]\n        case {\"status\": \"error\", \"message\": str(msg)}:\n            raise APIError(msg)\n        case _:\n            raise ValueError(\"Unexpected response format\")\n\n# Leverage improved async features\nasync def robust_async_operation():\n    async with asyncio.TaskGroup() as tg:\n        tasks = [tg.create_task(operation(i)) for i in range(5)]\n    return [task.result() for task in tasks]\n\n# Use new string methods for cleaner code\ndef clean_user_input(text):\n    return text.normalize_whitespace().strip()\nPython 3.14 represents a significant step forward in Python’s evolution, focusing on performance, developer experience, and language consistency. The improvements make Python more efficient and enjoyable to work with while maintaining the language’s commitment to readability and simplicity."
  },
  {
    "objectID": "posts/python-pi-code/index.html#performance-improvements",
    "href": "posts/python-pi-code/index.html#performance-improvements",
    "title": "Python 3.14: Key Improvements and New Features",
    "section": "",
    "text": "Python 3.14 continues the experimental free-threading support introduced in 3.13, with significant improvements:\n# Enable free-threading with --disable-gil flag\n# Better performance for CPU-bound multi-threaded applications\nimport threading\nimport time\n\ndef cpu_intensive_task(n):\n    total = 0\n    for i in range(n):\n        total += i ** 2\n    return total\n\n# Now benefits more from true parallelism\nthreads = []\nfor i in range(4):\n    t = threading.Thread(target=cpu_intensive_task, args=(1000000,))\n    threads.append(t)\n    t.start()\n\nfor t in threads:\n    t.join()\n\n\n\nEnhanced Just-In-Time compilation for better runtime performance:\n# Functions with hot loops see significant speedups\ndef fibonacci(n):\n    if n &lt;= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n\n# JIT compiler optimizes recursive calls more effectively\nresult = fibonacci(35)  # Noticeably faster than previous versions"
  },
  {
    "objectID": "posts/python-pi-code/index.html#language-features",
    "href": "posts/python-pi-code/index.html#language-features",
    "title": "Python 3.14: Key Improvements and New Features",
    "section": "",
    "text": "Enhanced support for generic types and better inference:\nfrom typing import Generic, TypeVar\n\nT = TypeVar('T')\n\nclass Stack(Generic[T]):\n    def __init__(self) -&gt; None:\n        self._items: list[T] = []\n    \n    def push(self, item: T) -&gt; None:\n        self._items.append(item)\n    \n    def pop(self) -&gt; T:\n        if not self._items:\n            raise IndexError(\"Stack is empty\")\n        return self._items.pop()\n\n# Better type inference\nstack = Stack[int]()\nstack.push(42)\nvalue = stack.pop()  # Type checker knows this is int\n\n\n\nImproved pattern matching with new syntax options:\ndef process_data(data):\n    match data:\n        case {\"type\": \"user\", \"id\": int(user_id), \"active\": True}:\n            return f\"Active user: {user_id}\"\n        case {\"type\": \"user\", \"id\": int(user_id), \"active\": False}:\n            return f\"Inactive user: {user_id}\"\n        case {\"type\": \"admin\", \"permissions\": list(perms)} if len(perms) &gt; 0:\n            return f\"Admin with {len(perms)} permissions\"\n        case _:\n            return \"Unknown data format\"\n\n# Enhanced guard conditions and destructuring\nresult = process_data({\"type\": \"user\", \"id\": 123, \"active\": True})\n\n\n\nAdditional string manipulation methods for common operations:\ntext = \"Hello, World! How are you today?\"\n\n# New methods for better string handling\nwords = text.split_keep_separator(\" \")  # Keeps separators\n# Result: [\"Hello,\", \" \", \"World!\", \" \", \"How\", \" \", \"are\", \" \", \"you\", \" \", \"today?\"]\n\n# Improved case conversion\ntitle_text = \"hello-world_example\"\nresult = title_text.to_title_case()  # \"Hello World Example\"\n\n# Better whitespace handling\nmessy_text = \"  \\t\\n  Hello  World  \\t\\n  \"\nclean_text = messy_text.normalize_whitespace()  # \"Hello World\""
  },
  {
    "objectID": "posts/python-pi-code/index.html#error-handling-improvements",
    "href": "posts/python-pi-code/index.html#error-handling-improvements",
    "title": "Python 3.14: Key Improvements and New Features",
    "section": "",
    "text": "Better support for handling multiple exceptions:\nimport asyncio\n\nasync def fetch_data(url):\n    if \"invalid\" in url:\n        raise ValueError(f\"Invalid URL: {url}\")\n    if \"timeout\" in url:\n        raise TimeoutError(f\"Timeout for: {url}\")\n    return f\"Data from {url}\"\n\nasync def fetch_multiple():\n    urls = [\"http://valid.com\", \"http://invalid.com\", \"http://timeout.com\"]\n    \n    try:\n        results = await asyncio.gather(\n            *[fetch_data(url) for url in urls],\n            return_exceptions=True\n        )\n    except* ValueError as eg:\n        print(f\"Value errors: {len(eg.exceptions)}\")\n        for exc in eg.exceptions:\n            print(f\"  - {exc}\")\n    except* TimeoutError as eg:\n        print(f\"Timeout errors: {len(eg.exceptions)}\")\n        for exc in eg.exceptions:\n            print(f\"  - {exc}\")\n\n\n\nMore detailed and helpful error messages:\ndef process_nested_data(data):\n    return data[\"users\"][0][\"profile\"][\"email\"].upper()\n\n# Better error messages show the exact path that failed\ntry:\n    result = process_nested_data({\"users\": []})\nexcept (KeyError, IndexError) as e:\n    # Error message now includes: \"Failed accessing: data['users'][0]\"\n    print(f\"Access error: {e}\")"
  },
  {
    "objectID": "posts/python-pi-code/index.html#standard-library-updates",
    "href": "posts/python-pi-code/index.html#standard-library-updates",
    "title": "Python 3.14: Key Improvements and New Features",
    "section": "",
    "text": "New methods for better file system operations:\nfrom pathlib import Path\n\npath = Path(\"./my_project\")\n\n# New methods for common operations\nif path.is_empty_dir():\n    print(\"Directory is empty\")\n\n# Better glob patterns\npython_files = path.rglob(\"*.py\", follow_symlinks=True)\n\n# Atomic operations\nconfig_file = path / \"config.json\"\nconfig_file.write_text_atomic('{\"version\": \"1.0\"}')  # Atomic write operation\n\n\n\nBetter async/await support and performance:\nimport asyncio\n\n# New context manager for better resource cleanup\nasync def main():\n    async with asyncio.TaskGroup() as tg:\n        task1 = tg.create_task(fetch_data(\"url1\"))\n        task2 = tg.create_task(fetch_data(\"url2\"))\n        task3 = tg.create_task(fetch_data(\"url3\"))\n    \n    # All tasks complete or all cancelled if one fails\n    print(\"All tasks completed successfully\")\n\n# Improved timeout handling\nasync def with_timeout():\n    try:\n        async with asyncio.timeout(5.0):\n            result = await slow_operation()\n            return result\n    except asyncio.TimeoutError:\n        print(\"Operation timed out\")\n        return None\n\n\n\nAdditional utilities for working with iterators:\nimport itertools\n\n# New batching function\ndata = range(15)\nbatches = list(itertools.batched(data, 4))\n# Result: [(0, 1, 2, 3), (4, 5, 6, 7), (8, 9, 10, 11), (12, 13, 14)]\n\n# Improved pairwise iteration\npoints = [(0, 0), (1, 1), (2, 4), (3, 9)]\nsegments = list(itertools.pairwise(points))\n# Result: [((0, 0), (1, 1)), ((1, 1), (2, 4)), ((2, 4), (3, 9))]"
  },
  {
    "objectID": "posts/python-pi-code/index.html#development-tools",
    "href": "posts/python-pi-code/index.html#development-tools",
    "title": "Python 3.14: Key Improvements and New Features",
    "section": "",
    "text": "Enhanced interactive Python shell:\n# Improved auto-completion and syntax highlighting\n# Better error recovery - continue working after syntax errors\n# Enhanced help system with examples\n\n# New REPL commands\n# %time &lt;expression&gt;  - Time execution\n# %edit &lt;function&gt;    - Edit function in external editor\n# %history           - Show command history\n\n\n\nBetter debugging capabilities:\nimport pdb\n\ndef complex_function(data):\n    # New breakpoint() enhancements\n    breakpoint()  # Now supports conditional breakpoints\n    \n    processed = []\n    for item in data:\n        # Enhanced step-through debugging\n        result = item * 2\n        processed.append(result)\n    \n    return processed\n\n# Better integration with IDE debuggers\n# Improved variable inspection\n# Enhanced stack trace navigation"
  },
  {
    "objectID": "posts/python-pi-code/index.html#migration-considerations",
    "href": "posts/python-pi-code/index.html#migration-considerations",
    "title": "Python 3.14: Key Improvements and New Features",
    "section": "",
    "text": "Features removed or deprecated in 3.14:\n# Deprecated: Old-style string formatting (still works but discouraged)\n# old_way = \"Hello %s\" % name\n# Better: Use f-strings or .format()\nnew_way = f\"Hello {name}\"\n\n# Removed: Some legacy asyncio APIs\n# Use modern async/await syntax consistently\n\n# Deprecated: Certain distutils modules\n# Use setuptools or build system alternatives\n\n\n\nImportant changes that might affect existing code:\n# Stricter type checking in some standard library functions\n# May need to update type annotations\n\n# Changed behavior in some edge cases for consistency\n# Review code that relies on previous undefined behavior\n\n# Updated default parameters for some functions\n# Check documentation for functions you use extensively"
  },
  {
    "objectID": "posts/python-pi-code/index.html#performance-benchmarks",
    "href": "posts/python-pi-code/index.html#performance-benchmarks",
    "title": "Python 3.14: Key Improvements and New Features",
    "section": "",
    "text": "Typical performance improvements you can expect:\n\nGeneral Python code: 10-15% faster execution\nMulti-threaded applications: Up to 40% improvement with free-threading\nString operations: 20-25% faster for common operations\nImport time: 15-20% faster module loading\nMemory usage: 5-10% reduction in typical applications"
  },
  {
    "objectID": "posts/python-pi-code/index.html#getting-started",
    "href": "posts/python-pi-code/index.html#getting-started",
    "title": "Python 3.14: Key Improvements and New Features",
    "section": "",
    "text": "# Install Python 3.14\npython3.14 -m pip install --upgrade pip\n\n# Check version\npython3.14 --version\n\n# Enable experimental features\npython3.14 --disable-gil script.py  # For free-threading\n\n\n\n\nTest your existing code with Python 3.14\nUpdate type annotations to use new features\nReview deprecated warnings in your codebase\nConsider enabling free-threading for CPU-bound applications\nUpdate development tools and IDE configurations\nBenchmark performance improvements in your applications"
  },
  {
    "objectID": "posts/python-pi-code/index.html#best-practices",
    "href": "posts/python-pi-code/index.html#best-practices",
    "title": "Python 3.14: Key Improvements and New Features",
    "section": "",
    "text": "# Use enhanced pattern matching for complex data processing\ndef process_api_response(response):\n    match response:\n        case {\"status\": \"success\", \"data\": list(items)} if len(items) &gt; 0:\n            return [process_item(item) for item in items]\n        case {\"status\": \"error\", \"message\": str(msg)}:\n            raise APIError(msg)\n        case _:\n            raise ValueError(\"Unexpected response format\")\n\n# Leverage improved async features\nasync def robust_async_operation():\n    async with asyncio.TaskGroup() as tg:\n        tasks = [tg.create_task(operation(i)) for i in range(5)]\n    return [task.result() for task in tasks]\n\n# Use new string methods for cleaner code\ndef clean_user_input(text):\n    return text.normalize_whitespace().strip()\nPython 3.14 represents a significant step forward in Python’s evolution, focusing on performance, developer experience, and language consistency. The improvements make Python more efficient and enjoyable to work with while maintaining the language’s commitment to readability and simplicity."
  },
  {
    "objectID": "posts/pytorch-lightning/index.html",
    "href": "posts/pytorch-lightning/index.html",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "PyTorch Lightning is a lightweight wrapper for PyTorch that helps organize code and reduce boilerplate while adding powerful features for research and production. This guide will walk you through the basics to advanced techniques.\n\n\n\nIntroduction to PyTorch Lightning\nInstallation\nBasic Structure: The LightningModule\nDataModules\nTraining with Trainer\nCallbacks\nLogging\nDistributed Training\nHyperparameter Tuning\nModel Checkpointing\nProduction Deployment\nBest Practices\n\n\n\n\nPyTorch Lightning separates research code from engineering code, making models more:\n\nReproducible: The same code works across different hardware\nReadable: Standard project structure makes collaboration easier\nScalable: Train on CPUs, GPUs, TPUs or clusters with no code changes\n\nLightning helps you focus on the science by handling the engineering details.\n\n\n\npip install pytorch-lightning\nFor the latest features, you can install from the source:\npip install git+https://github.com/Lightning-AI/lightning.git\n\n\n\nThe core component in Lightning is the LightningModule, which organizes your PyTorch code into a standardized structure:\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MNISTModel(pl.LightningModule):\n    def __init__(self, lr=0.001):\n        super().__init__()\n        self.save_hyperparameters()  # Saves learning_rate to hparams\n        self.layer_1 = nn.Linear(28 * 28, 128)\n        self.layer_2 = nn.Linear(128, 10)\n        self.lr = lr\n        \n    def forward(self, x):\n        batch_size, channels, width, height = x.size()\n        x = x.view(batch_size, -1)\n        x = self.layer_1(x)\n        x = F.relu(x)\n        x = self.layer_2(x)\n        return x\n        \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        self.log('train_loss', loss)\n        return loss\n        \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        self.log('val_loss', loss)\n        self.log('val_acc', acc)\n        \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        self.log('test_loss', loss)\n        self.log('test_acc', acc)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer\nThis basic structure includes:\n\nModel architecture (__init__ and forward)\nTraining logic (training_step)\nValidation logic (validation_step)\nTest logic (test_step)\nOptimization setup (configure_optimizers)\n\n\n\n\nLightning’s LightningDataModule encapsulates all data-related logic:\nfrom pytorch_lightning import LightningDataModule\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\n\nclass MNISTDataModule(LightningDataModule):\n    def __init__(self, data_dir='./data', batch_size=32):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n        \n    def prepare_data(self):\n        # Download data if needed\n        MNIST(self.data_dir, train=True, download=True)\n        MNIST(self.data_dir, train=False, download=True)\n        \n    def setup(self, stage=None):\n        # Assign train/val/test datasets\n        if stage == 'fit' or stage is None:\n            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n            \n        if stage == 'test' or stage is None:\n            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n            \n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=self.batch_size)\n        \n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n        \n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=self.batch_size)\nBenefits of LightningDataModule:\n\nEncapsulates all data preparation logic\nMakes data pipeline portable and reproducible\nSimplifies sharing data pipelines between projects\n\n\n\n\nThe Lightning Trainer handles the training loop and validation:\nfrom pytorch_lightning import Trainer\n\n# Create model and data module\nmodel = MNISTModel()\ndata_module = MNISTDataModule()\n\n# Initialize trainer\ntrainer = Trainer(\n    max_epochs=10,\n    accelerator=\"auto\",  # Automatically use available GPU\n    devices=\"auto\",\n    logger=True,         # Use TensorBoard logger by default\n)\n\n# Train model\ntrainer.fit(model, datamodule=data_module)\n\n# Test model\ntrainer.test(model, datamodule=data_module)\nThe Trainer automatically handles:\n\nEpoch and batch iteration\nOptimizer steps\nLogging metrics\nHardware acceleration (CPU, GPU, TPU)\nEarly stopping\nCheckpointing\nMulti-GPU training\n\n\n\ntrainer = Trainer(\n    max_epochs=10,                    # Maximum number of epochs\n    min_epochs=1,                     # Minimum number of epochs\n    accelerator=\"cpu\",                # Use CPU acceleration\n    devices=1,                        # Use 1 CPUs\n    precision=\"16-mixed\",             # Use mixed precision for faster training\n    gradient_clip_val=0.5,            # Clip gradients\n    accumulate_grad_batches=4,        # Accumulate gradients over 4 batches\n    log_every_n_steps=50,             # Log metrics every 50 steps\n    val_check_interval=0.25,          # Run validation 4 times per epoch\n    fast_dev_run=False,               # Debug mode (only run a few batches)\n    deterministic=True,               # Make training deterministic\n)\n\n\n\n\nCallbacks add functionality to the training loop without modifying the core code:\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n\n# Save the best model based on validation accuracy\ncheckpoint_callback = ModelCheckpoint(\n    monitor='val_acc',\n    dirpath='./checkpoints',\n    filename='mnist-{epoch:02d}-{val_acc:.2f}',\n    save_top_k=3,\n    mode='max',\n)\n\n# Stop training when validation loss stops improving\nearly_stop_callback = EarlyStopping(\n    monitor='val_loss',\n    patience=3,\n    verbose=True,\n    mode='min'\n)\n\n# Monitor learning rate\nlr_monitor = LearningRateMonitor(logging_interval='step')\n\n# Initialize trainer with callbacks\ntrainer = Trainer(\n    max_epochs=10,\n    callbacks=[checkpoint_callback, early_stop_callback, lr_monitor]\n)\n\n\nYou can create custom callbacks by extending the base Callback class:\nfrom pytorch_lightning.callbacks import Callback\n\nclass PrintingCallback(Callback):\n    def on_train_start(self, trainer, pl_module):\n        print(\"Training has started!\")\n        \n    def on_train_end(self, trainer, pl_module):\n        print(\"Training has finished!\")\n        \n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        if batch_idx % 100 == 0:\n            print(f\"Batch {batch_idx} completed\")\n\n\n\n\nLightning supports various loggers:\nfrom pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n\n# TensorBoard logger\ntensorboard_logger = TensorBoardLogger(save_dir=\"logs/\", name=\"mnist\")\n\n# Initialize trainer with loggers\ntrainer = Trainer(\n    max_epochs=10,\n    logger=[tensorboard_logger]\n)\nAdding metrics in your model:\ndef training_step(self, batch, batch_idx):\n    x, y = batch\n    logits = self(x)\n    loss = F.cross_entropy(logits, y)\n    \n    # Log scalar values\n    self.log('train_loss', loss)\n    \n    # Log learning rate\n    lr = self.optimizers().param_groups[0]['lr']\n    self.log('learning_rate', lr)\n    \n    # Log histograms (on GPU)\n    if batch_idx % 100 == 0:\n        self.logger.experiment.add_histogram('logits', logits, self.global_step)\n    \n    return loss\n\n\n\nLightning makes distributed training simple:\n# Single GPU\ntrainer = Trainer(accelerator=\"gpu\", devices=1)\n\n# Multiple GPUs (DDP strategy automatically selected)\ntrainer = Trainer(accelerator=\"gpu\", devices=4)\n\n# Specific GPU indices\ntrainer = Trainer(accelerator=\"gpu\", devices=[0, 1, 3])\n\n# TPU cores\ntrainer = Trainer(accelerator=\"tpu\", devices=8)\n\n# Advanced distributed training\ntrainer = Trainer(\n    accelerator=\"gpu\",\n    devices=4,\n    strategy=\"ddp\",  # Distributed Data Parallel\n    num_nodes=2      # Use 2 machines\n)\nOther distribution strategies:\n\nddp_spawn: Similar to DDP but uses spawn for multiprocessing\ndeepspeed: For very large models using DeepSpeed\nfsdp: Fully Sharded Data Parallel for huge models\n\n\n\n\nLightning integrates well with optimization libraries:\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.tuner import Tuner\n\n# Create model\nmodel = MNISTModel()\ndata_module = MNISTDataModule()\n\n# Create trainer\ntrainer = Trainer(max_epochs=10)\n\n# Use Lightning's Tuner for auto-scaling batch size\ntuner = Tuner(trainer)\ntuner.scale_batch_size(model, datamodule=data_module)\n\n# Find optimal learning rate\nlr_finder = tuner.lr_find(model, datamodule=data_module)\nmodel.learning_rate = lr_finder.suggestion()\n\n# Train with optimized parameters\ntrainer.fit(model, datamodule=data_module)\n\n\n\nSaving and loading model checkpoints:\n# Save checkpoints during training\ncheckpoint_callback = ModelCheckpoint(\n    dirpath='./checkpoints',\n    filename='{epoch}-{val_loss:.2f}',\n    monitor='val_loss',\n    save_top_k=3,\n    mode='min'\n)\n\ntrainer = Trainer(\n    max_epochs=10,\n    callbacks=[checkpoint_callback]\n)\n\ntrainer.fit(model, datamodule=data_module)\n\n# Path to best checkpoint\nbest_model_path = checkpoint_callback.best_model_path\n\n# Load checkpoint into model\nmodel = MNISTModel.load_from_checkpoint(best_model_path)\n\n# Continue training from checkpoint\ntrainer.fit(model, datamodule=data_module)\nCheckpointing for production:\n# Save model in production-ready format\ntrainer.save_checkpoint(\"model.ckpt\")\n\n# Extract state dict for production\ncheckpoint = torch.load(\"model.ckpt\")\nmodel_state_dict = checkpoint[\"state_dict\"]\n\n# Save just the PyTorch model for production\nmodel = MNISTModel()\nmodel.load_state_dict(model_state_dict)\ntorch.save(model.state_dict(), \"production_model.pt\")\n\n\n\nConverting Lightning models to production:\n# Option 1: Use the Lightning model directly\nmodel = MNISTModel.load_from_checkpoint(\"model.ckpt\")\nmodel.eval()\nmodel.freeze()  # Freeze the model parameters\n\n# Make predictions\nwith torch.no_grad():\n    x = torch.randn(1, 1, 28, 28)\n    y_hat = model(x)\n\n# Option 2: Extract the PyTorch model\nclass ProductionModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(28 * 28, 128)\n        self.layer_2 = nn.Linear(128, 10)\n    \n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.layer_1(x))\n        x = self.layer_2(x)\n        return x\n\n# Load weights from Lightning model\nlightning_model = MNISTModel.load_from_checkpoint(\"model.ckpt\")\nproduction_model = ProductionModel()\n\n# Copy weights\nproduction_model.layer_1.weight.data = lightning_model.layer_1.weight.data\nproduction_model.layer_1.bias.data = lightning_model.layer_1.bias.data\nproduction_model.layer_2.weight.data = lightning_model.layer_2.weight.data\nproduction_model.layer_2.bias.data = lightning_model.layer_2.bias.data\n\n# Save production model\ntorch.save(production_model.state_dict(), \"production_model.pt\")\n\n# Export to ONNX\ndummy_input = torch.randn(1, 1, 28, 28)\ntorch.onnx.export(\n    production_model,\n    dummy_input,\n    \"model.onnx\",\n    export_params=True,\n    opset_version=11,\n    input_names=['input'],\n    output_names=['output']\n)\n\n\n\n\n\nA well-organized Lightning project structure:\nproject/\n├── configs/              # Configuration files\n├── data/                 # Data files\n├── lightning_logs/       # Generated logs\n├── models/               # Model definitions\n│   ├── __init__.py\n│   └── mnist_model.py    # LightningModule\n├── data_modules/         # Data modules\n│   ├── __init__.py\n│   └── mnist_data.py     # LightningDataModule\n├── callbacks/            # Custom callbacks\n├── utils/                # Utility functions\n├── main.py               # Training script\n└── README.md\n\n\n\nLightning provides a CLI for running experiments from config files:\n# main.py\nfrom pytorch_lightning.cli import LightningCLI\n\ndef cli_main():\n    # Create CLI with LightningModule and LightningDataModule\n    cli = LightningCLI(\n        MNISTModel,\n        MNISTDataModule,\n        save_config_callback=None,\n    )\n\nif __name__ == \"__main__\":\n    cli_main()\nRun with:\npython main.py fit --config configs/mnist.yaml\nExample config file (mnist.yaml):\nmodel:\n  learning_rate: 0.001\n  hidden_size: 128\ndata:\n  data_dir: ./data\n  batch_size: 64\ntrainer:\n  max_epochs: 10\n  accelerator: gpu\n  devices: 1\n\n\n\nLightning includes tools to profile your code:\nfrom pytorch_lightning.profilers import PyTorchProfiler, SimpleProfiler\n\n# PyTorch Profiler\nprofiler = PyTorchProfiler(\n    on_trace_ready=torch.profiler.tensorboard_trace_handler(\"logs/profiler\"),\n    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2)\n)\n\n# Simple Profiler\n# profiler = SimpleProfiler()\nmodel = MNISTModel()\ndata_module = MNISTDataModule()\ntrainer = Trainer(\n    max_epochs=5,\n    profiler=profiler\n)\n\ntrainer.fit(model, datamodule=data_module)\n\n\n\n\n\ntrainer = Trainer(\n    accumulate_grad_batches=4  # Accumulate over 4 batches\n)\n\n\n\ndef configure_optimizers(self):\n    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, \n        mode='min', \n        factor=0.1, \n        patience=5\n    )\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\n            \"scheduler\": scheduler,\n            \"monitor\": \"val_loss\",\n            \"interval\": \"epoch\",\n            \"frequency\": 1\n        }\n    }\n\n\n\ntrainer = Trainer(\n    precision=\"16-mixed\"  # Use 16-bit mixed precision\n)\n\n\n\nclass TransferLearningModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        # Load pretrained model\n        self.feature_extractor = torchvision.models.resnet18(pretrained=True)\n        \n        # Freeze feature extractor\n        for param in self.feature_extractor.parameters():\n            param.requires_grad = False\n            \n        # Replace final layer\n        num_features = self.feature_extractor.fc.in_features\n        self.feature_extractor.fc = nn.Linear(num_features, 10)\n        \n    def unfreeze_features(self):\n        # Unfreeze model after some training\n        for param in self.feature_extractor.parameters():\n            param.requires_grad = True\n\n\n\n\n\nPyTorch Lightning provides an organized framework that separates research code from engineering boilerplate, making deep learning projects easier to develop, share, and scale. It’s especially valuable for research projects that need to be reproducible and scalable across different hardware configurations.\nFor further information:\n\nPyTorch Lightning Documentation\nLightning GitHub Repository\nLightning Tutorials"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#table-of-contents",
    "href": "posts/pytorch-lightning/index.html#table-of-contents",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Introduction to PyTorch Lightning\nInstallation\nBasic Structure: The LightningModule\nDataModules\nTraining with Trainer\nCallbacks\nLogging\nDistributed Training\nHyperparameter Tuning\nModel Checkpointing\nProduction Deployment\nBest Practices"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#introduction-to-pytorch-lightning",
    "href": "posts/pytorch-lightning/index.html#introduction-to-pytorch-lightning",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "PyTorch Lightning separates research code from engineering code, making models more:\n\nReproducible: The same code works across different hardware\nReadable: Standard project structure makes collaboration easier\nScalable: Train on CPUs, GPUs, TPUs or clusters with no code changes\n\nLightning helps you focus on the science by handling the engineering details."
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#installation",
    "href": "posts/pytorch-lightning/index.html#installation",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "pip install pytorch-lightning\nFor the latest features, you can install from the source:\npip install git+https://github.com/Lightning-AI/lightning.git"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#basic-structure-the-lightningmodule",
    "href": "posts/pytorch-lightning/index.html#basic-structure-the-lightningmodule",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "The core component in Lightning is the LightningModule, which organizes your PyTorch code into a standardized structure:\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MNISTModel(pl.LightningModule):\n    def __init__(self, lr=0.001):\n        super().__init__()\n        self.save_hyperparameters()  # Saves learning_rate to hparams\n        self.layer_1 = nn.Linear(28 * 28, 128)\n        self.layer_2 = nn.Linear(128, 10)\n        self.lr = lr\n        \n    def forward(self, x):\n        batch_size, channels, width, height = x.size()\n        x = x.view(batch_size, -1)\n        x = self.layer_1(x)\n        x = F.relu(x)\n        x = self.layer_2(x)\n        return x\n        \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        self.log('train_loss', loss)\n        return loss\n        \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        self.log('val_loss', loss)\n        self.log('val_acc', acc)\n        \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        self.log('test_loss', loss)\n        self.log('test_acc', acc)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer\nThis basic structure includes:\n\nModel architecture (__init__ and forward)\nTraining logic (training_step)\nValidation logic (validation_step)\nTest logic (test_step)\nOptimization setup (configure_optimizers)"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#datamodules",
    "href": "posts/pytorch-lightning/index.html#datamodules",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Lightning’s LightningDataModule encapsulates all data-related logic:\nfrom pytorch_lightning import LightningDataModule\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\n\nclass MNISTDataModule(LightningDataModule):\n    def __init__(self, data_dir='./data', batch_size=32):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n        \n    def prepare_data(self):\n        # Download data if needed\n        MNIST(self.data_dir, train=True, download=True)\n        MNIST(self.data_dir, train=False, download=True)\n        \n    def setup(self, stage=None):\n        # Assign train/val/test datasets\n        if stage == 'fit' or stage is None:\n            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n            \n        if stage == 'test' or stage is None:\n            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n            \n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=self.batch_size)\n        \n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n        \n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=self.batch_size)\nBenefits of LightningDataModule:\n\nEncapsulates all data preparation logic\nMakes data pipeline portable and reproducible\nSimplifies sharing data pipelines between projects"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#training-with-trainer",
    "href": "posts/pytorch-lightning/index.html#training-with-trainer",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "The Lightning Trainer handles the training loop and validation:\nfrom pytorch_lightning import Trainer\n\n# Create model and data module\nmodel = MNISTModel()\ndata_module = MNISTDataModule()\n\n# Initialize trainer\ntrainer = Trainer(\n    max_epochs=10,\n    accelerator=\"auto\",  # Automatically use available GPU\n    devices=\"auto\",\n    logger=True,         # Use TensorBoard logger by default\n)\n\n# Train model\ntrainer.fit(model, datamodule=data_module)\n\n# Test model\ntrainer.test(model, datamodule=data_module)\nThe Trainer automatically handles:\n\nEpoch and batch iteration\nOptimizer steps\nLogging metrics\nHardware acceleration (CPU, GPU, TPU)\nEarly stopping\nCheckpointing\nMulti-GPU training\n\n\n\ntrainer = Trainer(\n    max_epochs=10,                    # Maximum number of epochs\n    min_epochs=1,                     # Minimum number of epochs\n    accelerator=\"cpu\",                # Use CPU acceleration\n    devices=1,                        # Use 1 CPUs\n    precision=\"16-mixed\",             # Use mixed precision for faster training\n    gradient_clip_val=0.5,            # Clip gradients\n    accumulate_grad_batches=4,        # Accumulate gradients over 4 batches\n    log_every_n_steps=50,             # Log metrics every 50 steps\n    val_check_interval=0.25,          # Run validation 4 times per epoch\n    fast_dev_run=False,               # Debug mode (only run a few batches)\n    deterministic=True,               # Make training deterministic\n)"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#callbacks",
    "href": "posts/pytorch-lightning/index.html#callbacks",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Callbacks add functionality to the training loop without modifying the core code:\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n\n# Save the best model based on validation accuracy\ncheckpoint_callback = ModelCheckpoint(\n    monitor='val_acc',\n    dirpath='./checkpoints',\n    filename='mnist-{epoch:02d}-{val_acc:.2f}',\n    save_top_k=3,\n    mode='max',\n)\n\n# Stop training when validation loss stops improving\nearly_stop_callback = EarlyStopping(\n    monitor='val_loss',\n    patience=3,\n    verbose=True,\n    mode='min'\n)\n\n# Monitor learning rate\nlr_monitor = LearningRateMonitor(logging_interval='step')\n\n# Initialize trainer with callbacks\ntrainer = Trainer(\n    max_epochs=10,\n    callbacks=[checkpoint_callback, early_stop_callback, lr_monitor]\n)\n\n\nYou can create custom callbacks by extending the base Callback class:\nfrom pytorch_lightning.callbacks import Callback\n\nclass PrintingCallback(Callback):\n    def on_train_start(self, trainer, pl_module):\n        print(\"Training has started!\")\n        \n    def on_train_end(self, trainer, pl_module):\n        print(\"Training has finished!\")\n        \n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        if batch_idx % 100 == 0:\n            print(f\"Batch {batch_idx} completed\")"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#logging",
    "href": "posts/pytorch-lightning/index.html#logging",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Lightning supports various loggers:\nfrom pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n\n# TensorBoard logger\ntensorboard_logger = TensorBoardLogger(save_dir=\"logs/\", name=\"mnist\")\n\n# Initialize trainer with loggers\ntrainer = Trainer(\n    max_epochs=10,\n    logger=[tensorboard_logger]\n)\nAdding metrics in your model:\ndef training_step(self, batch, batch_idx):\n    x, y = batch\n    logits = self(x)\n    loss = F.cross_entropy(logits, y)\n    \n    # Log scalar values\n    self.log('train_loss', loss)\n    \n    # Log learning rate\n    lr = self.optimizers().param_groups[0]['lr']\n    self.log('learning_rate', lr)\n    \n    # Log histograms (on GPU)\n    if batch_idx % 100 == 0:\n        self.logger.experiment.add_histogram('logits', logits, self.global_step)\n    \n    return loss"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#distributed-training",
    "href": "posts/pytorch-lightning/index.html#distributed-training",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Lightning makes distributed training simple:\n# Single GPU\ntrainer = Trainer(accelerator=\"gpu\", devices=1)\n\n# Multiple GPUs (DDP strategy automatically selected)\ntrainer = Trainer(accelerator=\"gpu\", devices=4)\n\n# Specific GPU indices\ntrainer = Trainer(accelerator=\"gpu\", devices=[0, 1, 3])\n\n# TPU cores\ntrainer = Trainer(accelerator=\"tpu\", devices=8)\n\n# Advanced distributed training\ntrainer = Trainer(\n    accelerator=\"gpu\",\n    devices=4,\n    strategy=\"ddp\",  # Distributed Data Parallel\n    num_nodes=2      # Use 2 machines\n)\nOther distribution strategies:\n\nddp_spawn: Similar to DDP but uses spawn for multiprocessing\ndeepspeed: For very large models using DeepSpeed\nfsdp: Fully Sharded Data Parallel for huge models"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#hyperparameter-tuning",
    "href": "posts/pytorch-lightning/index.html#hyperparameter-tuning",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Lightning integrates well with optimization libraries:\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.tuner import Tuner\n\n# Create model\nmodel = MNISTModel()\ndata_module = MNISTDataModule()\n\n# Create trainer\ntrainer = Trainer(max_epochs=10)\n\n# Use Lightning's Tuner for auto-scaling batch size\ntuner = Tuner(trainer)\ntuner.scale_batch_size(model, datamodule=data_module)\n\n# Find optimal learning rate\nlr_finder = tuner.lr_find(model, datamodule=data_module)\nmodel.learning_rate = lr_finder.suggestion()\n\n# Train with optimized parameters\ntrainer.fit(model, datamodule=data_module)"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#model-checkpointing",
    "href": "posts/pytorch-lightning/index.html#model-checkpointing",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Saving and loading model checkpoints:\n# Save checkpoints during training\ncheckpoint_callback = ModelCheckpoint(\n    dirpath='./checkpoints',\n    filename='{epoch}-{val_loss:.2f}',\n    monitor='val_loss',\n    save_top_k=3,\n    mode='min'\n)\n\ntrainer = Trainer(\n    max_epochs=10,\n    callbacks=[checkpoint_callback]\n)\n\ntrainer.fit(model, datamodule=data_module)\n\n# Path to best checkpoint\nbest_model_path = checkpoint_callback.best_model_path\n\n# Load checkpoint into model\nmodel = MNISTModel.load_from_checkpoint(best_model_path)\n\n# Continue training from checkpoint\ntrainer.fit(model, datamodule=data_module)\nCheckpointing for production:\n# Save model in production-ready format\ntrainer.save_checkpoint(\"model.ckpt\")\n\n# Extract state dict for production\ncheckpoint = torch.load(\"model.ckpt\")\nmodel_state_dict = checkpoint[\"state_dict\"]\n\n# Save just the PyTorch model for production\nmodel = MNISTModel()\nmodel.load_state_dict(model_state_dict)\ntorch.save(model.state_dict(), \"production_model.pt\")"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#production-deployment",
    "href": "posts/pytorch-lightning/index.html#production-deployment",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Converting Lightning models to production:\n# Option 1: Use the Lightning model directly\nmodel = MNISTModel.load_from_checkpoint(\"model.ckpt\")\nmodel.eval()\nmodel.freeze()  # Freeze the model parameters\n\n# Make predictions\nwith torch.no_grad():\n    x = torch.randn(1, 1, 28, 28)\n    y_hat = model(x)\n\n# Option 2: Extract the PyTorch model\nclass ProductionModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(28 * 28, 128)\n        self.layer_2 = nn.Linear(128, 10)\n    \n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.layer_1(x))\n        x = self.layer_2(x)\n        return x\n\n# Load weights from Lightning model\nlightning_model = MNISTModel.load_from_checkpoint(\"model.ckpt\")\nproduction_model = ProductionModel()\n\n# Copy weights\nproduction_model.layer_1.weight.data = lightning_model.layer_1.weight.data\nproduction_model.layer_1.bias.data = lightning_model.layer_1.bias.data\nproduction_model.layer_2.weight.data = lightning_model.layer_2.weight.data\nproduction_model.layer_2.bias.data = lightning_model.layer_2.bias.data\n\n# Save production model\ntorch.save(production_model.state_dict(), \"production_model.pt\")\n\n# Export to ONNX\ndummy_input = torch.randn(1, 1, 28, 28)\ntorch.onnx.export(\n    production_model,\n    dummy_input,\n    \"model.onnx\",\n    export_params=True,\n    opset_version=11,\n    input_names=['input'],\n    output_names=['output']\n)"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#best-practices",
    "href": "posts/pytorch-lightning/index.html#best-practices",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "A well-organized Lightning project structure:\nproject/\n├── configs/              # Configuration files\n├── data/                 # Data files\n├── lightning_logs/       # Generated logs\n├── models/               # Model definitions\n│   ├── __init__.py\n│   └── mnist_model.py    # LightningModule\n├── data_modules/         # Data modules\n│   ├── __init__.py\n│   └── mnist_data.py     # LightningDataModule\n├── callbacks/            # Custom callbacks\n├── utils/                # Utility functions\n├── main.py               # Training script\n└── README.md\n\n\n\nLightning provides a CLI for running experiments from config files:\n# main.py\nfrom pytorch_lightning.cli import LightningCLI\n\ndef cli_main():\n    # Create CLI with LightningModule and LightningDataModule\n    cli = LightningCLI(\n        MNISTModel,\n        MNISTDataModule,\n        save_config_callback=None,\n    )\n\nif __name__ == \"__main__\":\n    cli_main()\nRun with:\npython main.py fit --config configs/mnist.yaml\nExample config file (mnist.yaml):\nmodel:\n  learning_rate: 0.001\n  hidden_size: 128\ndata:\n  data_dir: ./data\n  batch_size: 64\ntrainer:\n  max_epochs: 10\n  accelerator: gpu\n  devices: 1\n\n\n\nLightning includes tools to profile your code:\nfrom pytorch_lightning.profilers import PyTorchProfiler, SimpleProfiler\n\n# PyTorch Profiler\nprofiler = PyTorchProfiler(\n    on_trace_ready=torch.profiler.tensorboard_trace_handler(\"logs/profiler\"),\n    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2)\n)\n\n# Simple Profiler\n# profiler = SimpleProfiler()\nmodel = MNISTModel()\ndata_module = MNISTDataModule()\ntrainer = Trainer(\n    max_epochs=5,\n    profiler=profiler\n)\n\ntrainer.fit(model, datamodule=data_module)\n\n\n\n\n\ntrainer = Trainer(\n    accumulate_grad_batches=4  # Accumulate over 4 batches\n)\n\n\n\ndef configure_optimizers(self):\n    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, \n        mode='min', \n        factor=0.1, \n        patience=5\n    )\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\n            \"scheduler\": scheduler,\n            \"monitor\": \"val_loss\",\n            \"interval\": \"epoch\",\n            \"frequency\": 1\n        }\n    }\n\n\n\ntrainer = Trainer(\n    precision=\"16-mixed\"  # Use 16-bit mixed precision\n)\n\n\n\nclass TransferLearningModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        # Load pretrained model\n        self.feature_extractor = torchvision.models.resnet18(pretrained=True)\n        \n        # Freeze feature extractor\n        for param in self.feature_extractor.parameters():\n            param.requires_grad = False\n            \n        # Replace final layer\n        num_features = self.feature_extractor.fc.in_features\n        self.feature_extractor.fc = nn.Linear(num_features, 10)\n        \n    def unfreeze_features(self):\n        # Unfreeze model after some training\n        for param in self.feature_extractor.parameters():\n            param.requires_grad = True"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#conclusion",
    "href": "posts/pytorch-lightning/index.html#conclusion",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "PyTorch Lightning provides an organized framework that separates research code from engineering boilerplate, making deep learning projects easier to develop, share, and scale. It’s especially valuable for research projects that need to be reproducible and scalable across different hardware configurations.\nFor further information:\n\nPyTorch Lightning Documentation\nLightning GitHub Repository\nLightning Tutorials"
  },
  {
    "objectID": "posts/litserve-mobilenet/index.html",
    "href": "posts/litserve-mobilenet/index.html",
    "title": "LitServe with MobileNetV2 - Complete Code Guide",
    "section": "",
    "text": "This guide demonstrates how to deploy a MobileNetV2 image classification model using LitServe for efficient, scalable inference.\n\n\n\nInstallation\nBasic Implementation\nAdvanced Features\nPerformance Optimization\nDeployment\nTesting\n\n\n\n\n# Install required packages\npip install litserve torch torchvision pillow requests\n\n\n\n\n\n# mobilenet_api.py\nimport io\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom PIL import Image\nimport litserve as ls\n\n\nclass MobileNetV2API(ls.LitAPI):\n    def setup(self, device):\n        \"\"\"Initialize the model and preprocessing pipeline\"\"\"\n        # Load pre-trained MobileNetV2\n        self.model = models.mobilenet_v2(pretrained=True)\n        self.model.eval()\n        self.model.to(device)\n        \n        # Define image preprocessing\n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n        \n        # ImageNet class labels (first 10 for brevity)\n        self.class_labels = [\n            \"tench\", \"goldfish\", \"great white shark\", \"tiger shark\",\n            \"hammerhead\", \"electric ray\", \"stingray\", \"cock\", \n            \"hen\", \"ostrich\"\n            # ... add all 1000 ImageNet classes\n        ]\n\n    def decode_request(self, request):\n        \"\"\"Parse incoming request and prepare image\"\"\"\n        if isinstance(request, dict) and \"image\" in request:\n            # Handle base64 encoded image\n            import base64\n            image_data = base64.b64decode(request[\"image\"])\n            image = Image.open(io.BytesIO(image_data)).convert('RGB')\n        else:\n            # Handle direct image upload\n            image = Image.open(io.BytesIO(request)).convert('RGB')\n        \n        return image\n\n    def predict(self, image):\n        \"\"\"Run inference on the preprocessed image\"\"\"\n        # Preprocess image\n        input_tensor = self.transform(image).unsqueeze(0)\n        input_tensor = input_tensor.to(next(self.model.parameters()).device)\n        \n        # Run inference\n        with torch.no_grad():\n            outputs = self.model(input_tensor)\n            probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n        \n        return probabilities\n\n    def encode_response(self, probabilities):\n        \"\"\"Format the response\"\"\"\n        # Get top 5 predictions\n        top5_prob, top5_indices = torch.topk(probabilities, 5)\n        \n        results = []\n        for i in range(5):\n            idx = top5_indices[i].item()\n            prob = top5_prob[i].item()\n            label = self.class_labels[idx] if idx &lt; len(self.class_labels) else f\"class_{idx}\"\n            results.append({\n                \"class\": label,\n                \"confidence\": round(prob, 4),\n                \"class_id\": idx\n            })\n        \n        return {\n            \"predictions\": results,\n            \"model\": \"mobilenet_v2\"\n        }\n\n\nif __name__ == \"__main__\":\n    # Create and run the API\n    api = MobileNetV2API()\n    server = ls.LitServer(api, accelerator=\"auto\", max_batch_size=4)\n    server.run(port=8000)\n\n\n\n# client.py\nimport requests\nimport base64\nfrom PIL import Image\nimport io\n\n\ndef encode_image(image_path):\n    \"\"\"Encode image to base64\"\"\"\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode('utf-8')\n\n\ndef test_image_classification(image_path, server_url=\"http://localhost:8000/predict\"):\n    \"\"\"Test the MobileNetV2 API\"\"\"\n    # Method 1: Send as base64 in JSON\n    encoded_image = encode_image(image_path)\n    response = requests.post(\n        server_url,\n        json={\"image\": encoded_image}\n    )\n    \n    if response.status_code == 200:\n        result = response.json()\n        print(\"Top 5 Predictions:\")\n        for pred in result[\"predictions\"]:\n            print(f\"  {pred['class']}: {pred['confidence']:.4f}\")\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n\n\ndef test_direct_upload(image_path, server_url=\"http://localhost:8000/predict\"):\n    \"\"\"Test with direct image upload\"\"\"\n    with open(image_path, 'rb') as f:\n        files = {'file': f}\n        response = requests.post(server_url, files=files)\n    \n    if response.status_code == 200:\n        result = response.json()\n        print(\"Predictions:\", result)\n\n\nif __name__ == \"__main__\":\n    # Test with a sample image\n    test_image_classification(\"sample_image.jpg\")\n\n\n\n\n\n\n# advanced_mobilenet_api.py\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom PIL import Image\nimport litserve as ls\nimport json\nfrom typing import List, Any\n\n\nclass BatchedMobileNetV2API(ls.LitAPI):\n    def setup(self, device):\n        \"\"\"Initialize model with batch processing capabilities\"\"\"\n        self.model = models.mobilenet_v2(pretrained=True)\n        self.model.eval()\n        self.model.to(device)\n        self.device = device\n        \n        # Optimized transform for batch processing\n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n        \n        # Load class labels from file or define them\n        self.load_imagenet_labels()\n\n    def load_imagenet_labels(self):\n        \"\"\"Load ImageNet class labels\"\"\"\n        # You can download from: https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n        try:\n            with open('imagenet_classes.txt', 'r') as f:\n                self.class_labels = [line.strip() for line in f.readlines()]\n        except FileNotFoundError:\n            # Fallback to first few classes\n            self.class_labels = [f\"class_{i}\" for i in range(1000)]\n\n    def decode_request(self, request):\n        \"\"\"Handle both single images and batch requests\"\"\"\n        if isinstance(request, dict):\n            if \"images\" in request:  # Batch request\n                images = []\n                for img_data in request[\"images\"]:\n                    if isinstance(img_data, str):  # base64\n                        import base64\n                        image_data = base64.b64decode(img_data)\n                        image = Image.open(io.BytesIO(image_data)).convert('RGB')\n                    else:  # direct bytes\n                        image = Image.open(io.BytesIO(img_data)).convert('RGB')\n                    images.append(image)\n                return images\n            elif \"image\" in request:  # Single request\n                import base64\n                image_data = base64.b64decode(request[\"image\"])\n                image = Image.open(io.BytesIO(image_data)).convert('RGB')\n                return [image]  # Wrap in list for consistent handling\n        \n        # Direct upload\n        image = Image.open(io.BytesIO(request)).convert('RGB')\n        return [image]\n\n    def batch(self, inputs: List[Any]) -&gt; List[Any]:\n        \"\"\"Custom batching logic\"\"\"\n        # Flatten all images from all requests\n        all_images = []\n        batch_sizes = []\n        \n        for inp in inputs:\n            if isinstance(inp, list):\n                all_images.extend(inp)\n                batch_sizes.append(len(inp))\n            else:\n                all_images.append(inp)\n                batch_sizes.append(1)\n        \n        return all_images, batch_sizes\n\n    def predict(self, batch_data):\n        \"\"\"Process batch of images efficiently\"\"\"\n        images, batch_sizes = batch_data\n        \n        # Preprocess all images\n        batch_tensor = torch.stack([\n            self.transform(img) for img in images\n        ]).to(self.device)\n        \n        # Run batch inference\n        with torch.no_grad():\n            outputs = self.model(batch_tensor)\n            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n        \n        return probabilities, batch_sizes\n\n    def unbatch(self, output):\n        \"\"\"Split batch results back to individual responses\"\"\"\n        probabilities, batch_sizes = output\n        results = []\n        start_idx = 0\n        \n        for batch_size in batch_sizes:\n            batch_probs = probabilities[start_idx:start_idx + batch_size]\n            results.append(batch_probs)\n            start_idx += batch_size\n        \n        return results\n\n    def encode_response(self, probabilities):\n        \"\"\"Format response for batch or single predictions\"\"\"\n        if len(probabilities.shape) == 1:  # Single prediction\n            probabilities = probabilities.unsqueeze(0)\n        \n        all_results = []\n        for prob_vector in probabilities:\n            top5_prob, top5_indices = torch.topk(prob_vector, 5)\n            \n            predictions = []\n            for i in range(5):\n                idx = top5_indices[i].item()\n                prob = top5_prob[i].item()\n                predictions.append({\n                    \"class\": self.class_labels[idx],\n                    \"confidence\": round(prob, 4),\n                    \"class_id\": idx\n                })\n            \n            all_results.append(predictions)\n        \n        return {\n            \"predictions\": all_results[0] if len(all_results) == 1 else all_results,\n            \"model\": \"mobilenet_v2\",\n            \"batch_size\": len(all_results)\n        }\n\n\nif __name__ == \"__main__\":\n    api = BatchedMobileNetV2API()\n    server = ls.LitServer(\n        api,\n        accelerator=\"auto\",\n        max_batch_size=8,\n        batch_timeout=0.1,  # 100ms timeout for batching\n    )\n    server.run(port=8000, num_workers=2)\n\n\n\n# quantized_mobilenet_api.py\nimport torch\nimport torch.quantization\nfrom torchvision import models\nimport litserve as ls\n\n\nclass QuantizedMobileNetV2API(ls.LitAPI):\n    def setup(self, device):\n        \"\"\"Setup quantized MobileNetV2 for faster inference\"\"\"\n        # Load pre-trained model\n        self.model = models.mobilenet_v2(pretrained=True)\n        self.model.eval()\n        \n        # Apply dynamic quantization for CPU inference\n        if device == \"cpu\":\n            self.model = torch.quantization.quantize_dynamic(\n                self.model,\n                {torch.nn.Linear, torch.nn.Conv2d},\n                dtype=torch.qint8\n            )\n            print(\"Applied dynamic quantization for CPU\")\n        else:\n            self.model.to(device)\n            print(f\"Using device: {device}\")\n        \n        # Rest of setup code...\n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n\n    # ... rest of the methods remain the same\n\n\n\n\n\n\n# production_config.py\nimport litserve as ls\nfrom advanced_mobilenet_api import BatchedMobileNetV2API\n\ndef create_production_server():\n    \"\"\"Create optimized server for production\"\"\"\n    api = BatchedMobileNetV2API()\n    \n    server = ls.LitServer(\n        api,\n        accelerator=\"auto\",  # Auto-detect GPU/CPU\n        max_batch_size=16,   # Larger batches for throughput\n        batch_timeout=0.05,  # 50ms batching timeout\n        workers_per_device=2,  # Multiple workers per GPU\n        timeout=30,          # Request timeout\n    )\n    \n    return server\n\nif __name__ == \"__main__\":\n    server = create_production_server()\n    server.run(\n        port=8000,\n        host=\"0.0.0.0\",  # Accept external connections\n        num_workers=4     # Total number of workers\n    )\n\n\n\n# monitored_api.py\nimport time\nimport logging\nfrom collections import defaultdict\nimport litserve as ls\n\n\nclass MonitoredMobileNetV2API(BatchedMobileNetV2API):\n    def setup(self, device):\n        \"\"\"Setup with monitoring\"\"\"\n        super().setup(device)\n        \n        # Setup logging\n        logging.basicConfig(level=logging.INFO)\n        self.logger = logging.getLogger(__name__)\n        \n        # Metrics tracking\n        self.metrics = defaultdict(list)\n        self.request_count = 0\n\n    def predict(self, batch_data):\n        \"\"\"Predict with timing and metrics\"\"\"\n        start_time = time.time()\n        \n        result = super().predict(batch_data)\n        \n        # Log timing\n        inference_time = time.time() - start_time\n        batch_size = len(batch_data[0])\n        \n        self.metrics['inference_times'].append(inference_time)\n        self.metrics['batch_sizes'].append(batch_size)\n        self.request_count += 1\n        \n        self.logger.info(\n            f\"Processed batch of {batch_size} images in {inference_time:.3f}s \"\n            f\"(Total requests: {self.request_count})\"\n        )\n        \n        return result\n\n    def encode_response(self, probabilities):\n        \"\"\"Add metrics to response\"\"\"\n        response = super().encode_response(probabilities)\n        \n        # Add performance metrics every 100 requests\n        if self.request_count % 100 == 0:\n            avg_time = sum(self.metrics['inference_times'][-100:]) / min(100, len(self.metrics['inference_times']))\n            response['metrics'] = {\n                'avg_inference_time': round(avg_time, 4),\n                'total_requests': self.request_count\n            }\n        \n        return response\n\n\n\n\n\n\n# Dockerfile\nFROM python:3.9-slim\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    wget \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Download ImageNet labels\nRUN wget -O imagenet_classes.txt https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n\nEXPOSE 8000\n\nCMD [\"python\", \"production_config.py\"]\n# docker-compose.yml\nversion: '3.8'\n\nservices:\n  mobilenet-api:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - CUDA_VISIBLE_DEVICES=0  # Set GPU if available\n    volumes:\n      - ./models:/app/models  # Optional: for custom models\n    restart: unless-stopped\n    \n  # Optional: Add nginx for load balancing\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n    depends_on:\n      - mobilenet-api\n\n\n\n# k8s-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mobilenet-api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mobilenet-api\n  template:\n    metadata:\n      labels:\n        app: mobilenet-api\n    spec:\n      containers:\n      - name: mobilenet-api\n        image: your-registry/mobilenet-api:latest\n        ports:\n        - containerPort: 8000\n        resources:\n          requests:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"2Gi\"\n            cpu: \"1000m\"\n        env:\n        - name: WORKERS\n          value: \"2\"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mobilenet-service\nspec:\n  selector:\n    app: mobilenet-api\n  ports:\n  - port: 80\n    targetPort: 8000\n  type: LoadBalancer\n\n\n\n\n\n\n# test_api.py\nimport pytest\nimport requests\nimport base64\nimport numpy as np\nfrom PIL import Image\nimport io\n\n\nclass TestMobileNetV2API:\n    def setup_class(self):\n        \"\"\"Setup test configuration\"\"\"\n        self.base_url = \"http://localhost:8000\"\n        self.test_image = self.create_test_image()\n\n    def create_test_image(self):\n        \"\"\"Create a test image\"\"\"\n        # Create a simple test image\n        img = Image.new('RGB', (224, 224), color='red')\n        img_buffer = io.BytesIO()\n        img.save(img_buffer, format='JPEG')\n        img_buffer.seek(0)\n        return img_buffer.getvalue()\n\n    def test_single_prediction(self):\n        \"\"\"Test single image prediction\"\"\"\n        encoded_image = base64.b64encode(self.test_image).decode('utf-8')\n        \n        response = requests.post(\n            f\"{self.base_url}/predict\",\n            json={\"image\": encoded_image}\n        )\n        \n        assert response.status_code == 200\n        result = response.json()\n        assert \"predictions\" in result\n        assert len(result[\"predictions\"]) == 5\n        assert all(\"class\" in pred and \"confidence\" in pred for pred in result[\"predictions\"])\n\n    def test_batch_prediction(self):\n        \"\"\"Test batch prediction\"\"\"\n        encoded_image = base64.b64encode(self.test_image).decode('utf-8')\n        \n        response = requests.post(\n            f\"{self.base_url}/predict\",\n            json={\"images\": [encoded_image, encoded_image]}\n        )\n        \n        assert response.status_code == 200\n        result = response.json()\n        assert \"batch_size\" in result\n        assert result[\"batch_size\"] == 2\n\n    def test_performance(self):\n        \"\"\"Test API performance\"\"\"\n        import time\n        \n        encoded_image = base64.b64encode(self.test_image).decode('utf-8')\n        \n        # Warmup\n        requests.post(f\"{self.base_url}/predict\", json={\"image\": encoded_image})\n        \n        # Time multiple requests\n        times = []\n        for _ in range(10):\n            start = time.time()\n            response = requests.post(f\"{self.base_url}/predict\", json={\"image\": encoded_image})\n            times.append(time.time() - start)\n            assert response.status_code == 200\n        \n        avg_time = sum(times) / len(times)\n        print(f\"Average response time: {avg_time:.3f}s\")\n        assert avg_time &lt; 1.0  # Should respond within 1 second\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])\n\n\n\n# load_test.py\nimport asyncio\nimport aiohttp\nimport base64\nimport time\nfrom PIL import Image\nimport io\n\n\nasync def send_request(session, url, data):\n    \"\"\"Send a single request\"\"\"\n    try:\n        async with session.post(url, json=data) as response:\n            result = await response.json()\n            return response.status, time.time()\n    except Exception as e:\n        return 500, time.time()\n\n\nasync def load_test(num_requests=100, concurrent=10):\n    \"\"\"Run load test\"\"\"\n    # Create test image\n    img = Image.new('RGB', (224, 224), color='blue')\n    img_buffer = io.BytesIO()\n    img.save(img_buffer, format='JPEG')\n    encoded_image = base64.b64encode(img_buffer.getvalue()).decode('utf-8')\n    \n    url = \"http://localhost:8000/predict\"\n    data = {\"image\": encoded_image}\n    \n    start_time = time.time()\n    \n    async with aiohttp.ClientSession() as session:\n        # Create semaphore to limit concurrent requests\n        semaphore = asyncio.Semaphore(concurrent)\n        \n        async def bounded_request():\n            async with semaphore:\n                return await send_request(session, url, data)\n        \n        # Send all requests\n        tasks = [bounded_request() for _ in range(num_requests)]\n        results = await asyncio.gather(*tasks)\n    \n    total_time = time.time() - start_time\n    \n    # Analyze results\n    successful = sum(1 for status, _ in results if status == 200)\n    failed = num_requests - successful\n    \n    print(f\"Load Test Results:\")\n    print(f\"  Total Requests: {num_requests}\")\n    print(f\"  Successful: {successful}\")\n    print(f\"  Failed: {failed}\")\n    print(f\"  Total Time: {total_time:.2f}s\")\n    print(f\"  Requests/sec: {num_requests/total_time:.2f}\")\n    print(f\"  Concurrent: {concurrent}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(load_test(num_requests=200, concurrent=20))\n\n\n\n\n# requirements.txt\nlitserve&gt;=0.2.0\ntorch&gt;=1.9.0\ntorchvision&gt;=0.10.0\nPillow&gt;=8.0.0\nrequests&gt;=2.25.0\nnumpy&gt;=1.21.0\naiohttp&gt;=3.8.0  # For async testing\npytest&gt;=6.0.0  # For testing\n\n\n\n\nModel Optimization: Use quantization and TorchScript for production\nBatch Processing: Configure appropriate batch sizes based on your hardware\nError Handling: Implement comprehensive error handling for robustness\nMonitoring: Add logging and metrics collection for production monitoring\nSecurity: Implement authentication and input validation for production APIs\nCaching: Consider caching frequently requested predictions\nScaling: Use container orchestration for high-availability deployments\n\nThis guide provides a complete foundation for deploying MobileNetV2 with LitServe, from basic implementation to production-ready deployment with monitoring and testing."
  },
  {
    "objectID": "posts/litserve-mobilenet/index.html#table-of-contents",
    "href": "posts/litserve-mobilenet/index.html#table-of-contents",
    "title": "LitServe with MobileNetV2 - Complete Code Guide",
    "section": "",
    "text": "Installation\nBasic Implementation\nAdvanced Features\nPerformance Optimization\nDeployment\nTesting"
  },
  {
    "objectID": "posts/litserve-mobilenet/index.html#installation",
    "href": "posts/litserve-mobilenet/index.html#installation",
    "title": "LitServe with MobileNetV2 - Complete Code Guide",
    "section": "",
    "text": "# Install required packages\npip install litserve torch torchvision pillow requests"
  },
  {
    "objectID": "posts/litserve-mobilenet/index.html#basic-implementation",
    "href": "posts/litserve-mobilenet/index.html#basic-implementation",
    "title": "LitServe with MobileNetV2 - Complete Code Guide",
    "section": "",
    "text": "# mobilenet_api.py\nimport io\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom PIL import Image\nimport litserve as ls\n\n\nclass MobileNetV2API(ls.LitAPI):\n    def setup(self, device):\n        \"\"\"Initialize the model and preprocessing pipeline\"\"\"\n        # Load pre-trained MobileNetV2\n        self.model = models.mobilenet_v2(pretrained=True)\n        self.model.eval()\n        self.model.to(device)\n        \n        # Define image preprocessing\n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n        \n        # ImageNet class labels (first 10 for brevity)\n        self.class_labels = [\n            \"tench\", \"goldfish\", \"great white shark\", \"tiger shark\",\n            \"hammerhead\", \"electric ray\", \"stingray\", \"cock\", \n            \"hen\", \"ostrich\"\n            # ... add all 1000 ImageNet classes\n        ]\n\n    def decode_request(self, request):\n        \"\"\"Parse incoming request and prepare image\"\"\"\n        if isinstance(request, dict) and \"image\" in request:\n            # Handle base64 encoded image\n            import base64\n            image_data = base64.b64decode(request[\"image\"])\n            image = Image.open(io.BytesIO(image_data)).convert('RGB')\n        else:\n            # Handle direct image upload\n            image = Image.open(io.BytesIO(request)).convert('RGB')\n        \n        return image\n\n    def predict(self, image):\n        \"\"\"Run inference on the preprocessed image\"\"\"\n        # Preprocess image\n        input_tensor = self.transform(image).unsqueeze(0)\n        input_tensor = input_tensor.to(next(self.model.parameters()).device)\n        \n        # Run inference\n        with torch.no_grad():\n            outputs = self.model(input_tensor)\n            probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n        \n        return probabilities\n\n    def encode_response(self, probabilities):\n        \"\"\"Format the response\"\"\"\n        # Get top 5 predictions\n        top5_prob, top5_indices = torch.topk(probabilities, 5)\n        \n        results = []\n        for i in range(5):\n            idx = top5_indices[i].item()\n            prob = top5_prob[i].item()\n            label = self.class_labels[idx] if idx &lt; len(self.class_labels) else f\"class_{idx}\"\n            results.append({\n                \"class\": label,\n                \"confidence\": round(prob, 4),\n                \"class_id\": idx\n            })\n        \n        return {\n            \"predictions\": results,\n            \"model\": \"mobilenet_v2\"\n        }\n\n\nif __name__ == \"__main__\":\n    # Create and run the API\n    api = MobileNetV2API()\n    server = ls.LitServer(api, accelerator=\"auto\", max_batch_size=4)\n    server.run(port=8000)\n\n\n\n# client.py\nimport requests\nimport base64\nfrom PIL import Image\nimport io\n\n\ndef encode_image(image_path):\n    \"\"\"Encode image to base64\"\"\"\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode('utf-8')\n\n\ndef test_image_classification(image_path, server_url=\"http://localhost:8000/predict\"):\n    \"\"\"Test the MobileNetV2 API\"\"\"\n    # Method 1: Send as base64 in JSON\n    encoded_image = encode_image(image_path)\n    response = requests.post(\n        server_url,\n        json={\"image\": encoded_image}\n    )\n    \n    if response.status_code == 200:\n        result = response.json()\n        print(\"Top 5 Predictions:\")\n        for pred in result[\"predictions\"]:\n            print(f\"  {pred['class']}: {pred['confidence']:.4f}\")\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n\n\ndef test_direct_upload(image_path, server_url=\"http://localhost:8000/predict\"):\n    \"\"\"Test with direct image upload\"\"\"\n    with open(image_path, 'rb') as f:\n        files = {'file': f}\n        response = requests.post(server_url, files=files)\n    \n    if response.status_code == 200:\n        result = response.json()\n        print(\"Predictions:\", result)\n\n\nif __name__ == \"__main__\":\n    # Test with a sample image\n    test_image_classification(\"sample_image.jpg\")"
  },
  {
    "objectID": "posts/litserve-mobilenet/index.html#advanced-features",
    "href": "posts/litserve-mobilenet/index.html#advanced-features",
    "title": "LitServe with MobileNetV2 - Complete Code Guide",
    "section": "",
    "text": "# advanced_mobilenet_api.py\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom PIL import Image\nimport litserve as ls\nimport json\nfrom typing import List, Any\n\n\nclass BatchedMobileNetV2API(ls.LitAPI):\n    def setup(self, device):\n        \"\"\"Initialize model with batch processing capabilities\"\"\"\n        self.model = models.mobilenet_v2(pretrained=True)\n        self.model.eval()\n        self.model.to(device)\n        self.device = device\n        \n        # Optimized transform for batch processing\n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n        \n        # Load class labels from file or define them\n        self.load_imagenet_labels()\n\n    def load_imagenet_labels(self):\n        \"\"\"Load ImageNet class labels\"\"\"\n        # You can download from: https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n        try:\n            with open('imagenet_classes.txt', 'r') as f:\n                self.class_labels = [line.strip() for line in f.readlines()]\n        except FileNotFoundError:\n            # Fallback to first few classes\n            self.class_labels = [f\"class_{i}\" for i in range(1000)]\n\n    def decode_request(self, request):\n        \"\"\"Handle both single images and batch requests\"\"\"\n        if isinstance(request, dict):\n            if \"images\" in request:  # Batch request\n                images = []\n                for img_data in request[\"images\"]:\n                    if isinstance(img_data, str):  # base64\n                        import base64\n                        image_data = base64.b64decode(img_data)\n                        image = Image.open(io.BytesIO(image_data)).convert('RGB')\n                    else:  # direct bytes\n                        image = Image.open(io.BytesIO(img_data)).convert('RGB')\n                    images.append(image)\n                return images\n            elif \"image\" in request:  # Single request\n                import base64\n                image_data = base64.b64decode(request[\"image\"])\n                image = Image.open(io.BytesIO(image_data)).convert('RGB')\n                return [image]  # Wrap in list for consistent handling\n        \n        # Direct upload\n        image = Image.open(io.BytesIO(request)).convert('RGB')\n        return [image]\n\n    def batch(self, inputs: List[Any]) -&gt; List[Any]:\n        \"\"\"Custom batching logic\"\"\"\n        # Flatten all images from all requests\n        all_images = []\n        batch_sizes = []\n        \n        for inp in inputs:\n            if isinstance(inp, list):\n                all_images.extend(inp)\n                batch_sizes.append(len(inp))\n            else:\n                all_images.append(inp)\n                batch_sizes.append(1)\n        \n        return all_images, batch_sizes\n\n    def predict(self, batch_data):\n        \"\"\"Process batch of images efficiently\"\"\"\n        images, batch_sizes = batch_data\n        \n        # Preprocess all images\n        batch_tensor = torch.stack([\n            self.transform(img) for img in images\n        ]).to(self.device)\n        \n        # Run batch inference\n        with torch.no_grad():\n            outputs = self.model(batch_tensor)\n            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n        \n        return probabilities, batch_sizes\n\n    def unbatch(self, output):\n        \"\"\"Split batch results back to individual responses\"\"\"\n        probabilities, batch_sizes = output\n        results = []\n        start_idx = 0\n        \n        for batch_size in batch_sizes:\n            batch_probs = probabilities[start_idx:start_idx + batch_size]\n            results.append(batch_probs)\n            start_idx += batch_size\n        \n        return results\n\n    def encode_response(self, probabilities):\n        \"\"\"Format response for batch or single predictions\"\"\"\n        if len(probabilities.shape) == 1:  # Single prediction\n            probabilities = probabilities.unsqueeze(0)\n        \n        all_results = []\n        for prob_vector in probabilities:\n            top5_prob, top5_indices = torch.topk(prob_vector, 5)\n            \n            predictions = []\n            for i in range(5):\n                idx = top5_indices[i].item()\n                prob = top5_prob[i].item()\n                predictions.append({\n                    \"class\": self.class_labels[idx],\n                    \"confidence\": round(prob, 4),\n                    \"class_id\": idx\n                })\n            \n            all_results.append(predictions)\n        \n        return {\n            \"predictions\": all_results[0] if len(all_results) == 1 else all_results,\n            \"model\": \"mobilenet_v2\",\n            \"batch_size\": len(all_results)\n        }\n\n\nif __name__ == \"__main__\":\n    api = BatchedMobileNetV2API()\n    server = ls.LitServer(\n        api,\n        accelerator=\"auto\",\n        max_batch_size=8,\n        batch_timeout=0.1,  # 100ms timeout for batching\n    )\n    server.run(port=8000, num_workers=2)\n\n\n\n# quantized_mobilenet_api.py\nimport torch\nimport torch.quantization\nfrom torchvision import models\nimport litserve as ls\n\n\nclass QuantizedMobileNetV2API(ls.LitAPI):\n    def setup(self, device):\n        \"\"\"Setup quantized MobileNetV2 for faster inference\"\"\"\n        # Load pre-trained model\n        self.model = models.mobilenet_v2(pretrained=True)\n        self.model.eval()\n        \n        # Apply dynamic quantization for CPU inference\n        if device == \"cpu\":\n            self.model = torch.quantization.quantize_dynamic(\n                self.model,\n                {torch.nn.Linear, torch.nn.Conv2d},\n                dtype=torch.qint8\n            )\n            print(\"Applied dynamic quantization for CPU\")\n        else:\n            self.model.to(device)\n            print(f\"Using device: {device}\")\n        \n        # Rest of setup code...\n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n\n    # ... rest of the methods remain the same"
  },
  {
    "objectID": "posts/litserve-mobilenet/index.html#performance-optimization",
    "href": "posts/litserve-mobilenet/index.html#performance-optimization",
    "title": "LitServe with MobileNetV2 - Complete Code Guide",
    "section": "",
    "text": "# production_config.py\nimport litserve as ls\nfrom advanced_mobilenet_api import BatchedMobileNetV2API\n\ndef create_production_server():\n    \"\"\"Create optimized server for production\"\"\"\n    api = BatchedMobileNetV2API()\n    \n    server = ls.LitServer(\n        api,\n        accelerator=\"auto\",  # Auto-detect GPU/CPU\n        max_batch_size=16,   # Larger batches for throughput\n        batch_timeout=0.05,  # 50ms batching timeout\n        workers_per_device=2,  # Multiple workers per GPU\n        timeout=30,          # Request timeout\n    )\n    \n    return server\n\nif __name__ == \"__main__\":\n    server = create_production_server()\n    server.run(\n        port=8000,\n        host=\"0.0.0.0\",  # Accept external connections\n        num_workers=4     # Total number of workers\n    )\n\n\n\n# monitored_api.py\nimport time\nimport logging\nfrom collections import defaultdict\nimport litserve as ls\n\n\nclass MonitoredMobileNetV2API(BatchedMobileNetV2API):\n    def setup(self, device):\n        \"\"\"Setup with monitoring\"\"\"\n        super().setup(device)\n        \n        # Setup logging\n        logging.basicConfig(level=logging.INFO)\n        self.logger = logging.getLogger(__name__)\n        \n        # Metrics tracking\n        self.metrics = defaultdict(list)\n        self.request_count = 0\n\n    def predict(self, batch_data):\n        \"\"\"Predict with timing and metrics\"\"\"\n        start_time = time.time()\n        \n        result = super().predict(batch_data)\n        \n        # Log timing\n        inference_time = time.time() - start_time\n        batch_size = len(batch_data[0])\n        \n        self.metrics['inference_times'].append(inference_time)\n        self.metrics['batch_sizes'].append(batch_size)\n        self.request_count += 1\n        \n        self.logger.info(\n            f\"Processed batch of {batch_size} images in {inference_time:.3f}s \"\n            f\"(Total requests: {self.request_count})\"\n        )\n        \n        return result\n\n    def encode_response(self, probabilities):\n        \"\"\"Add metrics to response\"\"\"\n        response = super().encode_response(probabilities)\n        \n        # Add performance metrics every 100 requests\n        if self.request_count % 100 == 0:\n            avg_time = sum(self.metrics['inference_times'][-100:]) / min(100, len(self.metrics['inference_times']))\n            response['metrics'] = {\n                'avg_inference_time': round(avg_time, 4),\n                'total_requests': self.request_count\n            }\n        \n        return response"
  },
  {
    "objectID": "posts/litserve-mobilenet/index.html#deployment",
    "href": "posts/litserve-mobilenet/index.html#deployment",
    "title": "LitServe with MobileNetV2 - Complete Code Guide",
    "section": "",
    "text": "# Dockerfile\nFROM python:3.9-slim\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    wget \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Download ImageNet labels\nRUN wget -O imagenet_classes.txt https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n\nEXPOSE 8000\n\nCMD [\"python\", \"production_config.py\"]\n# docker-compose.yml\nversion: '3.8'\n\nservices:\n  mobilenet-api:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - CUDA_VISIBLE_DEVICES=0  # Set GPU if available\n    volumes:\n      - ./models:/app/models  # Optional: for custom models\n    restart: unless-stopped\n    \n  # Optional: Add nginx for load balancing\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n    depends_on:\n      - mobilenet-api\n\n\n\n# k8s-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mobilenet-api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mobilenet-api\n  template:\n    metadata:\n      labels:\n        app: mobilenet-api\n    spec:\n      containers:\n      - name: mobilenet-api\n        image: your-registry/mobilenet-api:latest\n        ports:\n        - containerPort: 8000\n        resources:\n          requests:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"2Gi\"\n            cpu: \"1000m\"\n        env:\n        - name: WORKERS\n          value: \"2\"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mobilenet-service\nspec:\n  selector:\n    app: mobilenet-api\n  ports:\n  - port: 80\n    targetPort: 8000\n  type: LoadBalancer"
  },
  {
    "objectID": "posts/litserve-mobilenet/index.html#testing",
    "href": "posts/litserve-mobilenet/index.html#testing",
    "title": "LitServe with MobileNetV2 - Complete Code Guide",
    "section": "",
    "text": "# test_api.py\nimport pytest\nimport requests\nimport base64\nimport numpy as np\nfrom PIL import Image\nimport io\n\n\nclass TestMobileNetV2API:\n    def setup_class(self):\n        \"\"\"Setup test configuration\"\"\"\n        self.base_url = \"http://localhost:8000\"\n        self.test_image = self.create_test_image()\n\n    def create_test_image(self):\n        \"\"\"Create a test image\"\"\"\n        # Create a simple test image\n        img = Image.new('RGB', (224, 224), color='red')\n        img_buffer = io.BytesIO()\n        img.save(img_buffer, format='JPEG')\n        img_buffer.seek(0)\n        return img_buffer.getvalue()\n\n    def test_single_prediction(self):\n        \"\"\"Test single image prediction\"\"\"\n        encoded_image = base64.b64encode(self.test_image).decode('utf-8')\n        \n        response = requests.post(\n            f\"{self.base_url}/predict\",\n            json={\"image\": encoded_image}\n        )\n        \n        assert response.status_code == 200\n        result = response.json()\n        assert \"predictions\" in result\n        assert len(result[\"predictions\"]) == 5\n        assert all(\"class\" in pred and \"confidence\" in pred for pred in result[\"predictions\"])\n\n    def test_batch_prediction(self):\n        \"\"\"Test batch prediction\"\"\"\n        encoded_image = base64.b64encode(self.test_image).decode('utf-8')\n        \n        response = requests.post(\n            f\"{self.base_url}/predict\",\n            json={\"images\": [encoded_image, encoded_image]}\n        )\n        \n        assert response.status_code == 200\n        result = response.json()\n        assert \"batch_size\" in result\n        assert result[\"batch_size\"] == 2\n\n    def test_performance(self):\n        \"\"\"Test API performance\"\"\"\n        import time\n        \n        encoded_image = base64.b64encode(self.test_image).decode('utf-8')\n        \n        # Warmup\n        requests.post(f\"{self.base_url}/predict\", json={\"image\": encoded_image})\n        \n        # Time multiple requests\n        times = []\n        for _ in range(10):\n            start = time.time()\n            response = requests.post(f\"{self.base_url}/predict\", json={\"image\": encoded_image})\n            times.append(time.time() - start)\n            assert response.status_code == 200\n        \n        avg_time = sum(times) / len(times)\n        print(f\"Average response time: {avg_time:.3f}s\")\n        assert avg_time &lt; 1.0  # Should respond within 1 second\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])\n\n\n\n# load_test.py\nimport asyncio\nimport aiohttp\nimport base64\nimport time\nfrom PIL import Image\nimport io\n\n\nasync def send_request(session, url, data):\n    \"\"\"Send a single request\"\"\"\n    try:\n        async with session.post(url, json=data) as response:\n            result = await response.json()\n            return response.status, time.time()\n    except Exception as e:\n        return 500, time.time()\n\n\nasync def load_test(num_requests=100, concurrent=10):\n    \"\"\"Run load test\"\"\"\n    # Create test image\n    img = Image.new('RGB', (224, 224), color='blue')\n    img_buffer = io.BytesIO()\n    img.save(img_buffer, format='JPEG')\n    encoded_image = base64.b64encode(img_buffer.getvalue()).decode('utf-8')\n    \n    url = \"http://localhost:8000/predict\"\n    data = {\"image\": encoded_image}\n    \n    start_time = time.time()\n    \n    async with aiohttp.ClientSession() as session:\n        # Create semaphore to limit concurrent requests\n        semaphore = asyncio.Semaphore(concurrent)\n        \n        async def bounded_request():\n            async with semaphore:\n                return await send_request(session, url, data)\n        \n        # Send all requests\n        tasks = [bounded_request() for _ in range(num_requests)]\n        results = await asyncio.gather(*tasks)\n    \n    total_time = time.time() - start_time\n    \n    # Analyze results\n    successful = sum(1 for status, _ in results if status == 200)\n    failed = num_requests - successful\n    \n    print(f\"Load Test Results:\")\n    print(f\"  Total Requests: {num_requests}\")\n    print(f\"  Successful: {successful}\")\n    print(f\"  Failed: {failed}\")\n    print(f\"  Total Time: {total_time:.2f}s\")\n    print(f\"  Requests/sec: {num_requests/total_time:.2f}\")\n    print(f\"  Concurrent: {concurrent}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(load_test(num_requests=200, concurrent=20))"
  },
  {
    "objectID": "posts/litserve-mobilenet/index.html#requirements-file",
    "href": "posts/litserve-mobilenet/index.html#requirements-file",
    "title": "LitServe with MobileNetV2 - Complete Code Guide",
    "section": "",
    "text": "# requirements.txt\nlitserve&gt;=0.2.0\ntorch&gt;=1.9.0\ntorchvision&gt;=0.10.0\nPillow&gt;=8.0.0\nrequests&gt;=2.25.0\nnumpy&gt;=1.21.0\naiohttp&gt;=3.8.0  # For async testing\npytest&gt;=6.0.0  # For testing"
  },
  {
    "objectID": "posts/litserve-mobilenet/index.html#best-practices",
    "href": "posts/litserve-mobilenet/index.html#best-practices",
    "title": "LitServe with MobileNetV2 - Complete Code Guide",
    "section": "",
    "text": "Model Optimization: Use quantization and TorchScript for production\nBatch Processing: Configure appropriate batch sizes based on your hardware\nError Handling: Implement comprehensive error handling for robustness\nMonitoring: Add logging and metrics collection for production monitoring\nSecurity: Implement authentication and input validation for production APIs\nCaching: Consider caching frequently requested predictions\nScaling: Use container orchestration for high-availability deployments\n\nThis guide provides a complete foundation for deploying MobileNetV2 with LitServe, from basic implementation to production-ready deployment with monitoring and testing."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My posts",
    "section": "",
    "text": "Albumentations vs TorchVision Transforms: Complete Code Guide\n\n\n\ncode\n\ntutorial\n\nbeginner\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 27, 2025\n\n\n\n\n\n\n\n\n\n\n\nLitServe Code Guide\n\n\n\ncode\n\nmlops\n\nbeginner\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 27, 2025\n\n\n\n\n\n\n\n\n\n\n\nLitServe with MobileNetV2 - Complete Code Guide\n\n\n\ncode\n\nmlops\n\nintermediate\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 27, 2025\n\n\n\n\n\n\n\n\n\n\n\nMobileNetV2 PyTorch Docker Deployment Guide\n\n\n\ncode\n\nmlops\n\nintermediate\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nPyTorch to PyTorch Lightning Migration Guide\n\n\n\ncode\n\ntutorial\n\nadvanced\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 25, 2025\n\n\n\n\n\n\n\n\n\n\n\nVision Transformers (ViT): A Simple Guide\n\n\n\nresearch\n\nbeginner\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 24, 2025\n\n\n\n\n\n\n\n\n\n\n\nPython 3.14: The Next Evolution in Python Development\n\n\n\nnews\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 24, 2025\n\n\n\n\n\n\n\n\n\n\n\nPython 3.14: Key Improvements and New Features\n\n\n\ncode\n\nbeginner\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 24, 2025\n\n\n\n\n\n\n\n\n\n\n\nDINOv2: A Deep Dive into Architecture and Training\n\n\n\nresearch\n\nintermediate\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 17, 2025\n\n\n\n\n\n\n\n\n\n\n\nDINO: Emerging Properties in Self-Supervised Vision Transformers\n\n\n\nresearch\n\nintermediate\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 13, 2025\n\n\n\n\n\n\n\n\n\n\n\nDINOv2: Comprehensive Implementation Guide\n\n\n\ncode\n\ntutorial\n\nbeginner\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 3, 2025\n\n\n\n\n\n\n\n\n\n\n\nVision Transformer (ViT) Implementation Guide\n\n\n\ncode\n\ntutorial\n\nadvanced\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nApr 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nActive Learning Influence Selection: A Comprehensive Guide\n\n\n\ncode\n\nresearch\n\nadvanced\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nApr 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nPython Data Visualization: Matplotlib vs Seaborn vs Altair\n\n\n\ncode\n\ntutorial\n\nbeginner\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nApr 12, 2025\n\n\n\n\n\n\n\n\n\n\n\nFrom Pandas to Polars\n\n\n\ncode\n\ntutorial\n\nadvanced\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nApr 5, 2025\n\n\n\n\n\n\n\n\n\n\n\nPyTorch Lightning: A Comprehensive Guide\n\n\n\ncode\n\ntutorial\n\nadvanced\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMar 29, 2025\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMar 22, 2025\n\n\n\n\n\n\nNo matching items"
  }
]