[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Hi 👋, I’m Krishnatheja Vanka\n\nMachine Learning Engineer (Applied Computer Vision)"
  },
  {
    "objectID": "posts/dinov2/index.html",
    "href": "posts/dinov2/index.html",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "DINOv2 is a state-of-the-art self-supervised vision model developed by Meta AI Research that builds upon the original DINO (Self-Distillation with No Labels) framework. This guide will walk you through understanding, implementing, and leveraging DINOv2 for various computer vision tasks.\n\n\n\nIntroduction to DINOv2\nInstallation and Setup\nLoading Pre-trained Models\nFeature Extraction\nFine-tuning for Downstream Tasks\nImage Classification Example\nSemantic Segmentation Example\nObject Detection Example\nAdvanced Usage and Customization\nPerformance Benchmarks\nTroubleshooting\n\n\n\n\nDINOv2 is a self-supervised learning method for vision that produces high-quality visual features without requiring labeled data. It extends the original DINO architecture with several improvements:\n\nTraining on a large and diverse dataset of images\nEnhanced teacher-student architecture\nImproved augmentation strategy\nMulti-scale feature learning\nSupport for various Vision Transformer (ViT) backbones\n\nThe result is a versatile foundation model that can be adapted to numerous vision tasks with minimal fine-tuning.\n\n\n\nTo use DINOv2, you’ll need to install the official implementation:\n# Install PyTorch first if not already installed\n# pip install torch torchvision\n\n# Install DINOv2\npip install git+https://github.com/facebookresearch/dinov2\nAlternatively, you can clone the repository and install it locally:\ngit clone https://github.com/facebookresearch/dinov2.git\ncd dinov2\npip install -e .\n\n\nDINOv2 requires: - Python 3.8+ - PyTorch 1.12+ - torchvision - CUDA (for GPU acceleration)\n\n\n\n\nDINOv2 provides several pre-trained models with different sizes and capabilities:\nimport torch\nfrom dinov2.models import build_model_from_cfg\nfrom dinov2.configs import get_config\n\n# Available model sizes: 'small', 'base', 'large', 'giant'\nmodel_size = 'base'\ncfg = get_config(f\"dinov2_{model_size}\")\nmodel = build_model_from_cfg(cfg)\n\n# Load pre-trained weights\ncheckpoint_path = f\"dinov2_{model_size}_pretrain.pth\"  # Download this from Meta AI's repository\ncheckpoint = torch.load(checkpoint_path, map_location=\"cpu\")\nmodel.load_state_dict(checkpoint[\"model\"])\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nmodel.eval()  # Set to evaluation mode\nYou can also use the Hugging Face Transformers library for an easier integration:\nfrom transformers import AutoImageProcessor, AutoModel\n\n# Available model sizes: 'small', 'base', 'large', 'giant'\nmodel_name = \"facebook/dinov2-base\"\nprocessor = AutoImageProcessor.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n\n\nOne of DINOv2’s key strengths is its ability to extract powerful visual features:\nimport torch\nfrom PIL import Image\nimport torchvision.transforms as T\nfrom transformers import AutoImageProcessor, AutoModel\n\n# Load model\nmodel_name = \"facebook/dinov2-base\"\nprocessor = AutoImageProcessor.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\nmodel.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\nmodel.eval()\n\n# Load and preprocess image\nimage = Image.open(\"path/to/your/image.jpg\").convert(\"RGB\")\ninputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n\n# Extract features\nwith torch.no_grad():\n    outputs = model(**inputs)\n    \n# Get CLS token features (useful for classification tasks)\ncls_features = outputs.last_hidden_state[:, 0]\n\n# Get patch features (useful for dense prediction tasks like segmentation)\npatch_features = outputs.last_hidden_state[:, 1:]\n\nprint(f\"CLS features shape: {cls_features.shape}\")\nprint(f\"Patch features shape: {patch_features.shape}\")\n\n\n\nDINOv2 can be fine-tuned for specific vision tasks:\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModel\n\n# Load pre-trained DINOv2 model\nbackbone = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n\n# Create a custom classification head\nclass ClassificationHead(nn.Module):\n    def __init__(self, backbone, num_classes=1000):\n        super().__init__()\n        self.backbone = backbone\n        self.classifier = nn.Linear(backbone.config.hidden_size, num_classes)\n        \n    def forward(self, x):\n        outputs = self.backbone(x)\n        cls_token = outputs.last_hidden_state[:, 0]\n        return self.classifier(cls_token)\n\n# Create the complete model\nmodel = ClassificationHead(backbone, num_classes=100)  # For 100 classes\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop example\ndef train_one_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    \n    for batch in dataloader:\n        images = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(dataloader)\n\n\n\nHere’s a complete example for image classification using DINOv2:\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as transforms\nfrom transformers import AutoImageProcessor, AutoModel\n\n# Define the dataset and transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Load your dataset (adjust the path)\ntrain_dataset = ImageFolder(root=\"path/to/train\", transform=transform)\nval_dataset = ImageFolder(root=\"path/to/val\", transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n\n# Create the model\nclass DINOv2Classifier(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.dinov2 = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n        self.classifier = nn.Linear(768, num_classes)  # 768 is the hidden size for base model\n        \n    def forward(self, x):\n        # Extract features\n        with torch.set_grad_enabled(self.training):\n            features = self.dinov2(x).last_hidden_state[:, 0]  # Get CLS token\n        \n        # Classify\n        logits = self.classifier(features)\n        return logits\n\n# Initialize model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DINOv2Classifier(num_classes=len(train_dataset.classes))\nmodel = model.to(device)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW([\n    {'params': model.classifier.parameters(), 'lr': 1e-3},\n    {'params': model.dinov2.parameters(), 'lr': 1e-5}\n])\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    # Training\n    model.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    \n    for inputs, targets in train_loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n    \n    train_accuracy = 100 * correct / total\n    \n    # Validation\n    model.eval()\n    val_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            \n            val_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    \n    val_accuracy = 100 * correct / total\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    print(f\"Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_accuracy:.2f}%\")\n    print(f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_accuracy:.2f}%\")\n\n\n\nDINOv2 is particularly powerful for segmentation tasks:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModel\n\nclass DINOv2Segmenter(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        # Load DINOv2 backbone\n        self.backbone = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n        \n        # Define segmentation head\n        hidden_dim = self.backbone.config.hidden_size\n        self.segmentation_head = nn.Sequential(\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(hidden_dim, num_classes, kernel_size=1)\n        )\n        \n        # Image size and patch size for reshaping\n        self.image_size = 224\n        self.patch_size = 14  # For ViT-Base\n        self.num_patches = (self.image_size // self.patch_size) ** 2\n        \n    def forward(self, x):\n        # Get patch features\n        outputs = self.backbone(x)\n        patch_features = outputs.last_hidden_state[:, 1:]  # Remove CLS token\n        \n        # Reshape to 2D spatial layout\n        B = x.shape[0]\n        H = W = self.image_size // self.patch_size\n        patch_features = patch_features.reshape(B, H, W, -1).permute(0, 3, 1, 2)\n        \n        # Apply segmentation head\n        segmentation_logits = self.segmentation_head(patch_features)\n        \n        # Upsample to original image size\n        segmentation_logits = F.interpolate(\n            segmentation_logits, \n            size=(self.image_size, self.image_size), \n            mode='bilinear', \n            align_corners=False\n        )\n        \n        return segmentation_logits\n\n# Create model and move to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DINOv2Segmenter(num_classes=21)  # 21 classes for Pascal VOC\nmodel = model.to(device)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW([\n    {'params': model.segmentation_head.parameters(), 'lr': 1e-3},\n    {'params': model.backbone.parameters(), 'lr': 1e-5}\n])\ncriterion = nn.CrossEntropyLoss(ignore_index=255)  # 255 is typically the ignore index\n\n# Rest of the training code would be similar to the classification example\n\n\n\nHere’s how to use DINOv2 features for object detection with a simple detection head:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModel\n\nclass DINOv2Detector(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        # Load DINOv2 backbone\n        self.backbone = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n        hidden_dim = self.backbone.config.hidden_size\n        \n        # Detection heads\n        self.box_predictor = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, 4)  # (x1, y1, x2, y2)\n        )\n        \n        self.class_predictor = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, num_classes + 1)  # +1 for background\n        )\n        \n        # Image size and patch size for feature map creation\n        self.image_size = 224\n        self.patch_size = 14  # For ViT-Base\n        \n    def forward(self, x):\n        # Get features\n        outputs = self.backbone(x)\n        features = outputs.last_hidden_state[:, 1:]  # Remove CLS token\n        \n        # Reshape to 2D spatial layout\n        B = x.shape[0]\n        H = W = self.image_size // self.patch_size\n        features = features.reshape(B, H, W, -1)\n        \n        # Flatten for prediction heads\n        features_flat = features.reshape(B, -1, features.shape[-1])\n        \n        # Predict boxes and classes\n        boxes = self.box_predictor(features_flat)\n        classes = self.class_predictor(features_flat)\n        \n        return {'boxes': boxes, 'classes': classes, 'features_map': features}\n\n# Create model and move to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DINOv2Detector(num_classes=80)  # 80 classes for COCO\nmodel = model.to(device)\n\n# Training would require a more complex detection pipeline with NMS, etc.\n\n\n\n\n\nYou can customize the DINOv2 model architecture:\nfrom dinov2.configs import get_config\nfrom dinov2.models import build_model_from_cfg\n\n# Get default configuration and modify it\ncfg = get_config(\"dinov2_base\")\n\n# Modify configuration\ncfg.student.drop_path_rate = 0.2  # Change stochastic depth rate\ncfg.student.num_registers = 16    # Change the number of registers\n\n# Build model from modified config\nmodel = build_model_from_cfg(cfg)\n\n\n\nFor some applications, you might want to extract features from intermediate layers:\nimport torch\nfrom transformers import AutoModel\nfrom torch.utils.hooks import RemovableHandle\n\nclass FeatureExtractor:\n    def __init__(self, model, layers=None):\n        self.model = model\n        self.features = {}\n        self.hooks = []\n        \n        # Default to extracting from the last block if no layers specified\n        self.layers = layers if layers is not None else [11]  # Base model has 12 blocks (0-11)\n        \n        # Register hooks\n        for idx in self.layers:\n            hook = self.model.encoder.layer[idx].register_forward_hook(\n                lambda module, input, output, idx=idx: self.features.update({f\"layer_{idx}\": output})\n            )\n            self.hooks.append(hook)\n    \n    def __call__(self, x):\n        self.features.clear()\n        with torch.no_grad():\n            outputs = self.model(x)\n        return self.features\n    \n    def remove_hooks(self):\n        for hook in self.hooks:\n            hook.remove()\n\n# Usage\nmodel = AutoModel.from_pretrained(\"facebook/dinov2-base\")\nextractor = FeatureExtractor(model, layers=[3, 7, 11])\n\n# Extract features\nfeatures = extractor(input_image)\nlayer_3_features = features[\"layer_3\"]\nlayer_7_features = features[\"layer_7\"]\nlayer_11_features = features[\"layer_11\"]\n\n# Clean up\nextractor.remove_hooks()\n\n\n\n\nDINOv2 achieves excellent results across various vision tasks. Here are typical performance metrics:\n\nImageNet-1K Classification (top-1 accuracy):\n\nDINOv2-Small: ~80.0%\nDINOv2-Base: ~84.5%\nDINOv2-Large: ~86.3%\nDINOv2-Giant: ~87.0%\n\nSemantic Segmentation (ADE20K) (mIoU):\n\nDINOv2-Small: ~47.5%\nDINOv2-Base: ~50.2%\nDINOv2-Large: ~52.5%\nDINOv2-Giant: ~53.8%\n\nObject Detection (COCO) (AP):\n\nDINOv2-Small: ~48.5%\nDINOv2-Base: ~51.3%\nDINOv2-Large: ~53.2%\nDINOv2-Giant: ~54.5%\n\n\n\n\n\n\n\n\nOut of Memory Errors\n\nReduce batch size\nUse gradient accumulation\nUse a smaller model variant (Small or Base)\nUse mixed precision training\n\n\n# Example of mixed precision training\nfrom torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nfor inputs, targets in train_loader:\n    inputs, targets = inputs.to(device), targets.to(device)\n    \n    optimizer.zero_grad()\n    \n    # Use autocast for mixed precision\n    with autocast():\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n    \n    # Scale loss and backprop\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n\nSlow Inference\n\nUse batch processing\nUse model.eval() and torch.no_grad()\nConsider model distillation or quantization\n\nPoor Performance on Downstream Tasks\n\nEnsure proper data preprocessing\nAdjust learning rates (lower for backbone, higher for heads)\nUse appropriate augmentations\nConsider using a larger variant of DINOv2\n\n\n\n\n\n\nVisualize model attention maps to understand what the model focuses on:\n\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom PIL import Image\nimport torchvision.transforms as T\n\ndef get_attention_map(model, img_tensor):\n    model.eval()\n    with torch.no_grad():\n        outputs = model(img_tensor.unsqueeze(0), output_attentions=True)\n    \n    # Get attention weights from the last layer\n    att_mat = outputs.attentions[-1]\n    \n    # Average attention across heads\n    att_mat = att_mat.mean(dim=1)\n    \n    # Extract attention for cls token to patch tokens\n    cls_att_map = att_mat[0, 0, 1:].reshape(14, 14)\n    \n    return cls_att_map.cpu().numpy()\n\n# Load and preprocess image\nimage = Image.open(\"path/to/image.jpg\").convert(\"RGB\")\ntransform = T.Compose([\n    T.Resize((224, 224)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\nimg_tensor = transform(image).to(device)\n\n# Get attention map\nfrom transformers import AutoModel\nmodel = AutoModel.from_pretrained(\"facebook/dinov2-base\", output_attentions=True)\nmodel.to(device)\nattention_map = get_attention_map(model, img_tensor)\n\n# Visualize\nplt.figure(figsize=(10, 10))\nplt.imshow(image.resize((224, 224)))\nplt.imshow(attention_map, alpha=0.5, cmap='jet')\nplt.axis('off')\nplt.colorbar()\nplt.savefig('attention_map.png')\nplt.close()\nThis guide should help you get started with DINOv2 and explore its capabilities for various computer vision tasks. As a self-supervised vision foundation model, DINOv2 provides a strong starting point for numerous applications with minimal labeled data requirements."
  },
  {
    "objectID": "posts/dinov2/index.html#table-of-contents",
    "href": "posts/dinov2/index.html#table-of-contents",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "Introduction to DINOv2\nInstallation and Setup\nLoading Pre-trained Models\nFeature Extraction\nFine-tuning for Downstream Tasks\nImage Classification Example\nSemantic Segmentation Example\nObject Detection Example\nAdvanced Usage and Customization\nPerformance Benchmarks\nTroubleshooting"
  },
  {
    "objectID": "posts/dinov2/index.html#introduction-to-dinov2",
    "href": "posts/dinov2/index.html#introduction-to-dinov2",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "DINOv2 is a self-supervised learning method for vision that produces high-quality visual features without requiring labeled data. It extends the original DINO architecture with several improvements:\n\nTraining on a large and diverse dataset of images\nEnhanced teacher-student architecture\nImproved augmentation strategy\nMulti-scale feature learning\nSupport for various Vision Transformer (ViT) backbones\n\nThe result is a versatile foundation model that can be adapted to numerous vision tasks with minimal fine-tuning."
  },
  {
    "objectID": "posts/dinov2/index.html#installation-and-setup",
    "href": "posts/dinov2/index.html#installation-and-setup",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "To use DINOv2, you’ll need to install the official implementation:\n# Install PyTorch first if not already installed\n# pip install torch torchvision\n\n# Install DINOv2\npip install git+https://github.com/facebookresearch/dinov2\nAlternatively, you can clone the repository and install it locally:\ngit clone https://github.com/facebookresearch/dinov2.git\ncd dinov2\npip install -e .\n\n\nDINOv2 requires: - Python 3.8+ - PyTorch 1.12+ - torchvision - CUDA (for GPU acceleration)"
  },
  {
    "objectID": "posts/dinov2/index.html#loading-pre-trained-models",
    "href": "posts/dinov2/index.html#loading-pre-trained-models",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "DINOv2 provides several pre-trained models with different sizes and capabilities:\nimport torch\nfrom dinov2.models import build_model_from_cfg\nfrom dinov2.configs import get_config\n\n# Available model sizes: 'small', 'base', 'large', 'giant'\nmodel_size = 'base'\ncfg = get_config(f\"dinov2_{model_size}\")\nmodel = build_model_from_cfg(cfg)\n\n# Load pre-trained weights\ncheckpoint_path = f\"dinov2_{model_size}_pretrain.pth\"  # Download this from Meta AI's repository\ncheckpoint = torch.load(checkpoint_path, map_location=\"cpu\")\nmodel.load_state_dict(checkpoint[\"model\"])\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nmodel.eval()  # Set to evaluation mode\nYou can also use the Hugging Face Transformers library for an easier integration:\nfrom transformers import AutoImageProcessor, AutoModel\n\n# Available model sizes: 'small', 'base', 'large', 'giant'\nmodel_name = \"facebook/dinov2-base\"\nprocessor = AutoImageProcessor.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)"
  },
  {
    "objectID": "posts/dinov2/index.html#feature-extraction",
    "href": "posts/dinov2/index.html#feature-extraction",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "One of DINOv2’s key strengths is its ability to extract powerful visual features:\nimport torch\nfrom PIL import Image\nimport torchvision.transforms as T\nfrom transformers import AutoImageProcessor, AutoModel\n\n# Load model\nmodel_name = \"facebook/dinov2-base\"\nprocessor = AutoImageProcessor.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\nmodel.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\nmodel.eval()\n\n# Load and preprocess image\nimage = Image.open(\"path/to/your/image.jpg\").convert(\"RGB\")\ninputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n\n# Extract features\nwith torch.no_grad():\n    outputs = model(**inputs)\n    \n# Get CLS token features (useful for classification tasks)\ncls_features = outputs.last_hidden_state[:, 0]\n\n# Get patch features (useful for dense prediction tasks like segmentation)\npatch_features = outputs.last_hidden_state[:, 1:]\n\nprint(f\"CLS features shape: {cls_features.shape}\")\nprint(f\"Patch features shape: {patch_features.shape}\")"
  },
  {
    "objectID": "posts/dinov2/index.html#fine-tuning-for-downstream-tasks",
    "href": "posts/dinov2/index.html#fine-tuning-for-downstream-tasks",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "DINOv2 can be fine-tuned for specific vision tasks:\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModel\n\n# Load pre-trained DINOv2 model\nbackbone = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n\n# Create a custom classification head\nclass ClassificationHead(nn.Module):\n    def __init__(self, backbone, num_classes=1000):\n        super().__init__()\n        self.backbone = backbone\n        self.classifier = nn.Linear(backbone.config.hidden_size, num_classes)\n        \n    def forward(self, x):\n        outputs = self.backbone(x)\n        cls_token = outputs.last_hidden_state[:, 0]\n        return self.classifier(cls_token)\n\n# Create the complete model\nmodel = ClassificationHead(backbone, num_classes=100)  # For 100 classes\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop example\ndef train_one_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    \n    for batch in dataloader:\n        images = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(dataloader)"
  },
  {
    "objectID": "posts/dinov2/index.html#image-classification-example",
    "href": "posts/dinov2/index.html#image-classification-example",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "Here’s a complete example for image classification using DINOv2:\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as transforms\nfrom transformers import AutoImageProcessor, AutoModel\n\n# Define the dataset and transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Load your dataset (adjust the path)\ntrain_dataset = ImageFolder(root=\"path/to/train\", transform=transform)\nval_dataset = ImageFolder(root=\"path/to/val\", transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n\n# Create the model\nclass DINOv2Classifier(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.dinov2 = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n        self.classifier = nn.Linear(768, num_classes)  # 768 is the hidden size for base model\n        \n    def forward(self, x):\n        # Extract features\n        with torch.set_grad_enabled(self.training):\n            features = self.dinov2(x).last_hidden_state[:, 0]  # Get CLS token\n        \n        # Classify\n        logits = self.classifier(features)\n        return logits\n\n# Initialize model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DINOv2Classifier(num_classes=len(train_dataset.classes))\nmodel = model.to(device)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW([\n    {'params': model.classifier.parameters(), 'lr': 1e-3},\n    {'params': model.dinov2.parameters(), 'lr': 1e-5}\n])\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    # Training\n    model.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    \n    for inputs, targets in train_loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n    \n    train_accuracy = 100 * correct / total\n    \n    # Validation\n    model.eval()\n    val_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            \n            val_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    \n    val_accuracy = 100 * correct / total\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    print(f\"Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_accuracy:.2f}%\")\n    print(f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_accuracy:.2f}%\")"
  },
  {
    "objectID": "posts/dinov2/index.html#semantic-segmentation-example",
    "href": "posts/dinov2/index.html#semantic-segmentation-example",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "DINOv2 is particularly powerful for segmentation tasks:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModel\n\nclass DINOv2Segmenter(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        # Load DINOv2 backbone\n        self.backbone = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n        \n        # Define segmentation head\n        hidden_dim = self.backbone.config.hidden_size\n        self.segmentation_head = nn.Sequential(\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(hidden_dim, num_classes, kernel_size=1)\n        )\n        \n        # Image size and patch size for reshaping\n        self.image_size = 224\n        self.patch_size = 14  # For ViT-Base\n        self.num_patches = (self.image_size // self.patch_size) ** 2\n        \n    def forward(self, x):\n        # Get patch features\n        outputs = self.backbone(x)\n        patch_features = outputs.last_hidden_state[:, 1:]  # Remove CLS token\n        \n        # Reshape to 2D spatial layout\n        B = x.shape[0]\n        H = W = self.image_size // self.patch_size\n        patch_features = patch_features.reshape(B, H, W, -1).permute(0, 3, 1, 2)\n        \n        # Apply segmentation head\n        segmentation_logits = self.segmentation_head(patch_features)\n        \n        # Upsample to original image size\n        segmentation_logits = F.interpolate(\n            segmentation_logits, \n            size=(self.image_size, self.image_size), \n            mode='bilinear', \n            align_corners=False\n        )\n        \n        return segmentation_logits\n\n# Create model and move to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DINOv2Segmenter(num_classes=21)  # 21 classes for Pascal VOC\nmodel = model.to(device)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW([\n    {'params': model.segmentation_head.parameters(), 'lr': 1e-3},\n    {'params': model.backbone.parameters(), 'lr': 1e-5}\n])\ncriterion = nn.CrossEntropyLoss(ignore_index=255)  # 255 is typically the ignore index\n\n# Rest of the training code would be similar to the classification example"
  },
  {
    "objectID": "posts/dinov2/index.html#object-detection-example",
    "href": "posts/dinov2/index.html#object-detection-example",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "Here’s how to use DINOv2 features for object detection with a simple detection head:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModel\n\nclass DINOv2Detector(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        # Load DINOv2 backbone\n        self.backbone = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n        hidden_dim = self.backbone.config.hidden_size\n        \n        # Detection heads\n        self.box_predictor = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, 4)  # (x1, y1, x2, y2)\n        )\n        \n        self.class_predictor = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, num_classes + 1)  # +1 for background\n        )\n        \n        # Image size and patch size for feature map creation\n        self.image_size = 224\n        self.patch_size = 14  # For ViT-Base\n        \n    def forward(self, x):\n        # Get features\n        outputs = self.backbone(x)\n        features = outputs.last_hidden_state[:, 1:]  # Remove CLS token\n        \n        # Reshape to 2D spatial layout\n        B = x.shape[0]\n        H = W = self.image_size // self.patch_size\n        features = features.reshape(B, H, W, -1)\n        \n        # Flatten for prediction heads\n        features_flat = features.reshape(B, -1, features.shape[-1])\n        \n        # Predict boxes and classes\n        boxes = self.box_predictor(features_flat)\n        classes = self.class_predictor(features_flat)\n        \n        return {'boxes': boxes, 'classes': classes, 'features_map': features}\n\n# Create model and move to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DINOv2Detector(num_classes=80)  # 80 classes for COCO\nmodel = model.to(device)\n\n# Training would require a more complex detection pipeline with NMS, etc."
  },
  {
    "objectID": "posts/dinov2/index.html#advanced-usage-and-customization",
    "href": "posts/dinov2/index.html#advanced-usage-and-customization",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "You can customize the DINOv2 model architecture:\nfrom dinov2.configs import get_config\nfrom dinov2.models import build_model_from_cfg\n\n# Get default configuration and modify it\ncfg = get_config(\"dinov2_base\")\n\n# Modify configuration\ncfg.student.drop_path_rate = 0.2  # Change stochastic depth rate\ncfg.student.num_registers = 16    # Change the number of registers\n\n# Build model from modified config\nmodel = build_model_from_cfg(cfg)\n\n\n\nFor some applications, you might want to extract features from intermediate layers:\nimport torch\nfrom transformers import AutoModel\nfrom torch.utils.hooks import RemovableHandle\n\nclass FeatureExtractor:\n    def __init__(self, model, layers=None):\n        self.model = model\n        self.features = {}\n        self.hooks = []\n        \n        # Default to extracting from the last block if no layers specified\n        self.layers = layers if layers is not None else [11]  # Base model has 12 blocks (0-11)\n        \n        # Register hooks\n        for idx in self.layers:\n            hook = self.model.encoder.layer[idx].register_forward_hook(\n                lambda module, input, output, idx=idx: self.features.update({f\"layer_{idx}\": output})\n            )\n            self.hooks.append(hook)\n    \n    def __call__(self, x):\n        self.features.clear()\n        with torch.no_grad():\n            outputs = self.model(x)\n        return self.features\n    \n    def remove_hooks(self):\n        for hook in self.hooks:\n            hook.remove()\n\n# Usage\nmodel = AutoModel.from_pretrained(\"facebook/dinov2-base\")\nextractor = FeatureExtractor(model, layers=[3, 7, 11])\n\n# Extract features\nfeatures = extractor(input_image)\nlayer_3_features = features[\"layer_3\"]\nlayer_7_features = features[\"layer_7\"]\nlayer_11_features = features[\"layer_11\"]\n\n# Clean up\nextractor.remove_hooks()"
  },
  {
    "objectID": "posts/dinov2/index.html#performance-benchmarks",
    "href": "posts/dinov2/index.html#performance-benchmarks",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "DINOv2 achieves excellent results across various vision tasks. Here are typical performance metrics:\n\nImageNet-1K Classification (top-1 accuracy):\n\nDINOv2-Small: ~80.0%\nDINOv2-Base: ~84.5%\nDINOv2-Large: ~86.3%\nDINOv2-Giant: ~87.0%\n\nSemantic Segmentation (ADE20K) (mIoU):\n\nDINOv2-Small: ~47.5%\nDINOv2-Base: ~50.2%\nDINOv2-Large: ~52.5%\nDINOv2-Giant: ~53.8%\n\nObject Detection (COCO) (AP):\n\nDINOv2-Small: ~48.5%\nDINOv2-Base: ~51.3%\nDINOv2-Large: ~53.2%\nDINOv2-Giant: ~54.5%"
  },
  {
    "objectID": "posts/dinov2/index.html#troubleshooting",
    "href": "posts/dinov2/index.html#troubleshooting",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "Out of Memory Errors\n\nReduce batch size\nUse gradient accumulation\nUse a smaller model variant (Small or Base)\nUse mixed precision training\n\n\n# Example of mixed precision training\nfrom torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nfor inputs, targets in train_loader:\n    inputs, targets = inputs.to(device), targets.to(device)\n    \n    optimizer.zero_grad()\n    \n    # Use autocast for mixed precision\n    with autocast():\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n    \n    # Scale loss and backprop\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n\nSlow Inference\n\nUse batch processing\nUse model.eval() and torch.no_grad()\nConsider model distillation or quantization\n\nPoor Performance on Downstream Tasks\n\nEnsure proper data preprocessing\nAdjust learning rates (lower for backbone, higher for heads)\nUse appropriate augmentations\nConsider using a larger variant of DINOv2\n\n\n\n\n\n\nVisualize model attention maps to understand what the model focuses on:\n\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom PIL import Image\nimport torchvision.transforms as T\n\ndef get_attention_map(model, img_tensor):\n    model.eval()\n    with torch.no_grad():\n        outputs = model(img_tensor.unsqueeze(0), output_attentions=True)\n    \n    # Get attention weights from the last layer\n    att_mat = outputs.attentions[-1]\n    \n    # Average attention across heads\n    att_mat = att_mat.mean(dim=1)\n    \n    # Extract attention for cls token to patch tokens\n    cls_att_map = att_mat[0, 0, 1:].reshape(14, 14)\n    \n    return cls_att_map.cpu().numpy()\n\n# Load and preprocess image\nimage = Image.open(\"path/to/image.jpg\").convert(\"RGB\")\ntransform = T.Compose([\n    T.Resize((224, 224)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\nimg_tensor = transform(image).to(device)\n\n# Get attention map\nfrom transformers import AutoModel\nmodel = AutoModel.from_pretrained(\"facebook/dinov2-base\", output_attentions=True)\nmodel.to(device)\nattention_map = get_attention_map(model, img_tensor)\n\n# Visualize\nplt.figure(figsize=(10, 10))\nplt.imshow(image.resize((224, 224)))\nplt.imshow(attention_map, alpha=0.5, cmap='jet')\nplt.axis('off')\nplt.colorbar()\nplt.savefig('attention_map.png')\nplt.close()\nThis guide should help you get started with DINOv2 and explore its capabilities for various computer vision tasks. As a self-supervised vision foundation model, DINOv2 provides a strong starting point for numerous applications with minimal labeled data requirements."
  },
  {
    "objectID": "posts/dino-v2-explained/index.html",
    "href": "posts/dino-v2-explained/index.html",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "In 2023, Meta AI Research unveiled DINOv2 (Self-Distillation with No Labels v2), a breakthrough in self-supervised visual learning that produces remarkably versatile and robust visual features. This article provides a detailed exploration of DINOv2’s architecture and training methodology, explaining how it achieves state-of-the-art performance across diverse visual tasks without task-specific supervision.\n\n\n\nAt the heart of DINOv2 is the Vision Transformer (ViT) architecture, which has proven highly effective for computer vision tasks. DINOv2 specifically uses:\n\n\n\nViT-S/14: Small model (22M parameters)\nViT-B/14: Base model (87M parameters)\n\nViT-L/14: Large model (304M parameters)\nViT-g/14: Giant model (1.1B parameters)\n\nThe “/14” indicates a patch size of 14×14 pixels. These patches are how images are tokenized before being processed by the transformer.\n\n\n\nDINOv2 incorporates several architectural improvements over the original DINO:\n\nImproved Layer Normalization: Uses a modified version of layer normalization that enhances stability during training at scale.\nSwiGLU Activation: Replaces standard ReLU or GELU activations with SwiGLU, which improves representation quality.\nRegister Tokens: Additional learnable tokens (alongside the [CLS] token) that capture different aspects of image information.\nAttention Bias: Incorporates relative position embeddings through attention biases instead of absolute positional encodings.\nPost-Normalization: Places the layer normalization after the multi-head attention and feed-forward blocks rather than before them.\n\n\n\n\n\nDINOv2’s training methodology centers around self-distillation, where a model essentially teaches itself. This is implemented through a student-teacher framework:\n\n\n\nStudent Network: The network being trained, updated via backpropagation\nTeacher Network: An exponential moving average (EMA) of the student’s parameters\nBoth networks share the same architecture but different parameters\n\nThis approach creates a moving target that continuously evolves as training progresses, preventing trivial solutions where the network collapses to outputting the same representation for all inputs.\n\n\n\nA key component of DINOv2’s training is its sophisticated multi-crop approach:\n\nGlobal Views: Two large crops covering significant portions of the image (224×224 pixels)\nLocal Views: Multiple smaller crops capturing image details (96×96 pixels)\n\nThe student network processes both global and local views, while the teacher network only processes global views. This forces the model to learn both global context and local details.\n\n\n\nThe training objective is a cross-entropy loss that encourages the student’s output distribution for local views to match the teacher’s output distribution for global views of the same image. Mathematically:\nL = H(Pt(g), Ps(l))\nWhere: - H is the cross-entropy - Pt(g) is the teacher’s prediction on global views - Ps(l) is the student’s prediction on local views\nThe teacher’s outputs are sharpened using a temperature parameter that gradually decreases throughout training, making the targets increasingly focused on specific features.\n\n\n\n\nDINOv2’s impressive performance comes not just from architecture but from meticulous data preparation:\n\n\nThe researchers curated a high-quality dataset of 142 million images from publicly available sources, with careful filtering to remove:\n\nDuplicate images\nLow-quality content\nInappropriate material\nText-heavy images\nHuman faces\n\n\n\n\nDuring training, DINOv2 employs a robust augmentation strategy:\n\nRandom resized cropping: Different sized views of the same image\nRandom horizontal flips: Mirroring images horizontally\nColor jittering: Altering brightness, contrast, saturation, and hue\nGaussian blur: Adding controlled blur to some views\nSolarization: Inverting pixels above a threshold (applied selectively)\n\nThese augmentations create diverse views while preserving the semantic content, forcing the model to learn invariance to these transformations.\n\n\n\n\nTraining a model of DINOv2’s scale requires sophisticated distributed computing approaches:\n\n\n\nOptimizer: AdamW with a cosine learning rate schedule\nGradient Accumulation: Used to handle effectively larger batch sizes\nMixed Precision: FP16 calculations to speed up training\nSharding: Model parameters distributed across multiple GPUs\n\n\n\n\nDINOv2 uses enormous effective batch sizes (up to 65,536 images) by leveraging distributed training across hundreds of GPUs. This large batch size is crucial for learning high-quality representations.\n\n\n\n\nTo prevent representation collapse and ensure diverse, meaningful features, DINOv2 employs:\n\nCentering: Ensuring the average output across the batch remains close to zero\nSharpening: Gradually decreasing the temperature parameter of the teacher’s softmax\nDALL-E VAE Integration: Using a pre-trained DALL-E VAE to improve representation quality\nWeight Decay: Applied differently to various components of the model\n\n\n\n\nAfter training, DINOv2 can be used in different ways:\n\n\n\n[CLS] Token: The class token representation serves as a global image descriptor\nRegister Tokens: Multiple specialized tokens that capture different aspects of the image\nPatch Tokens: Local features corresponding to specific regions of the image\n\n\n\n\nThe researchers also created smaller, distilled versions of DINOv2 that maintain much of the performance while requiring significantly fewer computational resources for deployment.\n\n\n\n\nDINOv2 represents a remarkable achievement in self-supervised visual learning. Its sophisticated architecture and training methodology enable it to learn general-purpose visual features that transfer exceptionally well across diverse tasks. The careful balance of architectural innovations, data curation, and training techniques creates a visual representation system that approaches the versatility and power that we’ve seen in large language models.\nThe success of DINOv2 highlights how self-supervised learning can leverage vast amounts of unlabeled data to create foundation models for computer vision that may eventually eliminate the need for task-specific supervised training in many applications."
  },
  {
    "objectID": "posts/dino-v2-explained/index.html#architectural-foundation-vision-transformers",
    "href": "posts/dino-v2-explained/index.html#architectural-foundation-vision-transformers",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "At the heart of DINOv2 is the Vision Transformer (ViT) architecture, which has proven highly effective for computer vision tasks. DINOv2 specifically uses:\n\n\n\nViT-S/14: Small model (22M parameters)\nViT-B/14: Base model (87M parameters)\n\nViT-L/14: Large model (304M parameters)\nViT-g/14: Giant model (1.1B parameters)\n\nThe “/14” indicates a patch size of 14×14 pixels. These patches are how images are tokenized before being processed by the transformer.\n\n\n\nDINOv2 incorporates several architectural improvements over the original DINO:\n\nImproved Layer Normalization: Uses a modified version of layer normalization that enhances stability during training at scale.\nSwiGLU Activation: Replaces standard ReLU or GELU activations with SwiGLU, which improves representation quality.\nRegister Tokens: Additional learnable tokens (alongside the [CLS] token) that capture different aspects of image information.\nAttention Bias: Incorporates relative position embeddings through attention biases instead of absolute positional encodings.\nPost-Normalization: Places the layer normalization after the multi-head attention and feed-forward blocks rather than before them."
  },
  {
    "objectID": "posts/dino-v2-explained/index.html#training-methodology-self-distillation-framework",
    "href": "posts/dino-v2-explained/index.html#training-methodology-self-distillation-framework",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "DINOv2’s training methodology centers around self-distillation, where a model essentially teaches itself. This is implemented through a student-teacher framework:\n\n\n\nStudent Network: The network being trained, updated via backpropagation\nTeacher Network: An exponential moving average (EMA) of the student’s parameters\nBoth networks share the same architecture but different parameters\n\nThis approach creates a moving target that continuously evolves as training progresses, preventing trivial solutions where the network collapses to outputting the same representation for all inputs.\n\n\n\nA key component of DINOv2’s training is its sophisticated multi-crop approach:\n\nGlobal Views: Two large crops covering significant portions of the image (224×224 pixels)\nLocal Views: Multiple smaller crops capturing image details (96×96 pixels)\n\nThe student network processes both global and local views, while the teacher network only processes global views. This forces the model to learn both global context and local details.\n\n\n\nThe training objective is a cross-entropy loss that encourages the student’s output distribution for local views to match the teacher’s output distribution for global views of the same image. Mathematically:\nL = H(Pt(g), Ps(l))\nWhere: - H is the cross-entropy - Pt(g) is the teacher’s prediction on global views - Ps(l) is the student’s prediction on local views\nThe teacher’s outputs are sharpened using a temperature parameter that gradually decreases throughout training, making the targets increasingly focused on specific features."
  },
  {
    "objectID": "posts/dino-v2-explained/index.html#data-curation-and-processing",
    "href": "posts/dino-v2-explained/index.html#data-curation-and-processing",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "DINOv2’s impressive performance comes not just from architecture but from meticulous data preparation:\n\n\nThe researchers curated a high-quality dataset of 142 million images from publicly available sources, with careful filtering to remove:\n\nDuplicate images\nLow-quality content\nInappropriate material\nText-heavy images\nHuman faces\n\n\n\n\nDuring training, DINOv2 employs a robust augmentation strategy:\n\nRandom resized cropping: Different sized views of the same image\nRandom horizontal flips: Mirroring images horizontally\nColor jittering: Altering brightness, contrast, saturation, and hue\nGaussian blur: Adding controlled blur to some views\nSolarization: Inverting pixels above a threshold (applied selectively)\n\nThese augmentations create diverse views while preserving the semantic content, forcing the model to learn invariance to these transformations."
  },
  {
    "objectID": "posts/dino-v2-explained/index.html#distributed-training-strategy",
    "href": "posts/dino-v2-explained/index.html#distributed-training-strategy",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "Training a model of DINOv2’s scale requires sophisticated distributed computing approaches:\n\n\n\nOptimizer: AdamW with a cosine learning rate schedule\nGradient Accumulation: Used to handle effectively larger batch sizes\nMixed Precision: FP16 calculations to speed up training\nSharding: Model parameters distributed across multiple GPUs\n\n\n\n\nDINOv2 uses enormous effective batch sizes (up to 65,536 images) by leveraging distributed training across hundreds of GPUs. This large batch size is crucial for learning high-quality representations."
  },
  {
    "objectID": "posts/dino-v2-explained/index.html#regularization-techniques",
    "href": "posts/dino-v2-explained/index.html#regularization-techniques",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "To prevent representation collapse and ensure diverse, meaningful features, DINOv2 employs:\n\nCentering: Ensuring the average output across the batch remains close to zero\nSharpening: Gradually decreasing the temperature parameter of the teacher’s softmax\nDALL-E VAE Integration: Using a pre-trained DALL-E VAE to improve representation quality\nWeight Decay: Applied differently to various components of the model"
  },
  {
    "objectID": "posts/dino-v2-explained/index.html#feature-extraction-and-deployment",
    "href": "posts/dino-v2-explained/index.html#feature-extraction-and-deployment",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "After training, DINOv2 can be used in different ways:\n\n\n\n[CLS] Token: The class token representation serves as a global image descriptor\nRegister Tokens: Multiple specialized tokens that capture different aspects of the image\nPatch Tokens: Local features corresponding to specific regions of the image\n\n\n\n\nThe researchers also created smaller, distilled versions of DINOv2 that maintain much of the performance while requiring significantly fewer computational resources for deployment."
  },
  {
    "objectID": "posts/dino-v2-explained/index.html#conclusion",
    "href": "posts/dino-v2-explained/index.html#conclusion",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "DINOv2 represents a remarkable achievement in self-supervised visual learning. Its sophisticated architecture and training methodology enable it to learn general-purpose visual features that transfer exceptionally well across diverse tasks. The careful balance of architectural innovations, data curation, and training techniques creates a visual representation system that approaches the versatility and power that we’ve seen in large language models.\nThe success of DINOv2 highlights how self-supervised learning can leverage vast amounts of unlabeled data to create foundation models for computer vision that may eventually eliminate the need for task-specific supervised training in many applications."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "👁️ Welcome to My Computer Vision Blog!\n\nThis is the first post in this blog.\nHello and welcome!\nI’m thrilled to kick off this blog dedicated to exploring the fascinating world of computer vision — a field where machines learn to see, interpret, and understand the visual world around us. Whether you’re a seasoned AI researcher, an aspiring developer, or simply curious about how technology can “see,” you’ll find something valuable here.\nFrom image processing techniques to deep learning breakthroughs, from real-world applications to hands-on tutorials — this blog will cover it all. My goal is to make computer vision approachable, insightful, and exciting for everyone.\nSo, whether you’re here to learn, build, or stay on top of the latest innovations, I’m glad to have you along for the journey. Let’s dive into the visual future together!\nStay tuned, and let’s make machines see the world like never before. 🚀\nCheers,  Krishna"
  },
  {
    "objectID": "posts/vision-transformers/index.html",
    "href": "posts/vision-transformers/index.html",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Vision Transformers (ViT) represent a significant paradigm shift in computer vision, applying the transformer architecture initially developed for NLP to image processing tasks. This guide walks through implementing a Vision Transformer from scratch using PyTorch.\n\n\n\nIntroduction to Vision Transformers\nUnderstanding the Architecture\nImplementation\n\nImage Patching\nPatch Embedding\nPosition Embedding\nTransformer Encoder\nMLP Head\n\nTraining the Model\nInference and Usage\nOptimization Techniques\nAdvanced Variants\n\n\n\n\nVision Transformers (ViT) were introduced in the paper “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale” by Dosovitskiy et al. in 2020. The core idea is to treat an image as a sequence of patches, similar to how words are treated in NLP, and process them using a transformer encoder.\nKey advantages of ViTs include: - Global receptive field from the start - Ability to capture long-range dependencies - Scalability to large datasets - No inductive bias towards local processing (unlike CNNs)\n\n\n\nThe ViT architecture consists of the following components:\n\nImage Patching: Dividing the input image into fixed-size patches\nPatch Embedding: Linear projection of flattened patches\nPosition Embedding: Adding positional information\nTransformer Encoder: Self-attention and feed-forward layers\nMLP Head: Final classification layer\n\n\n\n\n\nLet’s implement each component of the Vision Transformer step by step.\n\n\nFirst, we need to divide the input image into fixed-size patches. For a typical ViT, these are 16×16 pixel patches.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n        super().__init__()\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.num_patches = (image_size // patch_size) ** 2\n        \n        # Convert image into patches and embed them\n        # Instead of using einops, we'll use standard PyTorch operations\n        self.projection = nn.Conv2d(\n            in_channels, embed_dim, \n            kernel_size=patch_size, stride=patch_size\n        )\n        \n    def forward(self, x):\n        # x: (batch_size, channels, height, width)\n        # Convert image into patches using convolution\n        x = self.projection(x)  # (batch_size, embed_dim, grid_height, grid_width)\n        \n        # Flatten spatial dimensions and transpose to get \n        # (batch_size, num_patches, embed_dim)\n        batch_size = x.shape[0]\n        x = x.flatten(2)  # (batch_size, embed_dim, num_patches)\n        x = x.transpose(1, 2)  # (batch_size, num_patches, embed_dim)\n        \n        return x\n\n\n\nAfter patching, we need to add a learnable class token and position embeddings.\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, num_classes, \n                 embed_dim, depth, num_heads, mlp_ratio=4, \n                 dropout_rate=0.1):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(\n            image_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embedding.num_patches\n        \n        # Class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # Position embedding for patches + class token\n        self.pos_embedding = nn.Parameter(\n            torch.zeros(1, self.num_patches + 1, embed_dim))\n        \n        self.dropout = nn.Dropout(dropout_rate)\n\n\n\nThe position embeddings are added to provide spatial information:\n    def forward(self, x):\n        # Get patch embeddings\n        x = self.patch_embedding(x)  # (B, num_patches, embed_dim)\n        \n        # Add class token\n        batch_size = x.shape[0]\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (B, 1, embed_dim)\n        x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches + 1, embed_dim)\n        \n        # Add position embedding\n        x = x + self.pos_embedding\n        x = self.dropout(x)\n\n\n\nNext, let’s implement the transformer encoder blocks:\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout_rate=0.1):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        \n        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout_rate)\n        \n    def forward(self, x):\n        batch_size, seq_len, embed_dim = x.shape\n        \n        # Get query, key, and value projections\n        qkv = self.qkv(x)\n        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, H, N, D)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # Each is (B, H, N, D)\n        \n        # Attention\n        attn = (q @ k.transpose(-2, -1)) * self.scale  # (B, H, N, N)\n        attn = attn.softmax(dim=-1)\n        attn = self.dropout(attn)\n        \n        # Apply attention to values\n        out = (attn @ v).transpose(1, 2)  # (B, N, H, D)\n        out = out.reshape(batch_size, seq_len, embed_dim)  # (B, N, E)\n        out = self.proj(out)\n        \n        return out\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, embed_dim, num_heads, mlp_ratio=4, dropout_rate=0.1):\n        super().__init__()\n        \n        # Layer normalization\n        self.norm1 = nn.LayerNorm(embed_dim)\n        \n        # Multi-head self-attention\n        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout_rate)\n        self.dropout1 = nn.Dropout(dropout_rate)\n        \n        # Layer normalization\n        self.norm2 = nn.LayerNorm(embed_dim)\n        \n        # MLP block\n        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, mlp_hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(mlp_hidden_dim, embed_dim),\n            nn.Dropout(dropout_rate)\n        )\n        \n    def forward(self, x):\n        # Apply layer normalization and self-attention\n        attn_output = self.attn(self.norm1(x))\n        x = x + self.dropout1(attn_output)\n        \n        # Apply MLP block with residual connection\n        x = x + self.mlp(self.norm2(x))\n        return x\nNow, let’s update our main ViT class to include the transformer encoder blocks:\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, num_classes, \n                 embed_dim, depth, num_heads, mlp_ratio=4, \n                 dropout_rate=0.1):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(\n            image_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embedding.num_patches\n        \n        # Class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # Position embedding for patches + class token\n        self.pos_embedding = nn.Parameter(\n            torch.zeros(1, self.num_patches + 1, embed_dim))\n        \n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # Transformer encoder blocks\n        self.transformer_blocks = nn.ModuleList([\n            TransformerEncoder(embed_dim, num_heads, mlp_ratio, dropout_rate)\n            for _ in range(depth)\n        ])\n        \n        # Layer normalization\n        self.norm = nn.LayerNorm(embed_dim)\n\n\n\nFinally, let’s add the classification head and complete the forward pass:\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, num_classes, \n                 embed_dim, depth, num_heads, mlp_ratio=4, \n                 dropout_rate=0.1):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(\n            image_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embedding.num_patches\n        \n        # Class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # Position embedding for patches + class token\n        self.pos_embedding = nn.Parameter(\n            torch.zeros(1, self.num_patches + 1, embed_dim))\n        \n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # Transformer encoder blocks\n        self.transformer_blocks = nn.ModuleList([\n            TransformerEncoder(embed_dim, num_heads, mlp_ratio, dropout_rate)\n            for _ in range(depth)\n        ])\n        \n        # Layer normalization\n        self.norm = nn.LayerNorm(embed_dim)\n        \n        # Classification head\n        self.mlp_head = nn.Linear(embed_dim, num_classes)\n        \n        # Initialize weights\n        self._init_weights()\n        \n    def _init_weights(self):\n        # Initialize patch embedding and MLP heads\n        nn.init.normal_(self.cls_token, std=0.02)\n        nn.init.normal_(self.pos_embedding, std=0.02)\n        \n    def forward(self, x):\n        # Get patch embeddings\n        x = self.patch_embedding(x)  # (B, num_patches, embed_dim)\n        \n        # Add class token\n        batch_size = x.shape[0]\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (B, 1, embed_dim)\n        x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches + 1, embed_dim)\n        \n        # Add position embedding\n        x = x + self.pos_embedding\n        x = self.dropout(x)\n        \n        # Apply transformer blocks\n        for block in self.transformer_blocks:\n            x = block(x)\n        \n        # Apply final layer normalization\n        x = self.norm(x)\n        \n        # Take class token for classification\n        cls_token_final = x[:, 0]\n        \n        # Classification\n        return self.mlp_head(cls_token_final)\n\n\n\n\nLet’s implement a training function for our Vision Transformer:\ndef train_vit(model, train_loader, optimizer, criterion, device, epochs=10):\n    model.train()\n    for epoch in range(epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            # Zero the gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            \n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n            \n            # Statistics\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n            \n            # Print statistics every 100 batches\n            if batch_idx % 100 == 99:\n                print(f'Epoch: {epoch+1}, Batch: {batch_idx+1}, '\n                      f'Loss: {running_loss/100:.3f}, '\n                      f'Accuracy: {100.*correct/total:.2f}%')\n                running_loss = 0.0\n                \n        # Epoch statistics\n        print(f'Epoch {epoch+1} completed. '\n              f'Accuracy: {100.*correct/total:.2f}%')\nExample usage:\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Create ViT model\nmodel = VisionTransformer(\n    image_size=224,\n    patch_size=16,\n    in_channels=3,\n    num_classes=10,\n    embed_dim=768,\n    depth=12,\n    num_heads=12,\n    mlp_ratio=4,\n    dropout_rate=0.1\n).to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# Load data\ntransform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = datasets.FakeData(\n    transform=transforms.ToTensor()\n)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n\n# Train model\ntrain_vit(model, train_loader, optimizer, criterion, device, epochs=10)\n\n\n\nHere’s how to use the model for inference:\ndef inference(model, image_tensor, device):\n    model.eval()\n    with torch.no_grad():\n        image_tensor = image_tensor.unsqueeze(0).to(device)  # Add batch dimension\n        output = model(image_tensor)\n        probabilities = F.softmax(output, dim=1)\n        predicted_class = torch.argmax(probabilities, dim=1)\n    return predicted_class.item(), probabilities[0]\n\n# Example usage\nimage = transform(image).to(device)\npredicted_class, probabilities = inference(model, image, device)\nprint(f\"Predicted class: {predicted_class}\")\nprint(f\"Confidence: {probabilities[predicted_class]:.4f}\")\n\n\n\nTo improve the training and performance of ViT models, consider these optimization techniques:\n\n\nThe standard attention implementation can be memory-intensive. You can use a more efficient implementation:\ndef efficient_attention(q, k, v, mask=None):\n    # q, k, v: [batch_size, num_heads, seq_len, head_dim]\n    \n    # Scaled dot-product attention\n    scale = q.size(-1) ** -0.5\n    attention = torch.matmul(q, k.transpose(-2, -1)) * scale  # [B, H, L, L]\n    \n    if mask is not None:\n        attention = attention.masked_fill(mask == 0, -1e9)\n    \n    attention = F.softmax(attention, dim=-1)\n    output = torch.matmul(attention, v)  # [B, H, L, D]\n    \n    return output\n\n\n\nUse mixed precision training to reduce memory usage and increase training speed:\nfrom torch.cuda.amp import autocast, GradScaler\n\ndef train_with_mixed_precision(model, train_loader, optimizer, criterion, device, epochs=10):\n    scaler = GradScaler()\n    model.train()\n    \n    for epoch in range(epochs):\n        running_loss = 0.0\n        \n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Use autocast for mixed precision\n            with autocast():\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n            \n            # Scale gradients and optimize\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            running_loss += loss.item()\n            # Rest of the training loop...\n\n\n\nImplement regularization techniques such as stochastic depth to prevent overfitting:\nclass StochasticDepth(nn.Module):\n    def __init__(self, drop_prob=0.1):\n        super().__init__()\n        self.drop_prob = drop_prob\n        \n    def forward(self, x):\n        if not self.training or self.drop_prob == 0.:\n            return x\n        \n        keep_prob = 1 - self.drop_prob\n        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n        random_tensor.floor_()  # binarize\n        output = x.div(keep_prob) * random_tensor\n        return output\n\n\n\n\nSeveral advanced variants of Vision Transformers have been developed:\n\n\nDeiT introduces a distillation token and a teacher-student strategy:\nclass DeiT(VisionTransformer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        \n        # Distillation token\n        self.dist_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n        \n        # Update position embeddings to include distillation token\n        # Original position embeddings are for [class_token, patches]\n        # New position embeddings are for [class_token, dist_token, patches]\n        num_patches = self.patch_embedding.num_patches\n        new_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 2, self.embed_dim))\n        \n        # Initialize new position embeddings with the original ones\n        # Copy class token position embedding\n        new_pos_embed.data[:, 0:1, :] = self.pos_embedding.data[:, 0:1, :]\n        # Add a new position embedding for distillation token\n        new_pos_embed.data[:, 1:2, :] = self.pos_embedding.data[:, 0:1, :]\n        # Copy patch position embeddings\n        new_pos_embed.data[:, 2:, :] = self.pos_embedding.data[:, 1:, :]\n        \n        self.pos_embedding = new_pos_embed\n        \n        # Additional classification head for distillation\n        self.head_dist = nn.Linear(self.embed_dim, kwargs.get('num_classes', 1000))\n        \n    def forward(self, x):\n        # Get patch embeddings\n        x = self.patch_embedding(x)\n        \n        # Add class and distillation tokens\n        batch_size = x.shape[0]\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n        dist_tokens = self.dist_token.expand(batch_size, -1, -1)\n        x = torch.cat((cls_tokens, dist_tokens, x), dim=1)\n        \n        # Add position embedding and apply dropout\n        x = x + self.pos_embedding\n        x = self.dropout(x)\n        \n        # Apply transformer blocks\n        for block in self.transformer_blocks:\n            x = block(x)\n        \n        # Apply final layer normalization\n        x = self.norm(x)\n        \n        # Get class and distillation tokens\n        cls_token_final = x[:, 0]\n        dist_token_final = x[:, 1]\n        \n        # Apply classification heads\n        x_cls = self.mlp_head(cls_token_final)\n        x_dist = self.head_dist(dist_token_final)\n        \n        if self.training:\n            # During training, return both outputs\n            return x_cls, x_dist\n        else:\n            # During inference, return average\n            return (x_cls + x_dist) / 2\n\n\n\nSwin Transformer introduces hierarchical feature maps and shifted windows:\n# This is a simplified conceptual implementation of the Swin Transformer block\nclass WindowAttention(nn.Module):\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, dropout=0.):\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # (height, width)\n        self.num_heads = num_heads\n        self.scale = (dim // num_heads) ** -0.5\n        \n        # Linear projections\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n        \n        # Define relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))\n            \n        # Generate pair-wise relative position index for each token in the window\n        coords_h = torch.arange(self.window_size[0])\n        coords_w = torch.arange(self.window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        \n        # Calculate relative positions\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        \n        self.register_buffer(\"relative_position_index\", relative_position_index)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, mask=None):\n        B_, N, C = x.shape\n        \n        # Generate QKV matrices\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # 3, B_, num_heads, N, C//num_heads\n        q, k, v = qkv[0], qkv[1], qkv[2]  # each has shape [B_, num_heads, N, C//num_heads]\n        \n        # Scaled dot-product attention\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))  # B_, num_heads, N, N\n        \n        # Add relative position bias\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)]\n        relative_position_bias = relative_position_bias.view(\n            self.window_size[0] * self.window_size[1], \n            self.window_size[0] * self.window_size[1], \n            -1)  # Wh*Ww, Wh*Ww, num_heads\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # num_heads, Wh*Ww, Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0)\n        \n        # Apply mask if needed\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n        \n        # Apply softmax\n        attn = F.softmax(attn, dim=-1)\n        attn = self.dropout(attn)\n        \n        # Apply attention to values\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        \n        return x\n\n\n\n\nVision Transformers represent a significant advancement in computer vision, offering a different approach from traditional CNNs. This guide has covered the essential components for implementing a Vision Transformer from scratch, including image patching, position embeddings, transformer encoders, and classification heads.\nBy understanding these fundamentals, you can implement your own ViT models and experiment with various modifications to improve performance for specific tasks.\n\n\n\n\nDosovitskiy, A., et al. (2020). “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” arXiv:2010.11929.\nTouvron, H., et al. (2021). “Training data-efficient image transformers & distillation through attention.” arXiv:2012.12877.\nLiu, Z., et al. (2021). “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” arXiv:2103.14030."
  },
  {
    "objectID": "posts/vision-transformers/index.html#table-of-contents",
    "href": "posts/vision-transformers/index.html#table-of-contents",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Introduction to Vision Transformers\nUnderstanding the Architecture\nImplementation\n\nImage Patching\nPatch Embedding\nPosition Embedding\nTransformer Encoder\nMLP Head\n\nTraining the Model\nInference and Usage\nOptimization Techniques\nAdvanced Variants"
  },
  {
    "objectID": "posts/vision-transformers/index.html#introduction-to-vision-transformers",
    "href": "posts/vision-transformers/index.html#introduction-to-vision-transformers",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Vision Transformers (ViT) were introduced in the paper “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale” by Dosovitskiy et al. in 2020. The core idea is to treat an image as a sequence of patches, similar to how words are treated in NLP, and process them using a transformer encoder.\nKey advantages of ViTs include: - Global receptive field from the start - Ability to capture long-range dependencies - Scalability to large datasets - No inductive bias towards local processing (unlike CNNs)"
  },
  {
    "objectID": "posts/vision-transformers/index.html#understanding-the-architecture",
    "href": "posts/vision-transformers/index.html#understanding-the-architecture",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "The ViT architecture consists of the following components:\n\nImage Patching: Dividing the input image into fixed-size patches\nPatch Embedding: Linear projection of flattened patches\nPosition Embedding: Adding positional information\nTransformer Encoder: Self-attention and feed-forward layers\nMLP Head: Final classification layer"
  },
  {
    "objectID": "posts/vision-transformers/index.html#implementation",
    "href": "posts/vision-transformers/index.html#implementation",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Let’s implement each component of the Vision Transformer step by step.\n\n\nFirst, we need to divide the input image into fixed-size patches. For a typical ViT, these are 16×16 pixel patches.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n        super().__init__()\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.num_patches = (image_size // patch_size) ** 2\n        \n        # Convert image into patches and embed them\n        # Instead of using einops, we'll use standard PyTorch operations\n        self.projection = nn.Conv2d(\n            in_channels, embed_dim, \n            kernel_size=patch_size, stride=patch_size\n        )\n        \n    def forward(self, x):\n        # x: (batch_size, channels, height, width)\n        # Convert image into patches using convolution\n        x = self.projection(x)  # (batch_size, embed_dim, grid_height, grid_width)\n        \n        # Flatten spatial dimensions and transpose to get \n        # (batch_size, num_patches, embed_dim)\n        batch_size = x.shape[0]\n        x = x.flatten(2)  # (batch_size, embed_dim, num_patches)\n        x = x.transpose(1, 2)  # (batch_size, num_patches, embed_dim)\n        \n        return x\n\n\n\nAfter patching, we need to add a learnable class token and position embeddings.\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, num_classes, \n                 embed_dim, depth, num_heads, mlp_ratio=4, \n                 dropout_rate=0.1):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(\n            image_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embedding.num_patches\n        \n        # Class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # Position embedding for patches + class token\n        self.pos_embedding = nn.Parameter(\n            torch.zeros(1, self.num_patches + 1, embed_dim))\n        \n        self.dropout = nn.Dropout(dropout_rate)\n\n\n\nThe position embeddings are added to provide spatial information:\n    def forward(self, x):\n        # Get patch embeddings\n        x = self.patch_embedding(x)  # (B, num_patches, embed_dim)\n        \n        # Add class token\n        batch_size = x.shape[0]\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (B, 1, embed_dim)\n        x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches + 1, embed_dim)\n        \n        # Add position embedding\n        x = x + self.pos_embedding\n        x = self.dropout(x)\n\n\n\nNext, let’s implement the transformer encoder blocks:\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout_rate=0.1):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        \n        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout_rate)\n        \n    def forward(self, x):\n        batch_size, seq_len, embed_dim = x.shape\n        \n        # Get query, key, and value projections\n        qkv = self.qkv(x)\n        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, H, N, D)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # Each is (B, H, N, D)\n        \n        # Attention\n        attn = (q @ k.transpose(-2, -1)) * self.scale  # (B, H, N, N)\n        attn = attn.softmax(dim=-1)\n        attn = self.dropout(attn)\n        \n        # Apply attention to values\n        out = (attn @ v).transpose(1, 2)  # (B, N, H, D)\n        out = out.reshape(batch_size, seq_len, embed_dim)  # (B, N, E)\n        out = self.proj(out)\n        \n        return out\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, embed_dim, num_heads, mlp_ratio=4, dropout_rate=0.1):\n        super().__init__()\n        \n        # Layer normalization\n        self.norm1 = nn.LayerNorm(embed_dim)\n        \n        # Multi-head self-attention\n        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout_rate)\n        self.dropout1 = nn.Dropout(dropout_rate)\n        \n        # Layer normalization\n        self.norm2 = nn.LayerNorm(embed_dim)\n        \n        # MLP block\n        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, mlp_hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(mlp_hidden_dim, embed_dim),\n            nn.Dropout(dropout_rate)\n        )\n        \n    def forward(self, x):\n        # Apply layer normalization and self-attention\n        attn_output = self.attn(self.norm1(x))\n        x = x + self.dropout1(attn_output)\n        \n        # Apply MLP block with residual connection\n        x = x + self.mlp(self.norm2(x))\n        return x\nNow, let’s update our main ViT class to include the transformer encoder blocks:\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, num_classes, \n                 embed_dim, depth, num_heads, mlp_ratio=4, \n                 dropout_rate=0.1):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(\n            image_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embedding.num_patches\n        \n        # Class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # Position embedding for patches + class token\n        self.pos_embedding = nn.Parameter(\n            torch.zeros(1, self.num_patches + 1, embed_dim))\n        \n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # Transformer encoder blocks\n        self.transformer_blocks = nn.ModuleList([\n            TransformerEncoder(embed_dim, num_heads, mlp_ratio, dropout_rate)\n            for _ in range(depth)\n        ])\n        \n        # Layer normalization\n        self.norm = nn.LayerNorm(embed_dim)\n\n\n\nFinally, let’s add the classification head and complete the forward pass:\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, num_classes, \n                 embed_dim, depth, num_heads, mlp_ratio=4, \n                 dropout_rate=0.1):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(\n            image_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embedding.num_patches\n        \n        # Class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # Position embedding for patches + class token\n        self.pos_embedding = nn.Parameter(\n            torch.zeros(1, self.num_patches + 1, embed_dim))\n        \n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # Transformer encoder blocks\n        self.transformer_blocks = nn.ModuleList([\n            TransformerEncoder(embed_dim, num_heads, mlp_ratio, dropout_rate)\n            for _ in range(depth)\n        ])\n        \n        # Layer normalization\n        self.norm = nn.LayerNorm(embed_dim)\n        \n        # Classification head\n        self.mlp_head = nn.Linear(embed_dim, num_classes)\n        \n        # Initialize weights\n        self._init_weights()\n        \n    def _init_weights(self):\n        # Initialize patch embedding and MLP heads\n        nn.init.normal_(self.cls_token, std=0.02)\n        nn.init.normal_(self.pos_embedding, std=0.02)\n        \n    def forward(self, x):\n        # Get patch embeddings\n        x = self.patch_embedding(x)  # (B, num_patches, embed_dim)\n        \n        # Add class token\n        batch_size = x.shape[0]\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (B, 1, embed_dim)\n        x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches + 1, embed_dim)\n        \n        # Add position embedding\n        x = x + self.pos_embedding\n        x = self.dropout(x)\n        \n        # Apply transformer blocks\n        for block in self.transformer_blocks:\n            x = block(x)\n        \n        # Apply final layer normalization\n        x = self.norm(x)\n        \n        # Take class token for classification\n        cls_token_final = x[:, 0]\n        \n        # Classification\n        return self.mlp_head(cls_token_final)"
  },
  {
    "objectID": "posts/vision-transformers/index.html#training-the-model",
    "href": "posts/vision-transformers/index.html#training-the-model",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Let’s implement a training function for our Vision Transformer:\ndef train_vit(model, train_loader, optimizer, criterion, device, epochs=10):\n    model.train()\n    for epoch in range(epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            # Zero the gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            \n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n            \n            # Statistics\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n            \n            # Print statistics every 100 batches\n            if batch_idx % 100 == 99:\n                print(f'Epoch: {epoch+1}, Batch: {batch_idx+1}, '\n                      f'Loss: {running_loss/100:.3f}, '\n                      f'Accuracy: {100.*correct/total:.2f}%')\n                running_loss = 0.0\n                \n        # Epoch statistics\n        print(f'Epoch {epoch+1} completed. '\n              f'Accuracy: {100.*correct/total:.2f}%')\nExample usage:\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Create ViT model\nmodel = VisionTransformer(\n    image_size=224,\n    patch_size=16,\n    in_channels=3,\n    num_classes=10,\n    embed_dim=768,\n    depth=12,\n    num_heads=12,\n    mlp_ratio=4,\n    dropout_rate=0.1\n).to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# Load data\ntransform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = datasets.FakeData(\n    transform=transforms.ToTensor()\n)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n\n# Train model\ntrain_vit(model, train_loader, optimizer, criterion, device, epochs=10)"
  },
  {
    "objectID": "posts/vision-transformers/index.html#inference-and-usage",
    "href": "posts/vision-transformers/index.html#inference-and-usage",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Here’s how to use the model for inference:\ndef inference(model, image_tensor, device):\n    model.eval()\n    with torch.no_grad():\n        image_tensor = image_tensor.unsqueeze(0).to(device)  # Add batch dimension\n        output = model(image_tensor)\n        probabilities = F.softmax(output, dim=1)\n        predicted_class = torch.argmax(probabilities, dim=1)\n    return predicted_class.item(), probabilities[0]\n\n# Example usage\nimage = transform(image).to(device)\npredicted_class, probabilities = inference(model, image, device)\nprint(f\"Predicted class: {predicted_class}\")\nprint(f\"Confidence: {probabilities[predicted_class]:.4f}\")"
  },
  {
    "objectID": "posts/vision-transformers/index.html#optimization-techniques",
    "href": "posts/vision-transformers/index.html#optimization-techniques",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "To improve the training and performance of ViT models, consider these optimization techniques:\n\n\nThe standard attention implementation can be memory-intensive. You can use a more efficient implementation:\ndef efficient_attention(q, k, v, mask=None):\n    # q, k, v: [batch_size, num_heads, seq_len, head_dim]\n    \n    # Scaled dot-product attention\n    scale = q.size(-1) ** -0.5\n    attention = torch.matmul(q, k.transpose(-2, -1)) * scale  # [B, H, L, L]\n    \n    if mask is not None:\n        attention = attention.masked_fill(mask == 0, -1e9)\n    \n    attention = F.softmax(attention, dim=-1)\n    output = torch.matmul(attention, v)  # [B, H, L, D]\n    \n    return output\n\n\n\nUse mixed precision training to reduce memory usage and increase training speed:\nfrom torch.cuda.amp import autocast, GradScaler\n\ndef train_with_mixed_precision(model, train_loader, optimizer, criterion, device, epochs=10):\n    scaler = GradScaler()\n    model.train()\n    \n    for epoch in range(epochs):\n        running_loss = 0.0\n        \n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Use autocast for mixed precision\n            with autocast():\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n            \n            # Scale gradients and optimize\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            running_loss += loss.item()\n            # Rest of the training loop...\n\n\n\nImplement regularization techniques such as stochastic depth to prevent overfitting:\nclass StochasticDepth(nn.Module):\n    def __init__(self, drop_prob=0.1):\n        super().__init__()\n        self.drop_prob = drop_prob\n        \n    def forward(self, x):\n        if not self.training or self.drop_prob == 0.:\n            return x\n        \n        keep_prob = 1 - self.drop_prob\n        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n        random_tensor.floor_()  # binarize\n        output = x.div(keep_prob) * random_tensor\n        return output"
  },
  {
    "objectID": "posts/vision-transformers/index.html#advanced-variants",
    "href": "posts/vision-transformers/index.html#advanced-variants",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Several advanced variants of Vision Transformers have been developed:\n\n\nDeiT introduces a distillation token and a teacher-student strategy:\nclass DeiT(VisionTransformer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        \n        # Distillation token\n        self.dist_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n        \n        # Update position embeddings to include distillation token\n        # Original position embeddings are for [class_token, patches]\n        # New position embeddings are for [class_token, dist_token, patches]\n        num_patches = self.patch_embedding.num_patches\n        new_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 2, self.embed_dim))\n        \n        # Initialize new position embeddings with the original ones\n        # Copy class token position embedding\n        new_pos_embed.data[:, 0:1, :] = self.pos_embedding.data[:, 0:1, :]\n        # Add a new position embedding for distillation token\n        new_pos_embed.data[:, 1:2, :] = self.pos_embedding.data[:, 0:1, :]\n        # Copy patch position embeddings\n        new_pos_embed.data[:, 2:, :] = self.pos_embedding.data[:, 1:, :]\n        \n        self.pos_embedding = new_pos_embed\n        \n        # Additional classification head for distillation\n        self.head_dist = nn.Linear(self.embed_dim, kwargs.get('num_classes', 1000))\n        \n    def forward(self, x):\n        # Get patch embeddings\n        x = self.patch_embedding(x)\n        \n        # Add class and distillation tokens\n        batch_size = x.shape[0]\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n        dist_tokens = self.dist_token.expand(batch_size, -1, -1)\n        x = torch.cat((cls_tokens, dist_tokens, x), dim=1)\n        \n        # Add position embedding and apply dropout\n        x = x + self.pos_embedding\n        x = self.dropout(x)\n        \n        # Apply transformer blocks\n        for block in self.transformer_blocks:\n            x = block(x)\n        \n        # Apply final layer normalization\n        x = self.norm(x)\n        \n        # Get class and distillation tokens\n        cls_token_final = x[:, 0]\n        dist_token_final = x[:, 1]\n        \n        # Apply classification heads\n        x_cls = self.mlp_head(cls_token_final)\n        x_dist = self.head_dist(dist_token_final)\n        \n        if self.training:\n            # During training, return both outputs\n            return x_cls, x_dist\n        else:\n            # During inference, return average\n            return (x_cls + x_dist) / 2\n\n\n\nSwin Transformer introduces hierarchical feature maps and shifted windows:\n# This is a simplified conceptual implementation of the Swin Transformer block\nclass WindowAttention(nn.Module):\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, dropout=0.):\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # (height, width)\n        self.num_heads = num_heads\n        self.scale = (dim // num_heads) ** -0.5\n        \n        # Linear projections\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n        \n        # Define relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))\n            \n        # Generate pair-wise relative position index for each token in the window\n        coords_h = torch.arange(self.window_size[0])\n        coords_w = torch.arange(self.window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        \n        # Calculate relative positions\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        \n        self.register_buffer(\"relative_position_index\", relative_position_index)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, mask=None):\n        B_, N, C = x.shape\n        \n        # Generate QKV matrices\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # 3, B_, num_heads, N, C//num_heads\n        q, k, v = qkv[0], qkv[1], qkv[2]  # each has shape [B_, num_heads, N, C//num_heads]\n        \n        # Scaled dot-product attention\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))  # B_, num_heads, N, N\n        \n        # Add relative position bias\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)]\n        relative_position_bias = relative_position_bias.view(\n            self.window_size[0] * self.window_size[1], \n            self.window_size[0] * self.window_size[1], \n            -1)  # Wh*Ww, Wh*Ww, num_heads\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # num_heads, Wh*Ww, Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0)\n        \n        # Apply mask if needed\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n        \n        # Apply softmax\n        attn = F.softmax(attn, dim=-1)\n        attn = self.dropout(attn)\n        \n        # Apply attention to values\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        \n        return x"
  },
  {
    "objectID": "posts/vision-transformers/index.html#conclusion",
    "href": "posts/vision-transformers/index.html#conclusion",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Vision Transformers represent a significant advancement in computer vision, offering a different approach from traditional CNNs. This guide has covered the essential components for implementing a Vision Transformer from scratch, including image patching, position embeddings, transformer encoders, and classification heads.\nBy understanding these fundamentals, you can implement your own ViT models and experiment with various modifications to improve performance for specific tasks."
  },
  {
    "objectID": "posts/vision-transformers/index.html#references",
    "href": "posts/vision-transformers/index.html#references",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Dosovitskiy, A., et al. (2020). “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” arXiv:2010.11929.\nTouvron, H., et al. (2021). “Training data-efficient image transformers & distillation through attention.” arXiv:2012.12877.\nLiu, Z., et al. (2021). “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” arXiv:2103.14030."
  },
  {
    "objectID": "posts/pandas-to-polars/index.html",
    "href": "posts/pandas-to-polars/index.html",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "As datasets grow in size and complexity, performance and efficiency become critical in data processing. While Pandas has long been the go-to library for data manipulation in Python, it can struggle with speed and memory usage, especially on large datasets. Polars, a newer DataFrame library written in Rust, offers a faster, more memory-efficient alternative with support for lazy evaluation and multi-threading.\nThis guide explores how to convert Pandas DataFrames to Polars, and highlights key differences in syntax, performance, and functionality. Whether you’re looking to speed up your data workflows or just exploring modern tools, understanding the transition from Pandas to Polars is a valuable step.\n\n\n\nInstallation and Setup\nCreating DataFrames\nBasic Operations\nFiltering Data\nGrouping and Aggregation\nJoining/Merging DataFrames\nHandling Missing Values\nString Operations\nTime Series Operations\nPerformance Comparison\nAPI Philosophy Differences\nMigration Guide\n\n\n\n\n\n\n\n# Import pandas\nimport pandas as pd\n\n\n\n\n\n# Import polars\nimport polars as pl\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\n\n# Create DataFrame from dictionary\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'age': [25, 30, 35, 40],\n    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n}\ndf_pd = pd.DataFrame(data)\nprint(df_pd)\n\n      name  age         city\n0    Alice   25     New York\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n3    David   40      Houston\n\n\n\n\n\n\nimport polars as pl\n\n# Create DataFrame from dictionary\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'age': [25, 30, 35, 40],\n    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n}\ndf_pl = pl.DataFrame(data)\nprint(df_pl)\n\nshape: (4, 3)\n┌─────────┬─────┬─────────────┐\n│ name    ┆ age ┆ city        │\n│ ---     ┆ --- ┆ ---         │\n│ str     ┆ i64 ┆ str         │\n╞═════════╪═════╪═════════════╡\n│ Alice   ┆ 25  ┆ New York    │\n│ Bob     ┆ 30  ┆ Los Angeles │\n│ Charlie ┆ 35  ┆ Chicago     │\n│ David   ┆ 40  ┆ Houston     │\n└─────────┴─────┴─────────────┘\n\n\n\n\n\n\n\n\n\n\n\n\n# Select a single column (returns Series)\nseries = df_pd['name']\n\n# Select multiple columns\ndf_subset = df_pd[['name', 'age']]\n\n\n\n\n\n# Select a single column (returns Series)\nseries = df_pl['name']\n# Alternative method\nseries = df_pl.select(pl.col('name')).to_series()\n\n# Select multiple columns\ndf_subset = df_pl.select(['name', 'age'])\n# Alternative method\ndf_subset = df_pl.select(pl.col(['name', 'age']))\n\n\n\n\n\n\n\n\n# Add a new column\ndf_pd['is_adult'] = df_pd['age'] &gt;= 18\n\n# Using assign (creates a new DataFrame)\ndf_pd = df_pd.assign(age_squared=df_pd['age'] ** 2)\n\n\n\n\n\n# Add a new column\ndf_pl = df_pl.with_columns(\n    pl.when(pl.col('age') &gt;= 18).then(True).otherwise(False).alias('is_adult')\n)\n\n# Creating derived columns \ndf_pl = df_pl.with_columns(\n    (pl.col('age') ** 2).alias('age_squared')\n)\n\n# Multiple columns at once\ndf_pl = df_pl.with_columns([\n    pl.col('age').is_null().alias('age_is_null'),\n    (pl.col('age') * 2).alias('age_doubled')\n])\n\n\n\n\n\n\n\n\n# Get summary statistics\nsummary = df_pd.describe()\n\n# Individual statistics\nmean_age = df_pd['age'].mean()\nmedian_age = df_pd['age'].median()\nmin_age = df_pd['age'].min()\nmax_age = df_pd['age'].max()\n\n\n\n\n\n# Get summary statistics\nsummary = df_pl.describe()\n\n# Individual statistics\nmean_age = df_pl.select(pl.col('age').mean()).item()\nmedian_age = df_pl.select(pl.col('age').median()).item()\nmin_age = df_pl.select(pl.col('age').min()).item()\nmax_age = df_pl.select(pl.col('age').max()).item()\n\n\n\n\n\n\n\n\n\n\n\n# Filter rows\nadults = df_pd[df_pd['age'] &gt;= 18]\n\n# Multiple conditions\nfiltered = df_pd[(df_pd['age'] &gt; 30) & (df_pd['city'] == 'Chicago')]\n\n\n\n\n\n# Filter rows\nadults = df_pl.filter(pl.col('age') &gt;= 18)\n\n# Multiple conditions\nfiltered = df_pl.filter((pl.col('age') &gt; 30) & (pl.col('city') == 'Chicago'))\n\n\n\n\n\n\n\n\n# Filter with OR conditions\ndf_filtered = df_pd[(df_pd['city'] == 'New York') | (df_pd['city'] == 'Chicago')]\n\n# Using isin\ncities = ['New York', 'Chicago']\ndf_filtered = df_pd[df_pd['city'].isin(cities)]\n\n# String contains\ndf_filtered = df_pd[df_pd['name'].str.contains('li')]\n\n\n\n\n\n# Filter with OR conditions\ndf_filtered = df_pl.filter((pl.col('city') == 'New York') | (pl.col('city') == 'Chicago'))\n\n# Using is_in\ncities = ['New York', 'Chicago']\ndf_filtered = df_pl.filter(pl.col('city').is_in(cities))\n\n# String contains\ndf_filtered = df_pl.filter(pl.col('name').str.contains('li'))\n\n\n\n\n\n\n\n\n\n\n\n# Group by one column and aggregate\ncity_stats = df_pd.groupby('city').agg({\n    'age': ['mean', 'min', 'max', 'count']\n})\n\n# Reset index for flat DataFrame\ncity_stats = city_stats.reset_index()\n\n\n\n\n\n# Group by one column and aggregate\ncity_stats = df_pl.group_by('city').agg([\n    pl.col('age').mean().alias('age_mean'),\n    pl.col('age').min().alias('age_min'),\n    pl.col('age').max().alias('age_max'),\n    pl.col('age').count().alias('age_count')\n])\n\n\n\n\n\n\n\n\n\n\n\n# Create another DataFrame\nemployee_data = {\n    'emp_id': [1, 2, 3, 4],\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'dept': ['HR', 'IT', 'Finance', 'IT']\n}\nemployee_df_pd = pd.DataFrame(employee_data)\n\nsalary_data = {\n    'emp_id': [1, 2, 3, 5],\n    'salary': [50000, 60000, 70000, 80000]\n}\nsalary_df_pd = pd.DataFrame(salary_data)\n\n# Inner join\nmerged_df = employee_df_pd.merge(\n    salary_df_pd,\n    on='emp_id',\n    how='inner'\n)\n\n\n\n\n\n# Create another DataFrame\nemployee_data = {\n    'emp_id': [1, 2, 3, 4],\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'dept': ['HR', 'IT', 'Finance', 'IT']\n}\nemployee_df_pl = pl.DataFrame(employee_data)\n\nsalary_data = {\n    'emp_id': [1, 2, 3, 5],\n    'salary': [50000, 60000, 70000, 80000]\n}\nsalary_df_pl = pl.DataFrame(salary_data)\n\n# Inner join\nmerged_df = employee_df_pl.join(\n    salary_df_pl,\n    on='emp_id',\n    how='inner'\n)\n\n\n\n\n\n\n\n\n# Left join\nleft_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='left')\n\n# Right join\nright_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='right')\n\n# Outer join\nouter_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='outer')\n\n\n\n\n\n# Left join\nleft_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='left')\n\n# Right join\nright_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='right')\n\n# Outer join\nouter_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='full')\n\n\n\n\n\n\n\n\n\n\n\n# Check for missing values\nmissing_count = df_pd.isnull().sum()\n\n# Check if any column has missing values\nhas_missing = df_pd.isnull().any().any()\n\n\n\n\n\n# Check for missing values\nmissing_count = df_pl.null_count()\n\n# Check if specific column has missing values\nhas_missing = df_pl.select(pl.col('age').is_null().any()).item()\n\n\n\n\n\n\n\n\n# Drop rows with any missing values\ndf_pd_clean = df_pd.dropna()\n\n# Fill missing values\ndf_pd_filled = df_pd.fillna({\n    'age': 0,\n    'city': 'Unknown'\n})\n\n# Forward fill\ndf_pd_ffill = df_pd.ffill()\n\n\n\n\n\n# Drop rows with any missing values\ndf_pl_clean = df_pl.drop_nulls()\n\n# Fill missing values\ndf_pl_filled = df_pl.with_columns([\n    pl.col('age').fill_null(0),\n    pl.col('city').fill_null('Unknown')\n])\n\n# Forward fill\ndf_pl_ffill = df_pl.with_columns([\n    pl.col('age').fill_null(strategy='forward'),\n    pl.col('city').fill_null(strategy='forward')\n])\n\n\n\n\n\n\n\n\n\n\n\n# Convert to uppercase\ndf_pd['name_upper'] = df_pd['name'].str.upper()\n\n# Get string length\ndf_pd['name_length'] = df_pd['name'].str.len()\n\n# Extract substring\ndf_pd['name_first_char'] = df_pd['name'].str[0]\n\n# Replace substrings\ndf_pd['city_replaced'] = df_pd['city'].str.replace('New', 'Old')\n\n\n\n\n\n# Convert to uppercase\ndf_pl = df_pl.with_columns(pl.col('name').str.to_uppercase().alias('name_upper'))\n\n# Get string length\ndf_pl = df_pl.with_columns(pl.col('name').str.len_chars().alias('name_length'))\n\n# Extract substring \ndf_pl = df_pl.with_columns(pl.col('name').str.slice(0, 1).alias('name_first_char'))\n\n# Replace substrings\ndf_pl = df_pl.with_columns(pl.col('city').str.replace('New', 'Old').alias('city_replaced'))\n\n\n\n\n\n\n\n\n# Split string\ndf_pd['first_word'] = df_pd['city'].str.split(' ').str[0]\n\n# Pattern matching\nhas_new = df_pd['city'].str.contains('New')\n\n# Extract with regex\ndf_pd['extracted'] = df_pd['city'].str.extract(r'(\\w+)\\s')\n\n\n\n\n\n# Split string\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.split(' ').list.get(0).alias('first_word')\n)\n\n# Pattern matching\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.contains('New').alias('has_new')\n)\n\n# Extract with regex\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.extract(r'(\\w+)\\s').alias('extracted')\n)\n\n\n\n\n\n\n\n\n\n\n\n# Create DataFrame with dates\ndates_pd = pd.DataFrame({\n    'date_str': ['2023-01-01', '2023-02-15', '2023-03-30']\n})\n\n# Parse dates\ndates_pd['date'] = pd.to_datetime(dates_pd['date_str'])\n\n# Extract components\ndates_pd['year'] = dates_pd['date'].dt.year\ndates_pd['month'] = dates_pd['date'].dt.month\ndates_pd['day'] = dates_pd['date'].dt.day\ndates_pd['weekday'] = dates_pd['date'].dt.day_name()\n\n\n\n\n\n# Create DataFrame with dates\ndates_pl = pl.DataFrame({\n    'date_str': ['2023-01-01', '2023-02-15', '2023-03-30']\n})\n\n# Parse dates\ndates_pl = dates_pl.with_columns(\n    pl.col('date_str').str.strptime(pl.Datetime, '%Y-%m-%d').alias('date')\n)\n\n# Extract components\ndates_pl = dates_pl.with_columns([\n    pl.col('date').dt.year().alias('year'),\n    pl.col('date').dt.month().alias('month'),\n    pl.col('date').dt.day().alias('day'),\n    pl.col('date').dt.weekday().replace_strict({\n        0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', \n        3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'\n    }, default=\"unknown\").alias('weekday')\n])\n\n\n\n\n\n\n\n\n# Add days\ndates_pd['next_week'] = dates_pd['date'] + pd.Timedelta(days=7)\n\n# Date difference\ndate_range = pd.date_range(start='2023-01-01', end='2023-01-10')\ndf_dates = pd.DataFrame({'date': date_range})\ndf_dates['days_since_start'] = (df_dates['date'] - df_dates['date'].min()).dt.days\n\n\n\n\n\n# Add days\ndates_pl = dates_pl.with_columns(\n    (pl.col('date') + pl.duration(days=7)).alias('next_week')\n)\n\n# Date difference\ndate_range = pd.date_range(start='2023-01-01', end='2023-01-10')  # Using pandas to generate range\ndf_dates = pl.DataFrame({'date': date_range})\ndf_dates = df_dates.with_columns(\n    (pl.col('date') - pl.col('date').min()).dt.total_days().alias('days_since_start')\n)\n\n\n\n\n\n\nThis section demonstrates performance differences between pandas and polars for a large dataset operation.\n\nimport pandas as pd\nimport polars as pl\nimport time\nimport numpy as np\n\n# Generate a large dataset (10 million rows)\nn = 10_000_000\ndata = {\n    'id': np.arange(n),\n    'value': np.random.randn(n),\n    'group': np.random.choice(['A', 'B', 'C', 'D'], n)\n}\n\n# Convert to pandas DataFrame\ndf_pd = pd.DataFrame(data)\n\n# Convert to polars DataFrame\ndf_pl = pl.DataFrame(data)\n\n# Benchmark: Group by and calculate mean, min, max\nprint(\"Running pandas groupby...\")\nstart = time.time()\nresult_pd = df_pd.groupby('group').agg({\n    'value': ['mean', 'min', 'max', 'count']\n})\npd_time = time.time() - start\nprint(f\"Pandas time: {pd_time:.4f} seconds\")\n\nprint(\"Running polars groupby...\")\nstart = time.time()\nresult_pl = df_pl.group_by('group').agg([\n    pl.col('value').mean().alias('value_mean'),\n    pl.col('value').min().alias('value_min'),\n    pl.col('value').max().alias('value_max'),\n    pl.col('value').count().alias('value_count')\n])\npl_time = time.time() - start\nprint(f\"Polars time: {pl_time:.4f} seconds\")\n\nprint(f\"Polars is {pd_time / pl_time:.2f}x faster\")\n\nRunning pandas groupby...\nPandas time: 0.2320 seconds\nRunning polars groupby...\nPolars time: 0.0545 seconds\nPolars is 4.26x faster\n\n\nTypically, for operations like this, Polars will be 3-10x faster than pandas, especially as data sizes increase. The performance gap widens further with more complex operations that can benefit from query optimization.\n\n\n\nPandas and Polars differ in several fundamental aspects:\n\n\nPandas uses eager execution by default:\nPolars supports both eager and lazy execution:\n\n\n\nPandas often uses assignment operations:\n\n# Many pandas operations use in-place assignment\npd_df['new_col'] = pd_df['new_col'] * 2\npd_df['new_col'] = pd_df['new_col'].fillna(0)\n\n# Some operations return new DataFrames\npd_df = pd_df.sort_values('new_col')\n\nPolars consistently uses method chaining:\n\n# All operations return new DataFrames and can be chained\npl_df = (pl_df\n    .with_columns((pl.col('new_col') * 2).alias('new_col'))\n    .with_columns(pl.col('new_col').fill_null(0))\n    .sort('new_col')\n)\n\n\n\n\nPandas directly references columns:\n\npd_df['result'] = pd_df['age'] + pd_df['new_col']\nfiltered = pd_df[pd_df['age'] &gt; pd_df['age'].mean()]\n\nPolars uses an expression API:\n\npl_df = pl_df.with_columns(\n    (pl.col('age') + pl.col('new_col')).alias('result')\n)\nfiltered = pl_df.filter(pl.col('age') &gt; pl.col('age').mean())\n\n\n\n\n\nIf you’re transitioning from pandas to polars, here are key mappings between common operations:\n\n\n\n\n\n\n\n\nOperation\nPandas\nPolars\n\n\n\n\nRead CSV\npd.read_csv('file.csv')\npl.read_csv('file.csv')\n\n\nSelect columns\ndf[['col1', 'col2']]\ndf.select(['col1', 'col2'])\n\n\nAdd column\ndf['new'] = df['col1'] * 2\ndf.with_columns((pl.col('col1') * 2).alias('new'))\n\n\nFilter rows\ndf[df['col'] &gt; 5]\ndf.filter(pl.col('col') &gt; 5)\n\n\nSort\ndf.sort_values('col')\ndf.sort('col')\n\n\nGroup by\ndf.groupby('col').agg({'val': 'sum'})\ndf.group_by('col').agg(pl.col('val').sum())\n\n\nJoin\ndf1.merge(df2, on='key')\ndf1.join(df2, on='key')\n\n\nFill NA\ndf.fillna(0)\ndf.fill_null(0)\n\n\nDrop NA\ndf.dropna()\ndf.drop_nulls()\n\n\nRename\ndf.rename(columns={'a': 'b'})\ndf.rename({'a': 'b'})\n\n\nUnique values\ndf['col'].unique()\ndf.select(pl.col('col').unique())\n\n\nValue counts\ndf['col'].value_counts()\ndf.group_by('col').count()\n\n\n\n\n\n\nThink in expressions: Use pl.col() to reference columns in operations\nEmbrace method chaining: String operations together instead of intermediate variables\nTry lazy execution: For complex operations, use pl.scan_csv() and lazy operations\nUse with_columns(): Instead of direct assignment, use with_columns for adding/modifying columns\nLearn the expression functions: Many operations like string manipulation use different syntax\n\n\n\n\nDespite Polars’ advantages, pandas might still be preferred when:\n\nWorking with existing codebases heavily dependent on pandas\nUsing specialized libraries that only support pandas (some visualization tools)\nDealing with very small datasets where performance isn’t critical\nUsing pandas-specific features without polars equivalents\nWorking with time series data that benefits from pandas’ specialized functionality\n\n\n\n\n\nPolars offers significant performance improvements and a more consistent API compared to pandas, particularly for large datasets and complex operations. While the syntax differences require some adjustment, the benefits in speed and memory efficiency make it a compelling choice for modern data analysis workflows.\nBoth libraries have their place in the Python data ecosystem. Pandas remains the more mature option with broader ecosystem compatibility, while Polars represents the future of high-performance data processing. For new projects dealing with large datasets, Polars is increasingly becoming the recommended choice."
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#table-of-contents",
    "href": "posts/pandas-to-polars/index.html#table-of-contents",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "Installation and Setup\nCreating DataFrames\nBasic Operations\nFiltering Data\nGrouping and Aggregation\nJoining/Merging DataFrames\nHandling Missing Values\nString Operations\nTime Series Operations\nPerformance Comparison\nAPI Philosophy Differences\nMigration Guide"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#installation-and-setup",
    "href": "posts/pandas-to-polars/index.html#installation-and-setup",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Import pandas\nimport pandas as pd\n\n\n\n\n\n# Import polars\nimport polars as pl"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#creating-dataframes",
    "href": "posts/pandas-to-polars/index.html#creating-dataframes",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "import pandas as pd\n\n# Create DataFrame from dictionary\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'age': [25, 30, 35, 40],\n    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n}\ndf_pd = pd.DataFrame(data)\nprint(df_pd)\n\n      name  age         city\n0    Alice   25     New York\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n3    David   40      Houston\n\n\n\n\n\n\nimport polars as pl\n\n# Create DataFrame from dictionary\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'age': [25, 30, 35, 40],\n    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n}\ndf_pl = pl.DataFrame(data)\nprint(df_pl)\n\nshape: (4, 3)\n┌─────────┬─────┬─────────────┐\n│ name    ┆ age ┆ city        │\n│ ---     ┆ --- ┆ ---         │\n│ str     ┆ i64 ┆ str         │\n╞═════════╪═════╪═════════════╡\n│ Alice   ┆ 25  ┆ New York    │\n│ Bob     ┆ 30  ┆ Los Angeles │\n│ Charlie ┆ 35  ┆ Chicago     │\n│ David   ┆ 40  ┆ Houston     │\n└─────────┴─────┴─────────────┘"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#basic-operations",
    "href": "posts/pandas-to-polars/index.html#basic-operations",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Select a single column (returns Series)\nseries = df_pd['name']\n\n# Select multiple columns\ndf_subset = df_pd[['name', 'age']]\n\n\n\n\n\n# Select a single column (returns Series)\nseries = df_pl['name']\n# Alternative method\nseries = df_pl.select(pl.col('name')).to_series()\n\n# Select multiple columns\ndf_subset = df_pl.select(['name', 'age'])\n# Alternative method\ndf_subset = df_pl.select(pl.col(['name', 'age']))\n\n\n\n\n\n\n\n\n# Add a new column\ndf_pd['is_adult'] = df_pd['age'] &gt;= 18\n\n# Using assign (creates a new DataFrame)\ndf_pd = df_pd.assign(age_squared=df_pd['age'] ** 2)\n\n\n\n\n\n# Add a new column\ndf_pl = df_pl.with_columns(\n    pl.when(pl.col('age') &gt;= 18).then(True).otherwise(False).alias('is_adult')\n)\n\n# Creating derived columns \ndf_pl = df_pl.with_columns(\n    (pl.col('age') ** 2).alias('age_squared')\n)\n\n# Multiple columns at once\ndf_pl = df_pl.with_columns([\n    pl.col('age').is_null().alias('age_is_null'),\n    (pl.col('age') * 2).alias('age_doubled')\n])\n\n\n\n\n\n\n\n\n# Get summary statistics\nsummary = df_pd.describe()\n\n# Individual statistics\nmean_age = df_pd['age'].mean()\nmedian_age = df_pd['age'].median()\nmin_age = df_pd['age'].min()\nmax_age = df_pd['age'].max()\n\n\n\n\n\n# Get summary statistics\nsummary = df_pl.describe()\n\n# Individual statistics\nmean_age = df_pl.select(pl.col('age').mean()).item()\nmedian_age = df_pl.select(pl.col('age').median()).item()\nmin_age = df_pl.select(pl.col('age').min()).item()\nmax_age = df_pl.select(pl.col('age').max()).item()"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#filtering-data",
    "href": "posts/pandas-to-polars/index.html#filtering-data",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Filter rows\nadults = df_pd[df_pd['age'] &gt;= 18]\n\n# Multiple conditions\nfiltered = df_pd[(df_pd['age'] &gt; 30) & (df_pd['city'] == 'Chicago')]\n\n\n\n\n\n# Filter rows\nadults = df_pl.filter(pl.col('age') &gt;= 18)\n\n# Multiple conditions\nfiltered = df_pl.filter((pl.col('age') &gt; 30) & (pl.col('city') == 'Chicago'))\n\n\n\n\n\n\n\n\n# Filter with OR conditions\ndf_filtered = df_pd[(df_pd['city'] == 'New York') | (df_pd['city'] == 'Chicago')]\n\n# Using isin\ncities = ['New York', 'Chicago']\ndf_filtered = df_pd[df_pd['city'].isin(cities)]\n\n# String contains\ndf_filtered = df_pd[df_pd['name'].str.contains('li')]\n\n\n\n\n\n# Filter with OR conditions\ndf_filtered = df_pl.filter((pl.col('city') == 'New York') | (pl.col('city') == 'Chicago'))\n\n# Using is_in\ncities = ['New York', 'Chicago']\ndf_filtered = df_pl.filter(pl.col('city').is_in(cities))\n\n# String contains\ndf_filtered = df_pl.filter(pl.col('name').str.contains('li'))"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#grouping-and-aggregation",
    "href": "posts/pandas-to-polars/index.html#grouping-and-aggregation",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Group by one column and aggregate\ncity_stats = df_pd.groupby('city').agg({\n    'age': ['mean', 'min', 'max', 'count']\n})\n\n# Reset index for flat DataFrame\ncity_stats = city_stats.reset_index()\n\n\n\n\n\n# Group by one column and aggregate\ncity_stats = df_pl.group_by('city').agg([\n    pl.col('age').mean().alias('age_mean'),\n    pl.col('age').min().alias('age_min'),\n    pl.col('age').max().alias('age_max'),\n    pl.col('age').count().alias('age_count')\n])"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#joiningmerging-dataframes",
    "href": "posts/pandas-to-polars/index.html#joiningmerging-dataframes",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Create another DataFrame\nemployee_data = {\n    'emp_id': [1, 2, 3, 4],\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'dept': ['HR', 'IT', 'Finance', 'IT']\n}\nemployee_df_pd = pd.DataFrame(employee_data)\n\nsalary_data = {\n    'emp_id': [1, 2, 3, 5],\n    'salary': [50000, 60000, 70000, 80000]\n}\nsalary_df_pd = pd.DataFrame(salary_data)\n\n# Inner join\nmerged_df = employee_df_pd.merge(\n    salary_df_pd,\n    on='emp_id',\n    how='inner'\n)\n\n\n\n\n\n# Create another DataFrame\nemployee_data = {\n    'emp_id': [1, 2, 3, 4],\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'dept': ['HR', 'IT', 'Finance', 'IT']\n}\nemployee_df_pl = pl.DataFrame(employee_data)\n\nsalary_data = {\n    'emp_id': [1, 2, 3, 5],\n    'salary': [50000, 60000, 70000, 80000]\n}\nsalary_df_pl = pl.DataFrame(salary_data)\n\n# Inner join\nmerged_df = employee_df_pl.join(\n    salary_df_pl,\n    on='emp_id',\n    how='inner'\n)\n\n\n\n\n\n\n\n\n# Left join\nleft_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='left')\n\n# Right join\nright_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='right')\n\n# Outer join\nouter_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='outer')\n\n\n\n\n\n# Left join\nleft_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='left')\n\n# Right join\nright_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='right')\n\n# Outer join\nouter_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='full')"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#handling-missing-values",
    "href": "posts/pandas-to-polars/index.html#handling-missing-values",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Check for missing values\nmissing_count = df_pd.isnull().sum()\n\n# Check if any column has missing values\nhas_missing = df_pd.isnull().any().any()\n\n\n\n\n\n# Check for missing values\nmissing_count = df_pl.null_count()\n\n# Check if specific column has missing values\nhas_missing = df_pl.select(pl.col('age').is_null().any()).item()\n\n\n\n\n\n\n\n\n# Drop rows with any missing values\ndf_pd_clean = df_pd.dropna()\n\n# Fill missing values\ndf_pd_filled = df_pd.fillna({\n    'age': 0,\n    'city': 'Unknown'\n})\n\n# Forward fill\ndf_pd_ffill = df_pd.ffill()\n\n\n\n\n\n# Drop rows with any missing values\ndf_pl_clean = df_pl.drop_nulls()\n\n# Fill missing values\ndf_pl_filled = df_pl.with_columns([\n    pl.col('age').fill_null(0),\n    pl.col('city').fill_null('Unknown')\n])\n\n# Forward fill\ndf_pl_ffill = df_pl.with_columns([\n    pl.col('age').fill_null(strategy='forward'),\n    pl.col('city').fill_null(strategy='forward')\n])"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#string-operations",
    "href": "posts/pandas-to-polars/index.html#string-operations",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Convert to uppercase\ndf_pd['name_upper'] = df_pd['name'].str.upper()\n\n# Get string length\ndf_pd['name_length'] = df_pd['name'].str.len()\n\n# Extract substring\ndf_pd['name_first_char'] = df_pd['name'].str[0]\n\n# Replace substrings\ndf_pd['city_replaced'] = df_pd['city'].str.replace('New', 'Old')\n\n\n\n\n\n# Convert to uppercase\ndf_pl = df_pl.with_columns(pl.col('name').str.to_uppercase().alias('name_upper'))\n\n# Get string length\ndf_pl = df_pl.with_columns(pl.col('name').str.len_chars().alias('name_length'))\n\n# Extract substring \ndf_pl = df_pl.with_columns(pl.col('name').str.slice(0, 1).alias('name_first_char'))\n\n# Replace substrings\ndf_pl = df_pl.with_columns(pl.col('city').str.replace('New', 'Old').alias('city_replaced'))\n\n\n\n\n\n\n\n\n# Split string\ndf_pd['first_word'] = df_pd['city'].str.split(' ').str[0]\n\n# Pattern matching\nhas_new = df_pd['city'].str.contains('New')\n\n# Extract with regex\ndf_pd['extracted'] = df_pd['city'].str.extract(r'(\\w+)\\s')\n\n\n\n\n\n# Split string\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.split(' ').list.get(0).alias('first_word')\n)\n\n# Pattern matching\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.contains('New').alias('has_new')\n)\n\n# Extract with regex\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.extract(r'(\\w+)\\s').alias('extracted')\n)"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#time-series-operations",
    "href": "posts/pandas-to-polars/index.html#time-series-operations",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Create DataFrame with dates\ndates_pd = pd.DataFrame({\n    'date_str': ['2023-01-01', '2023-02-15', '2023-03-30']\n})\n\n# Parse dates\ndates_pd['date'] = pd.to_datetime(dates_pd['date_str'])\n\n# Extract components\ndates_pd['year'] = dates_pd['date'].dt.year\ndates_pd['month'] = dates_pd['date'].dt.month\ndates_pd['day'] = dates_pd['date'].dt.day\ndates_pd['weekday'] = dates_pd['date'].dt.day_name()\n\n\n\n\n\n# Create DataFrame with dates\ndates_pl = pl.DataFrame({\n    'date_str': ['2023-01-01', '2023-02-15', '2023-03-30']\n})\n\n# Parse dates\ndates_pl = dates_pl.with_columns(\n    pl.col('date_str').str.strptime(pl.Datetime, '%Y-%m-%d').alias('date')\n)\n\n# Extract components\ndates_pl = dates_pl.with_columns([\n    pl.col('date').dt.year().alias('year'),\n    pl.col('date').dt.month().alias('month'),\n    pl.col('date').dt.day().alias('day'),\n    pl.col('date').dt.weekday().replace_strict({\n        0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', \n        3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'\n    }, default=\"unknown\").alias('weekday')\n])\n\n\n\n\n\n\n\n\n# Add days\ndates_pd['next_week'] = dates_pd['date'] + pd.Timedelta(days=7)\n\n# Date difference\ndate_range = pd.date_range(start='2023-01-01', end='2023-01-10')\ndf_dates = pd.DataFrame({'date': date_range})\ndf_dates['days_since_start'] = (df_dates['date'] - df_dates['date'].min()).dt.days\n\n\n\n\n\n# Add days\ndates_pl = dates_pl.with_columns(\n    (pl.col('date') + pl.duration(days=7)).alias('next_week')\n)\n\n# Date difference\ndate_range = pd.date_range(start='2023-01-01', end='2023-01-10')  # Using pandas to generate range\ndf_dates = pl.DataFrame({'date': date_range})\ndf_dates = df_dates.with_columns(\n    (pl.col('date') - pl.col('date').min()).dt.total_days().alias('days_since_start')\n)"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#performance-comparison",
    "href": "posts/pandas-to-polars/index.html#performance-comparison",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "This section demonstrates performance differences between pandas and polars for a large dataset operation.\n\nimport pandas as pd\nimport polars as pl\nimport time\nimport numpy as np\n\n# Generate a large dataset (10 million rows)\nn = 10_000_000\ndata = {\n    'id': np.arange(n),\n    'value': np.random.randn(n),\n    'group': np.random.choice(['A', 'B', 'C', 'D'], n)\n}\n\n# Convert to pandas DataFrame\ndf_pd = pd.DataFrame(data)\n\n# Convert to polars DataFrame\ndf_pl = pl.DataFrame(data)\n\n# Benchmark: Group by and calculate mean, min, max\nprint(\"Running pandas groupby...\")\nstart = time.time()\nresult_pd = df_pd.groupby('group').agg({\n    'value': ['mean', 'min', 'max', 'count']\n})\npd_time = time.time() - start\nprint(f\"Pandas time: {pd_time:.4f} seconds\")\n\nprint(\"Running polars groupby...\")\nstart = time.time()\nresult_pl = df_pl.group_by('group').agg([\n    pl.col('value').mean().alias('value_mean'),\n    pl.col('value').min().alias('value_min'),\n    pl.col('value').max().alias('value_max'),\n    pl.col('value').count().alias('value_count')\n])\npl_time = time.time() - start\nprint(f\"Polars time: {pl_time:.4f} seconds\")\n\nprint(f\"Polars is {pd_time / pl_time:.2f}x faster\")\n\nRunning pandas groupby...\nPandas time: 0.2320 seconds\nRunning polars groupby...\nPolars time: 0.0545 seconds\nPolars is 4.26x faster\n\n\nTypically, for operations like this, Polars will be 3-10x faster than pandas, especially as data sizes increase. The performance gap widens further with more complex operations that can benefit from query optimization."
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#api-philosophy-differences",
    "href": "posts/pandas-to-polars/index.html#api-philosophy-differences",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "Pandas and Polars differ in several fundamental aspects:\n\n\nPandas uses eager execution by default:\nPolars supports both eager and lazy execution:\n\n\n\nPandas often uses assignment operations:\n\n# Many pandas operations use in-place assignment\npd_df['new_col'] = pd_df['new_col'] * 2\npd_df['new_col'] = pd_df['new_col'].fillna(0)\n\n# Some operations return new DataFrames\npd_df = pd_df.sort_values('new_col')\n\nPolars consistently uses method chaining:\n\n# All operations return new DataFrames and can be chained\npl_df = (pl_df\n    .with_columns((pl.col('new_col') * 2).alias('new_col'))\n    .with_columns(pl.col('new_col').fill_null(0))\n    .sort('new_col')\n)\n\n\n\n\nPandas directly references columns:\n\npd_df['result'] = pd_df['age'] + pd_df['new_col']\nfiltered = pd_df[pd_df['age'] &gt; pd_df['age'].mean()]\n\nPolars uses an expression API:\n\npl_df = pl_df.with_columns(\n    (pl.col('age') + pl.col('new_col')).alias('result')\n)\nfiltered = pl_df.filter(pl.col('age') &gt; pl.col('age').mean())"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#migration-guide",
    "href": "posts/pandas-to-polars/index.html#migration-guide",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "If you’re transitioning from pandas to polars, here are key mappings between common operations:\n\n\n\n\n\n\n\n\nOperation\nPandas\nPolars\n\n\n\n\nRead CSV\npd.read_csv('file.csv')\npl.read_csv('file.csv')\n\n\nSelect columns\ndf[['col1', 'col2']]\ndf.select(['col1', 'col2'])\n\n\nAdd column\ndf['new'] = df['col1'] * 2\ndf.with_columns((pl.col('col1') * 2).alias('new'))\n\n\nFilter rows\ndf[df['col'] &gt; 5]\ndf.filter(pl.col('col') &gt; 5)\n\n\nSort\ndf.sort_values('col')\ndf.sort('col')\n\n\nGroup by\ndf.groupby('col').agg({'val': 'sum'})\ndf.group_by('col').agg(pl.col('val').sum())\n\n\nJoin\ndf1.merge(df2, on='key')\ndf1.join(df2, on='key')\n\n\nFill NA\ndf.fillna(0)\ndf.fill_null(0)\n\n\nDrop NA\ndf.dropna()\ndf.drop_nulls()\n\n\nRename\ndf.rename(columns={'a': 'b'})\ndf.rename({'a': 'b'})\n\n\nUnique values\ndf['col'].unique()\ndf.select(pl.col('col').unique())\n\n\nValue counts\ndf['col'].value_counts()\ndf.group_by('col').count()\n\n\n\n\n\n\nThink in expressions: Use pl.col() to reference columns in operations\nEmbrace method chaining: String operations together instead of intermediate variables\nTry lazy execution: For complex operations, use pl.scan_csv() and lazy operations\nUse with_columns(): Instead of direct assignment, use with_columns for adding/modifying columns\nLearn the expression functions: Many operations like string manipulation use different syntax\n\n\n\n\nDespite Polars’ advantages, pandas might still be preferred when:\n\nWorking with existing codebases heavily dependent on pandas\nUsing specialized libraries that only support pandas (some visualization tools)\nDealing with very small datasets where performance isn’t critical\nUsing pandas-specific features without polars equivalents\nWorking with time series data that benefits from pandas’ specialized functionality"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#conclusion",
    "href": "posts/pandas-to-polars/index.html#conclusion",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "Polars offers significant performance improvements and a more consistent API compared to pandas, particularly for large datasets and complex operations. While the syntax differences require some adjustment, the benefits in speed and memory efficiency make it a compelling choice for modern data analysis workflows.\nBoth libraries have their place in the Python data ecosystem. Pandas remains the more mature option with broader ecosystem compatibility, while Polars represents the future of high-performance data processing. For new projects dealing with large datasets, Polars is increasingly becoming the recommended choice."
  },
  {
    "objectID": "posts/influence-selection/index.html",
    "href": "posts/influence-selection/index.html",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Active learning is a machine learning paradigm where the algorithm can interactively query an oracle (typically a human annotator) to label new data points. The key idea is to select the most informative samples to be labeled, reducing the overall labeling effort while maintaining or improving model performance. This guide focuses on influence selection methods used in active learning strategies.\n\n\n\n\nFundamentals of Active Learning\nInfluence Selection Strategies\nUncertainty-Based Methods\nDiversity-Based Methods\nExpected Model Change\nExpected Error Reduction\nInfluence Functions\nQuery-by-Committee\nImplementation Considerations\nEvaluation Metrics\nPractical Examples\nAdvanced Topics\n\n\n\n\n\n\nThe typical active learning process follows these steps:\n\nStart with a small labeled dataset and a large unlabeled pool\nTrain an initial model on the labeled data\nApply an influence selection strategy to choose informative samples from the unlabeled pool\nGet annotations for the selected samples\nAdd the newly labeled samples to the training set\nRetrain the model and repeat steps 3-6 until a stopping condition is met\n\n\n\n\n\nPool-based: The learner has access to a pool of unlabeled data and selects the most informative samples\nStream-based: Samples arrive sequentially, and the learner must decide on-the-fly whether to request labels\n\n\n\n\n\nInfluence selection is about identifying which unlabeled samples would be most beneficial to label next. Here are the main strategies:\n\n\n\nThese methods select samples that the model is most uncertain about.\n\n\n\ndef least_confidence(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    \n    # Get the confidence values for the most probable class\n    confidences = np.max(probabilities, axis=1)\n    \n    # Select the k samples with the lowest confidence\n    least_confident_indices = np.argsort(confidences)[:k]\n    \n    return unlabeled_pool[least_confident_indices]\n\n\n\n\nSelects samples with the smallest margin between the two most likely classes:\n\ndef margin_sampling(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    \n    # Sort the probabilities in descending order\n    sorted_probs = np.sort(probabilities, axis=1)[:, ::-1]\n    \n    # Calculate the margin between the first and second most probable classes\n    margins = sorted_probs[:, 0] - sorted_probs[:, 1]\n    \n    # Select the k samples with the smallest margins\n    smallest_margin_indices = np.argsort(margins)[:k]\n    \n    return unlabeled_pool[smallest_margin_indices]\n\n\n\n\nSelects samples with the highest predictive entropy:\n\ndef entropy_sampling(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    \n    # Calculate entropy for each sample\n    entropies = -np.sum(probabilities * np.log(probabilities + 1e-10), axis=1)\n    \n    # Select the k samples with the highest entropy\n    highest_entropy_indices = np.argsort(entropies)[::-1][:k]\n    \n    return unlabeled_pool[highest_entropy_indices]\n\n\n\n\nFor Bayesian models, BALD selects samples that maximize the mutual information between predictions and model parameters:\n\ndef bald_sampling(bayesian_model, unlabeled_pool, k, n_samples=100):\n    # Get multiple predictions by sampling from the model's posterior\n    probs_samples = []\n    for _ in range(n_samples):\n        probs = bayesian_model.predict_proba(unlabeled_pool)\n        probs_samples.append(probs)\n    \n    # Stack into a 3D array: (samples, data points, classes)\n    probs_samples = np.stack(probs_samples)\n    \n    # Calculate the average probability across all samples\n    mean_probs = np.mean(probs_samples, axis=0)\n    \n    # Calculate the entropy of the average prediction\n    entropy_mean = -np.sum(mean_probs * np.log(mean_probs + 1e-10), axis=1)\n    \n    # Calculate the average entropy across all samples\n    entropy_samples = -np.sum(probs_samples * np.log(probs_samples + 1e-10), axis=2)\n    mean_entropy = np.mean(entropy_samples, axis=0)\n    \n    # Mutual information = entropy of the mean - mean of entropies\n    bald_scores = entropy_mean - mean_entropy\n    \n    # Select the k samples with the highest BALD scores\n    highest_bald_indices = np.argsort(bald_scores)[::-1][:k]\n    \n    return unlabeled_pool[highest_bald_indices]\n\n\n\n\n\nThese methods aim to select a diverse set of examples to ensure broad coverage of the input space.\n\n\n\ndef clustering_based_sampling(unlabeled_pool, k, n_clusters=None):\n    if n_clusters is None:\n        n_clusters = k\n    \n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(unlabeled_pool)\n    \n    # Get the cluster centers and distances to each point\n    centers = kmeans.cluster_centers_\n    distances = kmeans.transform(unlabeled_pool)  # Distance to each cluster center\n    \n    # Select one sample from each cluster (closest to the center)\n    selected_indices = []\n    for i in range(n_clusters):\n        # Get the samples in this cluster\n        cluster_samples = np.where(kmeans.labels_ == i)[0]\n        \n        # Find the sample closest to the center\n        closest_sample = cluster_samples[np.argmin(distances[cluster_samples, i])]\n        selected_indices.append(closest_sample)\n    \n    # If we need more samples than clusters, fill with the most uncertain samples\n    if k &gt; n_clusters:\n        # Implementation depends on uncertainty measure\n        pass\n    \n    return unlabeled_pool[selected_indices[:k]]\n\n\n\n\nThe core-set approach aims to select a subset of data that best represents the whole dataset:\n\ndef core_set_sampling(labeled_pool, unlabeled_pool, k):\n    # Combine labeled and unlabeled data for distance calculations\n    all_data = np.vstack((labeled_pool, unlabeled_pool))\n    \n    # Compute pairwise distances\n    distances = pairwise_distances(all_data)\n    \n    # Split distances into labeled-unlabeled and unlabeled-unlabeled\n    n_labeled = labeled_pool.shape[0]\n    dist_labeled_unlabeled = distances[:n_labeled, n_labeled:]\n    \n    # For each unlabeled sample, find the minimum distance to any labeled sample\n    min_distances = np.min(dist_labeled_unlabeled, axis=0)\n    \n    # Select the k samples with the largest minimum distances\n    farthest_indices = np.argsort(min_distances)[::-1][:k]\n    \n    return unlabeled_pool[farthest_indices]\n\n\n\n\n\nThe Expected Model Change (EMC) method selects samples that would cause the greatest change in the model if they were labeled:\n\ndef expected_model_change(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    n_classes = probabilities.shape[1]\n    \n    # Calculate expected gradient length for each sample\n    expected_changes = []\n    for i, x in enumerate(unlabeled_pool):\n        # Calculate expected gradient length across all possible labels\n        change = 0\n        for c in range(n_classes):\n            # For each possible class, calculate the gradient if this was the true label\n            x_expanded = x.reshape(1, -1)\n            # Here we would compute the gradient of the model with respect to the sample\n            # For simplicity, we use a placeholder\n            gradient = compute_gradient(model, x_expanded, c)\n            norm_gradient = np.linalg.norm(gradient)\n            \n            # Weight by the probability of this class\n            change += probabilities[i, c] * norm_gradient\n        \n        expected_changes.append(change)\n    \n    # Select the k samples with the highest expected change\n    highest_change_indices = np.argsort(expected_changes)[::-1][:k]\n    \n    return unlabeled_pool[highest_change_indices]\n\nNote: The compute_gradient function would need to be implemented based on the specific model being used.\n\n\n\nThe Expected Error Reduction method selects samples that, when labeled, would minimally reduce the model’s expected error:\n\ndef expected_error_reduction(model, unlabeled_pool, unlabeled_pool_remaining, k):\n    # Predict probabilities for all remaining unlabeled data\n    current_probs = model.predict_proba(unlabeled_pool_remaining)\n    current_entropy = -np.sum(current_probs * np.log(current_probs + 1e-10), axis=1)\n    \n    expected_error_reductions = []\n    \n    # For each sample in the unlabeled pool we're considering\n    for i, x in enumerate(unlabeled_pool):\n        # Predict probabilities for this sample\n        probs = model.predict_proba(x.reshape(1, -1))[0]\n        \n        # Calculate the expected error reduction for each possible label\n        error_reduction = 0\n        for c in range(len(probs)):\n            # Create a hypothetical new model with this labeled sample\n            # For simplicity, we use a placeholder function\n            hypothetical_model = train_with_additional_sample(model, x, c)\n            \n            # Get new probabilities with this model\n            new_probs = hypothetical_model.predict_proba(unlabeled_pool_remaining)\n            new_entropy = -np.sum(new_probs * np.log(new_probs + 1e-10), axis=1)\n            \n            # Expected entropy reduction\n            reduction = np.sum(current_entropy - new_entropy)\n            \n            # Weight by the probability of this class\n            error_reduction += probs[c] * reduction\n        \n        expected_error_reductions.append(error_reduction)\n    \n    # Select the k samples with the highest expected error reduction\n    highest_reduction_indices = np.argsort(expected_error_reductions)[::-1][:k]\n    \n    return unlabeled_pool[highest_reduction_indices]\n\nNote: The train_with_additional_sample function would need to be implemented based on the specific model being used.\n\n\n\nInfluence functions approximate the effect of adding or removing a training example without retraining the model:\n\ndef influence_function_sampling(model, unlabeled_pool, labeled_pool, k, labels):\n    influences = []\n    \n    # For each unlabeled sample\n    for x_u in unlabeled_pool:\n        # Calculate the influence of adding this sample to the training set\n        influence = calculate_influence(model, x_u, labeled_pool, labels)\n        influences.append(influence)\n    \n    # Select the k samples with the highest influence\n    highest_influence_indices = np.argsort(influences)[::-1][:k]\n    \n    return unlabeled_pool[highest_influence_indices]\n\nNote: The calculate_influence function would need to be implemented based on the specific model and influence metric being used.\n\n\n\nQuery-by-Committee (QBC) methods train multiple models (a committee) and select samples where they disagree:\n\ndef query_by_committee(committee_models, unlabeled_pool, k):\n    # Get predictions from all committee members\n    all_predictions = []\n    for model in committee_models:\n        preds = model.predict(unlabeled_pool)\n        all_predictions.append(preds)\n    \n    # Stack predictions into a 2D array (committee members, data points)\n    all_predictions = np.stack(all_predictions)\n    \n    # Calculate disagreement (e.g., using vote entropy)\n    disagreements = []\n    for i in range(unlabeled_pool.shape[0]):\n        # Count votes for each class\n        votes = np.bincount(all_predictions[:, i])\n        # Normalize to get probabilities\n        vote_probs = votes / len(committee_models)\n        # Calculate entropy\n        entropy = -np.sum(vote_probs * np.log2(vote_probs + 1e-10))\n        disagreements.append(entropy)\n    \n    # Select the k samples with the highest disagreement\n    highest_disagreement_indices = np.argsort(disagreements)[::-1][:k]\n    \n    return unlabeled_pool[highest_disagreement_indices]\n\n\n\n\n\n\nIn practice, it’s often more efficient to select multiple samples at once. However, simply selecting the top-k samples may lead to redundancy. Consider using:\n\nGreedy Selection with Diversity: Select one sample at a time, then update the diversity metrics to avoid selecting similar samples.\n\n\ndef batch_selection_with_diversity(model, unlabeled_pool, k, lambda_diversity=0.5):\n    selected_indices = []\n    remaining_indices = list(range(len(unlabeled_pool)))\n    \n    # Calculate uncertainty scores for all samples\n    probabilities = model.predict_proba(unlabeled_pool)\n    entropies = -np.sum(probabilities * np.log(probabilities + 1e-10), axis=1)\n    \n    # Calculate distance matrix for diversity\n    distance_matrix = pairwise_distances(unlabeled_pool)\n    \n    for _ in range(k):\n        if not remaining_indices:\n            break\n        \n        scores = np.zeros(len(remaining_indices))\n        \n        # Calculate uncertainty scores\n        uncertainty_scores = entropies[remaining_indices]\n        \n        # Calculate diversity scores (if we have already selected some samples)\n        if selected_indices:\n            # For each remaining sample, calculate the minimum distance to any selected sample\n            diversity_scores = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n        else:\n            diversity_scores = np.zeros(len(remaining_indices))\n        \n        # Normalize scores\n        uncertainty_scores = (uncertainty_scores - np.min(uncertainty_scores)) / (np.max(uncertainty_scores) - np.min(uncertainty_scores) + 1e-10)\n        if selected_indices:\n            diversity_scores = (diversity_scores - np.min(diversity_scores)) / (np.max(diversity_scores) - np.min(diversity_scores) + 1e-10)\n        \n        # Combine scores\n        scores = (1 - lambda_diversity) * uncertainty_scores + lambda_diversity * diversity_scores\n        \n        # Select the sample with the highest score\n        best_idx = np.argmax(scores)\n        selected_idx = remaining_indices[best_idx]\n        \n        # Add to selected and remove from remaining\n        selected_indices.append(selected_idx)\n        remaining_indices.remove(selected_idx)\n    \n    return unlabeled_pool[selected_indices]\n\n\nSubmodular Function Maximization: Use a submodular function to ensure diversity in the selected batch.\n\n\n\n\nActive learning can inadvertently reinforce class imbalance. Consider:\n\nStratified Sampling: Ensure representation from all classes.\nHybrid Approaches: Combine uncertainty-based and density-based methods.\nDiversity Constraints: Explicitly enforce diversity in feature space.\n\n\n\n\nSome methods (like expected error reduction) can be computationally expensive. Consider:\n\nSubsample the Unlabeled Pool: Only consider a random subset for selection.\nPre-compute Embeddings: Use a fixed feature extractor to pre-compute embeddings.\nApproximate Methods: Use approximations for expensive operations.\n\n\n\n\n\n\n\nPlot model performance vs. number of labeled samples:\n\ndef plot_learning_curve(model_factory, X_train, y_train, X_test, y_test, \n                        active_learning_strategy, initial_size=10, \n                        batch_size=10, n_iterations=20):\n    # Initialize with a small labeled set\n    labeled_indices = np.random.choice(len(X_train), initial_size, replace=False)\n    unlabeled_indices = np.setdiff1d(np.arange(len(X_train)), labeled_indices)\n    \n    performance = []\n    \n    for i in range(n_iterations):\n        # Create a fresh model\n        model = model_factory()\n        \n        # Train on the currently labeled data\n        model.fit(X_train[labeled_indices], y_train[labeled_indices])\n        \n        # Evaluate on the test set\n        score = model.score(X_test, y_test)\n        performance.append((len(labeled_indices), score))\n        \n        # Select the next batch of samples\n        if len(unlabeled_indices) &gt; 0:\n            # Use the specified active learning strategy\n            selected_indices = active_learning_strategy(\n                model, X_train[unlabeled_indices], batch_size\n            )\n            \n            # Map back to original indices\n            selected_original_indices = unlabeled_indices[selected_indices]\n            \n            # Update labeled and unlabeled indices\n            labeled_indices = np.append(labeled_indices, selected_original_indices)\n            unlabeled_indices = np.setdiff1d(unlabeled_indices, selected_original_indices)\n    \n    # Plot the learning curve\n    counts, scores = zip(*performance)\n    plt.figure(figsize=(10, 6))\n    plt.plot(counts, scores, 'o-')\n    plt.xlabel('Number of labeled samples')\n    plt.ylabel('Model accuracy')\n    plt.title('Active Learning Performance')\n    plt.grid(True)\n    \n    return performance\n\n\n\n\nAlways compare your active learning strategy with random sampling as a baseline.\n\n\n\nCalculate how many annotations you saved compared to using the entire dataset.\n\n\n\n\n\n\n\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n# Load data\nmnist = fetch_openml('mnist_784', version=1, cache=True)\nX, y = mnist['data'], mnist['target']\n\n# Split into train and test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Initially, only a small portion is labeled\ninitial_size = 100\nlabeled_indices = np.random.choice(len(X_train), initial_size, replace=False)\nunlabeled_indices = np.setdiff1d(np.arange(len(X_train)), labeled_indices)\n\n# Tracking performance\nactive_learning_performance = []\nrandom_sampling_performance = []\n\n# Active learning loop\nfor i in range(10):  # 10 iterations\n    # Train a model on the currently labeled data\n    model = RandomForestClassifier(n_estimators=50, random_state=42)\n    model.fit(X_train.iloc[labeled_indices], y_train.iloc[labeled_indices])\n    \n    # Evaluate on the test set\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    active_learning_performance.append((len(labeled_indices), accuracy))\n    \n    print(f\"Iteration {i+1}: {len(labeled_indices)} labeled samples, \"\n          f\"accuracy: {accuracy:.4f}\")\n    \n    # Select 100 new samples using entropy sampling\n    if len(unlabeled_indices) &gt; 0:\n        # Predict probabilities for each unlabeled sample\n        probs = model.predict_proba(X_train.iloc[unlabeled_indices])\n        \n        # Calculate entropy\n        entropies = -np.sum(probs * np.log(probs + 1e-10), axis=1)\n        \n        # Select samples with the highest entropy\n        top_indices = np.argsort(entropies)[::-1][:100]\n        \n        # Update labeled and unlabeled indices\n        selected_indices = unlabeled_indices[top_indices]\n        labeled_indices = np.append(labeled_indices, selected_indices)\n        unlabeled_indices = np.setdiff1d(unlabeled_indices, selected_indices)\n\n# Plot learning curve\ncounts, scores = zip(*active_learning_performance)\nplt.figure(figsize=(10, 6))\nplt.plot(counts, scores, 'o-', label='Active Learning')\nplt.xlabel('Number of labeled samples')\nplt.ylabel('Model accuracy')\nplt.title('Active Learning Performance')\nplt.grid(True)\nplt.legend()\nplt.show()\n\nIteration 1: 100 labeled samples, accuracy: 0.6755\nIteration 2: 200 labeled samples, accuracy: 0.7300\nIteration 3: 300 labeled samples, accuracy: 0.7716\nIteration 4: 400 labeled samples, accuracy: 0.8097\nIteration 5: 500 labeled samples, accuracy: 0.8359\nIteration 6: 600 labeled samples, accuracy: 0.8366\nIteration 7: 700 labeled samples, accuracy: 0.8494\nIteration 8: 800 labeled samples, accuracy: 0.8509\nIteration 9: 900 labeled samples, accuracy: 0.8678\nIteration 10: 1000 labeled samples, accuracy: 0.8758\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.datasets import fetch_20newsgroups\n\n# Load data\ncategories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\ntwenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\ntwenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n\n# Feature extraction\nvectorizer = TfidfVectorizer(stop_words='english')\nX_train = vectorizer.fit_transform(twenty_train.data)\nX_test = vectorizer.transform(twenty_test.data)\ny_train = twenty_train.target\ny_test = twenty_test.target\n\n# Initially, only a small portion is labeled\ninitial_size = 20\nlabeled_indices = np.random.choice(len(X_train.toarray()), initial_size, replace=False)\nunlabeled_indices = np.setdiff1d(np.arange(len(X_train.toarray())), labeled_indices)\n\n# Create a committee of models\nmodels = [\n    ('nb', MultinomialNB()),\n    ('svm', SVC(kernel='linear', probability=True)),\n    ('svm2', SVC(kernel='rbf', probability=True))\n]\n\n# Active learning loop\nfor i in range(10):  # 10 iterations\n    # Train each model on the currently labeled data\n    committee_models = []\n    for name, model in models:\n        model.fit(X_train[labeled_indices], y_train[labeled_indices])\n        committee_models.append(model)\n    \n    # Evaluate using the VotingClassifier\n    voting_clf = VotingClassifier(estimators=models, voting='soft')\n    voting_clf.fit(X_train[labeled_indices], y_train[labeled_indices])\n    \n    accuracy = voting_clf.score(X_test, y_test)\n    print(f\"Iteration {i+1}: {len(labeled_indices)} labeled samples, \"\n          f\"accuracy: {accuracy:.4f}\")\n    \n    # Select 10 new samples using Query-by-Committee\n    if len(unlabeled_indices) &gt; 0:\n        # Get predictions from all committee members\n        all_predictions = []\n        for model in committee_models:\n            preds = model.predict(X_train[unlabeled_indices])\n            all_predictions.append(preds)\n        \n        # Calculate vote entropy\n        vote_entropies = []\n        all_predictions = np.array(all_predictions)\n        for i in range(len(unlabeled_indices)):\n            # Count votes for each class\n            votes = np.bincount(all_predictions[:, i], minlength=len(categories))\n            # Normalize to get probabilities\n            vote_probs = votes / len(committee_models)\n            # Calculate entropy\n            entropy = -np.sum(vote_probs * np.log2(vote_probs + 1e-10))\n            vote_entropies.append(entropy)\n        \n        # Select samples with the highest vote entropy\n        top_indices = np.argsort(vote_entropies)[::-1][:10]\n        \n        # Update labeled and unlabeled indices\n        selected_indices = unlabeled_indices[top_indices]\n        labeled_indices = np.append(labeled_indices, selected_indices)\n        unlabeled_indices = np.setdiff1d(unlabeled_indices, selected_indices)\n\nIteration 1: 20 labeled samples, accuracy: 0.2124\nIteration 2: 30 labeled samples, accuracy: 0.2310\nIteration 3: 40 labeled samples, accuracy: 0.3668\nIteration 4: 50 labeled samples, accuracy: 0.2983\nIteration 5: 60 labeled samples, accuracy: 0.4627\nIteration 6: 70 labeled samples, accuracy: 0.6312\nIteration 7: 80 labeled samples, accuracy: 0.6864\nIteration 8: 90 labeled samples, accuracy: 0.6125\nIteration 9: 100 labeled samples, accuracy: 0.7597\nIteration 10: 110 labeled samples, accuracy: 0.7870\n\n\n\n\n\n\n\n\nCombining transfer learning with active learning can be powerful:\n\nUse pre-trained models as feature extractors.\nApply active learning on the feature space.\nFine-tune the model on the selected samples.\n\n\n\n\nSpecial considerations for deep learning models:\n\nUncertainty Estimation: Use dropout or ensemble methods for better uncertainty estimation.\nBatch Normalization: Be careful with batch normalization layers when retraining.\nData Augmentation: Apply data augmentation to increase the effective size of the labeled pool.\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Subset\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\n# Define a simple CNN\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x, dropout=True):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        if dropout:\n            x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        if dropout:\n            x = self.dropout2(x)\n        x = self.fc2(x)\n        return x\n\n# MC Dropout for uncertainty estimation\ndef mc_dropout_uncertainty(model, data_loader, n_samples=10):\n    model.eval()\n    all_probs = []\n    \n    with torch.no_grad():\n        for _ in range(n_samples):\n            batch_probs = []\n            for data, _ in data_loader:\n                output = model(data, dropout=True)\n                probs = F.softmax(output, dim=1)\n                batch_probs.append(probs)\n            \n            # Concatenate batch probabilities\n            all_probs.append(torch.cat(batch_probs))\n    \n    # Stack along a new dimension\n    all_probs = torch.stack(all_probs)\n    \n    # Calculate the mean probabilities\n    mean_probs = torch.mean(all_probs, dim=0)\n    \n    # Calculate entropy of the mean prediction\n    entropy = -torch.sum(mean_probs * torch.log(mean_probs + 1e-10), dim=1)\n    \n    return entropy.numpy()\n\n\n\n\nLeverage both labeled and unlabeled data during training:\n\nSelf-Training: Use model predictions on unlabeled data as pseudo-labels.\nCo-Training: Train multiple models and use their predictions to teach each other.\nConsistency Regularization: Enforce consistent predictions across different perturbations.\n\n\ndef semi_supervised_active_learning(labeled_X, labeled_y, unlabeled_X, model, confidence_threshold=0.95):\n    # Train model on labeled data\n    model.fit(labeled_X, labeled_y)\n    \n    # Predict on unlabeled data\n    probabilities = model.predict_proba(unlabeled_X)\n    max_probs = np.max(probabilities, axis=1)\n    \n    # Get high confidence predictions\n    confident_indices = np.where(max_probs &gt;= confidence_threshold)[0]\n    \n    # Get pseudo-labels for confident predictions\n    pseudo_labels = model.predict(unlabeled_X[confident_indices])\n    \n    # Train on combined dataset\n    combined_X = np.vstack([labeled_X, unlabeled_X[confident_indices]])\n    combined_y = np.concatenate([labeled_y, pseudo_labels])\n    \n    model.fit(combined_X, combined_y)\n    \n    return model, confident_indices\n\n\n\n\nWhen labeled data from the target domain is scarce, active learning can help select the most informative samples:\n\nDomain Discrepancy Measures: Select samples that minimize domain discrepancy.\nAdversarial Selection: Select samples that the domain discriminator is most uncertain about.\nFeature Space Alignment: Select samples that help align feature spaces between domains.\n\n\n\n\n\nAnnotation Interface Design: Make the annotation process intuitive and efficient.\nCognitive Load Management: Group similar samples to reduce cognitive switching.\nExplanations: Provide model explanations to help annotators understand the current model’s decisions.\nQuality Control: Incorporate mechanisms to detect and correct annotation errors.\n\n\n\n\n\nActive learning provides a powerful framework for efficiently building machine learning models with limited labeled data. By selecting the most informative samples for annotation, active learning can significantly reduce the labeling effort while maintaining high model performance.\nThe key to successful active learning is choosing the right influence selection strategy for your specific problem and data characteristics. Consider the following when designing your active learning pipeline:\n\nData Characteristics: Dense vs. sparse data, balanced vs. imbalanced classes, feature distribution.\nModel Type: Linear models, tree-based models, deep learning models.\nComputational Resources: Available memory and processing power.\nAnnotation Budget: Number of samples that can be labeled.\nTask Complexity: Classification vs. regression, number of classes, difficulty of the task.\n\nBy carefully considering these factors and implementing the appropriate influence selection methods, you can build high-performance models with minimal annotation effort."
  },
  {
    "objectID": "posts/influence-selection/index.html#introduction",
    "href": "posts/influence-selection/index.html#introduction",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Active learning is a machine learning paradigm where the algorithm can interactively query an oracle (typically a human annotator) to label new data points. The key idea is to select the most informative samples to be labeled, reducing the overall labeling effort while maintaining or improving model performance. This guide focuses on influence selection methods used in active learning strategies."
  },
  {
    "objectID": "posts/influence-selection/index.html#table-of-contents",
    "href": "posts/influence-selection/index.html#table-of-contents",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Fundamentals of Active Learning\nInfluence Selection Strategies\nUncertainty-Based Methods\nDiversity-Based Methods\nExpected Model Change\nExpected Error Reduction\nInfluence Functions\nQuery-by-Committee\nImplementation Considerations\nEvaluation Metrics\nPractical Examples\nAdvanced Topics"
  },
  {
    "objectID": "posts/influence-selection/index.html#fundamentals-of-active-learning",
    "href": "posts/influence-selection/index.html#fundamentals-of-active-learning",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "The typical active learning process follows these steps:\n\nStart with a small labeled dataset and a large unlabeled pool\nTrain an initial model on the labeled data\nApply an influence selection strategy to choose informative samples from the unlabeled pool\nGet annotations for the selected samples\nAdd the newly labeled samples to the training set\nRetrain the model and repeat steps 3-6 until a stopping condition is met\n\n\n\n\n\nPool-based: The learner has access to a pool of unlabeled data and selects the most informative samples\nStream-based: Samples arrive sequentially, and the learner must decide on-the-fly whether to request labels"
  },
  {
    "objectID": "posts/influence-selection/index.html#influence-selection-strategies",
    "href": "posts/influence-selection/index.html#influence-selection-strategies",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Influence selection is about identifying which unlabeled samples would be most beneficial to label next. Here are the main strategies:"
  },
  {
    "objectID": "posts/influence-selection/index.html#uncertainty-based-methods",
    "href": "posts/influence-selection/index.html#uncertainty-based-methods",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "These methods select samples that the model is most uncertain about.\n\n\n\ndef least_confidence(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    \n    # Get the confidence values for the most probable class\n    confidences = np.max(probabilities, axis=1)\n    \n    # Select the k samples with the lowest confidence\n    least_confident_indices = np.argsort(confidences)[:k]\n    \n    return unlabeled_pool[least_confident_indices]\n\n\n\n\nSelects samples with the smallest margin between the two most likely classes:\n\ndef margin_sampling(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    \n    # Sort the probabilities in descending order\n    sorted_probs = np.sort(probabilities, axis=1)[:, ::-1]\n    \n    # Calculate the margin between the first and second most probable classes\n    margins = sorted_probs[:, 0] - sorted_probs[:, 1]\n    \n    # Select the k samples with the smallest margins\n    smallest_margin_indices = np.argsort(margins)[:k]\n    \n    return unlabeled_pool[smallest_margin_indices]\n\n\n\n\nSelects samples with the highest predictive entropy:\n\ndef entropy_sampling(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    \n    # Calculate entropy for each sample\n    entropies = -np.sum(probabilities * np.log(probabilities + 1e-10), axis=1)\n    \n    # Select the k samples with the highest entropy\n    highest_entropy_indices = np.argsort(entropies)[::-1][:k]\n    \n    return unlabeled_pool[highest_entropy_indices]\n\n\n\n\nFor Bayesian models, BALD selects samples that maximize the mutual information between predictions and model parameters:\n\ndef bald_sampling(bayesian_model, unlabeled_pool, k, n_samples=100):\n    # Get multiple predictions by sampling from the model's posterior\n    probs_samples = []\n    for _ in range(n_samples):\n        probs = bayesian_model.predict_proba(unlabeled_pool)\n        probs_samples.append(probs)\n    \n    # Stack into a 3D array: (samples, data points, classes)\n    probs_samples = np.stack(probs_samples)\n    \n    # Calculate the average probability across all samples\n    mean_probs = np.mean(probs_samples, axis=0)\n    \n    # Calculate the entropy of the average prediction\n    entropy_mean = -np.sum(mean_probs * np.log(mean_probs + 1e-10), axis=1)\n    \n    # Calculate the average entropy across all samples\n    entropy_samples = -np.sum(probs_samples * np.log(probs_samples + 1e-10), axis=2)\n    mean_entropy = np.mean(entropy_samples, axis=0)\n    \n    # Mutual information = entropy of the mean - mean of entropies\n    bald_scores = entropy_mean - mean_entropy\n    \n    # Select the k samples with the highest BALD scores\n    highest_bald_indices = np.argsort(bald_scores)[::-1][:k]\n    \n    return unlabeled_pool[highest_bald_indices]"
  },
  {
    "objectID": "posts/influence-selection/index.html#diversity-based-methods",
    "href": "posts/influence-selection/index.html#diversity-based-methods",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "These methods aim to select a diverse set of examples to ensure broad coverage of the input space.\n\n\n\ndef clustering_based_sampling(unlabeled_pool, k, n_clusters=None):\n    if n_clusters is None:\n        n_clusters = k\n    \n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(unlabeled_pool)\n    \n    # Get the cluster centers and distances to each point\n    centers = kmeans.cluster_centers_\n    distances = kmeans.transform(unlabeled_pool)  # Distance to each cluster center\n    \n    # Select one sample from each cluster (closest to the center)\n    selected_indices = []\n    for i in range(n_clusters):\n        # Get the samples in this cluster\n        cluster_samples = np.where(kmeans.labels_ == i)[0]\n        \n        # Find the sample closest to the center\n        closest_sample = cluster_samples[np.argmin(distances[cluster_samples, i])]\n        selected_indices.append(closest_sample)\n    \n    # If we need more samples than clusters, fill with the most uncertain samples\n    if k &gt; n_clusters:\n        # Implementation depends on uncertainty measure\n        pass\n    \n    return unlabeled_pool[selected_indices[:k]]\n\n\n\n\nThe core-set approach aims to select a subset of data that best represents the whole dataset:\n\ndef core_set_sampling(labeled_pool, unlabeled_pool, k):\n    # Combine labeled and unlabeled data for distance calculations\n    all_data = np.vstack((labeled_pool, unlabeled_pool))\n    \n    # Compute pairwise distances\n    distances = pairwise_distances(all_data)\n    \n    # Split distances into labeled-unlabeled and unlabeled-unlabeled\n    n_labeled = labeled_pool.shape[0]\n    dist_labeled_unlabeled = distances[:n_labeled, n_labeled:]\n    \n    # For each unlabeled sample, find the minimum distance to any labeled sample\n    min_distances = np.min(dist_labeled_unlabeled, axis=0)\n    \n    # Select the k samples with the largest minimum distances\n    farthest_indices = np.argsort(min_distances)[::-1][:k]\n    \n    return unlabeled_pool[farthest_indices]"
  },
  {
    "objectID": "posts/influence-selection/index.html#expected-model-change",
    "href": "posts/influence-selection/index.html#expected-model-change",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "The Expected Model Change (EMC) method selects samples that would cause the greatest change in the model if they were labeled:\n\ndef expected_model_change(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    n_classes = probabilities.shape[1]\n    \n    # Calculate expected gradient length for each sample\n    expected_changes = []\n    for i, x in enumerate(unlabeled_pool):\n        # Calculate expected gradient length across all possible labels\n        change = 0\n        for c in range(n_classes):\n            # For each possible class, calculate the gradient if this was the true label\n            x_expanded = x.reshape(1, -1)\n            # Here we would compute the gradient of the model with respect to the sample\n            # For simplicity, we use a placeholder\n            gradient = compute_gradient(model, x_expanded, c)\n            norm_gradient = np.linalg.norm(gradient)\n            \n            # Weight by the probability of this class\n            change += probabilities[i, c] * norm_gradient\n        \n        expected_changes.append(change)\n    \n    # Select the k samples with the highest expected change\n    highest_change_indices = np.argsort(expected_changes)[::-1][:k]\n    \n    return unlabeled_pool[highest_change_indices]\n\nNote: The compute_gradient function would need to be implemented based on the specific model being used."
  },
  {
    "objectID": "posts/influence-selection/index.html#expected-error-reduction",
    "href": "posts/influence-selection/index.html#expected-error-reduction",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "The Expected Error Reduction method selects samples that, when labeled, would minimally reduce the model’s expected error:\n\ndef expected_error_reduction(model, unlabeled_pool, unlabeled_pool_remaining, k):\n    # Predict probabilities for all remaining unlabeled data\n    current_probs = model.predict_proba(unlabeled_pool_remaining)\n    current_entropy = -np.sum(current_probs * np.log(current_probs + 1e-10), axis=1)\n    \n    expected_error_reductions = []\n    \n    # For each sample in the unlabeled pool we're considering\n    for i, x in enumerate(unlabeled_pool):\n        # Predict probabilities for this sample\n        probs = model.predict_proba(x.reshape(1, -1))[0]\n        \n        # Calculate the expected error reduction for each possible label\n        error_reduction = 0\n        for c in range(len(probs)):\n            # Create a hypothetical new model with this labeled sample\n            # For simplicity, we use a placeholder function\n            hypothetical_model = train_with_additional_sample(model, x, c)\n            \n            # Get new probabilities with this model\n            new_probs = hypothetical_model.predict_proba(unlabeled_pool_remaining)\n            new_entropy = -np.sum(new_probs * np.log(new_probs + 1e-10), axis=1)\n            \n            # Expected entropy reduction\n            reduction = np.sum(current_entropy - new_entropy)\n            \n            # Weight by the probability of this class\n            error_reduction += probs[c] * reduction\n        \n        expected_error_reductions.append(error_reduction)\n    \n    # Select the k samples with the highest expected error reduction\n    highest_reduction_indices = np.argsort(expected_error_reductions)[::-1][:k]\n    \n    return unlabeled_pool[highest_reduction_indices]\n\nNote: The train_with_additional_sample function would need to be implemented based on the specific model being used."
  },
  {
    "objectID": "posts/influence-selection/index.html#influence-functions",
    "href": "posts/influence-selection/index.html#influence-functions",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Influence functions approximate the effect of adding or removing a training example without retraining the model:\n\ndef influence_function_sampling(model, unlabeled_pool, labeled_pool, k, labels):\n    influences = []\n    \n    # For each unlabeled sample\n    for x_u in unlabeled_pool:\n        # Calculate the influence of adding this sample to the training set\n        influence = calculate_influence(model, x_u, labeled_pool, labels)\n        influences.append(influence)\n    \n    # Select the k samples with the highest influence\n    highest_influence_indices = np.argsort(influences)[::-1][:k]\n    \n    return unlabeled_pool[highest_influence_indices]\n\nNote: The calculate_influence function would need to be implemented based on the specific model and influence metric being used."
  },
  {
    "objectID": "posts/influence-selection/index.html#query-by-committee",
    "href": "posts/influence-selection/index.html#query-by-committee",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Query-by-Committee (QBC) methods train multiple models (a committee) and select samples where they disagree:\n\ndef query_by_committee(committee_models, unlabeled_pool, k):\n    # Get predictions from all committee members\n    all_predictions = []\n    for model in committee_models:\n        preds = model.predict(unlabeled_pool)\n        all_predictions.append(preds)\n    \n    # Stack predictions into a 2D array (committee members, data points)\n    all_predictions = np.stack(all_predictions)\n    \n    # Calculate disagreement (e.g., using vote entropy)\n    disagreements = []\n    for i in range(unlabeled_pool.shape[0]):\n        # Count votes for each class\n        votes = np.bincount(all_predictions[:, i])\n        # Normalize to get probabilities\n        vote_probs = votes / len(committee_models)\n        # Calculate entropy\n        entropy = -np.sum(vote_probs * np.log2(vote_probs + 1e-10))\n        disagreements.append(entropy)\n    \n    # Select the k samples with the highest disagreement\n    highest_disagreement_indices = np.argsort(disagreements)[::-1][:k]\n    \n    return unlabeled_pool[highest_disagreement_indices]"
  },
  {
    "objectID": "posts/influence-selection/index.html#implementation-considerations",
    "href": "posts/influence-selection/index.html#implementation-considerations",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "In practice, it’s often more efficient to select multiple samples at once. However, simply selecting the top-k samples may lead to redundancy. Consider using:\n\nGreedy Selection with Diversity: Select one sample at a time, then update the diversity metrics to avoid selecting similar samples.\n\n\ndef batch_selection_with_diversity(model, unlabeled_pool, k, lambda_diversity=0.5):\n    selected_indices = []\n    remaining_indices = list(range(len(unlabeled_pool)))\n    \n    # Calculate uncertainty scores for all samples\n    probabilities = model.predict_proba(unlabeled_pool)\n    entropies = -np.sum(probabilities * np.log(probabilities + 1e-10), axis=1)\n    \n    # Calculate distance matrix for diversity\n    distance_matrix = pairwise_distances(unlabeled_pool)\n    \n    for _ in range(k):\n        if not remaining_indices:\n            break\n        \n        scores = np.zeros(len(remaining_indices))\n        \n        # Calculate uncertainty scores\n        uncertainty_scores = entropies[remaining_indices]\n        \n        # Calculate diversity scores (if we have already selected some samples)\n        if selected_indices:\n            # For each remaining sample, calculate the minimum distance to any selected sample\n            diversity_scores = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n        else:\n            diversity_scores = np.zeros(len(remaining_indices))\n        \n        # Normalize scores\n        uncertainty_scores = (uncertainty_scores - np.min(uncertainty_scores)) / (np.max(uncertainty_scores) - np.min(uncertainty_scores) + 1e-10)\n        if selected_indices:\n            diversity_scores = (diversity_scores - np.min(diversity_scores)) / (np.max(diversity_scores) - np.min(diversity_scores) + 1e-10)\n        \n        # Combine scores\n        scores = (1 - lambda_diversity) * uncertainty_scores + lambda_diversity * diversity_scores\n        \n        # Select the sample with the highest score\n        best_idx = np.argmax(scores)\n        selected_idx = remaining_indices[best_idx]\n        \n        # Add to selected and remove from remaining\n        selected_indices.append(selected_idx)\n        remaining_indices.remove(selected_idx)\n    \n    return unlabeled_pool[selected_indices]\n\n\nSubmodular Function Maximization: Use a submodular function to ensure diversity in the selected batch.\n\n\n\n\nActive learning can inadvertently reinforce class imbalance. Consider:\n\nStratified Sampling: Ensure representation from all classes.\nHybrid Approaches: Combine uncertainty-based and density-based methods.\nDiversity Constraints: Explicitly enforce diversity in feature space.\n\n\n\n\nSome methods (like expected error reduction) can be computationally expensive. Consider:\n\nSubsample the Unlabeled Pool: Only consider a random subset for selection.\nPre-compute Embeddings: Use a fixed feature extractor to pre-compute embeddings.\nApproximate Methods: Use approximations for expensive operations."
  },
  {
    "objectID": "posts/influence-selection/index.html#evaluation-metrics",
    "href": "posts/influence-selection/index.html#evaluation-metrics",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Plot model performance vs. number of labeled samples:\n\ndef plot_learning_curve(model_factory, X_train, y_train, X_test, y_test, \n                        active_learning_strategy, initial_size=10, \n                        batch_size=10, n_iterations=20):\n    # Initialize with a small labeled set\n    labeled_indices = np.random.choice(len(X_train), initial_size, replace=False)\n    unlabeled_indices = np.setdiff1d(np.arange(len(X_train)), labeled_indices)\n    \n    performance = []\n    \n    for i in range(n_iterations):\n        # Create a fresh model\n        model = model_factory()\n        \n        # Train on the currently labeled data\n        model.fit(X_train[labeled_indices], y_train[labeled_indices])\n        \n        # Evaluate on the test set\n        score = model.score(X_test, y_test)\n        performance.append((len(labeled_indices), score))\n        \n        # Select the next batch of samples\n        if len(unlabeled_indices) &gt; 0:\n            # Use the specified active learning strategy\n            selected_indices = active_learning_strategy(\n                model, X_train[unlabeled_indices], batch_size\n            )\n            \n            # Map back to original indices\n            selected_original_indices = unlabeled_indices[selected_indices]\n            \n            # Update labeled and unlabeled indices\n            labeled_indices = np.append(labeled_indices, selected_original_indices)\n            unlabeled_indices = np.setdiff1d(unlabeled_indices, selected_original_indices)\n    \n    # Plot the learning curve\n    counts, scores = zip(*performance)\n    plt.figure(figsize=(10, 6))\n    plt.plot(counts, scores, 'o-')\n    plt.xlabel('Number of labeled samples')\n    plt.ylabel('Model accuracy')\n    plt.title('Active Learning Performance')\n    plt.grid(True)\n    \n    return performance\n\n\n\n\nAlways compare your active learning strategy with random sampling as a baseline.\n\n\n\nCalculate how many annotations you saved compared to using the entire dataset."
  },
  {
    "objectID": "posts/influence-selection/index.html#practical-examples",
    "href": "posts/influence-selection/index.html#practical-examples",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "import numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n# Load data\nmnist = fetch_openml('mnist_784', version=1, cache=True)\nX, y = mnist['data'], mnist['target']\n\n# Split into train and test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Initially, only a small portion is labeled\ninitial_size = 100\nlabeled_indices = np.random.choice(len(X_train), initial_size, replace=False)\nunlabeled_indices = np.setdiff1d(np.arange(len(X_train)), labeled_indices)\n\n# Tracking performance\nactive_learning_performance = []\nrandom_sampling_performance = []\n\n# Active learning loop\nfor i in range(10):  # 10 iterations\n    # Train a model on the currently labeled data\n    model = RandomForestClassifier(n_estimators=50, random_state=42)\n    model.fit(X_train.iloc[labeled_indices], y_train.iloc[labeled_indices])\n    \n    # Evaluate on the test set\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    active_learning_performance.append((len(labeled_indices), accuracy))\n    \n    print(f\"Iteration {i+1}: {len(labeled_indices)} labeled samples, \"\n          f\"accuracy: {accuracy:.4f}\")\n    \n    # Select 100 new samples using entropy sampling\n    if len(unlabeled_indices) &gt; 0:\n        # Predict probabilities for each unlabeled sample\n        probs = model.predict_proba(X_train.iloc[unlabeled_indices])\n        \n        # Calculate entropy\n        entropies = -np.sum(probs * np.log(probs + 1e-10), axis=1)\n        \n        # Select samples with the highest entropy\n        top_indices = np.argsort(entropies)[::-1][:100]\n        \n        # Update labeled and unlabeled indices\n        selected_indices = unlabeled_indices[top_indices]\n        labeled_indices = np.append(labeled_indices, selected_indices)\n        unlabeled_indices = np.setdiff1d(unlabeled_indices, selected_indices)\n\n# Plot learning curve\ncounts, scores = zip(*active_learning_performance)\nplt.figure(figsize=(10, 6))\nplt.plot(counts, scores, 'o-', label='Active Learning')\nplt.xlabel('Number of labeled samples')\nplt.ylabel('Model accuracy')\nplt.title('Active Learning Performance')\nplt.grid(True)\nplt.legend()\nplt.show()\n\nIteration 1: 100 labeled samples, accuracy: 0.6755\nIteration 2: 200 labeled samples, accuracy: 0.7300\nIteration 3: 300 labeled samples, accuracy: 0.7716\nIteration 4: 400 labeled samples, accuracy: 0.8097\nIteration 5: 500 labeled samples, accuracy: 0.8359\nIteration 6: 600 labeled samples, accuracy: 0.8366\nIteration 7: 700 labeled samples, accuracy: 0.8494\nIteration 8: 800 labeled samples, accuracy: 0.8509\nIteration 9: 900 labeled samples, accuracy: 0.8678\nIteration 10: 1000 labeled samples, accuracy: 0.8758\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.datasets import fetch_20newsgroups\n\n# Load data\ncategories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\ntwenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\ntwenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n\n# Feature extraction\nvectorizer = TfidfVectorizer(stop_words='english')\nX_train = vectorizer.fit_transform(twenty_train.data)\nX_test = vectorizer.transform(twenty_test.data)\ny_train = twenty_train.target\ny_test = twenty_test.target\n\n# Initially, only a small portion is labeled\ninitial_size = 20\nlabeled_indices = np.random.choice(len(X_train.toarray()), initial_size, replace=False)\nunlabeled_indices = np.setdiff1d(np.arange(len(X_train.toarray())), labeled_indices)\n\n# Create a committee of models\nmodels = [\n    ('nb', MultinomialNB()),\n    ('svm', SVC(kernel='linear', probability=True)),\n    ('svm2', SVC(kernel='rbf', probability=True))\n]\n\n# Active learning loop\nfor i in range(10):  # 10 iterations\n    # Train each model on the currently labeled data\n    committee_models = []\n    for name, model in models:\n        model.fit(X_train[labeled_indices], y_train[labeled_indices])\n        committee_models.append(model)\n    \n    # Evaluate using the VotingClassifier\n    voting_clf = VotingClassifier(estimators=models, voting='soft')\n    voting_clf.fit(X_train[labeled_indices], y_train[labeled_indices])\n    \n    accuracy = voting_clf.score(X_test, y_test)\n    print(f\"Iteration {i+1}: {len(labeled_indices)} labeled samples, \"\n          f\"accuracy: {accuracy:.4f}\")\n    \n    # Select 10 new samples using Query-by-Committee\n    if len(unlabeled_indices) &gt; 0:\n        # Get predictions from all committee members\n        all_predictions = []\n        for model in committee_models:\n            preds = model.predict(X_train[unlabeled_indices])\n            all_predictions.append(preds)\n        \n        # Calculate vote entropy\n        vote_entropies = []\n        all_predictions = np.array(all_predictions)\n        for i in range(len(unlabeled_indices)):\n            # Count votes for each class\n            votes = np.bincount(all_predictions[:, i], minlength=len(categories))\n            # Normalize to get probabilities\n            vote_probs = votes / len(committee_models)\n            # Calculate entropy\n            entropy = -np.sum(vote_probs * np.log2(vote_probs + 1e-10))\n            vote_entropies.append(entropy)\n        \n        # Select samples with the highest vote entropy\n        top_indices = np.argsort(vote_entropies)[::-1][:10]\n        \n        # Update labeled and unlabeled indices\n        selected_indices = unlabeled_indices[top_indices]\n        labeled_indices = np.append(labeled_indices, selected_indices)\n        unlabeled_indices = np.setdiff1d(unlabeled_indices, selected_indices)\n\nIteration 1: 20 labeled samples, accuracy: 0.2124\nIteration 2: 30 labeled samples, accuracy: 0.2310\nIteration 3: 40 labeled samples, accuracy: 0.3668\nIteration 4: 50 labeled samples, accuracy: 0.2983\nIteration 5: 60 labeled samples, accuracy: 0.4627\nIteration 6: 70 labeled samples, accuracy: 0.6312\nIteration 7: 80 labeled samples, accuracy: 0.6864\nIteration 8: 90 labeled samples, accuracy: 0.6125\nIteration 9: 100 labeled samples, accuracy: 0.7597\nIteration 10: 110 labeled samples, accuracy: 0.7870"
  },
  {
    "objectID": "posts/influence-selection/index.html#advanced-topics",
    "href": "posts/influence-selection/index.html#advanced-topics",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Combining transfer learning with active learning can be powerful:\n\nUse pre-trained models as feature extractors.\nApply active learning on the feature space.\nFine-tune the model on the selected samples.\n\n\n\n\nSpecial considerations for deep learning models:\n\nUncertainty Estimation: Use dropout or ensemble methods for better uncertainty estimation.\nBatch Normalization: Be careful with batch normalization layers when retraining.\nData Augmentation: Apply data augmentation to increase the effective size of the labeled pool.\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Subset\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\n# Define a simple CNN\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x, dropout=True):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        if dropout:\n            x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        if dropout:\n            x = self.dropout2(x)\n        x = self.fc2(x)\n        return x\n\n# MC Dropout for uncertainty estimation\ndef mc_dropout_uncertainty(model, data_loader, n_samples=10):\n    model.eval()\n    all_probs = []\n    \n    with torch.no_grad():\n        for _ in range(n_samples):\n            batch_probs = []\n            for data, _ in data_loader:\n                output = model(data, dropout=True)\n                probs = F.softmax(output, dim=1)\n                batch_probs.append(probs)\n            \n            # Concatenate batch probabilities\n            all_probs.append(torch.cat(batch_probs))\n    \n    # Stack along a new dimension\n    all_probs = torch.stack(all_probs)\n    \n    # Calculate the mean probabilities\n    mean_probs = torch.mean(all_probs, dim=0)\n    \n    # Calculate entropy of the mean prediction\n    entropy = -torch.sum(mean_probs * torch.log(mean_probs + 1e-10), dim=1)\n    \n    return entropy.numpy()\n\n\n\n\nLeverage both labeled and unlabeled data during training:\n\nSelf-Training: Use model predictions on unlabeled data as pseudo-labels.\nCo-Training: Train multiple models and use their predictions to teach each other.\nConsistency Regularization: Enforce consistent predictions across different perturbations.\n\n\ndef semi_supervised_active_learning(labeled_X, labeled_y, unlabeled_X, model, confidence_threshold=0.95):\n    # Train model on labeled data\n    model.fit(labeled_X, labeled_y)\n    \n    # Predict on unlabeled data\n    probabilities = model.predict_proba(unlabeled_X)\n    max_probs = np.max(probabilities, axis=1)\n    \n    # Get high confidence predictions\n    confident_indices = np.where(max_probs &gt;= confidence_threshold)[0]\n    \n    # Get pseudo-labels for confident predictions\n    pseudo_labels = model.predict(unlabeled_X[confident_indices])\n    \n    # Train on combined dataset\n    combined_X = np.vstack([labeled_X, unlabeled_X[confident_indices]])\n    combined_y = np.concatenate([labeled_y, pseudo_labels])\n    \n    model.fit(combined_X, combined_y)\n    \n    return model, confident_indices\n\n\n\n\nWhen labeled data from the target domain is scarce, active learning can help select the most informative samples:\n\nDomain Discrepancy Measures: Select samples that minimize domain discrepancy.\nAdversarial Selection: Select samples that the domain discriminator is most uncertain about.\nFeature Space Alignment: Select samples that help align feature spaces between domains.\n\n\n\n\n\nAnnotation Interface Design: Make the annotation process intuitive and efficient.\nCognitive Load Management: Group similar samples to reduce cognitive switching.\nExplanations: Provide model explanations to help annotators understand the current model’s decisions.\nQuality Control: Incorporate mechanisms to detect and correct annotation errors."
  },
  {
    "objectID": "posts/influence-selection/index.html#conclusion",
    "href": "posts/influence-selection/index.html#conclusion",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Active learning provides a powerful framework for efficiently building machine learning models with limited labeled data. By selecting the most informative samples for annotation, active learning can significantly reduce the labeling effort while maintaining high model performance.\nThe key to successful active learning is choosing the right influence selection strategy for your specific problem and data characteristics. Consider the following when designing your active learning pipeline:\n\nData Characteristics: Dense vs. sparse data, balanced vs. imbalanced classes, feature distribution.\nModel Type: Linear models, tree-based models, deep learning models.\nComputational Resources: Available memory and processing power.\nAnnotation Budget: Number of samples that can be labeled.\nTask Complexity: Classification vs. regression, number of classes, difficulty of the task.\n\nBy carefully considering these factors and implementing the appropriate influence selection methods, you can build high-performance models with minimal annotation effort."
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html",
    "href": "posts/data-visualization-tutorial/index.html",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "This guide compares three popular Python data visualization libraries: Matplotlib, Seaborn, and Altair (Vega-Altair). Each library has its own strengths, weaknesses, and ideal use cases. This comparison will help you choose the right tool for your specific visualization needs.\n\n\n\n\n\n\n\n\n\n\n\nFeature\nMatplotlib\nSeaborn\nAltair\n\n\n\n\nRelease Year\n2003\n2013\n2016\n\n\nFoundation\nStandalone\nBuilt on Matplotlib\nBased on Vega-Lite\n\n\nPhilosophy\nImperative\nStatistical\nDeclarative\n\n\nAbstraction Level\nLow\nMedium\nHigh\n\n\nLearning Curve\nSteep\nModerate\nGentle\n\n\nCode Verbosity\nHigh\nMedium\nLow\n\n\nCustomization\nExtensive\nGood\nLimited\n\n\nStatistical Integration\nManual\nBuilt-in\nGood\n\n\nInteractive Features\nLimited\nLimited\nExcellent\n\n\nPerformance with Large Data\nGood\nModerate\nLimited\n\n\nCommunity & Resources\nExtensive\nGood\nGrowing\n\n\n\n\n\n\nMatplotlib is the foundational plotting library in Python’s data visualization ecosystem.\n\n\n\nFine-grained control: Almost every aspect of a visualization can be customized\nVersatility: Can create virtually any type of static plot\nMaturity: Extensive documentation and community support\nEcosystem integration: Many libraries integrate with or build upon Matplotlib\nPerformance: Handles large datasets well\n\n\n\n\n\nVerbose syntax: Requires many lines of code for complex visualizations\nSteep learning curve: Many functions and parameters to learn\nDefault aesthetics: Basic default styling (though this has improved)\nLimited interactivity: Primarily designed for static plots\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\n# Create figure and axis\nfig, ax = plt.subplots(figsize=(8, 4))\n\n# Plot data\nax.plot(x, y, label='Sine Wave')\n\n# Add grid, legend, title and labels\nax.grid(True)\nax.set_xlabel('X-axis')\nax.set_ylabel('Y-axis')\nax.set_title('Simple Sine Wave Plot')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nYou need complete control over every aspect of your visualization\nYou’re creating complex, publication-quality figures\nYou’re working with specialized plot types not available in higher-level libraries\nYou need to integrate with many other Python libraries\nYou’re working with large datasets\n\n\n\n\n\nSeaborn is a statistical visualization library built on top of Matplotlib.\n\n\n\nAesthetic defaults: Beautiful out-of-the-box styling\nStatistical integration: Built-in support for statistical visualizations\nDataset awareness: Works well with pandas DataFrames\nSimplicity: Fewer lines of code than Matplotlib for common plots\nHigh-level functions: Specialized plots like lmplot, catplot, etc.\n\n\n\n\n\nLimited customization: Some advanced customizations require falling back to Matplotlib\nPerformance: Can be slower with very large datasets\nRestricted scope: Focused on statistical visualization, not general-purpose plotting\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Create sample data\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.2, size=len(x))\ndata = pd.DataFrame({'x': x, 'y': y})\n\n# Set the aesthetic style\nsns.set_theme(style=\"whitegrid\")\n\n# Create the plot\nplt.figure(figsize=(8, 4))\nsns.lineplot(data=data, x='x', y='y', label='Noisy Sine Wave')\nsns.regplot(data=data, x='x', y='y', scatter=False, label='Regression Line')\n\n# Add title and labels\nplt.title('Seaborn Line Plot with Regression')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nYou want attractive visualizations with minimal code\nYou’re performing statistical analysis\nYou’re working with pandas DataFrames\nYou’re creating common statistical plots (distributions, relationships, categorical plots)\nYou want the power of Matplotlib with a simpler interface\n\n\n\n\n\nAltair is a declarative statistical visualization library based on Vega-Lite.\n\n\n\nDeclarative approach: Focus on what to visualize, not how to draw it\nConcise syntax: Very readable, clear code\nLayered grammar of graphics: Intuitive composition of plots\nInteractive visualizations: Built-in support for interactive features\nJSON output: Visualizations can be saved as JSON specifications\n\n\n\n\n\nPerformance limitations: Not ideal for very large datasets (&gt;5000 points)\nLimited customization: Less fine-grained control than Matplotlib\nLearning curve: Different paradigm from traditional plotting libraries\nBrowser dependency: Uses JavaScript rendering for advanced features\n\n\n\n\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\n# Create sample data\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.2, size=len(x))\ndata = pd.DataFrame({'x': x, 'y': y})\n\n# Create a simple scatter plot with interactive tooltips\nchart = alt.Chart(data).mark_circle().encode(\n    x='x',\n    y='y',\n    tooltip=['x', 'y']\n).properties(\n    width=600,\n    height=300,\n    title='Interactive Altair Scatter Plot'\n).interactive()\n\n# Add a regression line\nregression = alt.Chart(data).transform_regression(\n    'x', 'y'\n).mark_line(color='red').encode(\n    x='x',\n    y='y'\n)\n\n# Combine the plots\nfinal_chart = chart + regression\n\n# Display the chart\nfinal_chart\n\n\n\n\n\n\n\n\n\n\n\nYou want interactive visualizations\nYou prefer a declarative approach to visualization\nYou’re working with small to medium-sized datasets\nYou want to publish visualizations on the web\nYou appreciate a consistent grammar of graphics\n\n\n\n\n\n\n\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.random.randn(100)\ny = np.random.randn(100)\n\nplt.figure(figsize=(8, 6))\nplt.scatter(x, y, alpha=0.7)\nplt.title('Matplotlib Scatter Plot')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndata = pd.DataFrame({\n    'x': np.random.randn(100),\n    'y': np.random.randn(100)\n})\n\nsns.set_theme(style=\"whitegrid\")\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=data, x='x', y='y', alpha=0.7)\nplt.title('Seaborn Scatter Plot')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame({\n    'x': np.random.randn(100),\n    'y': np.random.randn(100)\n})\n\nalt.Chart(data).mark_circle(opacity=0.7).encode(\n    x='x',\n    y='y'\n).properties(\n    width=500,\n    height=400,\n    title='Altair Scatter Plot'\n)\n\n\n\n\n\n\n\n\n\n\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.randn(1000)\n\nplt.figure(figsize=(8, 6))\nplt.hist(data, bins=30, alpha=0.7, edgecolor='black')\nplt.title('Matplotlib Histogram')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.randn(1000)\n\nsns.set_theme(style=\"whitegrid\")\nplt.figure(figsize=(8, 6))\nsns.histplot(data=data, bins=30, kde=True)\nplt.title('Seaborn Histogram with KDE')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame({'value': np.random.randn(1000)})\n\nalt.Chart(data).mark_bar().encode(\n    alt.X('value', bin=alt.Bin(maxbins=30)),\n    y='count()'\n).properties(\n    width=500,\n    height=400,\n    title='Altair Histogram'\n)\n\n\n\n\n\n\n\n\n\n\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny1 = np.sin(x)\ny2 = np.cos(x)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y1, label='Sine')\nplt.plot(x, y2, label='Cosine')\nplt.title('Matplotlib Line Plot')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nx = np.linspace(0, 10, 100)\ndata = pd.DataFrame({\n    'x': np.concatenate([x, x]),\n    'y': np.concatenate([np.sin(x), np.cos(x)]),\n    'function': ['Sine']*100 + ['Cosine']*100\n})\n\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=data, x='x', y='y', hue='function')\nplt.title('Seaborn Line Plot')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ndata = pd.DataFrame({\n    'x': np.concatenate([x, x]),\n    'y': np.concatenate([np.sin(x), np.cos(x)]),\n    'function': ['Sine']*100 + ['Cosine']*100\n})\n\nalt.Chart(data).mark_line().encode(\n    x='x',\n    y='y',\n    color='function'\n).properties(\n    width=600,\n    height=400,\n    title='Altair Line Plot'\n)\n\n\n\n\n\n\n\n\n\n\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.rand(10, 12)\n\nplt.figure(figsize=(10, 8))\nplt.imshow(data, cmap='viridis')\nplt.colorbar(label='Value')\nplt.title('Matplotlib Heatmap')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.rand(10, 12)\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(data, annot=True, cmap='viridis', fmt='.2f')\nplt.title('Seaborn Heatmap')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\n# Create sample data\ndata = np.random.rand(10, 12)\ndf = pd.DataFrame(data)\n\n# Reshape for Altair\ndf_long = df.reset_index().melt(id_vars='index')\ndf_long.columns = ['y', 'x', 'value']\n\nalt.Chart(df_long).mark_rect().encode(\n    x='x:O',\n    y='y:O',\n    color='value:Q'\n).properties(\n    width=500,\n    height=400,\n    title='Altair Heatmap'\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou need complete control over every detail of your visualization\nYou’re creating complex, custom plots\nYour visualizations will be included in scientific publications\nYou’re working with very large datasets\nYou need to create animations or specialized chart types\n\n\n\n\n\nYou want attractive plots with minimal code\nYou’re performing statistical analysis\nYou want to create common statistical plots quickly\nYou need to visualize relationships between variables\nYou want good-looking defaults but still need some customization\n\n\n\n\n\nYou want interactive visualizations\nYou prefer a declarative approach to visualization\nYou want concise, readable code\nYou’re creating dashboards or web-based visualizations\nYou’re working with small to medium-sized datasets\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\n# Create sample data\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'x': np.random.normal(0, 1, 100),\n    'y': np.random.normal(0, 1, 100),\n    'category': np.random.choice(['A', 'B', 'C'], 100)\n})\n\n# Create a figure with Matplotlib\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Use Seaborn for the main plot\nsns.scatterplot(data=data, x='x', y='y', hue='category', ax=ax)\n\n# Add Matplotlib customizations\nax.set_title('Combining Matplotlib and Seaborn', fontsize=16)\nax.grid(True, linestyle='--', alpha=0.7)\nax.set_xlabel('X Variable', fontsize=12)\nax.set_ylabel('Y Variable', fontsize=12)\n\n# Add annotations using Matplotlib\nax.annotate('Interesting Point', xy=(-1, 1), xytext=(-2, 1.5),\n            arrowprops=dict(facecolor='black', shrink=0.05))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\n# Create sample data with pandas\nnp.random.seed(42)\ndf = pd.DataFrame({\n    'date': pd.date_range('2023-01-01', periods=100),\n    'value': np.cumsum(np.random.randn(100)),\n    'category': np.random.choice(['Group A', 'Group B'], 100)\n})\n\n# Use pandas to prepare the data\ndf['month'] = df['date'].dt.month\nmonthly_avg = df.groupby(['month', 'category'])['value'].mean().reset_index()\n\n# Create the Altair visualization\nchart = alt.Chart(monthly_avg).mark_line(point=True).encode(\n    x='month:O',\n    y='value:Q',\n    color='category:N',\n    tooltip=['month', 'value', 'category']\n).properties(\n    width=600,\n    height=400,\n    title='Monthly Averages by Category'\n).interactive()\n\nchart\n\n\n\n\n\n\n\n\n\n\n\nFor libraries like Matplotlib, Seaborn, and Altair, performance can vary widely depending on the size of your dataset and the complexity of your visualization. Here’s a general overview:\n\n\n\nAll three libraries perform well\nAltair might have slightly more overhead due to its JSON specification generation\n\n\n\n\n\nMatplotlib and Seaborn continue to perform well\nAltair starts to slow down but remains usable\n\n\n\n\n\nMatplotlib performs best for large static visualizations\nSeaborn becomes slower as it adds statistical computations\nAltair significantly slows down and may require data aggregation\n\n\n\n\n\nMatplotlib: Use plot() instead of scatter() for line plots, or try hexbin() for density plots\nSeaborn: Use sample() or aggregation methods before plotting\nAltair: Use transform_sample() or pre-aggregate your data\n\n\n\n\n\nThe Python visualization ecosystem offers tools for every need, from low-level control to high-level abstraction:\n\nMatplotlib provides ultimate flexibility and control but requires more code and knowledge\nSeaborn offers a perfect middle ground with statistical integration and clean defaults\nAltair delivers a concise, declarative approach with built-in interactivity\n\nRather than picking just one library, consider becoming familiar with all three and selecting the right tool for each visualization task. Many data scientists use a combination of these libraries, leveraging the strengths of each one as needed.\nFor those just starting, Seaborn provides a gentle entry point with attractive results for common visualization needs. As your skills advance, you can incorporate Matplotlib for customization and Altair for interactive visualizations."
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#quick-reference-comparison",
    "href": "posts/data-visualization-tutorial/index.html#quick-reference-comparison",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "Feature\nMatplotlib\nSeaborn\nAltair\n\n\n\n\nRelease Year\n2003\n2013\n2016\n\n\nFoundation\nStandalone\nBuilt on Matplotlib\nBased on Vega-Lite\n\n\nPhilosophy\nImperative\nStatistical\nDeclarative\n\n\nAbstraction Level\nLow\nMedium\nHigh\n\n\nLearning Curve\nSteep\nModerate\nGentle\n\n\nCode Verbosity\nHigh\nMedium\nLow\n\n\nCustomization\nExtensive\nGood\nLimited\n\n\nStatistical Integration\nManual\nBuilt-in\nGood\n\n\nInteractive Features\nLimited\nLimited\nExcellent\n\n\nPerformance with Large Data\nGood\nModerate\nLimited\n\n\nCommunity & Resources\nExtensive\nGood\nGrowing"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#matplotlib",
    "href": "posts/data-visualization-tutorial/index.html#matplotlib",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "Matplotlib is the foundational plotting library in Python’s data visualization ecosystem.\n\n\n\nFine-grained control: Almost every aspect of a visualization can be customized\nVersatility: Can create virtually any type of static plot\nMaturity: Extensive documentation and community support\nEcosystem integration: Many libraries integrate with or build upon Matplotlib\nPerformance: Handles large datasets well\n\n\n\n\n\nVerbose syntax: Requires many lines of code for complex visualizations\nSteep learning curve: Many functions and parameters to learn\nDefault aesthetics: Basic default styling (though this has improved)\nLimited interactivity: Primarily designed for static plots\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\n# Create figure and axis\nfig, ax = plt.subplots(figsize=(8, 4))\n\n# Plot data\nax.plot(x, y, label='Sine Wave')\n\n# Add grid, legend, title and labels\nax.grid(True)\nax.set_xlabel('X-axis')\nax.set_ylabel('Y-axis')\nax.set_title('Simple Sine Wave Plot')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nYou need complete control over every aspect of your visualization\nYou’re creating complex, publication-quality figures\nYou’re working with specialized plot types not available in higher-level libraries\nYou need to integrate with many other Python libraries\nYou’re working with large datasets"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#seaborn",
    "href": "posts/data-visualization-tutorial/index.html#seaborn",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "Seaborn is a statistical visualization library built on top of Matplotlib.\n\n\n\nAesthetic defaults: Beautiful out-of-the-box styling\nStatistical integration: Built-in support for statistical visualizations\nDataset awareness: Works well with pandas DataFrames\nSimplicity: Fewer lines of code than Matplotlib for common plots\nHigh-level functions: Specialized plots like lmplot, catplot, etc.\n\n\n\n\n\nLimited customization: Some advanced customizations require falling back to Matplotlib\nPerformance: Can be slower with very large datasets\nRestricted scope: Focused on statistical visualization, not general-purpose plotting\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Create sample data\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.2, size=len(x))\ndata = pd.DataFrame({'x': x, 'y': y})\n\n# Set the aesthetic style\nsns.set_theme(style=\"whitegrid\")\n\n# Create the plot\nplt.figure(figsize=(8, 4))\nsns.lineplot(data=data, x='x', y='y', label='Noisy Sine Wave')\nsns.regplot(data=data, x='x', y='y', scatter=False, label='Regression Line')\n\n# Add title and labels\nplt.title('Seaborn Line Plot with Regression')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nYou want attractive visualizations with minimal code\nYou’re performing statistical analysis\nYou’re working with pandas DataFrames\nYou’re creating common statistical plots (distributions, relationships, categorical plots)\nYou want the power of Matplotlib with a simpler interface"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#altair-vega-altair",
    "href": "posts/data-visualization-tutorial/index.html#altair-vega-altair",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "Altair is a declarative statistical visualization library based on Vega-Lite.\n\n\n\nDeclarative approach: Focus on what to visualize, not how to draw it\nConcise syntax: Very readable, clear code\nLayered grammar of graphics: Intuitive composition of plots\nInteractive visualizations: Built-in support for interactive features\nJSON output: Visualizations can be saved as JSON specifications\n\n\n\n\n\nPerformance limitations: Not ideal for very large datasets (&gt;5000 points)\nLimited customization: Less fine-grained control than Matplotlib\nLearning curve: Different paradigm from traditional plotting libraries\nBrowser dependency: Uses JavaScript rendering for advanced features\n\n\n\n\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\n# Create sample data\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.2, size=len(x))\ndata = pd.DataFrame({'x': x, 'y': y})\n\n# Create a simple scatter plot with interactive tooltips\nchart = alt.Chart(data).mark_circle().encode(\n    x='x',\n    y='y',\n    tooltip=['x', 'y']\n).properties(\n    width=600,\n    height=300,\n    title='Interactive Altair Scatter Plot'\n).interactive()\n\n# Add a regression line\nregression = alt.Chart(data).transform_regression(\n    'x', 'y'\n).mark_line(color='red').encode(\n    x='x',\n    y='y'\n)\n\n# Combine the plots\nfinal_chart = chart + regression\n\n# Display the chart\nfinal_chart\n\n\n\n\n\n\n\n\n\n\n\nYou want interactive visualizations\nYou prefer a declarative approach to visualization\nYou’re working with small to medium-sized datasets\nYou want to publish visualizations on the web\nYou appreciate a consistent grammar of graphics"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#common-visualization-types-comparison",
    "href": "posts/data-visualization-tutorial/index.html#common-visualization-types-comparison",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "Matplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.random.randn(100)\ny = np.random.randn(100)\n\nplt.figure(figsize=(8, 6))\nplt.scatter(x, y, alpha=0.7)\nplt.title('Matplotlib Scatter Plot')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndata = pd.DataFrame({\n    'x': np.random.randn(100),\n    'y': np.random.randn(100)\n})\n\nsns.set_theme(style=\"whitegrid\")\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=data, x='x', y='y', alpha=0.7)\nplt.title('Seaborn Scatter Plot')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame({\n    'x': np.random.randn(100),\n    'y': np.random.randn(100)\n})\n\nalt.Chart(data).mark_circle(opacity=0.7).encode(\n    x='x',\n    y='y'\n).properties(\n    width=500,\n    height=400,\n    title='Altair Scatter Plot'\n)\n\n\n\n\n\n\n\n\n\n\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.randn(1000)\n\nplt.figure(figsize=(8, 6))\nplt.hist(data, bins=30, alpha=0.7, edgecolor='black')\nplt.title('Matplotlib Histogram')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.randn(1000)\n\nsns.set_theme(style=\"whitegrid\")\nplt.figure(figsize=(8, 6))\nsns.histplot(data=data, bins=30, kde=True)\nplt.title('Seaborn Histogram with KDE')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame({'value': np.random.randn(1000)})\n\nalt.Chart(data).mark_bar().encode(\n    alt.X('value', bin=alt.Bin(maxbins=30)),\n    y='count()'\n).properties(\n    width=500,\n    height=400,\n    title='Altair Histogram'\n)\n\n\n\n\n\n\n\n\n\n\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny1 = np.sin(x)\ny2 = np.cos(x)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y1, label='Sine')\nplt.plot(x, y2, label='Cosine')\nplt.title('Matplotlib Line Plot')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nx = np.linspace(0, 10, 100)\ndata = pd.DataFrame({\n    'x': np.concatenate([x, x]),\n    'y': np.concatenate([np.sin(x), np.cos(x)]),\n    'function': ['Sine']*100 + ['Cosine']*100\n})\n\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=data, x='x', y='y', hue='function')\nplt.title('Seaborn Line Plot')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ndata = pd.DataFrame({\n    'x': np.concatenate([x, x]),\n    'y': np.concatenate([np.sin(x), np.cos(x)]),\n    'function': ['Sine']*100 + ['Cosine']*100\n})\n\nalt.Chart(data).mark_line().encode(\n    x='x',\n    y='y',\n    color='function'\n).properties(\n    width=600,\n    height=400,\n    title='Altair Line Plot'\n)\n\n\n\n\n\n\n\n\n\n\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.rand(10, 12)\n\nplt.figure(figsize=(10, 8))\nplt.imshow(data, cmap='viridis')\nplt.colorbar(label='Value')\nplt.title('Matplotlib Heatmap')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.rand(10, 12)\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(data, annot=True, cmap='viridis', fmt='.2f')\nplt.title('Seaborn Heatmap')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\n# Create sample data\ndata = np.random.rand(10, 12)\ndf = pd.DataFrame(data)\n\n# Reshape for Altair\ndf_long = df.reset_index().melt(id_vars='index')\ndf_long.columns = ['y', 'x', 'value']\n\nalt.Chart(df_long).mark_rect().encode(\n    x='x:O',\n    y='y:O',\n    color='value:Q'\n).properties(\n    width=500,\n    height=400,\n    title='Altair Heatmap'\n)"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#decision-framework-for-choosing-a-library",
    "href": "posts/data-visualization-tutorial/index.html#decision-framework-for-choosing-a-library",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "You need complete control over every detail of your visualization\nYou’re creating complex, custom plots\nYour visualizations will be included in scientific publications\nYou’re working with very large datasets\nYou need to create animations or specialized chart types\n\n\n\n\n\nYou want attractive plots with minimal code\nYou’re performing statistical analysis\nYou want to create common statistical plots quickly\nYou need to visualize relationships between variables\nYou want good-looking defaults but still need some customization\n\n\n\n\n\nYou want interactive visualizations\nYou prefer a declarative approach to visualization\nYou want concise, readable code\nYou’re creating dashboards or web-based visualizations\nYou’re working with small to medium-sized datasets"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#integration-examples",
    "href": "posts/data-visualization-tutorial/index.html#integration-examples",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\n# Create sample data\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'x': np.random.normal(0, 1, 100),\n    'y': np.random.normal(0, 1, 100),\n    'category': np.random.choice(['A', 'B', 'C'], 100)\n})\n\n# Create a figure with Matplotlib\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Use Seaborn for the main plot\nsns.scatterplot(data=data, x='x', y='y', hue='category', ax=ax)\n\n# Add Matplotlib customizations\nax.set_title('Combining Matplotlib and Seaborn', fontsize=16)\nax.grid(True, linestyle='--', alpha=0.7)\nax.set_xlabel('X Variable', fontsize=12)\nax.set_ylabel('Y Variable', fontsize=12)\n\n# Add annotations using Matplotlib\nax.annotate('Interesting Point', xy=(-1, 1), xytext=(-2, 1.5),\n            arrowprops=dict(facecolor='black', shrink=0.05))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\n# Create sample data with pandas\nnp.random.seed(42)\ndf = pd.DataFrame({\n    'date': pd.date_range('2023-01-01', periods=100),\n    'value': np.cumsum(np.random.randn(100)),\n    'category': np.random.choice(['Group A', 'Group B'], 100)\n})\n\n# Use pandas to prepare the data\ndf['month'] = df['date'].dt.month\nmonthly_avg = df.groupby(['month', 'category'])['value'].mean().reset_index()\n\n# Create the Altair visualization\nchart = alt.Chart(monthly_avg).mark_line(point=True).encode(\n    x='month:O',\n    y='value:Q',\n    color='category:N',\n    tooltip=['month', 'value', 'category']\n).properties(\n    width=600,\n    height=400,\n    title='Monthly Averages by Category'\n).interactive()\n\nchart"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#performance-comparison",
    "href": "posts/data-visualization-tutorial/index.html#performance-comparison",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "For libraries like Matplotlib, Seaborn, and Altair, performance can vary widely depending on the size of your dataset and the complexity of your visualization. Here’s a general overview:\n\n\n\nAll three libraries perform well\nAltair might have slightly more overhead due to its JSON specification generation\n\n\n\n\n\nMatplotlib and Seaborn continue to perform well\nAltair starts to slow down but remains usable\n\n\n\n\n\nMatplotlib performs best for large static visualizations\nSeaborn becomes slower as it adds statistical computations\nAltair significantly slows down and may require data aggregation\n\n\n\n\n\nMatplotlib: Use plot() instead of scatter() for line plots, or try hexbin() for density plots\nSeaborn: Use sample() or aggregation methods before plotting\nAltair: Use transform_sample() or pre-aggregate your data"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#conclusion",
    "href": "posts/data-visualization-tutorial/index.html#conclusion",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "The Python visualization ecosystem offers tools for every need, from low-level control to high-level abstraction:\n\nMatplotlib provides ultimate flexibility and control but requires more code and knowledge\nSeaborn offers a perfect middle ground with statistical integration and clean defaults\nAltair delivers a concise, declarative approach with built-in interactivity\n\nRather than picking just one library, consider becoming familiar with all three and selecting the right tool for each visualization task. Many data scientists use a combination of these libraries, leveraging the strengths of each one as needed.\nFor those just starting, Seaborn provides a gentle entry point with attractive results for common visualization needs. As your skills advance, you can incorporate Matplotlib for customization and Altair for interactive visualizations."
  },
  {
    "objectID": "posts/pytorch-lightning/index.html",
    "href": "posts/pytorch-lightning/index.html",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "PyTorch Lightning is a lightweight wrapper for PyTorch that helps organize code and reduce boilerplate while adding powerful features for research and production. This guide will walk you through the basics to advanced techniques.\n\n\n\nIntroduction to PyTorch Lightning\nInstallation\nBasic Structure: The LightningModule\nDataModules\nTraining with Trainer\nCallbacks\nLogging\nDistributed Training\nHyperparameter Tuning\nModel Checkpointing\nProduction Deployment\nBest Practices\n\n\n\n\nPyTorch Lightning separates research code from engineering code, making models more: - Reproducible: The same code works across different hardware - Readable: Standard project structure makes collaboration easier - Scalable: Train on CPUs, GPUs, TPUs or clusters with no code changes\nLightning helps you focus on the science by handling the engineering details.\n\n\n\npip install pytorch-lightning\nFor the latest features, you can install from the source:\npip install git+https://github.com/Lightning-AI/lightning.git\n\n\n\nThe core component in Lightning is the LightningModule, which organizes your PyTorch code into a standardized structure:\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MNISTModel(pl.LightningModule):\n    def __init__(self, lr=0.001):\n        super().__init__()\n        self.save_hyperparameters()  # Saves learning_rate to hparams\n        self.layer_1 = nn.Linear(28 * 28, 128)\n        self.layer_2 = nn.Linear(128, 10)\n        self.lr = lr\n        \n    def forward(self, x):\n        batch_size, channels, width, height = x.size()\n        x = x.view(batch_size, -1)\n        x = self.layer_1(x)\n        x = F.relu(x)\n        x = self.layer_2(x)\n        return x\n        \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        self.log('train_loss', loss)\n        return loss\n        \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        self.log('val_loss', loss)\n        self.log('val_acc', acc)\n        \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        self.log('test_loss', loss)\n        self.log('test_acc', acc)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer\nThis basic structure includes: - Model architecture (__init__ and forward) - Training logic (training_step) - Validation logic (validation_step) - Test logic (test_step) - Optimization setup (configure_optimizers)\n\n\n\nLightning’s LightningDataModule encapsulates all data-related logic:\nfrom pytorch_lightning import LightningDataModule\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\n\nclass MNISTDataModule(LightningDataModule):\n    def __init__(self, data_dir='./data', batch_size=32):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n        \n    def prepare_data(self):\n        # Download data if needed\n        MNIST(self.data_dir, train=True, download=True)\n        MNIST(self.data_dir, train=False, download=True)\n        \n    def setup(self, stage=None):\n        # Assign train/val/test datasets\n        if stage == 'fit' or stage is None:\n            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n            \n        if stage == 'test' or stage is None:\n            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n            \n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=self.batch_size)\n        \n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n        \n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=self.batch_size)\nBenefits of LightningDataModule: - Encapsulates all data preparation logic - Makes data pipeline portable and reproducible - Simplifies sharing data pipelines between projects\n\n\n\nThe Lightning Trainer handles the training loop and validation:\nfrom pytorch_lightning import Trainer\n\n# Create model and data module\nmodel = MNISTModel()\ndata_module = MNISTDataModule()\n\n# Initialize trainer\ntrainer = Trainer(\n    max_epochs=10,\n    accelerator=\"auto\",  # Automatically use available GPU\n    devices=\"auto\",\n    logger=True,         # Use TensorBoard logger by default\n)\n\n# Train model\ntrainer.fit(model, datamodule=data_module)\n\n# Test model\ntrainer.test(model, datamodule=data_module)\nThe Trainer automatically handles: - Epoch and batch iteration - Optimizer steps - Logging metrics - Hardware acceleration (CPU, GPU, TPU) - Early stopping - Checkpointing - Multi-GPU training\n\n\ntrainer = Trainer(\n    max_epochs=10,                    # Maximum number of epochs\n    min_epochs=1,                     # Minimum number of epochs\n    accelerator=\"cpu\",                # Use CPU acceleration\n    devices=1,                        # Use 1 CPUs\n    precision=\"16-mixed\",             # Use mixed precision for faster training\n    gradient_clip_val=0.5,            # Clip gradients\n    accumulate_grad_batches=4,        # Accumulate gradients over 4 batches\n    log_every_n_steps=50,             # Log metrics every 50 steps\n    val_check_interval=0.25,          # Run validation 4 times per epoch\n    fast_dev_run=False,               # Debug mode (only run a few batches)\n    deterministic=True,               # Make training deterministic\n)\n\n\n\n\nCallbacks add functionality to the training loop without modifying the core code:\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n\n# Save the best model based on validation accuracy\ncheckpoint_callback = ModelCheckpoint(\n    monitor='val_acc',\n    dirpath='./checkpoints',\n    filename='mnist-{epoch:02d}-{val_acc:.2f}',\n    save_top_k=3,\n    mode='max',\n)\n\n# Stop training when validation loss stops improving\nearly_stop_callback = EarlyStopping(\n    monitor='val_loss',\n    patience=3,\n    verbose=True,\n    mode='min'\n)\n\n# Monitor learning rate\nlr_monitor = LearningRateMonitor(logging_interval='step')\n\n# Initialize trainer with callbacks\ntrainer = Trainer(\n    max_epochs=10,\n    callbacks=[checkpoint_callback, early_stop_callback, lr_monitor]\n)\n\n\nYou can create custom callbacks by extending the base Callback class:\nfrom pytorch_lightning.callbacks import Callback\n\nclass PrintingCallback(Callback):\n    def on_train_start(self, trainer, pl_module):\n        print(\"Training has started!\")\n        \n    def on_train_end(self, trainer, pl_module):\n        print(\"Training has finished!\")\n        \n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        if batch_idx % 100 == 0:\n            print(f\"Batch {batch_idx} completed\")\n\n\n\n\nLightning supports various loggers:\nfrom pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n\n# TensorBoard logger\ntensorboard_logger = TensorBoardLogger(save_dir=\"logs/\", name=\"mnist\")\n\n# Initialize trainer with loggers\ntrainer = Trainer(\n    max_epochs=10,\n    logger=[tensorboard_logger]\n)\nAdding metrics in your model:\ndef training_step(self, batch, batch_idx):\n    x, y = batch\n    logits = self(x)\n    loss = F.cross_entropy(logits, y)\n    \n    # Log scalar values\n    self.log('train_loss', loss)\n    \n    # Log learning rate\n    lr = self.optimizers().param_groups[0]['lr']\n    self.log('learning_rate', lr)\n    \n    # Log histograms (on GPU)\n    if batch_idx % 100 == 0:\n        self.logger.experiment.add_histogram('logits', logits, self.global_step)\n    \n    return loss\n\n\n\nLightning makes distributed training simple:\n# Single GPU\ntrainer = Trainer(accelerator=\"gpu\", devices=1)\n\n# Multiple GPUs (DDP strategy automatically selected)\ntrainer = Trainer(accelerator=\"gpu\", devices=4)\n\n# Specific GPU indices\ntrainer = Trainer(accelerator=\"gpu\", devices=[0, 1, 3])\n\n# TPU cores\ntrainer = Trainer(accelerator=\"tpu\", devices=8)\n\n# Advanced distributed training\ntrainer = Trainer(\n    accelerator=\"gpu\",\n    devices=4,\n    strategy=\"ddp\",  # Distributed Data Parallel\n    num_nodes=2      # Use 2 machines\n)\nOther distribution strategies: - ddp_spawn: Similar to DDP but uses spawn for multiprocessing - deepspeed: For very large models using DeepSpeed - fsdp: Fully Sharded Data Parallel for huge models\n\n\n\nLightning integrates well with optimization libraries:\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.tuner import Tuner\n\n# Create model\nmodel = MNISTModel()\ndata_module = MNISTDataModule()\n\n# Create trainer\ntrainer = Trainer(max_epochs=10)\n\n# Use Lightning's Tuner for auto-scaling batch size\ntuner = Tuner(trainer)\ntuner.scale_batch_size(model, datamodule=data_module)\n\n# Find optimal learning rate\nlr_finder = tuner.lr_find(model, datamodule=data_module)\nmodel.learning_rate = lr_finder.suggestion()\n\n# Train with optimized parameters\ntrainer.fit(model, datamodule=data_module)\n\n\n\nSaving and loading model checkpoints:\n# Save checkpoints during training\ncheckpoint_callback = ModelCheckpoint(\n    dirpath='./checkpoints',\n    filename='{epoch}-{val_loss:.2f}',\n    monitor='val_loss',\n    save_top_k=3,\n    mode='min'\n)\n\ntrainer = Trainer(\n    max_epochs=10,\n    callbacks=[checkpoint_callback]\n)\n\ntrainer.fit(model, datamodule=data_module)\n\n# Path to best checkpoint\nbest_model_path = checkpoint_callback.best_model_path\n\n# Load checkpoint into model\nmodel = MNISTModel.load_from_checkpoint(best_model_path)\n\n# Continue training from checkpoint\ntrainer.fit(model, datamodule=data_module)\nCheckpointing for production:\n# Save model in production-ready format\ntrainer.save_checkpoint(\"model.ckpt\")\n\n# Extract state dict for production\ncheckpoint = torch.load(\"model.ckpt\")\nmodel_state_dict = checkpoint[\"state_dict\"]\n\n# Save just the PyTorch model for production\nmodel = MNISTModel()\nmodel.load_state_dict(model_state_dict)\ntorch.save(model.state_dict(), \"production_model.pt\")\n\n\n\nConverting Lightning models to production:\n# Option 1: Use the Lightning model directly\nmodel = MNISTModel.load_from_checkpoint(\"model.ckpt\")\nmodel.eval()\nmodel.freeze()  # Freeze the model parameters\n\n# Make predictions\nwith torch.no_grad():\n    x = torch.randn(1, 1, 28, 28)\n    y_hat = model(x)\n\n# Option 2: Extract the PyTorch model\nclass ProductionModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(28 * 28, 128)\n        self.layer_2 = nn.Linear(128, 10)\n    \n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.layer_1(x))\n        x = self.layer_2(x)\n        return x\n\n# Load weights from Lightning model\nlightning_model = MNISTModel.load_from_checkpoint(\"model.ckpt\")\nproduction_model = ProductionModel()\n\n# Copy weights\nproduction_model.layer_1.weight.data = lightning_model.layer_1.weight.data\nproduction_model.layer_1.bias.data = lightning_model.layer_1.bias.data\nproduction_model.layer_2.weight.data = lightning_model.layer_2.weight.data\nproduction_model.layer_2.bias.data = lightning_model.layer_2.bias.data\n\n# Save production model\ntorch.save(production_model.state_dict(), \"production_model.pt\")\n\n# Export to ONNX\ndummy_input = torch.randn(1, 1, 28, 28)\ntorch.onnx.export(\n    production_model,\n    dummy_input,\n    \"model.onnx\",\n    export_params=True,\n    opset_version=11,\n    input_names=['input'],\n    output_names=['output']\n)\n\n\n\n\n\nA well-organized Lightning project structure:\nproject/\n├── configs/              # Configuration files\n├── data/                 # Data files\n├── lightning_logs/       # Generated logs\n├── models/               # Model definitions\n│   ├── __init__.py\n│   └── mnist_model.py    # LightningModule\n├── data_modules/         # Data modules\n│   ├── __init__.py\n│   └── mnist_data.py     # LightningDataModule\n├── callbacks/            # Custom callbacks\n├── utils/                # Utility functions\n├── main.py               # Training script\n└── README.md\n\n\n\nLightning provides a CLI for running experiments from config files:\n# main.py\nfrom pytorch_lightning.cli import LightningCLI\n\ndef cli_main():\n    # Create CLI with LightningModule and LightningDataModule\n    cli = LightningCLI(\n        MNISTModel,\n        MNISTDataModule,\n        save_config_callback=None,\n    )\n\nif __name__ == \"__main__\":\n    cli_main()\nRun with:\npython main.py fit --config configs/mnist.yaml\nExample config file (mnist.yaml):\nmodel:\n  learning_rate: 0.001\n  hidden_size: 128\ndata:\n  data_dir: ./data\n  batch_size: 64\ntrainer:\n  max_epochs: 10\n  accelerator: gpu\n  devices: 1\n\n\n\nLightning includes tools to profile your code:\nfrom pytorch_lightning.profilers import PyTorchProfiler, SimpleProfiler\n\n# PyTorch Profiler\nprofiler = PyTorchProfiler(\n    on_trace_ready=torch.profiler.tensorboard_trace_handler(\"logs/profiler\"),\n    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2)\n)\n\n# Simple Profiler\n# profiler = SimpleProfiler()\nmodel = MNISTModel()\ndata_module = MNISTDataModule()\ntrainer = Trainer(\n    max_epochs=5,\n    profiler=profiler\n)\n\ntrainer.fit(model, datamodule=data_module)\n\n\n\n\n\ntrainer = Trainer(\n    accumulate_grad_batches=4  # Accumulate over 4 batches\n)\n\n\n\ndef configure_optimizers(self):\n    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, \n        mode='min', \n        factor=0.1, \n        patience=5\n    )\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\n            \"scheduler\": scheduler,\n            \"monitor\": \"val_loss\",\n            \"interval\": \"epoch\",\n            \"frequency\": 1\n        }\n    }\n\n\n\ntrainer = Trainer(\n    precision=\"16-mixed\"  # Use 16-bit mixed precision\n)\n\n\n\nclass TransferLearningModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        # Load pretrained model\n        self.feature_extractor = torchvision.models.resnet18(pretrained=True)\n        \n        # Freeze feature extractor\n        for param in self.feature_extractor.parameters():\n            param.requires_grad = False\n            \n        # Replace final layer\n        num_features = self.feature_extractor.fc.in_features\n        self.feature_extractor.fc = nn.Linear(num_features, 10)\n        \n    def unfreeze_features(self):\n        # Unfreeze model after some training\n        for param in self.feature_extractor.parameters():\n            param.requires_grad = True\n\n\n\n\n\nPyTorch Lightning provides an organized framework that separates research code from engineering boilerplate, making deep learning projects easier to develop, share, and scale. It’s especially valuable for research projects that need to be reproducible and scalable across different hardware configurations.\nFor further information: - PyTorch Lightning Documentation - Lightning GitHub Repository - Lightning Tutorials"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#table-of-contents",
    "href": "posts/pytorch-lightning/index.html#table-of-contents",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Introduction to PyTorch Lightning\nInstallation\nBasic Structure: The LightningModule\nDataModules\nTraining with Trainer\nCallbacks\nLogging\nDistributed Training\nHyperparameter Tuning\nModel Checkpointing\nProduction Deployment\nBest Practices"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#introduction-to-pytorch-lightning",
    "href": "posts/pytorch-lightning/index.html#introduction-to-pytorch-lightning",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "PyTorch Lightning separates research code from engineering code, making models more: - Reproducible: The same code works across different hardware - Readable: Standard project structure makes collaboration easier - Scalable: Train on CPUs, GPUs, TPUs or clusters with no code changes\nLightning helps you focus on the science by handling the engineering details."
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#installation",
    "href": "posts/pytorch-lightning/index.html#installation",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "pip install pytorch-lightning\nFor the latest features, you can install from the source:\npip install git+https://github.com/Lightning-AI/lightning.git"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#basic-structure-the-lightningmodule",
    "href": "posts/pytorch-lightning/index.html#basic-structure-the-lightningmodule",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "The core component in Lightning is the LightningModule, which organizes your PyTorch code into a standardized structure:\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MNISTModel(pl.LightningModule):\n    def __init__(self, lr=0.001):\n        super().__init__()\n        self.save_hyperparameters()  # Saves learning_rate to hparams\n        self.layer_1 = nn.Linear(28 * 28, 128)\n        self.layer_2 = nn.Linear(128, 10)\n        self.lr = lr\n        \n    def forward(self, x):\n        batch_size, channels, width, height = x.size()\n        x = x.view(batch_size, -1)\n        x = self.layer_1(x)\n        x = F.relu(x)\n        x = self.layer_2(x)\n        return x\n        \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        self.log('train_loss', loss)\n        return loss\n        \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        self.log('val_loss', loss)\n        self.log('val_acc', acc)\n        \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        self.log('test_loss', loss)\n        self.log('test_acc', acc)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer\nThis basic structure includes: - Model architecture (__init__ and forward) - Training logic (training_step) - Validation logic (validation_step) - Test logic (test_step) - Optimization setup (configure_optimizers)"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#datamodules",
    "href": "posts/pytorch-lightning/index.html#datamodules",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Lightning’s LightningDataModule encapsulates all data-related logic:\nfrom pytorch_lightning import LightningDataModule\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\n\nclass MNISTDataModule(LightningDataModule):\n    def __init__(self, data_dir='./data', batch_size=32):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n        \n    def prepare_data(self):\n        # Download data if needed\n        MNIST(self.data_dir, train=True, download=True)\n        MNIST(self.data_dir, train=False, download=True)\n        \n    def setup(self, stage=None):\n        # Assign train/val/test datasets\n        if stage == 'fit' or stage is None:\n            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n            \n        if stage == 'test' or stage is None:\n            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n            \n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=self.batch_size)\n        \n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n        \n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=self.batch_size)\nBenefits of LightningDataModule: - Encapsulates all data preparation logic - Makes data pipeline portable and reproducible - Simplifies sharing data pipelines between projects"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#training-with-trainer",
    "href": "posts/pytorch-lightning/index.html#training-with-trainer",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "The Lightning Trainer handles the training loop and validation:\nfrom pytorch_lightning import Trainer\n\n# Create model and data module\nmodel = MNISTModel()\ndata_module = MNISTDataModule()\n\n# Initialize trainer\ntrainer = Trainer(\n    max_epochs=10,\n    accelerator=\"auto\",  # Automatically use available GPU\n    devices=\"auto\",\n    logger=True,         # Use TensorBoard logger by default\n)\n\n# Train model\ntrainer.fit(model, datamodule=data_module)\n\n# Test model\ntrainer.test(model, datamodule=data_module)\nThe Trainer automatically handles: - Epoch and batch iteration - Optimizer steps - Logging metrics - Hardware acceleration (CPU, GPU, TPU) - Early stopping - Checkpointing - Multi-GPU training\n\n\ntrainer = Trainer(\n    max_epochs=10,                    # Maximum number of epochs\n    min_epochs=1,                     # Minimum number of epochs\n    accelerator=\"cpu\",                # Use CPU acceleration\n    devices=1,                        # Use 1 CPUs\n    precision=\"16-mixed\",             # Use mixed precision for faster training\n    gradient_clip_val=0.5,            # Clip gradients\n    accumulate_grad_batches=4,        # Accumulate gradients over 4 batches\n    log_every_n_steps=50,             # Log metrics every 50 steps\n    val_check_interval=0.25,          # Run validation 4 times per epoch\n    fast_dev_run=False,               # Debug mode (only run a few batches)\n    deterministic=True,               # Make training deterministic\n)"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#callbacks",
    "href": "posts/pytorch-lightning/index.html#callbacks",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Callbacks add functionality to the training loop without modifying the core code:\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n\n# Save the best model based on validation accuracy\ncheckpoint_callback = ModelCheckpoint(\n    monitor='val_acc',\n    dirpath='./checkpoints',\n    filename='mnist-{epoch:02d}-{val_acc:.2f}',\n    save_top_k=3,\n    mode='max',\n)\n\n# Stop training when validation loss stops improving\nearly_stop_callback = EarlyStopping(\n    monitor='val_loss',\n    patience=3,\n    verbose=True,\n    mode='min'\n)\n\n# Monitor learning rate\nlr_monitor = LearningRateMonitor(logging_interval='step')\n\n# Initialize trainer with callbacks\ntrainer = Trainer(\n    max_epochs=10,\n    callbacks=[checkpoint_callback, early_stop_callback, lr_monitor]\n)\n\n\nYou can create custom callbacks by extending the base Callback class:\nfrom pytorch_lightning.callbacks import Callback\n\nclass PrintingCallback(Callback):\n    def on_train_start(self, trainer, pl_module):\n        print(\"Training has started!\")\n        \n    def on_train_end(self, trainer, pl_module):\n        print(\"Training has finished!\")\n        \n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        if batch_idx % 100 == 0:\n            print(f\"Batch {batch_idx} completed\")"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#logging",
    "href": "posts/pytorch-lightning/index.html#logging",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Lightning supports various loggers:\nfrom pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n\n# TensorBoard logger\ntensorboard_logger = TensorBoardLogger(save_dir=\"logs/\", name=\"mnist\")\n\n# Initialize trainer with loggers\ntrainer = Trainer(\n    max_epochs=10,\n    logger=[tensorboard_logger]\n)\nAdding metrics in your model:\ndef training_step(self, batch, batch_idx):\n    x, y = batch\n    logits = self(x)\n    loss = F.cross_entropy(logits, y)\n    \n    # Log scalar values\n    self.log('train_loss', loss)\n    \n    # Log learning rate\n    lr = self.optimizers().param_groups[0]['lr']\n    self.log('learning_rate', lr)\n    \n    # Log histograms (on GPU)\n    if batch_idx % 100 == 0:\n        self.logger.experiment.add_histogram('logits', logits, self.global_step)\n    \n    return loss"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#distributed-training",
    "href": "posts/pytorch-lightning/index.html#distributed-training",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Lightning makes distributed training simple:\n# Single GPU\ntrainer = Trainer(accelerator=\"gpu\", devices=1)\n\n# Multiple GPUs (DDP strategy automatically selected)\ntrainer = Trainer(accelerator=\"gpu\", devices=4)\n\n# Specific GPU indices\ntrainer = Trainer(accelerator=\"gpu\", devices=[0, 1, 3])\n\n# TPU cores\ntrainer = Trainer(accelerator=\"tpu\", devices=8)\n\n# Advanced distributed training\ntrainer = Trainer(\n    accelerator=\"gpu\",\n    devices=4,\n    strategy=\"ddp\",  # Distributed Data Parallel\n    num_nodes=2      # Use 2 machines\n)\nOther distribution strategies: - ddp_spawn: Similar to DDP but uses spawn for multiprocessing - deepspeed: For very large models using DeepSpeed - fsdp: Fully Sharded Data Parallel for huge models"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#hyperparameter-tuning",
    "href": "posts/pytorch-lightning/index.html#hyperparameter-tuning",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Lightning integrates well with optimization libraries:\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.tuner import Tuner\n\n# Create model\nmodel = MNISTModel()\ndata_module = MNISTDataModule()\n\n# Create trainer\ntrainer = Trainer(max_epochs=10)\n\n# Use Lightning's Tuner for auto-scaling batch size\ntuner = Tuner(trainer)\ntuner.scale_batch_size(model, datamodule=data_module)\n\n# Find optimal learning rate\nlr_finder = tuner.lr_find(model, datamodule=data_module)\nmodel.learning_rate = lr_finder.suggestion()\n\n# Train with optimized parameters\ntrainer.fit(model, datamodule=data_module)"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#model-checkpointing",
    "href": "posts/pytorch-lightning/index.html#model-checkpointing",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Saving and loading model checkpoints:\n# Save checkpoints during training\ncheckpoint_callback = ModelCheckpoint(\n    dirpath='./checkpoints',\n    filename='{epoch}-{val_loss:.2f}',\n    monitor='val_loss',\n    save_top_k=3,\n    mode='min'\n)\n\ntrainer = Trainer(\n    max_epochs=10,\n    callbacks=[checkpoint_callback]\n)\n\ntrainer.fit(model, datamodule=data_module)\n\n# Path to best checkpoint\nbest_model_path = checkpoint_callback.best_model_path\n\n# Load checkpoint into model\nmodel = MNISTModel.load_from_checkpoint(best_model_path)\n\n# Continue training from checkpoint\ntrainer.fit(model, datamodule=data_module)\nCheckpointing for production:\n# Save model in production-ready format\ntrainer.save_checkpoint(\"model.ckpt\")\n\n# Extract state dict for production\ncheckpoint = torch.load(\"model.ckpt\")\nmodel_state_dict = checkpoint[\"state_dict\"]\n\n# Save just the PyTorch model for production\nmodel = MNISTModel()\nmodel.load_state_dict(model_state_dict)\ntorch.save(model.state_dict(), \"production_model.pt\")"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#production-deployment",
    "href": "posts/pytorch-lightning/index.html#production-deployment",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Converting Lightning models to production:\n# Option 1: Use the Lightning model directly\nmodel = MNISTModel.load_from_checkpoint(\"model.ckpt\")\nmodel.eval()\nmodel.freeze()  # Freeze the model parameters\n\n# Make predictions\nwith torch.no_grad():\n    x = torch.randn(1, 1, 28, 28)\n    y_hat = model(x)\n\n# Option 2: Extract the PyTorch model\nclass ProductionModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(28 * 28, 128)\n        self.layer_2 = nn.Linear(128, 10)\n    \n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.layer_1(x))\n        x = self.layer_2(x)\n        return x\n\n# Load weights from Lightning model\nlightning_model = MNISTModel.load_from_checkpoint(\"model.ckpt\")\nproduction_model = ProductionModel()\n\n# Copy weights\nproduction_model.layer_1.weight.data = lightning_model.layer_1.weight.data\nproduction_model.layer_1.bias.data = lightning_model.layer_1.bias.data\nproduction_model.layer_2.weight.data = lightning_model.layer_2.weight.data\nproduction_model.layer_2.bias.data = lightning_model.layer_2.bias.data\n\n# Save production model\ntorch.save(production_model.state_dict(), \"production_model.pt\")\n\n# Export to ONNX\ndummy_input = torch.randn(1, 1, 28, 28)\ntorch.onnx.export(\n    production_model,\n    dummy_input,\n    \"model.onnx\",\n    export_params=True,\n    opset_version=11,\n    input_names=['input'],\n    output_names=['output']\n)"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#best-practices",
    "href": "posts/pytorch-lightning/index.html#best-practices",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "A well-organized Lightning project structure:\nproject/\n├── configs/              # Configuration files\n├── data/                 # Data files\n├── lightning_logs/       # Generated logs\n├── models/               # Model definitions\n│   ├── __init__.py\n│   └── mnist_model.py    # LightningModule\n├── data_modules/         # Data modules\n│   ├── __init__.py\n│   └── mnist_data.py     # LightningDataModule\n├── callbacks/            # Custom callbacks\n├── utils/                # Utility functions\n├── main.py               # Training script\n└── README.md\n\n\n\nLightning provides a CLI for running experiments from config files:\n# main.py\nfrom pytorch_lightning.cli import LightningCLI\n\ndef cli_main():\n    # Create CLI with LightningModule and LightningDataModule\n    cli = LightningCLI(\n        MNISTModel,\n        MNISTDataModule,\n        save_config_callback=None,\n    )\n\nif __name__ == \"__main__\":\n    cli_main()\nRun with:\npython main.py fit --config configs/mnist.yaml\nExample config file (mnist.yaml):\nmodel:\n  learning_rate: 0.001\n  hidden_size: 128\ndata:\n  data_dir: ./data\n  batch_size: 64\ntrainer:\n  max_epochs: 10\n  accelerator: gpu\n  devices: 1\n\n\n\nLightning includes tools to profile your code:\nfrom pytorch_lightning.profilers import PyTorchProfiler, SimpleProfiler\n\n# PyTorch Profiler\nprofiler = PyTorchProfiler(\n    on_trace_ready=torch.profiler.tensorboard_trace_handler(\"logs/profiler\"),\n    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2)\n)\n\n# Simple Profiler\n# profiler = SimpleProfiler()\nmodel = MNISTModel()\ndata_module = MNISTDataModule()\ntrainer = Trainer(\n    max_epochs=5,\n    profiler=profiler\n)\n\ntrainer.fit(model, datamodule=data_module)\n\n\n\n\n\ntrainer = Trainer(\n    accumulate_grad_batches=4  # Accumulate over 4 batches\n)\n\n\n\ndef configure_optimizers(self):\n    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, \n        mode='min', \n        factor=0.1, \n        patience=5\n    )\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\n            \"scheduler\": scheduler,\n            \"monitor\": \"val_loss\",\n            \"interval\": \"epoch\",\n            \"frequency\": 1\n        }\n    }\n\n\n\ntrainer = Trainer(\n    precision=\"16-mixed\"  # Use 16-bit mixed precision\n)\n\n\n\nclass TransferLearningModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        # Load pretrained model\n        self.feature_extractor = torchvision.models.resnet18(pretrained=True)\n        \n        # Freeze feature extractor\n        for param in self.feature_extractor.parameters():\n            param.requires_grad = False\n            \n        # Replace final layer\n        num_features = self.feature_extractor.fc.in_features\n        self.feature_extractor.fc = nn.Linear(num_features, 10)\n        \n    def unfreeze_features(self):\n        # Unfreeze model after some training\n        for param in self.feature_extractor.parameters():\n            param.requires_grad = True"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#conclusion",
    "href": "posts/pytorch-lightning/index.html#conclusion",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "PyTorch Lightning provides an organized framework that separates research code from engineering boilerplate, making deep learning projects easier to develop, share, and scale. It’s especially valuable for research projects that need to be reproducible and scalable across different hardware configurations.\nFor further information: - PyTorch Lightning Documentation - Lightning GitHub Repository - Lightning Tutorials"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My posts",
    "section": "",
    "text": "DINOv2: A Deep Dive into Architecture and Training\n\n\n\nresearch\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 17, 2025\n\n\n\n\n\n\n\n\n\n\n\nDINOv2: Comprehensive Implementation Guide\n\n\n\ncode\n\ntutorial\n\nadvanced\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 3, 2025\n\n\n\n\n\n\n\n\n\n\n\nVision Transformer (ViT) Implementation Guide\n\n\n\ncode\n\ntutorial\n\nadvanced\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nApr 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nActive Learning Influence Selection: A Comprehensive Guide\n\n\n\ncode\n\nresearch\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nApr 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nPython Data Visualization: Matplotlib vs Seaborn vs Altair\n\n\n\ncode\n\ntutorial\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nApr 12, 2025\n\n\n\n\n\n\n\n\n\n\n\nFrom Pandas to Polars\n\n\n\ncode\n\ntutorial\n\nadvanced\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nApr 5, 2025\n\n\n\n\n\n\n\n\n\n\n\nPyTorch Lightning: A Comprehensive Guide\n\n\n\ncode\n\ntutorial\n\nadvanced\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMar 29, 2025\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMar 22, 2025\n\n\n\n\n\n\nNo matching items"
  }
]