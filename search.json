[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Hi ðŸ‘‹, Iâ€™m Krishnatheja Vanka\n\nMachine Learning Engineer (Applied Computer Vision)\nMachine Learning Engineer with a strong focus on computer vision, generative AI, and deep learning. I specialize in building and deploying end-to-end ML solutionsâ€”from data curation and model training to real-world deployment using PyTorch, AWS, and modern MLOps tools. Currently at Lytx, I work on visual models for challenging driving conditions and fatigue detection. Previously built intelligent vision systems in manufacturing and healthcare domains. Active open-source contributor (PyTorch Lightning, TorchMetrics) and community mentor in AI/ML spaces."
  },
  {
    "objectID": "posts/self-supervised-explained/index.html",
    "href": "posts/self-supervised-explained/index.html",
    "title": "Self-Supervised Learning: Training AI Without Labels",
    "section": "",
    "text": "Machine learning has traditionally relied on vast amounts of labeled data to train models effectively. However, acquiring high-quality labeled datasets is expensive, time-consuming, and often impractical for many real-world applications. Self-supervised learning has emerged as a revolutionary paradigm that addresses these challenges by learning meaningful representations from unlabeled data itself.\n\n\nSelf-supervised learning is a machine learning approach where models learn to understand and represent data by predicting parts of the input from other parts, without requiring external labels or human annotations. Instead of relying on manually created labels, the model generates its own supervisory signal from the inherent structure and patterns within the data.\nThe key insight behind self-supervised learning is that data contains rich internal structure and relationships that can serve as teaching signals. By designing tasks that require the model to understand these relationships, we can train systems that develop sophisticated representations of the underlying data distribution.\n\n\n\nSelf-supervised learning operates on several fundamental principles that distinguish it from traditional supervised learning approaches.\nPretext Tasks: The foundation of self-supervised learning lies in carefully designed pretext tasks. These are artificial objectives created from the data itself, such as predicting missing words in a sentence, reconstructing masked portions of an image, or forecasting future frames in a video sequence. While these tasks may seem simple, they force the model to develop deep understanding of the dataâ€™s underlying structure.\nRepresentation Learning: Rather than training models for specific end tasks, self-supervised learning focuses on learning general-purpose representations that capture the essential characteristics of the data. These learned representations can then be transferred to downstream tasks with minimal additional training, making them highly versatile and efficient.\nData Efficiency: By leveraging the vast amounts of unlabeled data available in the real world, self-supervised learning can achieve performance comparable to or exceeding supervised methods while requiring significantly fewer labeled examples for fine-tuning on specific tasks.\n\n\n\nThe training process for self-supervised learning involves several distinct phases, each designed to maximize the modelâ€™s ability to extract meaningful patterns from unlabeled data.\n\n\nThe success of self-supervised learning heavily depends on the choice and design of pretext tasks. Effective pretext tasks must strike a delicate balance: they should be challenging enough to require sophisticated understanding of the data, yet solvable enough to provide clear learning signals.\nIn natural language processing, common pretext tasks include masked language modeling, where random words in sentences are hidden and the model must predict them based on context. For computer vision, popular approaches include image inpainting, where portions of images are masked and must be reconstructed, or contrastive learning, where the model learns to distinguish between similar and dissimilar image pairs.\n\n\n\nSelf-supervised learning models typically employ architectures specifically designed to excel at the chosen pretext tasks. Transformer architectures have proven particularly effective for language tasks due to their ability to capture long-range dependencies and contextual relationships. For vision tasks, convolutional neural networks, vision transformers, and hybrid architectures are commonly used depending on the specific requirements.\nThe architecture must be capable of processing the input data format while being flexible enough to handle the artificial constraints imposed by the pretext task. Many self-supervised models use encoder-decoder structures, where the encoder learns compressed representations and the decoder reconstructs or predicts the target output.\n\n\n\nDuring training, the model processes large quantities of unlabeled data, continuously solving the pretext task and adjusting its parameters through backpropagation. The training objective is typically formulated as minimizing a loss function that measures how well the model performs on the pretext task.\nUnlike supervised learning, where the model sees explicit input-output pairs, self-supervised training involves creating these pairs automatically from the data itself. For example, in masked language modeling, the complete sentence serves as both input (with masks) and target output (original words), while in image reconstruction tasks, corrupted images are inputs and clean images are targets.\n\n\n\nAfter pretraining on the self-supervised task, the learned representations are adapted for specific downstream applications through fine-tuning. This process typically requires only small amounts of labeled data and relatively few training iterations, as the model has already learned to extract relevant features from the pretraining phase.\nThe fine-tuning process often involves adding task-specific layers on top of the pretrained encoder and training the entire system on the target task. Alternatively, the pretrained representations can be used as fixed feature extractors, with only the final classification or regression layers being trained.\n\n\n\n\nSeveral proven strategies have emerged for training effective self-supervised models across different domains.\nContrastive Learning has become one of the most successful approaches, particularly in computer vision. This method teaches models to distinguish between positive pairs (similar or related data points) and negative pairs (dissimilar or unrelated data points). By maximizing agreement between positive pairs while minimizing agreement between negative pairs, models learn representations that capture semantic similarity and difference.\nMasked Modeling represents another highly effective strategy, where portions of the input are randomly hidden and the model must predict the missing content. This approach forces the model to develop understanding of context and relationships within the data, leading to rich representational learning.\nPredictive Modeling involves training models to forecast future states or missing information based on available context. This could include predicting future video frames, completing partial sequences, or inferring hidden attributes from observable features.\n\n\n\nSelf-supervised learning offers several compelling advantages over traditional supervised approaches. The most significant benefit is the ability to leverage vast amounts of unlabeled data that would otherwise remain unused, dramatically expanding the available training resources. This approach also reduces dependence on expensive human annotation processes and can discover patterns and relationships that might not be obvious to human labelers.\nThe versatility of self-supervised representations makes them valuable across numerous applications. In natural language processing, models like BERT and GPT have revolutionized tasks ranging from translation and summarization to question answering and text generation. Computer vision applications include object recognition, image segmentation, and visual reasoning, while in other domains, self-supervised learning has shown promise for speech recognition, drug discovery, and robotic control.\n\n\n\nDespite its promise, self-supervised learning faces several important challenges. Designing effective pretext tasks requires deep understanding of the data domain and careful consideration of what patterns the model should learn. Poor pretext task design can lead to models that excel at artificial objectives but fail to capture semantically meaningful representations.\nThe computational requirements for self-supervised learning can be substantial, as these models often require processing massive datasets and training large architectures for extended periods. Additionally, evaluation of self-supervised models can be complex, as their quality is ultimately measured by performance on downstream tasks rather than the pretext task itself.\n\n\n\nThe field of self-supervised learning continues to evolve rapidly, with researchers exploring new pretext tasks, architectural innovations, and training methodologies. Emerging trends include multi-modal self-supervised learning that combines different data types, more sophisticated contrastive learning strategies, and the development of unified frameworks that can handle diverse self-supervised objectives.\nAs computational resources continue to grow and new algorithmic innovations emerge, self-supervised learning is poised to play an increasingly central role in artificial intelligence, potentially reducing our dependence on labeled data while improving model performance and generalization capabilities.\nSelf-supervised learning represents a fundamental shift in how we approach machine learning, moving from explicit supervision toward learning from the inherent structure of data itself. This paradigm promises to unlock the vast potential of unlabeled data while creating more robust and generalizable AI systems."
  },
  {
    "objectID": "posts/self-supervised-explained/index.html#what-is-self-supervised-learning",
    "href": "posts/self-supervised-explained/index.html#what-is-self-supervised-learning",
    "title": "Self-Supervised Learning: Training AI Without Labels",
    "section": "",
    "text": "Self-supervised learning is a machine learning approach where models learn to understand and represent data by predicting parts of the input from other parts, without requiring external labels or human annotations. Instead of relying on manually created labels, the model generates its own supervisory signal from the inherent structure and patterns within the data.\nThe key insight behind self-supervised learning is that data contains rich internal structure and relationships that can serve as teaching signals. By designing tasks that require the model to understand these relationships, we can train systems that develop sophisticated representations of the underlying data distribution."
  },
  {
    "objectID": "posts/self-supervised-explained/index.html#core-principles-and-mechanisms",
    "href": "posts/self-supervised-explained/index.html#core-principles-and-mechanisms",
    "title": "Self-Supervised Learning: Training AI Without Labels",
    "section": "",
    "text": "Self-supervised learning operates on several fundamental principles that distinguish it from traditional supervised learning approaches.\nPretext Tasks: The foundation of self-supervised learning lies in carefully designed pretext tasks. These are artificial objectives created from the data itself, such as predicting missing words in a sentence, reconstructing masked portions of an image, or forecasting future frames in a video sequence. While these tasks may seem simple, they force the model to develop deep understanding of the dataâ€™s underlying structure.\nRepresentation Learning: Rather than training models for specific end tasks, self-supervised learning focuses on learning general-purpose representations that capture the essential characteristics of the data. These learned representations can then be transferred to downstream tasks with minimal additional training, making them highly versatile and efficient.\nData Efficiency: By leveraging the vast amounts of unlabeled data available in the real world, self-supervised learning can achieve performance comparable to or exceeding supervised methods while requiring significantly fewer labeled examples for fine-tuning on specific tasks."
  },
  {
    "objectID": "posts/self-supervised-explained/index.html#training-methodology",
    "href": "posts/self-supervised-explained/index.html#training-methodology",
    "title": "Self-Supervised Learning: Training AI Without Labels",
    "section": "",
    "text": "The training process for self-supervised learning involves several distinct phases, each designed to maximize the modelâ€™s ability to extract meaningful patterns from unlabeled data.\n\n\nThe success of self-supervised learning heavily depends on the choice and design of pretext tasks. Effective pretext tasks must strike a delicate balance: they should be challenging enough to require sophisticated understanding of the data, yet solvable enough to provide clear learning signals.\nIn natural language processing, common pretext tasks include masked language modeling, where random words in sentences are hidden and the model must predict them based on context. For computer vision, popular approaches include image inpainting, where portions of images are masked and must be reconstructed, or contrastive learning, where the model learns to distinguish between similar and dissimilar image pairs.\n\n\n\nSelf-supervised learning models typically employ architectures specifically designed to excel at the chosen pretext tasks. Transformer architectures have proven particularly effective for language tasks due to their ability to capture long-range dependencies and contextual relationships. For vision tasks, convolutional neural networks, vision transformers, and hybrid architectures are commonly used depending on the specific requirements.\nThe architecture must be capable of processing the input data format while being flexible enough to handle the artificial constraints imposed by the pretext task. Many self-supervised models use encoder-decoder structures, where the encoder learns compressed representations and the decoder reconstructs or predicts the target output.\n\n\n\nDuring training, the model processes large quantities of unlabeled data, continuously solving the pretext task and adjusting its parameters through backpropagation. The training objective is typically formulated as minimizing a loss function that measures how well the model performs on the pretext task.\nUnlike supervised learning, where the model sees explicit input-output pairs, self-supervised training involves creating these pairs automatically from the data itself. For example, in masked language modeling, the complete sentence serves as both input (with masks) and target output (original words), while in image reconstruction tasks, corrupted images are inputs and clean images are targets.\n\n\n\nAfter pretraining on the self-supervised task, the learned representations are adapted for specific downstream applications through fine-tuning. This process typically requires only small amounts of labeled data and relatively few training iterations, as the model has already learned to extract relevant features from the pretraining phase.\nThe fine-tuning process often involves adding task-specific layers on top of the pretrained encoder and training the entire system on the target task. Alternatively, the pretrained representations can be used as fixed feature extractors, with only the final classification or regression layers being trained."
  },
  {
    "objectID": "posts/self-supervised-explained/index.html#common-training-strategies",
    "href": "posts/self-supervised-explained/index.html#common-training-strategies",
    "title": "Self-Supervised Learning: Training AI Without Labels",
    "section": "",
    "text": "Several proven strategies have emerged for training effective self-supervised models across different domains.\nContrastive Learning has become one of the most successful approaches, particularly in computer vision. This method teaches models to distinguish between positive pairs (similar or related data points) and negative pairs (dissimilar or unrelated data points). By maximizing agreement between positive pairs while minimizing agreement between negative pairs, models learn representations that capture semantic similarity and difference.\nMasked Modeling represents another highly effective strategy, where portions of the input are randomly hidden and the model must predict the missing content. This approach forces the model to develop understanding of context and relationships within the data, leading to rich representational learning.\nPredictive Modeling involves training models to forecast future states or missing information based on available context. This could include predicting future video frames, completing partial sequences, or inferring hidden attributes from observable features."
  },
  {
    "objectID": "posts/self-supervised-explained/index.html#advantages-and-applications",
    "href": "posts/self-supervised-explained/index.html#advantages-and-applications",
    "title": "Self-Supervised Learning: Training AI Without Labels",
    "section": "",
    "text": "Self-supervised learning offers several compelling advantages over traditional supervised approaches. The most significant benefit is the ability to leverage vast amounts of unlabeled data that would otherwise remain unused, dramatically expanding the available training resources. This approach also reduces dependence on expensive human annotation processes and can discover patterns and relationships that might not be obvious to human labelers.\nThe versatility of self-supervised representations makes them valuable across numerous applications. In natural language processing, models like BERT and GPT have revolutionized tasks ranging from translation and summarization to question answering and text generation. Computer vision applications include object recognition, image segmentation, and visual reasoning, while in other domains, self-supervised learning has shown promise for speech recognition, drug discovery, and robotic control."
  },
  {
    "objectID": "posts/self-supervised-explained/index.html#challenges-and-limitations",
    "href": "posts/self-supervised-explained/index.html#challenges-and-limitations",
    "title": "Self-Supervised Learning: Training AI Without Labels",
    "section": "",
    "text": "Despite its promise, self-supervised learning faces several important challenges. Designing effective pretext tasks requires deep understanding of the data domain and careful consideration of what patterns the model should learn. Poor pretext task design can lead to models that excel at artificial objectives but fail to capture semantically meaningful representations.\nThe computational requirements for self-supervised learning can be substantial, as these models often require processing massive datasets and training large architectures for extended periods. Additionally, evaluation of self-supervised models can be complex, as their quality is ultimately measured by performance on downstream tasks rather than the pretext task itself."
  },
  {
    "objectID": "posts/self-supervised-explained/index.html#future-directions",
    "href": "posts/self-supervised-explained/index.html#future-directions",
    "title": "Self-Supervised Learning: Training AI Without Labels",
    "section": "",
    "text": "The field of self-supervised learning continues to evolve rapidly, with researchers exploring new pretext tasks, architectural innovations, and training methodologies. Emerging trends include multi-modal self-supervised learning that combines different data types, more sophisticated contrastive learning strategies, and the development of unified frameworks that can handle diverse self-supervised objectives.\nAs computational resources continue to grow and new algorithmic innovations emerge, self-supervised learning is poised to play an increasingly central role in artificial intelligence, potentially reducing our dependence on labeled data while improving model performance and generalization capabilities.\nSelf-supervised learning represents a fundamental shift in how we approach machine learning, moving from explicit supervision toward learning from the inherent structure of data itself. This paradigm promises to unlock the vast potential of unlabeled data while creating more robust and generalizable AI systems."
  },
  {
    "objectID": "posts/litserve-mobilenet/index.html",
    "href": "posts/litserve-mobilenet/index.html",
    "title": "LitServe with MobileNetV2 - Complete Code Guide",
    "section": "",
    "text": "This guide demonstrates how to deploy a MobileNetV2 image classification model using LitServe for efficient, scalable inference.\n\n\n\nInstallation\nBasic Implementation\nAdvanced Features\nPerformance Optimization\nDeployment\nTesting\n\n\n\n\n# Install required packages\npip install litserve torch torchvision pillow requests\n\n\n\n\n\n# mobilenet_api.py\nimport io\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom PIL import Image\nimport litserve as ls\n\n\nclass MobileNetV2API(ls.LitAPI):\n    def setup(self, device):\n        \"\"\"Initialize the model and preprocessing pipeline\"\"\"\n        # Load pre-trained MobileNetV2\n        self.model = models.mobilenet_v2(pretrained=True)\n        self.model.eval()\n        self.model.to(device)\n        \n        # Define image preprocessing\n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n        \n        # ImageNet class labels (first 10 for brevity)\n        self.class_labels = [\n            \"tench\", \"goldfish\", \"great white shark\", \"tiger shark\",\n            \"hammerhead\", \"electric ray\", \"stingray\", \"cock\", \n            \"hen\", \"ostrich\"\n            # ... add all 1000 ImageNet classes\n        ]\n\n    def decode_request(self, request):\n        \"\"\"Parse incoming request and prepare image\"\"\"\n        if isinstance(request, dict) and \"image\" in request:\n            # Handle base64 encoded image\n            import base64\n            image_data = base64.b64decode(request[\"image\"])\n            image = Image.open(io.BytesIO(image_data)).convert('RGB')\n        else:\n            # Handle direct image upload\n            image = Image.open(io.BytesIO(request)).convert('RGB')\n        \n        return image\n\n    def predict(self, image):\n        \"\"\"Run inference on the preprocessed image\"\"\"\n        # Preprocess image\n        input_tensor = self.transform(image).unsqueeze(0)\n        input_tensor = input_tensor.to(next(self.model.parameters()).device)\n        \n        # Run inference\n        with torch.no_grad():\n            outputs = self.model(input_tensor)\n            probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n        \n        return probabilities\n\n    def encode_response(self, probabilities):\n        \"\"\"Format the response\"\"\"\n        # Get top 5 predictions\n        top5_prob, top5_indices = torch.topk(probabilities, 5)\n        \n        results = []\n        for i in range(5):\n            idx = top5_indices[i].item()\n            prob = top5_prob[i].item()\n            label = self.class_labels[idx] if idx &lt; len(self.class_labels) else f\"class_{idx}\"\n            results.append({\n                \"class\": label,\n                \"confidence\": round(prob, 4),\n                \"class_id\": idx\n            })\n        \n        return {\n            \"predictions\": results,\n            \"model\": \"mobilenet_v2\"\n        }\n\n\nif __name__ == \"__main__\":\n    # Create and run the API\n    api = MobileNetV2API()\n    server = ls.LitServer(api, accelerator=\"auto\", max_batch_size=4)\n    server.run(port=8000)\n\n\n\n# client.py\nimport requests\nimport base64\nfrom PIL import Image\nimport io\n\n\ndef encode_image(image_path):\n    \"\"\"Encode image to base64\"\"\"\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode('utf-8')\n\n\ndef test_image_classification(image_path, server_url=\"http://localhost:8000/predict\"):\n    \"\"\"Test the MobileNetV2 API\"\"\"\n    # Method 1: Send as base64 in JSON\n    encoded_image = encode_image(image_path)\n    response = requests.post(\n        server_url,\n        json={\"image\": encoded_image}\n    )\n    \n    if response.status_code == 200:\n        result = response.json()\n        print(\"Top 5 Predictions:\")\n        for pred in result[\"predictions\"]:\n            print(f\"  {pred['class']}: {pred['confidence']:.4f}\")\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n\n\ndef test_direct_upload(image_path, server_url=\"http://localhost:8000/predict\"):\n    \"\"\"Test with direct image upload\"\"\"\n    with open(image_path, 'rb') as f:\n        files = {'file': f}\n        response = requests.post(server_url, files=files)\n    \n    if response.status_code == 200:\n        result = response.json()\n        print(\"Predictions:\", result)\n\n\nif __name__ == \"__main__\":\n    # Test with a sample image\n    test_image_classification(\"sample_image.jpg\")\n\n\n\n\n\n\n# advanced_mobilenet_api.py\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom PIL import Image\nimport litserve as ls\nimport json\nfrom typing import List, Any\n\n\nclass BatchedMobileNetV2API(ls.LitAPI):\n    def setup(self, device):\n        \"\"\"Initialize model with batch processing capabilities\"\"\"\n        self.model = models.mobilenet_v2(pretrained=True)\n        self.model.eval()\n        self.model.to(device)\n        self.device = device\n        \n        # Optimized transform for batch processing\n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n        \n        # Load class labels from file or define them\n        self.load_imagenet_labels()\n\n    def load_imagenet_labels(self):\n        \"\"\"Load ImageNet class labels\"\"\"\n        # You can download from: https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n        try:\n            with open('imagenet_classes.txt', 'r') as f:\n                self.class_labels = [line.strip() for line in f.readlines()]\n        except FileNotFoundError:\n            # Fallback to first few classes\n            self.class_labels = [f\"class_{i}\" for i in range(1000)]\n\n    def decode_request(self, request):\n        \"\"\"Handle both single images and batch requests\"\"\"\n        if isinstance(request, dict):\n            if \"images\" in request:  # Batch request\n                images = []\n                for img_data in request[\"images\"]:\n                    if isinstance(img_data, str):  # base64\n                        import base64\n                        image_data = base64.b64decode(img_data)\n                        image = Image.open(io.BytesIO(image_data)).convert('RGB')\n                    else:  # direct bytes\n                        image = Image.open(io.BytesIO(img_data)).convert('RGB')\n                    images.append(image)\n                return images\n            elif \"image\" in request:  # Single request\n                import base64\n                image_data = base64.b64decode(request[\"image\"])\n                image = Image.open(io.BytesIO(image_data)).convert('RGB')\n                return [image]  # Wrap in list for consistent handling\n        \n        # Direct upload\n        image = Image.open(io.BytesIO(request)).convert('RGB')\n        return [image]\n\n    def batch(self, inputs: List[Any]) -&gt; List[Any]:\n        \"\"\"Custom batching logic\"\"\"\n        # Flatten all images from all requests\n        all_images = []\n        batch_sizes = []\n        \n        for inp in inputs:\n            if isinstance(inp, list):\n                all_images.extend(inp)\n                batch_sizes.append(len(inp))\n            else:\n                all_images.append(inp)\n                batch_sizes.append(1)\n        \n        return all_images, batch_sizes\n\n    def predict(self, batch_data):\n        \"\"\"Process batch of images efficiently\"\"\"\n        images, batch_sizes = batch_data\n        \n        # Preprocess all images\n        batch_tensor = torch.stack([\n            self.transform(img) for img in images\n        ]).to(self.device)\n        \n        # Run batch inference\n        with torch.no_grad():\n            outputs = self.model(batch_tensor)\n            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n        \n        return probabilities, batch_sizes\n\n    def unbatch(self, output):\n        \"\"\"Split batch results back to individual responses\"\"\"\n        probabilities, batch_sizes = output\n        results = []\n        start_idx = 0\n        \n        for batch_size in batch_sizes:\n            batch_probs = probabilities[start_idx:start_idx + batch_size]\n            results.append(batch_probs)\n            start_idx += batch_size\n        \n        return results\n\n    def encode_response(self, probabilities):\n        \"\"\"Format response for batch or single predictions\"\"\"\n        if len(probabilities.shape) == 1:  # Single prediction\n            probabilities = probabilities.unsqueeze(0)\n        \n        all_results = []\n        for prob_vector in probabilities:\n            top5_prob, top5_indices = torch.topk(prob_vector, 5)\n            \n            predictions = []\n            for i in range(5):\n                idx = top5_indices[i].item()\n                prob = top5_prob[i].item()\n                predictions.append({\n                    \"class\": self.class_labels[idx],\n                    \"confidence\": round(prob, 4),\n                    \"class_id\": idx\n                })\n            \n            all_results.append(predictions)\n        \n        return {\n            \"predictions\": all_results[0] if len(all_results) == 1 else all_results,\n            \"model\": \"mobilenet_v2\",\n            \"batch_size\": len(all_results)\n        }\n\n\nif __name__ == \"__main__\":\n    api = BatchedMobileNetV2API()\n    server = ls.LitServer(\n        api,\n        accelerator=\"auto\",\n        max_batch_size=8,\n        batch_timeout=0.1,  # 100ms timeout for batching\n    )\n    server.run(port=8000, num_workers=2)\n\n\n\n# quantized_mobilenet_api.py\nimport torch\nimport torch.quantization\nfrom torchvision import models\nimport litserve as ls\n\n\nclass QuantizedMobileNetV2API(ls.LitAPI):\n    def setup(self, device):\n        \"\"\"Setup quantized MobileNetV2 for faster inference\"\"\"\n        # Load pre-trained model\n        self.model = models.mobilenet_v2(pretrained=True)\n        self.model.eval()\n        \n        # Apply dynamic quantization for CPU inference\n        if device == \"cpu\":\n            self.model = torch.quantization.quantize_dynamic(\n                self.model,\n                {torch.nn.Linear, torch.nn.Conv2d},\n                dtype=torch.qint8\n            )\n            print(\"Applied dynamic quantization for CPU\")\n        else:\n            self.model.to(device)\n            print(f\"Using device: {device}\")\n        \n        # Rest of setup code...\n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n\n    # ... rest of the methods remain the same\n\n\n\n\n\n\n# production_config.py\nimport litserve as ls\nfrom advanced_mobilenet_api import BatchedMobileNetV2API\n\ndef create_production_server():\n    \"\"\"Create optimized server for production\"\"\"\n    api = BatchedMobileNetV2API()\n    \n    server = ls.LitServer(\n        api,\n        accelerator=\"auto\",  # Auto-detect GPU/CPU\n        max_batch_size=16,   # Larger batches for throughput\n        batch_timeout=0.05,  # 50ms batching timeout\n        workers_per_device=2,  # Multiple workers per GPU\n        timeout=30,          # Request timeout\n    )\n    \n    return server\n\nif __name__ == \"__main__\":\n    server = create_production_server()\n    server.run(\n        port=8000,\n        host=\"0.0.0.0\",  # Accept external connections\n        num_workers=4     # Total number of workers\n    )\n\n\n\n# monitored_api.py\nimport time\nimport logging\nfrom collections import defaultdict\nimport litserve as ls\n\n\nclass MonitoredMobileNetV2API(BatchedMobileNetV2API):\n    def setup(self, device):\n        \"\"\"Setup with monitoring\"\"\"\n        super().setup(device)\n        \n        # Setup logging\n        logging.basicConfig(level=logging.INFO)\n        self.logger = logging.getLogger(__name__)\n        \n        # Metrics tracking\n        self.metrics = defaultdict(list)\n        self.request_count = 0\n\n    def predict(self, batch_data):\n        \"\"\"Predict with timing and metrics\"\"\"\n        start_time = time.time()\n        \n        result = super().predict(batch_data)\n        \n        # Log timing\n        inference_time = time.time() - start_time\n        batch_size = len(batch_data[0])\n        \n        self.metrics['inference_times'].append(inference_time)\n        self.metrics['batch_sizes'].append(batch_size)\n        self.request_count += 1\n        \n        self.logger.info(\n            f\"Processed batch of {batch_size} images in {inference_time:.3f}s \"\n            f\"(Total requests: {self.request_count})\"\n        )\n        \n        return result\n\n    def encode_response(self, probabilities):\n        \"\"\"Add metrics to response\"\"\"\n        response = super().encode_response(probabilities)\n        \n        # Add performance metrics every 100 requests\n        if self.request_count % 100 == 0:\n            avg_time = sum(self.metrics['inference_times'][-100:]) / min(100, len(self.metrics['inference_times']))\n            response['metrics'] = {\n                'avg_inference_time': round(avg_time, 4),\n                'total_requests': self.request_count\n            }\n        \n        return response\n\n\n\n\n\n\n# Dockerfile\nFROM python:3.9-slim\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    wget \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Download ImageNet labels\nRUN wget -O imagenet_classes.txt https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n\nEXPOSE 8000\n\nCMD [\"python\", \"production_config.py\"]\n# docker-compose.yml\nversion: '3.8'\n\nservices:\n  mobilenet-api:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - CUDA_VISIBLE_DEVICES=0  # Set GPU if available\n    volumes:\n      - ./models:/app/models  # Optional: for custom models\n    restart: unless-stopped\n    \n  # Optional: Add nginx for load balancing\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n    depends_on:\n      - mobilenet-api\n\n\n\n# k8s-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mobilenet-api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mobilenet-api\n  template:\n    metadata:\n      labels:\n        app: mobilenet-api\n    spec:\n      containers:\n      - name: mobilenet-api\n        image: your-registry/mobilenet-api:latest\n        ports:\n        - containerPort: 8000\n        resources:\n          requests:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"2Gi\"\n            cpu: \"1000m\"\n        env:\n        - name: WORKERS\n          value: \"2\"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mobilenet-service\nspec:\n  selector:\n    app: mobilenet-api\n  ports:\n  - port: 80\n    targetPort: 8000\n  type: LoadBalancer\n\n\n\n\n\n\n# test_api.py\nimport pytest\nimport requests\nimport base64\nimport numpy as np\nfrom PIL import Image\nimport io\n\n\nclass TestMobileNetV2API:\n    def setup_class(self):\n        \"\"\"Setup test configuration\"\"\"\n        self.base_url = \"http://localhost:8000\"\n        self.test_image = self.create_test_image()\n\n    def create_test_image(self):\n        \"\"\"Create a test image\"\"\"\n        # Create a simple test image\n        img = Image.new('RGB', (224, 224), color='red')\n        img_buffer = io.BytesIO()\n        img.save(img_buffer, format='JPEG')\n        img_buffer.seek(0)\n        return img_buffer.getvalue()\n\n    def test_single_prediction(self):\n        \"\"\"Test single image prediction\"\"\"\n        encoded_image = base64.b64encode(self.test_image).decode('utf-8')\n        \n        response = requests.post(\n            f\"{self.base_url}/predict\",\n            json={\"image\": encoded_image}\n        )\n        \n        assert response.status_code == 200\n        result = response.json()\n        assert \"predictions\" in result\n        assert len(result[\"predictions\"]) == 5\n        assert all(\"class\" in pred and \"confidence\" in pred for pred in result[\"predictions\"])\n\n    def test_batch_prediction(self):\n        \"\"\"Test batch prediction\"\"\"\n        encoded_image = base64.b64encode(self.test_image).decode('utf-8')\n        \n        response = requests.post(\n            f\"{self.base_url}/predict\",\n            json={\"images\": [encoded_image, encoded_image]}\n        )\n        \n        assert response.status_code == 200\n        result = response.json()\n        assert \"batch_size\" in result\n        assert result[\"batch_size\"] == 2\n\n    def test_performance(self):\n        \"\"\"Test API performance\"\"\"\n        import time\n        \n        encoded_image = base64.b64encode(self.test_image).decode('utf-8')\n        \n        # Warmup\n        requests.post(f\"{self.base_url}/predict\", json={\"image\": encoded_image})\n        \n        # Time multiple requests\n        times = []\n        for _ in range(10):\n            start = time.time()\n            response = requests.post(f\"{self.base_url}/predict\", json={\"image\": encoded_image})\n            times.append(time.time() - start)\n            assert response.status_code == 200\n        \n        avg_time = sum(times) / len(times)\n        print(f\"Average response time: {avg_time:.3f}s\")\n        assert avg_time &lt; 1.0  # Should respond within 1 second\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])\n\n\n\n# load_test.py\nimport asyncio\nimport aiohttp\nimport base64\nimport time\nfrom PIL import Image\nimport io\n\n\nasync def send_request(session, url, data):\n    \"\"\"Send a single request\"\"\"\n    try:\n        async with session.post(url, json=data) as response:\n            result = await response.json()\n            return response.status, time.time()\n    except Exception as e:\n        return 500, time.time()\n\n\nasync def load_test(num_requests=100, concurrent=10):\n    \"\"\"Run load test\"\"\"\n    # Create test image\n    img = Image.new('RGB', (224, 224), color='blue')\n    img_buffer = io.BytesIO()\n    img.save(img_buffer, format='JPEG')\n    encoded_image = base64.b64encode(img_buffer.getvalue()).decode('utf-8')\n    \n    url = \"http://localhost:8000/predict\"\n    data = {\"image\": encoded_image}\n    \n    start_time = time.time()\n    \n    async with aiohttp.ClientSession() as session:\n        # Create semaphore to limit concurrent requests\n        semaphore = asyncio.Semaphore(concurrent)\n        \n        async def bounded_request():\n            async with semaphore:\n                return await send_request(session, url, data)\n        \n        # Send all requests\n        tasks = [bounded_request() for _ in range(num_requests)]\n        results = await asyncio.gather(*tasks)\n    \n    total_time = time.time() - start_time\n    \n    # Analyze results\n    successful = sum(1 for status, _ in results if status == 200)\n    failed = num_requests - successful\n    \n    print(f\"Load Test Results:\")\n    print(f\"  Total Requests: {num_requests}\")\n    print(f\"  Successful: {successful}\")\n    print(f\"  Failed: {failed}\")\n    print(f\"  Total Time: {total_time:.2f}s\")\n    print(f\"  Requests/sec: {num_requests/total_time:.2f}\")\n    print(f\"  Concurrent: {concurrent}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(load_test(num_requests=200, concurrent=20))\n\n\n\n\n# requirements.txt\nlitserve&gt;=0.2.0\ntorch&gt;=1.9.0\ntorchvision&gt;=0.10.0\nPillow&gt;=8.0.0\nrequests&gt;=2.25.0\nnumpy&gt;=1.21.0\naiohttp&gt;=3.8.0  # For async testing\npytest&gt;=6.0.0  # For testing\n\n\n\n\nModel Optimization: Use quantization and TorchScript for production\nBatch Processing: Configure appropriate batch sizes based on your hardware\nError Handling: Implement comprehensive error handling for robustness\nMonitoring: Add logging and metrics collection for production monitoring\nSecurity: Implement authentication and input validation for production APIs\nCaching: Consider caching frequently requested predictions\nScaling: Use container orchestration for high-availability deployments\n\nThis guide provides a complete foundation for deploying MobileNetV2 with LitServe, from basic implementation to production-ready deployment with monitoring and testing."
  },
  {
    "objectID": "posts/litserve-mobilenet/index.html#table-of-contents",
    "href": "posts/litserve-mobilenet/index.html#table-of-contents",
    "title": "LitServe with MobileNetV2 - Complete Code Guide",
    "section": "",
    "text": "Installation\nBasic Implementation\nAdvanced Features\nPerformance Optimization\nDeployment\nTesting"
  },
  {
    "objectID": "posts/litserve-mobilenet/index.html#installation",
    "href": "posts/litserve-mobilenet/index.html#installation",
    "title": "LitServe with MobileNetV2 - Complete Code Guide",
    "section": "",
    "text": "# Install required packages\npip install litserve torch torchvision pillow requests"
  },
  {
    "objectID": "posts/litserve-mobilenet/index.html#basic-implementation",
    "href": "posts/litserve-mobilenet/index.html#basic-implementation",
    "title": "LitServe with MobileNetV2 - Complete Code Guide",
    "section": "",
    "text": "# mobilenet_api.py\nimport io\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom PIL import Image\nimport litserve as ls\n\n\nclass MobileNetV2API(ls.LitAPI):\n    def setup(self, device):\n        \"\"\"Initialize the model and preprocessing pipeline\"\"\"\n        # Load pre-trained MobileNetV2\n        self.model = models.mobilenet_v2(pretrained=True)\n        self.model.eval()\n        self.model.to(device)\n        \n        # Define image preprocessing\n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n        \n        # ImageNet class labels (first 10 for brevity)\n        self.class_labels = [\n            \"tench\", \"goldfish\", \"great white shark\", \"tiger shark\",\n            \"hammerhead\", \"electric ray\", \"stingray\", \"cock\", \n            \"hen\", \"ostrich\"\n            # ... add all 1000 ImageNet classes\n        ]\n\n    def decode_request(self, request):\n        \"\"\"Parse incoming request and prepare image\"\"\"\n        if isinstance(request, dict) and \"image\" in request:\n            # Handle base64 encoded image\n            import base64\n            image_data = base64.b64decode(request[\"image\"])\n            image = Image.open(io.BytesIO(image_data)).convert('RGB')\n        else:\n            # Handle direct image upload\n            image = Image.open(io.BytesIO(request)).convert('RGB')\n        \n        return image\n\n    def predict(self, image):\n        \"\"\"Run inference on the preprocessed image\"\"\"\n        # Preprocess image\n        input_tensor = self.transform(image).unsqueeze(0)\n        input_tensor = input_tensor.to(next(self.model.parameters()).device)\n        \n        # Run inference\n        with torch.no_grad():\n            outputs = self.model(input_tensor)\n            probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n        \n        return probabilities\n\n    def encode_response(self, probabilities):\n        \"\"\"Format the response\"\"\"\n        # Get top 5 predictions\n        top5_prob, top5_indices = torch.topk(probabilities, 5)\n        \n        results = []\n        for i in range(5):\n            idx = top5_indices[i].item()\n            prob = top5_prob[i].item()\n            label = self.class_labels[idx] if idx &lt; len(self.class_labels) else f\"class_{idx}\"\n            results.append({\n                \"class\": label,\n                \"confidence\": round(prob, 4),\n                \"class_id\": idx\n            })\n        \n        return {\n            \"predictions\": results,\n            \"model\": \"mobilenet_v2\"\n        }\n\n\nif __name__ == \"__main__\":\n    # Create and run the API\n    api = MobileNetV2API()\n    server = ls.LitServer(api, accelerator=\"auto\", max_batch_size=4)\n    server.run(port=8000)\n\n\n\n# client.py\nimport requests\nimport base64\nfrom PIL import Image\nimport io\n\n\ndef encode_image(image_path):\n    \"\"\"Encode image to base64\"\"\"\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode('utf-8')\n\n\ndef test_image_classification(image_path, server_url=\"http://localhost:8000/predict\"):\n    \"\"\"Test the MobileNetV2 API\"\"\"\n    # Method 1: Send as base64 in JSON\n    encoded_image = encode_image(image_path)\n    response = requests.post(\n        server_url,\n        json={\"image\": encoded_image}\n    )\n    \n    if response.status_code == 200:\n        result = response.json()\n        print(\"Top 5 Predictions:\")\n        for pred in result[\"predictions\"]:\n            print(f\"  {pred['class']}: {pred['confidence']:.4f}\")\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n\n\ndef test_direct_upload(image_path, server_url=\"http://localhost:8000/predict\"):\n    \"\"\"Test with direct image upload\"\"\"\n    with open(image_path, 'rb') as f:\n        files = {'file': f}\n        response = requests.post(server_url, files=files)\n    \n    if response.status_code == 200:\n        result = response.json()\n        print(\"Predictions:\", result)\n\n\nif __name__ == \"__main__\":\n    # Test with a sample image\n    test_image_classification(\"sample_image.jpg\")"
  },
  {
    "objectID": "posts/litserve-mobilenet/index.html#advanced-features",
    "href": "posts/litserve-mobilenet/index.html#advanced-features",
    "title": "LitServe with MobileNetV2 - Complete Code Guide",
    "section": "",
    "text": "# advanced_mobilenet_api.py\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom PIL import Image\nimport litserve as ls\nimport json\nfrom typing import List, Any\n\n\nclass BatchedMobileNetV2API(ls.LitAPI):\n    def setup(self, device):\n        \"\"\"Initialize model with batch processing capabilities\"\"\"\n        self.model = models.mobilenet_v2(pretrained=True)\n        self.model.eval()\n        self.model.to(device)\n        self.device = device\n        \n        # Optimized transform for batch processing\n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n        \n        # Load class labels from file or define them\n        self.load_imagenet_labels()\n\n    def load_imagenet_labels(self):\n        \"\"\"Load ImageNet class labels\"\"\"\n        # You can download from: https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n        try:\n            with open('imagenet_classes.txt', 'r') as f:\n                self.class_labels = [line.strip() for line in f.readlines()]\n        except FileNotFoundError:\n            # Fallback to first few classes\n            self.class_labels = [f\"class_{i}\" for i in range(1000)]\n\n    def decode_request(self, request):\n        \"\"\"Handle both single images and batch requests\"\"\"\n        if isinstance(request, dict):\n            if \"images\" in request:  # Batch request\n                images = []\n                for img_data in request[\"images\"]:\n                    if isinstance(img_data, str):  # base64\n                        import base64\n                        image_data = base64.b64decode(img_data)\n                        image = Image.open(io.BytesIO(image_data)).convert('RGB')\n                    else:  # direct bytes\n                        image = Image.open(io.BytesIO(img_data)).convert('RGB')\n                    images.append(image)\n                return images\n            elif \"image\" in request:  # Single request\n                import base64\n                image_data = base64.b64decode(request[\"image\"])\n                image = Image.open(io.BytesIO(image_data)).convert('RGB')\n                return [image]  # Wrap in list for consistent handling\n        \n        # Direct upload\n        image = Image.open(io.BytesIO(request)).convert('RGB')\n        return [image]\n\n    def batch(self, inputs: List[Any]) -&gt; List[Any]:\n        \"\"\"Custom batching logic\"\"\"\n        # Flatten all images from all requests\n        all_images = []\n        batch_sizes = []\n        \n        for inp in inputs:\n            if isinstance(inp, list):\n                all_images.extend(inp)\n                batch_sizes.append(len(inp))\n            else:\n                all_images.append(inp)\n                batch_sizes.append(1)\n        \n        return all_images, batch_sizes\n\n    def predict(self, batch_data):\n        \"\"\"Process batch of images efficiently\"\"\"\n        images, batch_sizes = batch_data\n        \n        # Preprocess all images\n        batch_tensor = torch.stack([\n            self.transform(img) for img in images\n        ]).to(self.device)\n        \n        # Run batch inference\n        with torch.no_grad():\n            outputs = self.model(batch_tensor)\n            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n        \n        return probabilities, batch_sizes\n\n    def unbatch(self, output):\n        \"\"\"Split batch results back to individual responses\"\"\"\n        probabilities, batch_sizes = output\n        results = []\n        start_idx = 0\n        \n        for batch_size in batch_sizes:\n            batch_probs = probabilities[start_idx:start_idx + batch_size]\n            results.append(batch_probs)\n            start_idx += batch_size\n        \n        return results\n\n    def encode_response(self, probabilities):\n        \"\"\"Format response for batch or single predictions\"\"\"\n        if len(probabilities.shape) == 1:  # Single prediction\n            probabilities = probabilities.unsqueeze(0)\n        \n        all_results = []\n        for prob_vector in probabilities:\n            top5_prob, top5_indices = torch.topk(prob_vector, 5)\n            \n            predictions = []\n            for i in range(5):\n                idx = top5_indices[i].item()\n                prob = top5_prob[i].item()\n                predictions.append({\n                    \"class\": self.class_labels[idx],\n                    \"confidence\": round(prob, 4),\n                    \"class_id\": idx\n                })\n            \n            all_results.append(predictions)\n        \n        return {\n            \"predictions\": all_results[0] if len(all_results) == 1 else all_results,\n            \"model\": \"mobilenet_v2\",\n            \"batch_size\": len(all_results)\n        }\n\n\nif __name__ == \"__main__\":\n    api = BatchedMobileNetV2API()\n    server = ls.LitServer(\n        api,\n        accelerator=\"auto\",\n        max_batch_size=8,\n        batch_timeout=0.1,  # 100ms timeout for batching\n    )\n    server.run(port=8000, num_workers=2)\n\n\n\n# quantized_mobilenet_api.py\nimport torch\nimport torch.quantization\nfrom torchvision import models\nimport litserve as ls\n\n\nclass QuantizedMobileNetV2API(ls.LitAPI):\n    def setup(self, device):\n        \"\"\"Setup quantized MobileNetV2 for faster inference\"\"\"\n        # Load pre-trained model\n        self.model = models.mobilenet_v2(pretrained=True)\n        self.model.eval()\n        \n        # Apply dynamic quantization for CPU inference\n        if device == \"cpu\":\n            self.model = torch.quantization.quantize_dynamic(\n                self.model,\n                {torch.nn.Linear, torch.nn.Conv2d},\n                dtype=torch.qint8\n            )\n            print(\"Applied dynamic quantization for CPU\")\n        else:\n            self.model.to(device)\n            print(f\"Using device: {device}\")\n        \n        # Rest of setup code...\n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n\n    # ... rest of the methods remain the same"
  },
  {
    "objectID": "posts/litserve-mobilenet/index.html#performance-optimization",
    "href": "posts/litserve-mobilenet/index.html#performance-optimization",
    "title": "LitServe with MobileNetV2 - Complete Code Guide",
    "section": "",
    "text": "# production_config.py\nimport litserve as ls\nfrom advanced_mobilenet_api import BatchedMobileNetV2API\n\ndef create_production_server():\n    \"\"\"Create optimized server for production\"\"\"\n    api = BatchedMobileNetV2API()\n    \n    server = ls.LitServer(\n        api,\n        accelerator=\"auto\",  # Auto-detect GPU/CPU\n        max_batch_size=16,   # Larger batches for throughput\n        batch_timeout=0.05,  # 50ms batching timeout\n        workers_per_device=2,  # Multiple workers per GPU\n        timeout=30,          # Request timeout\n    )\n    \n    return server\n\nif __name__ == \"__main__\":\n    server = create_production_server()\n    server.run(\n        port=8000,\n        host=\"0.0.0.0\",  # Accept external connections\n        num_workers=4     # Total number of workers\n    )\n\n\n\n# monitored_api.py\nimport time\nimport logging\nfrom collections import defaultdict\nimport litserve as ls\n\n\nclass MonitoredMobileNetV2API(BatchedMobileNetV2API):\n    def setup(self, device):\n        \"\"\"Setup with monitoring\"\"\"\n        super().setup(device)\n        \n        # Setup logging\n        logging.basicConfig(level=logging.INFO)\n        self.logger = logging.getLogger(__name__)\n        \n        # Metrics tracking\n        self.metrics = defaultdict(list)\n        self.request_count = 0\n\n    def predict(self, batch_data):\n        \"\"\"Predict with timing and metrics\"\"\"\n        start_time = time.time()\n        \n        result = super().predict(batch_data)\n        \n        # Log timing\n        inference_time = time.time() - start_time\n        batch_size = len(batch_data[0])\n        \n        self.metrics['inference_times'].append(inference_time)\n        self.metrics['batch_sizes'].append(batch_size)\n        self.request_count += 1\n        \n        self.logger.info(\n            f\"Processed batch of {batch_size} images in {inference_time:.3f}s \"\n            f\"(Total requests: {self.request_count})\"\n        )\n        \n        return result\n\n    def encode_response(self, probabilities):\n        \"\"\"Add metrics to response\"\"\"\n        response = super().encode_response(probabilities)\n        \n        # Add performance metrics every 100 requests\n        if self.request_count % 100 == 0:\n            avg_time = sum(self.metrics['inference_times'][-100:]) / min(100, len(self.metrics['inference_times']))\n            response['metrics'] = {\n                'avg_inference_time': round(avg_time, 4),\n                'total_requests': self.request_count\n            }\n        \n        return response"
  },
  {
    "objectID": "posts/litserve-mobilenet/index.html#deployment",
    "href": "posts/litserve-mobilenet/index.html#deployment",
    "title": "LitServe with MobileNetV2 - Complete Code Guide",
    "section": "",
    "text": "# Dockerfile\nFROM python:3.9-slim\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    wget \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Download ImageNet labels\nRUN wget -O imagenet_classes.txt https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n\nEXPOSE 8000\n\nCMD [\"python\", \"production_config.py\"]\n# docker-compose.yml\nversion: '3.8'\n\nservices:\n  mobilenet-api:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - CUDA_VISIBLE_DEVICES=0  # Set GPU if available\n    volumes:\n      - ./models:/app/models  # Optional: for custom models\n    restart: unless-stopped\n    \n  # Optional: Add nginx for load balancing\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n    depends_on:\n      - mobilenet-api\n\n\n\n# k8s-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mobilenet-api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mobilenet-api\n  template:\n    metadata:\n      labels:\n        app: mobilenet-api\n    spec:\n      containers:\n      - name: mobilenet-api\n        image: your-registry/mobilenet-api:latest\n        ports:\n        - containerPort: 8000\n        resources:\n          requests:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"2Gi\"\n            cpu: \"1000m\"\n        env:\n        - name: WORKERS\n          value: \"2\"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mobilenet-service\nspec:\n  selector:\n    app: mobilenet-api\n  ports:\n  - port: 80\n    targetPort: 8000\n  type: LoadBalancer"
  },
  {
    "objectID": "posts/litserve-mobilenet/index.html#testing",
    "href": "posts/litserve-mobilenet/index.html#testing",
    "title": "LitServe with MobileNetV2 - Complete Code Guide",
    "section": "",
    "text": "# test_api.py\nimport pytest\nimport requests\nimport base64\nimport numpy as np\nfrom PIL import Image\nimport io\n\n\nclass TestMobileNetV2API:\n    def setup_class(self):\n        \"\"\"Setup test configuration\"\"\"\n        self.base_url = \"http://localhost:8000\"\n        self.test_image = self.create_test_image()\n\n    def create_test_image(self):\n        \"\"\"Create a test image\"\"\"\n        # Create a simple test image\n        img = Image.new('RGB', (224, 224), color='red')\n        img_buffer = io.BytesIO()\n        img.save(img_buffer, format='JPEG')\n        img_buffer.seek(0)\n        return img_buffer.getvalue()\n\n    def test_single_prediction(self):\n        \"\"\"Test single image prediction\"\"\"\n        encoded_image = base64.b64encode(self.test_image).decode('utf-8')\n        \n        response = requests.post(\n            f\"{self.base_url}/predict\",\n            json={\"image\": encoded_image}\n        )\n        \n        assert response.status_code == 200\n        result = response.json()\n        assert \"predictions\" in result\n        assert len(result[\"predictions\"]) == 5\n        assert all(\"class\" in pred and \"confidence\" in pred for pred in result[\"predictions\"])\n\n    def test_batch_prediction(self):\n        \"\"\"Test batch prediction\"\"\"\n        encoded_image = base64.b64encode(self.test_image).decode('utf-8')\n        \n        response = requests.post(\n            f\"{self.base_url}/predict\",\n            json={\"images\": [encoded_image, encoded_image]}\n        )\n        \n        assert response.status_code == 200\n        result = response.json()\n        assert \"batch_size\" in result\n        assert result[\"batch_size\"] == 2\n\n    def test_performance(self):\n        \"\"\"Test API performance\"\"\"\n        import time\n        \n        encoded_image = base64.b64encode(self.test_image).decode('utf-8')\n        \n        # Warmup\n        requests.post(f\"{self.base_url}/predict\", json={\"image\": encoded_image})\n        \n        # Time multiple requests\n        times = []\n        for _ in range(10):\n            start = time.time()\n            response = requests.post(f\"{self.base_url}/predict\", json={\"image\": encoded_image})\n            times.append(time.time() - start)\n            assert response.status_code == 200\n        \n        avg_time = sum(times) / len(times)\n        print(f\"Average response time: {avg_time:.3f}s\")\n        assert avg_time &lt; 1.0  # Should respond within 1 second\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])\n\n\n\n# load_test.py\nimport asyncio\nimport aiohttp\nimport base64\nimport time\nfrom PIL import Image\nimport io\n\n\nasync def send_request(session, url, data):\n    \"\"\"Send a single request\"\"\"\n    try:\n        async with session.post(url, json=data) as response:\n            result = await response.json()\n            return response.status, time.time()\n    except Exception as e:\n        return 500, time.time()\n\n\nasync def load_test(num_requests=100, concurrent=10):\n    \"\"\"Run load test\"\"\"\n    # Create test image\n    img = Image.new('RGB', (224, 224), color='blue')\n    img_buffer = io.BytesIO()\n    img.save(img_buffer, format='JPEG')\n    encoded_image = base64.b64encode(img_buffer.getvalue()).decode('utf-8')\n    \n    url = \"http://localhost:8000/predict\"\n    data = {\"image\": encoded_image}\n    \n    start_time = time.time()\n    \n    async with aiohttp.ClientSession() as session:\n        # Create semaphore to limit concurrent requests\n        semaphore = asyncio.Semaphore(concurrent)\n        \n        async def bounded_request():\n            async with semaphore:\n                return await send_request(session, url, data)\n        \n        # Send all requests\n        tasks = [bounded_request() for _ in range(num_requests)]\n        results = await asyncio.gather(*tasks)\n    \n    total_time = time.time() - start_time\n    \n    # Analyze results\n    successful = sum(1 for status, _ in results if status == 200)\n    failed = num_requests - successful\n    \n    print(f\"Load Test Results:\")\n    print(f\"  Total Requests: {num_requests}\")\n    print(f\"  Successful: {successful}\")\n    print(f\"  Failed: {failed}\")\n    print(f\"  Total Time: {total_time:.2f}s\")\n    print(f\"  Requests/sec: {num_requests/total_time:.2f}\")\n    print(f\"  Concurrent: {concurrent}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(load_test(num_requests=200, concurrent=20))"
  },
  {
    "objectID": "posts/litserve-mobilenet/index.html#requirements-file",
    "href": "posts/litserve-mobilenet/index.html#requirements-file",
    "title": "LitServe with MobileNetV2 - Complete Code Guide",
    "section": "",
    "text": "# requirements.txt\nlitserve&gt;=0.2.0\ntorch&gt;=1.9.0\ntorchvision&gt;=0.10.0\nPillow&gt;=8.0.0\nrequests&gt;=2.25.0\nnumpy&gt;=1.21.0\naiohttp&gt;=3.8.0  # For async testing\npytest&gt;=6.0.0  # For testing"
  },
  {
    "objectID": "posts/litserve-mobilenet/index.html#best-practices",
    "href": "posts/litserve-mobilenet/index.html#best-practices",
    "title": "LitServe with MobileNetV2 - Complete Code Guide",
    "section": "",
    "text": "Model Optimization: Use quantization and TorchScript for production\nBatch Processing: Configure appropriate batch sizes based on your hardware\nError Handling: Implement comprehensive error handling for robustness\nMonitoring: Add logging and metrics collection for production monitoring\nSecurity: Implement authentication and input validation for production APIs\nCaching: Consider caching frequently requested predictions\nScaling: Use container orchestration for high-availability deployments\n\nThis guide provides a complete foundation for deploying MobileNetV2 with LitServe, from basic implementation to production-ready deployment with monitoring and testing."
  },
  {
    "objectID": "posts/clip-code/index.html",
    "href": "posts/clip-code/index.html",
    "title": "CLIP Code Guide: Complete Implementation and Usage",
    "section": "",
    "text": "Introduction to CLIP\nArchitecture Overview\nSetting Up the Environment\nBasic CLIP Usage\nCustom CLIP Implementation\nTraining CLIP from Scratch\nFine-tuning CLIP\nAdvanced Applications\nPerformance Optimization\nCommon Issues and Solutions\n\n\n\n\nCLIP (Contrastive Language-Image Pre-training) is a neural network architecture developed by OpenAI that learns visual concepts from natural language supervision. It can understand images in the context of natural language descriptions, enabling zero-shot classification and multimodal understanding.\n\n\n\nZero-shot image classification\nText-image similarity computation\nMultimodal embeddings\nTransfer learning capabilities\n\n\n\n\n\nCLIP consists of two main components: 1. Text Encoder: Processes text descriptions (typically a Transformer) 2. Image Encoder: Processes images (typically a Vision Transformer or ResNet)\nThe model learns to maximize the cosine similarity between corresponding text-image pairs while minimizing it for non-corresponding pairs.\n\n\n\n\n\n# Basic installation\npip install torch torchvision transformers\npip install clip-by-openai  # Official OpenAI CLIP\npip install open-clip-torch  # OpenCLIP (more models)\n\n# For development and training\npip install wandb datasets accelerate\npip install matplotlib pillow requests\n\n\n\n# Install from source\ngit clone https://github.com/openai/CLIP.git\ncd CLIP\npip install -e .\n\n\n\n\n\n\nimport clip\nimport torch\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\n# Load model and preprocessing\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\n# Available models: ViT-B/32, ViT-B/16, ViT-L/14, RN50, RN101, RN50x4, etc.\nprint(f\"Available models: {clip.available_models()}\")\n\n\n\ndef zero_shot_classification(image_path, text_options):\n    # Load and preprocess image\n    image = Image.open(image_path)\n    image_input = preprocess(image).unsqueeze(0).to(device)\n    \n    # Tokenize text options\n    text_inputs = clip.tokenize(text_options).to(device)\n    \n    # Get predictions\n    with torch.no_grad():\n        image_features = model.encode_image(image_input)\n        text_features = model.encode_text(text_inputs)\n        \n        # Calculate similarities\n        similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n        \n    # Get results\n    values, indices = similarities[0].topk(len(text_options))\n    \n    results = []\n    for value, index in zip(values, indices):\n        results.append({\n            'label': text_options[index],\n            'confidence': value.item()\n        })\n    \n    return results\n\n# Example usage\ntext_options = [\"a dog\", \"a cat\", \"a car\", \"a bird\", \"a house\"]\nresults = zero_shot_classification(\"path/to/image.jpg\", text_options)\n\nfor result in results:\n    print(f\"{result['label']}: {result['confidence']:.2%}\")\n\n\n\ndef compute_similarity(image_path, text_description):\n    # Load image\n    image = Image.open(image_path)\n    image_input = preprocess(image).unsqueeze(0).to(device)\n    \n    # Tokenize text\n    text_input = clip.tokenize([text_description]).to(device)\n    \n    # Get features\n    with torch.no_grad():\n        image_features = model.encode_image(image_input)\n        text_features = model.encode_text(text_input)\n        \n        # Normalize features\n        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n        \n        # Compute similarity\n        similarity = (image_features @ text_features.T).item()\n    \n    return similarity\n\n# Example usage\nsimilarity = compute_similarity(\"dog.jpg\", \"a golden retriever sitting in grass\")\nprint(f\"Similarity: {similarity:.4f}\")\n\n\n\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import GPT2Model, GPT2Tokenizer\nimport timm\n\nclass CLIPModel(nn.Module):\n    def __init__(self, \n                 image_encoder_name='resnet50',\n                 text_encoder_name='gpt2',\n                 embed_dim=512,\n                 image_resolution=224,\n                 vocab_size=49408):\n        super().__init__()\n        \n        self.embed_dim = embed_dim\n        self.image_resolution = image_resolution\n        \n        # Image encoder\n        self.visual = timm.create_model(image_encoder_name, pretrained=True, num_classes=0)\n        visual_dim = self.visual.num_features\n        \n        # Text encoder\n        self.text_encoder = GPT2Model.from_pretrained(text_encoder_name)\n        text_dim = self.text_encoder.config.n_embd\n        \n        # Projection layers\n        self.visual_projection = nn.Linear(visual_dim, embed_dim, bias=False)\n        self.text_projection = nn.Linear(text_dim, embed_dim, bias=False)\n        \n        # Learnable temperature parameter\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n        \n        self.initialize_parameters()\n    \n    def initialize_parameters(self):\n        # Initialize projection layers\n        nn.init.normal_(self.visual_projection.weight, std=0.02)\n        nn.init.normal_(self.text_projection.weight, std=0.02)\n    \n    def encode_image(self, image):\n        # Extract visual features\n        visual_features = self.visual(image)\n        # Project to common embedding space\n        image_features = self.visual_projection(visual_features)\n        # Normalize\n        image_features = F.normalize(image_features, dim=-1)\n        return image_features\n    \n    def encode_text(self, text):\n        # Get text features from last token\n        text_outputs = self.text_encoder(text)\n        # Use last token's representation\n        text_features = text_outputs.last_hidden_state[:, -1, :]\n        # Project to common embedding space\n        text_features = self.text_projection(text_features)\n        # Normalize\n        text_features = F.normalize(text_features, dim=-1)\n        return text_features\n    \n    def forward(self, image, text):\n        image_features = self.encode_image(image)\n        text_features = self.encode_text(text)\n        \n        # Compute logits\n        logit_scale = self.logit_scale.exp()\n        logits_per_image = logit_scale * image_features @ text_features.t()\n        logits_per_text = logits_per_image.t()\n        \n        return logits_per_image, logits_per_text\n\n\n\ndef clip_loss(logits_per_image, logits_per_text):\n    \"\"\"\n    Contrastive loss for CLIP training\n    \"\"\"\n    batch_size = logits_per_image.shape[0]\n    labels = torch.arange(batch_size, device=logits_per_image.device)\n    \n    # Cross-entropy loss for both directions\n    loss_i = F.cross_entropy(logits_per_image, labels)\n    loss_t = F.cross_entropy(logits_per_text, labels)\n    \n    # Average the losses\n    loss = (loss_i + loss_t) / 2\n    return loss\n\n\n\n\n\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport json\n\nclass ImageTextDataset(Dataset):\n    def __init__(self, data_path, image_dir, transform=None, tokenizer=None, max_length=77):\n        with open(data_path, 'r') as f:\n            self.data = json.load(f)\n        \n        self.image_dir = image_dir\n        self.transform = transform\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        # Load image\n        image_path = os.path.join(self.image_dir, item['image'])\n        image = Image.open(image_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        # Tokenize text\n        text = item['caption']\n        if self.tokenizer:\n            text_tokens = self.tokenizer(\n                text,\n                max_length=self.max_length,\n                padding='max_length',\n                truncation=True,\n                return_tensors='pt'\n            )['input_ids'].squeeze(0)\n        else:\n            # Simple tokenization for demonstration\n            text_tokens = torch.zeros(self.max_length, dtype=torch.long)\n        \n        return image, text_tokens, text\n\n\n\ndef train_clip(model, dataloader, optimizer, scheduler, device, num_epochs):\n    model.train()\n    \n    for epoch in range(num_epochs):\n        total_loss = 0\n        num_batches = 0\n        \n        for batch_idx, (images, text_tokens, _) in enumerate(dataloader):\n            images = images.to(device)\n            text_tokens = text_tokens.to(device)\n            \n            # Forward pass\n            logits_per_image, logits_per_text = model(images, text_tokens)\n            \n            # Compute loss\n            loss = clip_loss(logits_per_image, logits_per_text)\n            \n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            num_batches += 1\n            \n            # Logging\n            if batch_idx % 100 == 0:\n                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n        \n        # Update learning rate\n        scheduler.step()\n        \n        avg_loss = total_loss / num_batches\n        print(f'Epoch {epoch} completed. Average Loss: {avg_loss:.4f}')\n\n# Training setup\nmodel = CLIPModel().to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n\n# Start training\ntrain_clip(model, dataloader, optimizer, scheduler, device, num_epochs=100)\n\n\n\n\n\n\ndef fine_tune_clip(pretrained_model, dataloader, num_epochs=10, lr=1e-5):\n    # Freeze most layers, only fine-tune projection layers\n    for param in pretrained_model.visual.parameters():\n        param.requires_grad = False\n    \n    for param in pretrained_model.text_encoder.parameters():\n        param.requires_grad = False\n    \n    # Only train projection layers\n    optimizer = torch.optim.Adam([\n        {'params': pretrained_model.visual_projection.parameters()},\n        {'params': pretrained_model.text_projection.parameters()},\n        {'params': [pretrained_model.logit_scale]}\n    ], lr=lr)\n    \n    pretrained_model.train()\n    \n    for epoch in range(num_epochs):\n        for batch_idx, (images, text_tokens, _) in enumerate(dataloader):\n            images = images.to(device)\n            text_tokens = text_tokens.to(device)\n            \n            logits_per_image, logits_per_text = pretrained_model(images, text_tokens)\n            loss = clip_loss(logits_per_image, logits_per_text)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            if batch_idx % 50 == 0:\n                print(f'Fine-tune Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n\n\n\n\n\n\nclass CLIPImageSearch:\n    def __init__(self, model, preprocess):\n        self.model = model\n        self.preprocess = preprocess\n        self.image_features = None\n        self.image_paths = None\n    \n    def index_images(self, image_paths):\n        \"\"\"Pre-compute features for all images\"\"\"\n        self.image_paths = image_paths\n        features = []\n        \n        for img_path in image_paths:\n            image = Image.open(img_path)\n            image_input = self.preprocess(image).unsqueeze(0).to(device)\n            \n            with torch.no_grad():\n                image_feature = self.model.encode_image(image_input)\n                features.append(image_feature)\n        \n        self.image_features = torch.cat(features, dim=0)\n        self.image_features = self.image_features / self.image_features.norm(dim=-1, keepdim=True)\n    \n    def search(self, query_text, top_k=5):\n        \"\"\"Search for images matching the text query\"\"\"\n        text_input = clip.tokenize([query_text]).to(device)\n        \n        with torch.no_grad():\n            text_features = self.model.encode_text(text_input)\n            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n            \n            # Compute similarities\n            similarities = (text_features @ self.image_features.T).squeeze(0)\n            \n            # Get top-k results\n            top_similarities, top_indices = similarities.topk(top_k)\n        \n        results = []\n        for sim, idx in zip(top_similarities, top_indices):\n            results.append({\n                'path': self.image_paths[idx],\n                'similarity': sim.item()\n            })\n        \n        return results\n\n# Usage example\nsearch_engine = CLIPImageSearch(model, preprocess)\nsearch_engine.index_images(list_of_image_paths)\nresults = search_engine.search(\"a red sports car\", top_k=10)\n\n\n\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\ndef cluster_images_by_content(image_paths, n_clusters=5):\n    # Extract features for all images\n    features = []\n    \n    for img_path in image_paths:\n        image = Image.open(img_path)\n        image_input = preprocess(image).unsqueeze(0).to(device)\n        \n        with torch.no_grad():\n            feature = model.encode_image(image_input)\n            features.append(feature.cpu().numpy())\n    \n    # Convert to numpy array\n    features = np.vstack(features)\n    \n    # Perform clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    cluster_labels = kmeans.fit_predict(features)\n    \n    # Organize results\n    clusters = {}\n    for i, label in enumerate(cluster_labels):\n        if label not in clusters:\n            clusters[label] = []\n        clusters[label].append(image_paths[i])\n    \n    return clusters\n\n\n\ndef visual_qa(image_path, question, answer_choices):\n    \"\"\"Simple VQA using CLIP\"\"\"\n    image = Image.open(image_path)\n    image_input = preprocess(image).unsqueeze(0).to(device)\n    \n    # Create prompts combining question with each answer\n    prompts = [f\"Question: {question} Answer: {choice}\" for choice in answer_choices]\n    text_inputs = clip.tokenize(prompts).to(device)\n    \n    with torch.no_grad():\n        image_features = model.encode_image(image_input)\n        text_features = model.encode_text(text_inputs)\n        \n        # Compute similarities\n        similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n    \n    # Return the most likely answer\n    best_idx = similarities.argmax().item()\n    return answer_choices[best_idx], similarities[0][best_idx].item()\n\n# Example usage\nanswer, confidence = visual_qa(\n    \"image.jpg\",\n    \"What color is the car?\",\n    [\"red\", \"blue\", \"green\", \"yellow\", \"black\"]\n)\nprint(f\"Answer: {answer}, Confidence: {confidence:.2%}\")\n\n\n\n\n\n\ndef batch_encode_images(image_paths, batch_size=32):\n    \"\"\"Process images in batches for better efficiency\"\"\"\n    all_features = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_paths = image_paths[i:i+batch_size]\n        batch_images = []\n        \n        for path in batch_paths:\n            image = Image.open(path)\n            image_input = preprocess(image)\n            batch_images.append(image_input)\n        \n        batch_tensor = torch.stack(batch_images).to(device)\n        \n        with torch.no_grad():\n            batch_features = model.encode_image(batch_tensor)\n            all_features.append(batch_features.cpu())\n    \n    return torch.cat(all_features, dim=0)\n\n\n\nfrom torch.cuda.amp import autocast, GradScaler\n\ndef train_with_mixed_precision(model, dataloader, optimizer, num_epochs):\n    scaler = GradScaler()\n    model.train()\n    \n    for epoch in range(num_epochs):\n        for images, text_tokens, _ in dataloader:\n            images = images.to(device)\n            text_tokens = text_tokens.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Forward pass with autocast\n            with autocast():\n                logits_per_image, logits_per_text = model(images, text_tokens)\n                loss = clip_loss(logits_per_image, logits_per_text)\n            \n            # Backward pass with scaling\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n\n\nimport torch.quantization as quantization\n\ndef quantize_clip_model(model):\n    \"\"\"Quantize CLIP model for inference\"\"\"\n    model.eval()\n    \n    # Specify quantization configuration\n    model.qconfig = quantization.get_default_qconfig('fbgemm')\n    \n    # Prepare model for quantization\n    model_prepared = quantization.prepare(model, inplace=False)\n    \n    # Calibrate with sample data (you need to provide calibration data)\n    # ... calibration code here ...\n    \n    # Convert to quantized model\n    model_quantized = quantization.convert(model_prepared, inplace=False)\n    \n    return model_quantized\n\n\n\n\n\n\n# Clear GPU cache\ntorch.cuda.empty_cache()\n\n# Use gradient checkpointing for large models\ndef enable_gradient_checkpointing(model):\n    if hasattr(model.visual, 'set_grad_checkpointing'):\n        model.visual.set_grad_checkpointing(True)\n    if hasattr(model.text_encoder, 'gradient_checkpointing_enable'):\n        model.text_encoder.gradient_checkpointing_enable()\n\n\n\nfrom torchvision import transforms\n\ndef create_adaptive_transform(target_size=224):\n    return transforms.Compose([\n        transforms.Resize(target_size, interpolation=transforms.InterpolationMode.BICUBIC),\n        transforms.CenterCrop(target_size),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        )\n    ])\n\n\n\nimport re\n\ndef preprocess_text(text, max_length=77):\n    \"\"\"Clean and preprocess text for CLIP\"\"\"\n    # Remove special characters and extra whitespace\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = ' '.join(text.split())\n    \n    # Truncate if too long\n    words = text.split()\n    if len(words) &gt; max_length - 2:  # Account for special tokens\n        text = ' '.join(words[:max_length-2])\n    \n    return text\n\n\n\ndef evaluate_zero_shot_accuracy(model, preprocess, test_loader, class_names):\n    \"\"\"Evaluate zero-shot classification accuracy\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    \n    # Encode class names\n    text_inputs = clip.tokenize([f\"a photo of a {name}\" for name in class_names]).to(device)\n    \n    with torch.no_grad():\n        text_features = model.encode_text(text_inputs)\n        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n        \n        for images, labels in test_loader:\n            images = images.to(device)\n            \n            # Encode images\n            image_features = model.encode_image(images)\n            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n            \n            # Compute similarities\n            similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n            predictions = similarities.argmax(dim=-1)\n            \n            correct += (predictions == labels.to(device)).sum().item()\n            total += labels.size(0)\n    \n    accuracy = correct / total\n    return accuracy\n\n# Usage\naccuracy = evaluate_zero_shot_accuracy(model, preprocess, test_loader, class_names)\nprint(f\"Zero-shot accuracy: {accuracy:.2%}\")\n\n\n\n\nThis guide covers the essential aspects of working with CLIP, from basic usage to advanced implementations. Key takeaways:\n\nStart Simple: Use pre-trained models for most applications\nUnderstand the Architecture: CLIPâ€™s power comes from joint text-image training\nOptimize for Your Use Case: Fine-tune or customize based on your specific needs\nMonitor Performance: Use proper evaluation metrics and optimization techniques\nHandle Edge Cases: Implement robust preprocessing and error handling\n\nFor production deployments, consider:\n\nModel quantization for faster inference\nBatch processing for efficiency\nProper error handling and fallbacks\nMonitoring and logging for performance tracking\n\nThe field of multimodal AI is rapidly evolving, so stay updated with the latest research and implementations to leverage CLIPâ€™s full potential in your applications."
  },
  {
    "objectID": "posts/clip-code/index.html#table-of-contents",
    "href": "posts/clip-code/index.html#table-of-contents",
    "title": "CLIP Code Guide: Complete Implementation and Usage",
    "section": "",
    "text": "Introduction to CLIP\nArchitecture Overview\nSetting Up the Environment\nBasic CLIP Usage\nCustom CLIP Implementation\nTraining CLIP from Scratch\nFine-tuning CLIP\nAdvanced Applications\nPerformance Optimization\nCommon Issues and Solutions"
  },
  {
    "objectID": "posts/clip-code/index.html#introduction-to-clip",
    "href": "posts/clip-code/index.html#introduction-to-clip",
    "title": "CLIP Code Guide: Complete Implementation and Usage",
    "section": "",
    "text": "CLIP (Contrastive Language-Image Pre-training) is a neural network architecture developed by OpenAI that learns visual concepts from natural language supervision. It can understand images in the context of natural language descriptions, enabling zero-shot classification and multimodal understanding.\n\n\n\nZero-shot image classification\nText-image similarity computation\nMultimodal embeddings\nTransfer learning capabilities"
  },
  {
    "objectID": "posts/clip-code/index.html#architecture-overview",
    "href": "posts/clip-code/index.html#architecture-overview",
    "title": "CLIP Code Guide: Complete Implementation and Usage",
    "section": "",
    "text": "CLIP consists of two main components: 1. Text Encoder: Processes text descriptions (typically a Transformer) 2. Image Encoder: Processes images (typically a Vision Transformer or ResNet)\nThe model learns to maximize the cosine similarity between corresponding text-image pairs while minimizing it for non-corresponding pairs."
  },
  {
    "objectID": "posts/clip-code/index.html#setting-up-the-environment",
    "href": "posts/clip-code/index.html#setting-up-the-environment",
    "title": "CLIP Code Guide: Complete Implementation and Usage",
    "section": "",
    "text": "# Basic installation\npip install torch torchvision transformers\npip install clip-by-openai  # Official OpenAI CLIP\npip install open-clip-torch  # OpenCLIP (more models)\n\n# For development and training\npip install wandb datasets accelerate\npip install matplotlib pillow requests\n\n\n\n# Install from source\ngit clone https://github.com/openai/CLIP.git\ncd CLIP\npip install -e ."
  },
  {
    "objectID": "posts/clip-code/index.html#basic-clip-usage",
    "href": "posts/clip-code/index.html#basic-clip-usage",
    "title": "CLIP Code Guide: Complete Implementation and Usage",
    "section": "",
    "text": "import clip\nimport torch\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\n# Load model and preprocessing\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\n# Available models: ViT-B/32, ViT-B/16, ViT-L/14, RN50, RN101, RN50x4, etc.\nprint(f\"Available models: {clip.available_models()}\")\n\n\n\ndef zero_shot_classification(image_path, text_options):\n    # Load and preprocess image\n    image = Image.open(image_path)\n    image_input = preprocess(image).unsqueeze(0).to(device)\n    \n    # Tokenize text options\n    text_inputs = clip.tokenize(text_options).to(device)\n    \n    # Get predictions\n    with torch.no_grad():\n        image_features = model.encode_image(image_input)\n        text_features = model.encode_text(text_inputs)\n        \n        # Calculate similarities\n        similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n        \n    # Get results\n    values, indices = similarities[0].topk(len(text_options))\n    \n    results = []\n    for value, index in zip(values, indices):\n        results.append({\n            'label': text_options[index],\n            'confidence': value.item()\n        })\n    \n    return results\n\n# Example usage\ntext_options = [\"a dog\", \"a cat\", \"a car\", \"a bird\", \"a house\"]\nresults = zero_shot_classification(\"path/to/image.jpg\", text_options)\n\nfor result in results:\n    print(f\"{result['label']}: {result['confidence']:.2%}\")\n\n\n\ndef compute_similarity(image_path, text_description):\n    # Load image\n    image = Image.open(image_path)\n    image_input = preprocess(image).unsqueeze(0).to(device)\n    \n    # Tokenize text\n    text_input = clip.tokenize([text_description]).to(device)\n    \n    # Get features\n    with torch.no_grad():\n        image_features = model.encode_image(image_input)\n        text_features = model.encode_text(text_input)\n        \n        # Normalize features\n        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n        \n        # Compute similarity\n        similarity = (image_features @ text_features.T).item()\n    \n    return similarity\n\n# Example usage\nsimilarity = compute_similarity(\"dog.jpg\", \"a golden retriever sitting in grass\")\nprint(f\"Similarity: {similarity:.4f}\")"
  },
  {
    "objectID": "posts/clip-code/index.html#custom-clip-implementation",
    "href": "posts/clip-code/index.html#custom-clip-implementation",
    "title": "CLIP Code Guide: Complete Implementation and Usage",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import GPT2Model, GPT2Tokenizer\nimport timm\n\nclass CLIPModel(nn.Module):\n    def __init__(self, \n                 image_encoder_name='resnet50',\n                 text_encoder_name='gpt2',\n                 embed_dim=512,\n                 image_resolution=224,\n                 vocab_size=49408):\n        super().__init__()\n        \n        self.embed_dim = embed_dim\n        self.image_resolution = image_resolution\n        \n        # Image encoder\n        self.visual = timm.create_model(image_encoder_name, pretrained=True, num_classes=0)\n        visual_dim = self.visual.num_features\n        \n        # Text encoder\n        self.text_encoder = GPT2Model.from_pretrained(text_encoder_name)\n        text_dim = self.text_encoder.config.n_embd\n        \n        # Projection layers\n        self.visual_projection = nn.Linear(visual_dim, embed_dim, bias=False)\n        self.text_projection = nn.Linear(text_dim, embed_dim, bias=False)\n        \n        # Learnable temperature parameter\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n        \n        self.initialize_parameters()\n    \n    def initialize_parameters(self):\n        # Initialize projection layers\n        nn.init.normal_(self.visual_projection.weight, std=0.02)\n        nn.init.normal_(self.text_projection.weight, std=0.02)\n    \n    def encode_image(self, image):\n        # Extract visual features\n        visual_features = self.visual(image)\n        # Project to common embedding space\n        image_features = self.visual_projection(visual_features)\n        # Normalize\n        image_features = F.normalize(image_features, dim=-1)\n        return image_features\n    \n    def encode_text(self, text):\n        # Get text features from last token\n        text_outputs = self.text_encoder(text)\n        # Use last token's representation\n        text_features = text_outputs.last_hidden_state[:, -1, :]\n        # Project to common embedding space\n        text_features = self.text_projection(text_features)\n        # Normalize\n        text_features = F.normalize(text_features, dim=-1)\n        return text_features\n    \n    def forward(self, image, text):\n        image_features = self.encode_image(image)\n        text_features = self.encode_text(text)\n        \n        # Compute logits\n        logit_scale = self.logit_scale.exp()\n        logits_per_image = logit_scale * image_features @ text_features.t()\n        logits_per_text = logits_per_image.t()\n        \n        return logits_per_image, logits_per_text\n\n\n\ndef clip_loss(logits_per_image, logits_per_text):\n    \"\"\"\n    Contrastive loss for CLIP training\n    \"\"\"\n    batch_size = logits_per_image.shape[0]\n    labels = torch.arange(batch_size, device=logits_per_image.device)\n    \n    # Cross-entropy loss for both directions\n    loss_i = F.cross_entropy(logits_per_image, labels)\n    loss_t = F.cross_entropy(logits_per_text, labels)\n    \n    # Average the losses\n    loss = (loss_i + loss_t) / 2\n    return loss"
  },
  {
    "objectID": "posts/clip-code/index.html#training-clip-from-scratch",
    "href": "posts/clip-code/index.html#training-clip-from-scratch",
    "title": "CLIP Code Guide: Complete Implementation and Usage",
    "section": "",
    "text": "import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport json\n\nclass ImageTextDataset(Dataset):\n    def __init__(self, data_path, image_dir, transform=None, tokenizer=None, max_length=77):\n        with open(data_path, 'r') as f:\n            self.data = json.load(f)\n        \n        self.image_dir = image_dir\n        self.transform = transform\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        # Load image\n        image_path = os.path.join(self.image_dir, item['image'])\n        image = Image.open(image_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        # Tokenize text\n        text = item['caption']\n        if self.tokenizer:\n            text_tokens = self.tokenizer(\n                text,\n                max_length=self.max_length,\n                padding='max_length',\n                truncation=True,\n                return_tensors='pt'\n            )['input_ids'].squeeze(0)\n        else:\n            # Simple tokenization for demonstration\n            text_tokens = torch.zeros(self.max_length, dtype=torch.long)\n        \n        return image, text_tokens, text\n\n\n\ndef train_clip(model, dataloader, optimizer, scheduler, device, num_epochs):\n    model.train()\n    \n    for epoch in range(num_epochs):\n        total_loss = 0\n        num_batches = 0\n        \n        for batch_idx, (images, text_tokens, _) in enumerate(dataloader):\n            images = images.to(device)\n            text_tokens = text_tokens.to(device)\n            \n            # Forward pass\n            logits_per_image, logits_per_text = model(images, text_tokens)\n            \n            # Compute loss\n            loss = clip_loss(logits_per_image, logits_per_text)\n            \n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            num_batches += 1\n            \n            # Logging\n            if batch_idx % 100 == 0:\n                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n        \n        # Update learning rate\n        scheduler.step()\n        \n        avg_loss = total_loss / num_batches\n        print(f'Epoch {epoch} completed. Average Loss: {avg_loss:.4f}')\n\n# Training setup\nmodel = CLIPModel().to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n\n# Start training\ntrain_clip(model, dataloader, optimizer, scheduler, device, num_epochs=100)"
  },
  {
    "objectID": "posts/clip-code/index.html#fine-tuning-clip",
    "href": "posts/clip-code/index.html#fine-tuning-clip",
    "title": "CLIP Code Guide: Complete Implementation and Usage",
    "section": "",
    "text": "def fine_tune_clip(pretrained_model, dataloader, num_epochs=10, lr=1e-5):\n    # Freeze most layers, only fine-tune projection layers\n    for param in pretrained_model.visual.parameters():\n        param.requires_grad = False\n    \n    for param in pretrained_model.text_encoder.parameters():\n        param.requires_grad = False\n    \n    # Only train projection layers\n    optimizer = torch.optim.Adam([\n        {'params': pretrained_model.visual_projection.parameters()},\n        {'params': pretrained_model.text_projection.parameters()},\n        {'params': [pretrained_model.logit_scale]}\n    ], lr=lr)\n    \n    pretrained_model.train()\n    \n    for epoch in range(num_epochs):\n        for batch_idx, (images, text_tokens, _) in enumerate(dataloader):\n            images = images.to(device)\n            text_tokens = text_tokens.to(device)\n            \n            logits_per_image, logits_per_text = pretrained_model(images, text_tokens)\n            loss = clip_loss(logits_per_image, logits_per_text)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            if batch_idx % 50 == 0:\n                print(f'Fine-tune Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')"
  },
  {
    "objectID": "posts/clip-code/index.html#advanced-applications",
    "href": "posts/clip-code/index.html#advanced-applications",
    "title": "CLIP Code Guide: Complete Implementation and Usage",
    "section": "",
    "text": "class CLIPImageSearch:\n    def __init__(self, model, preprocess):\n        self.model = model\n        self.preprocess = preprocess\n        self.image_features = None\n        self.image_paths = None\n    \n    def index_images(self, image_paths):\n        \"\"\"Pre-compute features for all images\"\"\"\n        self.image_paths = image_paths\n        features = []\n        \n        for img_path in image_paths:\n            image = Image.open(img_path)\n            image_input = self.preprocess(image).unsqueeze(0).to(device)\n            \n            with torch.no_grad():\n                image_feature = self.model.encode_image(image_input)\n                features.append(image_feature)\n        \n        self.image_features = torch.cat(features, dim=0)\n        self.image_features = self.image_features / self.image_features.norm(dim=-1, keepdim=True)\n    \n    def search(self, query_text, top_k=5):\n        \"\"\"Search for images matching the text query\"\"\"\n        text_input = clip.tokenize([query_text]).to(device)\n        \n        with torch.no_grad():\n            text_features = self.model.encode_text(text_input)\n            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n            \n            # Compute similarities\n            similarities = (text_features @ self.image_features.T).squeeze(0)\n            \n            # Get top-k results\n            top_similarities, top_indices = similarities.topk(top_k)\n        \n        results = []\n        for sim, idx in zip(top_similarities, top_indices):\n            results.append({\n                'path': self.image_paths[idx],\n                'similarity': sim.item()\n            })\n        \n        return results\n\n# Usage example\nsearch_engine = CLIPImageSearch(model, preprocess)\nsearch_engine.index_images(list_of_image_paths)\nresults = search_engine.search(\"a red sports car\", top_k=10)\n\n\n\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\ndef cluster_images_by_content(image_paths, n_clusters=5):\n    # Extract features for all images\n    features = []\n    \n    for img_path in image_paths:\n        image = Image.open(img_path)\n        image_input = preprocess(image).unsqueeze(0).to(device)\n        \n        with torch.no_grad():\n            feature = model.encode_image(image_input)\n            features.append(feature.cpu().numpy())\n    \n    # Convert to numpy array\n    features = np.vstack(features)\n    \n    # Perform clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    cluster_labels = kmeans.fit_predict(features)\n    \n    # Organize results\n    clusters = {}\n    for i, label in enumerate(cluster_labels):\n        if label not in clusters:\n            clusters[label] = []\n        clusters[label].append(image_paths[i])\n    \n    return clusters\n\n\n\ndef visual_qa(image_path, question, answer_choices):\n    \"\"\"Simple VQA using CLIP\"\"\"\n    image = Image.open(image_path)\n    image_input = preprocess(image).unsqueeze(0).to(device)\n    \n    # Create prompts combining question with each answer\n    prompts = [f\"Question: {question} Answer: {choice}\" for choice in answer_choices]\n    text_inputs = clip.tokenize(prompts).to(device)\n    \n    with torch.no_grad():\n        image_features = model.encode_image(image_input)\n        text_features = model.encode_text(text_inputs)\n        \n        # Compute similarities\n        similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n    \n    # Return the most likely answer\n    best_idx = similarities.argmax().item()\n    return answer_choices[best_idx], similarities[0][best_idx].item()\n\n# Example usage\nanswer, confidence = visual_qa(\n    \"image.jpg\",\n    \"What color is the car?\",\n    [\"red\", \"blue\", \"green\", \"yellow\", \"black\"]\n)\nprint(f\"Answer: {answer}, Confidence: {confidence:.2%}\")"
  },
  {
    "objectID": "posts/clip-code/index.html#performance-optimization",
    "href": "posts/clip-code/index.html#performance-optimization",
    "title": "CLIP Code Guide: Complete Implementation and Usage",
    "section": "",
    "text": "def batch_encode_images(image_paths, batch_size=32):\n    \"\"\"Process images in batches for better efficiency\"\"\"\n    all_features = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_paths = image_paths[i:i+batch_size]\n        batch_images = []\n        \n        for path in batch_paths:\n            image = Image.open(path)\n            image_input = preprocess(image)\n            batch_images.append(image_input)\n        \n        batch_tensor = torch.stack(batch_images).to(device)\n        \n        with torch.no_grad():\n            batch_features = model.encode_image(batch_tensor)\n            all_features.append(batch_features.cpu())\n    \n    return torch.cat(all_features, dim=0)\n\n\n\nfrom torch.cuda.amp import autocast, GradScaler\n\ndef train_with_mixed_precision(model, dataloader, optimizer, num_epochs):\n    scaler = GradScaler()\n    model.train()\n    \n    for epoch in range(num_epochs):\n        for images, text_tokens, _ in dataloader:\n            images = images.to(device)\n            text_tokens = text_tokens.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Forward pass with autocast\n            with autocast():\n                logits_per_image, logits_per_text = model(images, text_tokens)\n                loss = clip_loss(logits_per_image, logits_per_text)\n            \n            # Backward pass with scaling\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n\n\nimport torch.quantization as quantization\n\ndef quantize_clip_model(model):\n    \"\"\"Quantize CLIP model for inference\"\"\"\n    model.eval()\n    \n    # Specify quantization configuration\n    model.qconfig = quantization.get_default_qconfig('fbgemm')\n    \n    # Prepare model for quantization\n    model_prepared = quantization.prepare(model, inplace=False)\n    \n    # Calibrate with sample data (you need to provide calibration data)\n    # ... calibration code here ...\n    \n    # Convert to quantized model\n    model_quantized = quantization.convert(model_prepared, inplace=False)\n    \n    return model_quantized"
  },
  {
    "objectID": "posts/clip-code/index.html#common-issues-and-solutions",
    "href": "posts/clip-code/index.html#common-issues-and-solutions",
    "title": "CLIP Code Guide: Complete Implementation and Usage",
    "section": "",
    "text": "# Clear GPU cache\ntorch.cuda.empty_cache()\n\n# Use gradient checkpointing for large models\ndef enable_gradient_checkpointing(model):\n    if hasattr(model.visual, 'set_grad_checkpointing'):\n        model.visual.set_grad_checkpointing(True)\n    if hasattr(model.text_encoder, 'gradient_checkpointing_enable'):\n        model.text_encoder.gradient_checkpointing_enable()\n\n\n\nfrom torchvision import transforms\n\ndef create_adaptive_transform(target_size=224):\n    return transforms.Compose([\n        transforms.Resize(target_size, interpolation=transforms.InterpolationMode.BICUBIC),\n        transforms.CenterCrop(target_size),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        )\n    ])\n\n\n\nimport re\n\ndef preprocess_text(text, max_length=77):\n    \"\"\"Clean and preprocess text for CLIP\"\"\"\n    # Remove special characters and extra whitespace\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = ' '.join(text.split())\n    \n    # Truncate if too long\n    words = text.split()\n    if len(words) &gt; max_length - 2:  # Account for special tokens\n        text = ' '.join(words[:max_length-2])\n    \n    return text\n\n\n\ndef evaluate_zero_shot_accuracy(model, preprocess, test_loader, class_names):\n    \"\"\"Evaluate zero-shot classification accuracy\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    \n    # Encode class names\n    text_inputs = clip.tokenize([f\"a photo of a {name}\" for name in class_names]).to(device)\n    \n    with torch.no_grad():\n        text_features = model.encode_text(text_inputs)\n        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n        \n        for images, labels in test_loader:\n            images = images.to(device)\n            \n            # Encode images\n            image_features = model.encode_image(images)\n            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n            \n            # Compute similarities\n            similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n            predictions = similarities.argmax(dim=-1)\n            \n            correct += (predictions == labels.to(device)).sum().item()\n            total += labels.size(0)\n    \n    accuracy = correct / total\n    return accuracy\n\n# Usage\naccuracy = evaluate_zero_shot_accuracy(model, preprocess, test_loader, class_names)\nprint(f\"Zero-shot accuracy: {accuracy:.2%}\")"
  },
  {
    "objectID": "posts/clip-code/index.html#conclusion",
    "href": "posts/clip-code/index.html#conclusion",
    "title": "CLIP Code Guide: Complete Implementation and Usage",
    "section": "",
    "text": "This guide covers the essential aspects of working with CLIP, from basic usage to advanced implementations. Key takeaways:\n\nStart Simple: Use pre-trained models for most applications\nUnderstand the Architecture: CLIPâ€™s power comes from joint text-image training\nOptimize for Your Use Case: Fine-tune or customize based on your specific needs\nMonitor Performance: Use proper evaluation metrics and optimization techniques\nHandle Edge Cases: Implement robust preprocessing and error handling\n\nFor production deployments, consider:\n\nModel quantization for faster inference\nBatch processing for efficiency\nProper error handling and fallbacks\nMonitoring and logging for performance tracking\n\nThe field of multimodal AI is rapidly evolving, so stay updated with the latest research and implementations to leverage CLIPâ€™s full potential in your applications."
  },
  {
    "objectID": "posts/dino-v2-scratch/index.html",
    "href": "posts/dino-v2-scratch/index.html",
    "title": "DINOv2 Student-Teacher Network Training Guide",
    "section": "",
    "text": "This guide provides a complete implementation for training a DINOv2 (DINO version 2) student-teacher network from scratch using PyTorch. DINOv2 is a self-supervised learning method that trains vision transformers without labels using a teacher-student distillation framework.\n\n\n\nOverview\nArchitecture Components\nImplementation\nTraining Loop\nUsage Example\n\n\n\n\nDINOv2 uses a student-teacher framework where:\n\nTeacher network: Provides stable targets (EMA of student weights)\nStudent network: Learns to match teacher outputs\nMulti-crop strategy: Uses different image crops for robustness\nCentering mechanism: Prevents mode collapse\n\n\n\n\n\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.distributed as dist\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nimport math\nimport numpy as np\nfrom typing import Optional, List, Tuple\n\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding\"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n        \n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n    \n    def forward(self, x):\n        B, C, H, W = x.shape\n        x = self.proj(x).flatten(2).transpose(1, 2)  # [B, N, D]\n        return x\n\n\n\nclass MultiheadAttention(nn.Module):\n    \"\"\"Multi-head Self Attention\"\"\"\n    def __init__(self, embed_dim, num_heads, dropout=0.0):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        B, N, D = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n        attn = attn.softmax(dim=-1)\n        attn = self.dropout(attn)\n        \n        x = (attn @ v).transpose(1, 2).reshape(B, N, D)\n        x = self.proj(x)\n        return x\n\n\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Transformer Block\"\"\"\n    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.0):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn = MultiheadAttention(embed_dim, num_heads, dropout)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        \n        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, mlp_hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(mlp_hidden_dim, embed_dim),\n            nn.Dropout(dropout)\n        )\n    \n    def forward(self, x):\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n\n\n\nclass VisionTransformer(nn.Module):\n    \"\"\"Vision Transformer\"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, \n                 depth=12, num_heads=12, mlp_ratio=4.0, dropout=0.0):\n        super().__init__()\n        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n        num_patches = self.patch_embed.num_patches\n        \n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        self.dropout = nn.Dropout(dropout)\n        \n        self.blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n            for _ in range(depth)\n        ])\n        \n        self.norm = nn.LayerNorm(embed_dim)\n        \n        # Initialize weights\n        self._init_weights()\n    \n    def _init_weights(self):\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n        \n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.trunc_normal_(m.weight, std=0.02)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.LayerNorm):\n                nn.init.constant_(m.bias, 0)\n                nn.init.constant_(m.weight, 1.0)\n    \n    def forward(self, x):\n        B = x.shape[0]\n        x = self.patch_embed(x)\n        \n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed\n        x = self.dropout(x)\n        \n        for block in self.blocks:\n            x = block(x)\n        \n        x = self.norm(x)\n        return x[:, 0]  # Return CLS token\n\n\n\nclass DINOHead(nn.Module):\n    \"\"\"DINO Projection Head\"\"\"\n    def __init__(self, in_dim, out_dim, hidden_dim=2048, bottleneck_dim=256, \n                 num_layers=3, use_bn=False, norm_last_layer=True):\n        super().__init__()\n        \n        if num_layers == 1:\n            self.mlp = nn.Linear(in_dim, bottleneck_dim)\n        else:\n            layers = [nn.Linear(in_dim, hidden_dim)]\n            if use_bn:\n                layers.append(nn.BatchNorm1d(hidden_dim))\n            layers.append(nn.GELU())\n            \n            for _ in range(num_layers - 2):\n                layers.append(nn.Linear(hidden_dim, hidden_dim))\n                if use_bn:\n                    layers.append(nn.BatchNorm1d(hidden_dim))\n                layers.append(nn.GELU())\n            \n            layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n            self.mlp = nn.Sequential(*layers)\n        \n        self.apply(self._init_weights)\n        \n        self.last_layer = nn.utils.weight_norm(\n            nn.Linear(bottleneck_dim, out_dim, bias=False)\n        )\n        self.last_layer.weight_g.data.fill_(1)\n        if norm_last_layer:\n            self.last_layer.weight_g.requires_grad = False\n    \n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n    \n    def forward(self, x):\n        x = self.mlp(x)\n        x = nn.functional.normalize(x, dim=-1, p=2)\n        x = self.last_layer(x)\n        return x\n\n\n\nclass DINOv2(nn.Module):\n    \"\"\"Complete DINOv2 Model\"\"\"\n    def __init__(self, backbone_args, head_args):\n        super().__init__()\n        self.backbone = VisionTransformer(**backbone_args)\n        self.head = DINOHead(**head_args)\n    \n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.head(x)\n        return x\n\n\n\n\n\n\nclass MultiCropDataAugmentation:\n    \"\"\"Multi-crop data augmentation for DINOv2\"\"\"\n    def __init__(self, global_crops_scale=(0.4, 1.0), local_crops_scale=(0.05, 0.4),\n                 global_crops_number=2, local_crops_number=6, size_crops=(224, 96)):\n        self.global_crops_number = global_crops_number\n        self.local_crops_number = local_crops_number\n        \n        # Global crops (teacher and student)\n        self.global_transform = transforms.Compose([\n            transforms.RandomResizedCrop(size_crops[0], scale=global_crops_scale, \n                                       interpolation=transforms.InterpolationMode.BICUBIC),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1),\n            transforms.RandomGrayscale(p=0.2),\n            GaussianBlur(p=1.0),\n            Solarization(p=0.0),\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        ])\n        \n        # Local crops (student only)\n        self.local_transform = transforms.Compose([\n            transforms.RandomResizedCrop(size_crops[1], scale=local_crops_scale,\n                                       interpolation=transforms.InterpolationMode.BICUBIC),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1),\n            transforms.RandomGrayscale(p=0.2),\n            GaussianBlur(p=0.5),\n            Solarization(p=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        ])\n    \n    def __call__(self, image):\n        crops = []\n        \n        # Global crops\n        for _ in range(self.global_crops_number):\n            crops.append(self.global_transform(image))\n        \n        # Local crops\n        for _ in range(self.local_crops_number):\n            crops.append(self.local_transform(image))\n        \n        return crops\n\n\n\nclass GaussianBlur:\n    \"\"\"Gaussian blur augmentation\"\"\"\n    def __init__(self, p=0.5, radius_min=0.1, radius_max=2.0):\n        self.prob = p\n        self.radius_min = radius_min\n        self.radius_max = radius_max\n    \n    def __call__(self, img):\n        if torch.rand(1) &lt; self.prob:\n            radius = self.radius_min + torch.rand(1) * (self.radius_max - self.radius_min)\n            return transforms.functional.gaussian_blur(img, kernel_size=9, sigma=radius.item())\n        return img\n\nclass Solarization:\n    \"\"\"Solarization augmentation\"\"\"\n    def __init__(self, p=0.2):\n        self.p = p\n    \n    def __call__(self, img):\n        if torch.rand(1) &lt; self.p:\n            return transforms.functional.solarize(img, threshold=128)\n        return img\n\n\n\n\nclass DINOLoss(nn.Module):\n    \"\"\"DINO Loss with centering and sharpening\"\"\"\n    def __init__(self, out_dim, ncrops, warmup_teacher_temp=0.04, \n                 teacher_temp=0.04, warmup_teacher_temp_epochs=0, \n                 student_temp=0.1, center_momentum=0.9):\n        super().__init__()\n        self.student_temp = student_temp\n        self.center_momentum = center_momentum\n        self.ncrops = ncrops\n        self.register_buffer(\"center\", torch.zeros(1, out_dim))\n        \n        # Temperature schedule\n        self.teacher_temp_schedule = np.concatenate((\n            np.linspace(warmup_teacher_temp, teacher_temp, warmup_teacher_temp_epochs),\n            np.ones(1000) * teacher_temp  # Assume max 1000 epochs\n        ))\n    \n    def forward(self, student_output, teacher_output, epoch):\n        \"\"\"\n        Cross-entropy between softmax outputs of the teacher and student networks.\n        \"\"\"\n        student_out = student_output / self.student_temp\n        student_out = student_out.chunk(self.ncrops)\n        \n        # Teacher centering and sharpening\n        temp = self.teacher_temp_schedule[epoch]\n        teacher_out = F.softmax((teacher_output - self.center) / temp, dim=-1)\n        teacher_out = teacher_out.detach().chunk(2)  # Only 2 global crops for teacher\n        \n        total_loss = 0\n        n_loss_terms = 0\n        \n        for iq, q in enumerate(teacher_out):\n            for v in range(len(student_out)):\n                if v == iq:\n                    continue  # Skip same crop\n                loss = torch.sum(-q * F.log_softmax(student_out[v], dim=-1), dim=-1)\n                total_loss += loss.mean()\n                n_loss_terms += 1\n        \n        total_loss /= n_loss_terms\n        self.update_center(teacher_output)\n        return total_loss\n    \n    @torch.no_grad()\n    def update_center(self, teacher_output):\n        \"\"\"Update center used for teacher output.\"\"\"\n        batch_center = torch.sum(teacher_output, dim=0, keepdim=True)\n        batch_center = batch_center / len(teacher_output)\n        \n        # EMA update\n        self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)\n\n\n@torch.no_grad()\ndef update_teacher(student, teacher, momentum):\n    \"\"\"EMA update of the teacher network.\"\"\"\n    for param_student, param_teacher in zip(student.parameters(), teacher.parameters()):\n        param_teacher.data.mul_(momentum).add_(param_student.data, alpha=1 - momentum)\n\ndef cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0):\n    \"\"\"Cosine learning rate schedule with linear warmup.\"\"\"\n    warmup_schedule = np.array([])\n    warmup_iters = warmup_epochs * niter_per_ep\n    \n    if warmup_epochs &gt; 0:\n        warmup_schedule = np.linspace(0, base_value, warmup_iters)\n    \n    iters = np.arange(epochs * niter_per_ep - warmup_iters)\n    schedule = final_value + 0.5 * (base_value - final_value) * (1 + np.cos(np.pi * iters / len(iters)))\n    \n    schedule = np.concatenate((warmup_schedule, schedule))\n    assert len(schedule) == epochs * niter_per_ep\n    return schedule\n\n\n\n\n\nclass DINOv2Trainer:\n    \"\"\"DINOv2 Training Pipeline\"\"\"\n    def __init__(self, config):\n        self.config = config\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        # Model architecture configs\n        backbone_args = {\n            'img_size': 224,\n            'patch_size': 16,\n            'embed_dim': 768,\n            'depth': 12,\n            'num_heads': 12,\n            'mlp_ratio': 4.0,\n            'dropout': 0.0\n        }\n        \n        head_args = {\n            'in_dim': 768,\n            'out_dim': 65536,  # Large output dimension\n            'hidden_dim': 2048,\n            'bottleneck_dim': 256\n        }\n        \n        # Initialize student and teacher networks\n        self.student = DINOv2(backbone_args, head_args).to(self.device)\n        self.teacher = DINOv2(backbone_args, head_args).to(self.device)\n        \n        # Teacher starts as copy of student\n        self.teacher.load_state_dict(self.student.state_dict())\n        \n        # Teacher parameters are not updated by gradients\n        for p in self.teacher.parameters():\n            p.requires_grad = False\n        \n        # Loss function\n        self.dino_loss = DINOLoss(\n            out_dim=head_args['out_dim'],\n            ncrops=8,  # 2 global + 6 local crops\n            student_temp=0.1,\n            teacher_temp=0.04,\n            center_momentum=0.9\n        ).to(self.device)\n        \n        # Optimizer\n        self.optimizer = torch.optim.AdamW(\n            self.student.parameters(),\n            lr=config['base_lr'],\n            weight_decay=config['weight_decay']\n        )\n        \n        # Learning rate scheduler\n        self.lr_schedule = cosine_scheduler(\n            config['base_lr'],\n            config['final_lr'],\n            config['epochs'],\n            config['niter_per_ep'],\n            config['warmup_epochs']\n        )\n        \n        # Momentum schedule for teacher updates\n        self.momentum_schedule = cosine_scheduler(\n            config['momentum_teacher'],\n            1.0,\n            config['epochs'],\n            config['niter_per_ep']\n        )\n    \n    def train_epoch(self, dataloader, epoch):\n        \"\"\"Train for one epoch\"\"\"\n        self.student.train()\n        self.teacher.eval()\n        \n        total_loss = 0\n        num_batches = len(dataloader)\n        \n        for it, (images, _) in enumerate(dataloader):\n            # Update learning rate\n            lr = self.lr_schedule[epoch * num_batches + it]\n            for param_group in self.optimizer.param_groups:\n                param_group['lr'] = lr\n            \n            # Move to device and prepare crops\n            images = [im.to(self.device, non_blocking=True) for im in images]\n            \n            # Teacher forward pass (only on global crops)\n            teacher_output = self.teacher(torch.cat(images[:2]))\n            \n            # Student forward pass (on all crops)\n            student_output = self.student(torch.cat(images))\n            \n            # Compute loss\n            loss = self.dino_loss(student_output, teacher_output, epoch)\n            \n            # Backward pass\n            self.optimizer.zero_grad()\n            loss.backward()\n            \n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(self.student.parameters(), max_norm=3.0)\n            \n            self.optimizer.step()\n            \n            # Update teacher with EMA\n            momentum = self.momentum_schedule[epoch * num_batches + it]\n            update_teacher(self.student, self.teacher, momentum)\n            \n            total_loss += loss.item()\n            \n            if it % 100 == 0:\n                print(f'Epoch {epoch}, Iter {it}/{num_batches}, Loss: {loss.item():.4f}, LR: {lr:.6f}')\n        \n        return total_loss / num_batches\n    \n    def train(self, dataloader):\n        \"\"\"Full training loop\"\"\"\n        for epoch in range(self.config['epochs']):\n            avg_loss = self.train_epoch(dataloader, epoch)\n            print(f'Epoch {epoch}/{self.config[\"epochs\"]}, Average Loss: {avg_loss:.4f}')\n            \n            # Save checkpoint\n            if epoch % self.config['save_every'] == 0:\n                self.save_checkpoint(epoch)\n    \n    def save_checkpoint(self, epoch):\n        \"\"\"Save model checkpoint\"\"\"\n        checkpoint = {\n            'epoch': epoch,\n            'student_state_dict': self.student.state_dict(),\n            'teacher_state_dict': self.teacher.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'config': self.config\n        }\n        torch.save(checkpoint, f'dinov2_checkpoint_epoch_{epoch}.pth')\n\n\n\ndef main():\n    # Training configuration\n    config = {\n        'base_lr': 5e-4,\n        'final_lr': 1e-6,\n        'weight_decay': 0.04,\n        'momentum_teacher': 0.996,\n        'epochs': 100,\n        'warmup_epochs': 10,\n        'batch_size': 64,\n        'save_every': 10,\n        'niter_per_ep': None  # Will be set after dataloader creation\n    }\n    \n    # Data setup\n    transform = MultiCropDataAugmentation()\n    dataset = ImageFolder(root='path/to/your/dataset', transform=transform)\n    dataloader = DataLoader(\n        dataset, \n        batch_size=config['batch_size'], \n        shuffle=True, \n        num_workers=4,\n        pin_memory=True,\n        drop_last=True\n    )\n    \n    config['niter_per_ep'] = len(dataloader)\n    \n    # Initialize trainer and start training\n    trainer = DINOv2Trainer(config)\n    trainer.train(dataloader)\n\nmain()\n\n\n\n\nVision Transformer Backbone: Complete ViT implementation with patch embedding, multi-head attention, and transformer blocks\nMulti-crop Strategy: Global and local crops with different augmentations\nTeacher-Student Framework: EMA updates for teacher network\nDINO Loss: Cross-entropy loss with centering mechanism to prevent collapse\nLearning Rate Scheduling: Cosine annealing with warmup\nGradient Clipping: Stability during training\nCheckpointing: Save/load model states\n\n\n\n\n\nBatch Size: Use large batch sizes (256-1024) for better performance\nData Augmentation: Strong augmentations are crucial for self-supervised learning\nTemperature Scheduling: Gradually increase teacher temperature\nMomentum Scheduling: Start with high momentum and decrease over time\nMulti-GPU Training: Use DistributedDataParallel for faster training\n\nThis implementation provides a solid foundation for training DINOv2 models. Adjust hyperparameters based on your dataset size and computational resources."
  },
  {
    "objectID": "posts/dino-v2-scratch/index.html#table-of-contents",
    "href": "posts/dino-v2-scratch/index.html#table-of-contents",
    "title": "DINOv2 Student-Teacher Network Training Guide",
    "section": "",
    "text": "Overview\nArchitecture Components\nImplementation\nTraining Loop\nUsage Example"
  },
  {
    "objectID": "posts/dino-v2-scratch/index.html#overview",
    "href": "posts/dino-v2-scratch/index.html#overview",
    "title": "DINOv2 Student-Teacher Network Training Guide",
    "section": "",
    "text": "DINOv2 uses a student-teacher framework where:\n\nTeacher network: Provides stable targets (EMA of student weights)\nStudent network: Learns to match teacher outputs\nMulti-crop strategy: Uses different image crops for robustness\nCentering mechanism: Prevents mode collapse"
  },
  {
    "objectID": "posts/dino-v2-scratch/index.html#architecture-components",
    "href": "posts/dino-v2-scratch/index.html#architecture-components",
    "title": "DINOv2 Student-Teacher Network Training Guide",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.distributed as dist\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nimport math\nimport numpy as np\nfrom typing import Optional, List, Tuple\n\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding\"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n        \n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n    \n    def forward(self, x):\n        B, C, H, W = x.shape\n        x = self.proj(x).flatten(2).transpose(1, 2)  # [B, N, D]\n        return x\n\n\n\nclass MultiheadAttention(nn.Module):\n    \"\"\"Multi-head Self Attention\"\"\"\n    def __init__(self, embed_dim, num_heads, dropout=0.0):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        B, N, D = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n        attn = attn.softmax(dim=-1)\n        attn = self.dropout(attn)\n        \n        x = (attn @ v).transpose(1, 2).reshape(B, N, D)\n        x = self.proj(x)\n        return x\n\n\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Transformer Block\"\"\"\n    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.0):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn = MultiheadAttention(embed_dim, num_heads, dropout)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        \n        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, mlp_hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(mlp_hidden_dim, embed_dim),\n            nn.Dropout(dropout)\n        )\n    \n    def forward(self, x):\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n\n\n\nclass VisionTransformer(nn.Module):\n    \"\"\"Vision Transformer\"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, \n                 depth=12, num_heads=12, mlp_ratio=4.0, dropout=0.0):\n        super().__init__()\n        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n        num_patches = self.patch_embed.num_patches\n        \n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        self.dropout = nn.Dropout(dropout)\n        \n        self.blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n            for _ in range(depth)\n        ])\n        \n        self.norm = nn.LayerNorm(embed_dim)\n        \n        # Initialize weights\n        self._init_weights()\n    \n    def _init_weights(self):\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n        \n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.trunc_normal_(m.weight, std=0.02)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.LayerNorm):\n                nn.init.constant_(m.bias, 0)\n                nn.init.constant_(m.weight, 1.0)\n    \n    def forward(self, x):\n        B = x.shape[0]\n        x = self.patch_embed(x)\n        \n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed\n        x = self.dropout(x)\n        \n        for block in self.blocks:\n            x = block(x)\n        \n        x = self.norm(x)\n        return x[:, 0]  # Return CLS token\n\n\n\nclass DINOHead(nn.Module):\n    \"\"\"DINO Projection Head\"\"\"\n    def __init__(self, in_dim, out_dim, hidden_dim=2048, bottleneck_dim=256, \n                 num_layers=3, use_bn=False, norm_last_layer=True):\n        super().__init__()\n        \n        if num_layers == 1:\n            self.mlp = nn.Linear(in_dim, bottleneck_dim)\n        else:\n            layers = [nn.Linear(in_dim, hidden_dim)]\n            if use_bn:\n                layers.append(nn.BatchNorm1d(hidden_dim))\n            layers.append(nn.GELU())\n            \n            for _ in range(num_layers - 2):\n                layers.append(nn.Linear(hidden_dim, hidden_dim))\n                if use_bn:\n                    layers.append(nn.BatchNorm1d(hidden_dim))\n                layers.append(nn.GELU())\n            \n            layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n            self.mlp = nn.Sequential(*layers)\n        \n        self.apply(self._init_weights)\n        \n        self.last_layer = nn.utils.weight_norm(\n            nn.Linear(bottleneck_dim, out_dim, bias=False)\n        )\n        self.last_layer.weight_g.data.fill_(1)\n        if norm_last_layer:\n            self.last_layer.weight_g.requires_grad = False\n    \n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n    \n    def forward(self, x):\n        x = self.mlp(x)\n        x = nn.functional.normalize(x, dim=-1, p=2)\n        x = self.last_layer(x)\n        return x\n\n\n\nclass DINOv2(nn.Module):\n    \"\"\"Complete DINOv2 Model\"\"\"\n    def __init__(self, backbone_args, head_args):\n        super().__init__()\n        self.backbone = VisionTransformer(**backbone_args)\n        self.head = DINOHead(**head_args)\n    \n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.head(x)\n        return x\n\n\n\n\n\n\nclass MultiCropDataAugmentation:\n    \"\"\"Multi-crop data augmentation for DINOv2\"\"\"\n    def __init__(self, global_crops_scale=(0.4, 1.0), local_crops_scale=(0.05, 0.4),\n                 global_crops_number=2, local_crops_number=6, size_crops=(224, 96)):\n        self.global_crops_number = global_crops_number\n        self.local_crops_number = local_crops_number\n        \n        # Global crops (teacher and student)\n        self.global_transform = transforms.Compose([\n            transforms.RandomResizedCrop(size_crops[0], scale=global_crops_scale, \n                                       interpolation=transforms.InterpolationMode.BICUBIC),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1),\n            transforms.RandomGrayscale(p=0.2),\n            GaussianBlur(p=1.0),\n            Solarization(p=0.0),\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        ])\n        \n        # Local crops (student only)\n        self.local_transform = transforms.Compose([\n            transforms.RandomResizedCrop(size_crops[1], scale=local_crops_scale,\n                                       interpolation=transforms.InterpolationMode.BICUBIC),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1),\n            transforms.RandomGrayscale(p=0.2),\n            GaussianBlur(p=0.5),\n            Solarization(p=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        ])\n    \n    def __call__(self, image):\n        crops = []\n        \n        # Global crops\n        for _ in range(self.global_crops_number):\n            crops.append(self.global_transform(image))\n        \n        # Local crops\n        for _ in range(self.local_crops_number):\n            crops.append(self.local_transform(image))\n        \n        return crops\n\n\n\nclass GaussianBlur:\n    \"\"\"Gaussian blur augmentation\"\"\"\n    def __init__(self, p=0.5, radius_min=0.1, radius_max=2.0):\n        self.prob = p\n        self.radius_min = radius_min\n        self.radius_max = radius_max\n    \n    def __call__(self, img):\n        if torch.rand(1) &lt; self.prob:\n            radius = self.radius_min + torch.rand(1) * (self.radius_max - self.radius_min)\n            return transforms.functional.gaussian_blur(img, kernel_size=9, sigma=radius.item())\n        return img\n\nclass Solarization:\n    \"\"\"Solarization augmentation\"\"\"\n    def __init__(self, p=0.2):\n        self.p = p\n    \n    def __call__(self, img):\n        if torch.rand(1) &lt; self.p:\n            return transforms.functional.solarize(img, threshold=128)\n        return img\n\n\n\n\nclass DINOLoss(nn.Module):\n    \"\"\"DINO Loss with centering and sharpening\"\"\"\n    def __init__(self, out_dim, ncrops, warmup_teacher_temp=0.04, \n                 teacher_temp=0.04, warmup_teacher_temp_epochs=0, \n                 student_temp=0.1, center_momentum=0.9):\n        super().__init__()\n        self.student_temp = student_temp\n        self.center_momentum = center_momentum\n        self.ncrops = ncrops\n        self.register_buffer(\"center\", torch.zeros(1, out_dim))\n        \n        # Temperature schedule\n        self.teacher_temp_schedule = np.concatenate((\n            np.linspace(warmup_teacher_temp, teacher_temp, warmup_teacher_temp_epochs),\n            np.ones(1000) * teacher_temp  # Assume max 1000 epochs\n        ))\n    \n    def forward(self, student_output, teacher_output, epoch):\n        \"\"\"\n        Cross-entropy between softmax outputs of the teacher and student networks.\n        \"\"\"\n        student_out = student_output / self.student_temp\n        student_out = student_out.chunk(self.ncrops)\n        \n        # Teacher centering and sharpening\n        temp = self.teacher_temp_schedule[epoch]\n        teacher_out = F.softmax((teacher_output - self.center) / temp, dim=-1)\n        teacher_out = teacher_out.detach().chunk(2)  # Only 2 global crops for teacher\n        \n        total_loss = 0\n        n_loss_terms = 0\n        \n        for iq, q in enumerate(teacher_out):\n            for v in range(len(student_out)):\n                if v == iq:\n                    continue  # Skip same crop\n                loss = torch.sum(-q * F.log_softmax(student_out[v], dim=-1), dim=-1)\n                total_loss += loss.mean()\n                n_loss_terms += 1\n        \n        total_loss /= n_loss_terms\n        self.update_center(teacher_output)\n        return total_loss\n    \n    @torch.no_grad()\n    def update_center(self, teacher_output):\n        \"\"\"Update center used for teacher output.\"\"\"\n        batch_center = torch.sum(teacher_output, dim=0, keepdim=True)\n        batch_center = batch_center / len(teacher_output)\n        \n        # EMA update\n        self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)\n\n\n@torch.no_grad()\ndef update_teacher(student, teacher, momentum):\n    \"\"\"EMA update of the teacher network.\"\"\"\n    for param_student, param_teacher in zip(student.parameters(), teacher.parameters()):\n        param_teacher.data.mul_(momentum).add_(param_student.data, alpha=1 - momentum)\n\ndef cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0):\n    \"\"\"Cosine learning rate schedule with linear warmup.\"\"\"\n    warmup_schedule = np.array([])\n    warmup_iters = warmup_epochs * niter_per_ep\n    \n    if warmup_epochs &gt; 0:\n        warmup_schedule = np.linspace(0, base_value, warmup_iters)\n    \n    iters = np.arange(epochs * niter_per_ep - warmup_iters)\n    schedule = final_value + 0.5 * (base_value - final_value) * (1 + np.cos(np.pi * iters / len(iters)))\n    \n    schedule = np.concatenate((warmup_schedule, schedule))\n    assert len(schedule) == epochs * niter_per_ep\n    return schedule"
  },
  {
    "objectID": "posts/dino-v2-scratch/index.html#training-loop-implementation",
    "href": "posts/dino-v2-scratch/index.html#training-loop-implementation",
    "title": "DINOv2 Student-Teacher Network Training Guide",
    "section": "",
    "text": "class DINOv2Trainer:\n    \"\"\"DINOv2 Training Pipeline\"\"\"\n    def __init__(self, config):\n        self.config = config\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        # Model architecture configs\n        backbone_args = {\n            'img_size': 224,\n            'patch_size': 16,\n            'embed_dim': 768,\n            'depth': 12,\n            'num_heads': 12,\n            'mlp_ratio': 4.0,\n            'dropout': 0.0\n        }\n        \n        head_args = {\n            'in_dim': 768,\n            'out_dim': 65536,  # Large output dimension\n            'hidden_dim': 2048,\n            'bottleneck_dim': 256\n        }\n        \n        # Initialize student and teacher networks\n        self.student = DINOv2(backbone_args, head_args).to(self.device)\n        self.teacher = DINOv2(backbone_args, head_args).to(self.device)\n        \n        # Teacher starts as copy of student\n        self.teacher.load_state_dict(self.student.state_dict())\n        \n        # Teacher parameters are not updated by gradients\n        for p in self.teacher.parameters():\n            p.requires_grad = False\n        \n        # Loss function\n        self.dino_loss = DINOLoss(\n            out_dim=head_args['out_dim'],\n            ncrops=8,  # 2 global + 6 local crops\n            student_temp=0.1,\n            teacher_temp=0.04,\n            center_momentum=0.9\n        ).to(self.device)\n        \n        # Optimizer\n        self.optimizer = torch.optim.AdamW(\n            self.student.parameters(),\n            lr=config['base_lr'],\n            weight_decay=config['weight_decay']\n        )\n        \n        # Learning rate scheduler\n        self.lr_schedule = cosine_scheduler(\n            config['base_lr'],\n            config['final_lr'],\n            config['epochs'],\n            config['niter_per_ep'],\n            config['warmup_epochs']\n        )\n        \n        # Momentum schedule for teacher updates\n        self.momentum_schedule = cosine_scheduler(\n            config['momentum_teacher'],\n            1.0,\n            config['epochs'],\n            config['niter_per_ep']\n        )\n    \n    def train_epoch(self, dataloader, epoch):\n        \"\"\"Train for one epoch\"\"\"\n        self.student.train()\n        self.teacher.eval()\n        \n        total_loss = 0\n        num_batches = len(dataloader)\n        \n        for it, (images, _) in enumerate(dataloader):\n            # Update learning rate\n            lr = self.lr_schedule[epoch * num_batches + it]\n            for param_group in self.optimizer.param_groups:\n                param_group['lr'] = lr\n            \n            # Move to device and prepare crops\n            images = [im.to(self.device, non_blocking=True) for im in images]\n            \n            # Teacher forward pass (only on global crops)\n            teacher_output = self.teacher(torch.cat(images[:2]))\n            \n            # Student forward pass (on all crops)\n            student_output = self.student(torch.cat(images))\n            \n            # Compute loss\n            loss = self.dino_loss(student_output, teacher_output, epoch)\n            \n            # Backward pass\n            self.optimizer.zero_grad()\n            loss.backward()\n            \n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(self.student.parameters(), max_norm=3.0)\n            \n            self.optimizer.step()\n            \n            # Update teacher with EMA\n            momentum = self.momentum_schedule[epoch * num_batches + it]\n            update_teacher(self.student, self.teacher, momentum)\n            \n            total_loss += loss.item()\n            \n            if it % 100 == 0:\n                print(f'Epoch {epoch}, Iter {it}/{num_batches}, Loss: {loss.item():.4f}, LR: {lr:.6f}')\n        \n        return total_loss / num_batches\n    \n    def train(self, dataloader):\n        \"\"\"Full training loop\"\"\"\n        for epoch in range(self.config['epochs']):\n            avg_loss = self.train_epoch(dataloader, epoch)\n            print(f'Epoch {epoch}/{self.config[\"epochs\"]}, Average Loss: {avg_loss:.4f}')\n            \n            # Save checkpoint\n            if epoch % self.config['save_every'] == 0:\n                self.save_checkpoint(epoch)\n    \n    def save_checkpoint(self, epoch):\n        \"\"\"Save model checkpoint\"\"\"\n        checkpoint = {\n            'epoch': epoch,\n            'student_state_dict': self.student.state_dict(),\n            'teacher_state_dict': self.teacher.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'config': self.config\n        }\n        torch.save(checkpoint, f'dinov2_checkpoint_epoch_{epoch}.pth')"
  },
  {
    "objectID": "posts/dino-v2-scratch/index.html#usage-example",
    "href": "posts/dino-v2-scratch/index.html#usage-example",
    "title": "DINOv2 Student-Teacher Network Training Guide",
    "section": "",
    "text": "def main():\n    # Training configuration\n    config = {\n        'base_lr': 5e-4,\n        'final_lr': 1e-6,\n        'weight_decay': 0.04,\n        'momentum_teacher': 0.996,\n        'epochs': 100,\n        'warmup_epochs': 10,\n        'batch_size': 64,\n        'save_every': 10,\n        'niter_per_ep': None  # Will be set after dataloader creation\n    }\n    \n    # Data setup\n    transform = MultiCropDataAugmentation()\n    dataset = ImageFolder(root='path/to/your/dataset', transform=transform)\n    dataloader = DataLoader(\n        dataset, \n        batch_size=config['batch_size'], \n        shuffle=True, \n        num_workers=4,\n        pin_memory=True,\n        drop_last=True\n    )\n    \n    config['niter_per_ep'] = len(dataloader)\n    \n    # Initialize trainer and start training\n    trainer = DINOv2Trainer(config)\n    trainer.train(dataloader)\n\nmain()"
  },
  {
    "objectID": "posts/dino-v2-scratch/index.html#key-features-implemented",
    "href": "posts/dino-v2-scratch/index.html#key-features-implemented",
    "title": "DINOv2 Student-Teacher Network Training Guide",
    "section": "",
    "text": "Vision Transformer Backbone: Complete ViT implementation with patch embedding, multi-head attention, and transformer blocks\nMulti-crop Strategy: Global and local crops with different augmentations\nTeacher-Student Framework: EMA updates for teacher network\nDINO Loss: Cross-entropy loss with centering mechanism to prevent collapse\nLearning Rate Scheduling: Cosine annealing with warmup\nGradient Clipping: Stability during training\nCheckpointing: Save/load model states"
  },
  {
    "objectID": "posts/dino-v2-scratch/index.html#training-tips",
    "href": "posts/dino-v2-scratch/index.html#training-tips",
    "title": "DINOv2 Student-Teacher Network Training Guide",
    "section": "",
    "text": "Batch Size: Use large batch sizes (256-1024) for better performance\nData Augmentation: Strong augmentations are crucial for self-supervised learning\nTemperature Scheduling: Gradually increase teacher temperature\nMomentum Scheduling: Start with high momentum and decrease over time\nMulti-GPU Training: Use DistributedDataParallel for faster training\n\nThis implementation provides a solid foundation for training DINOv2 models. Adjust hyperparameters based on your dataset size and computational resources."
  },
  {
    "objectID": "posts/pytorch-lightning/index.html",
    "href": "posts/pytorch-lightning/index.html",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "PyTorch Lightning is a lightweight wrapper for PyTorch that helps organize code and reduce boilerplate while adding powerful features for research and production. This guide will walk you through the basics to advanced techniques.\n\n\n\nIntroduction to PyTorch Lightning\nInstallation\nBasic Structure: The LightningModule\nDataModules\nTraining with Trainer\nCallbacks\nLogging\nDistributed Training\nHyperparameter Tuning\nModel Checkpointing\nProduction Deployment\nBest Practices\n\n\n\n\nPyTorch Lightning separates research code from engineering code, making models more:\n\nReproducible: The same code works across different hardware\nReadable: Standard project structure makes collaboration easier\nScalable: Train on CPUs, GPUs, TPUs or clusters with no code changes\n\nLightning helps you focus on the science by handling the engineering details.\n\n\n\npip install pytorch-lightning\nFor the latest features, you can install from the source:\npip install git+https://github.com/Lightning-AI/lightning.git\n\n\n\nThe core component in Lightning is the LightningModule, which organizes your PyTorch code into a standardized structure:\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MNISTModel(pl.LightningModule):\n    def __init__(self, lr=0.001):\n        super().__init__()\n        self.save_hyperparameters()  # Saves learning_rate to hparams\n        self.layer_1 = nn.Linear(28 * 28, 128)\n        self.layer_2 = nn.Linear(128, 10)\n        self.lr = lr\n        \n    def forward(self, x):\n        batch_size, channels, width, height = x.size()\n        x = x.view(batch_size, -1)\n        x = self.layer_1(x)\n        x = F.relu(x)\n        x = self.layer_2(x)\n        return x\n        \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        self.log('train_loss', loss)\n        return loss\n        \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        self.log('val_loss', loss)\n        self.log('val_acc', acc)\n        \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        self.log('test_loss', loss)\n        self.log('test_acc', acc)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer\nThis basic structure includes:\n\nModel architecture (__init__ and forward)\nTraining logic (training_step)\nValidation logic (validation_step)\nTest logic (test_step)\nOptimization setup (configure_optimizers)\n\n\n\n\nLightningâ€™s LightningDataModule encapsulates all data-related logic:\nfrom pytorch_lightning import LightningDataModule\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\n\nclass MNISTDataModule(LightningDataModule):\n    def __init__(self, data_dir='./data', batch_size=32):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n        \n    def prepare_data(self):\n        # Download data if needed\n        MNIST(self.data_dir, train=True, download=True)\n        MNIST(self.data_dir, train=False, download=True)\n        \n    def setup(self, stage=None):\n        # Assign train/val/test datasets\n        if stage == 'fit' or stage is None:\n            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n            \n        if stage == 'test' or stage is None:\n            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n            \n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=self.batch_size)\n        \n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n        \n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=self.batch_size)\nBenefits of LightningDataModule:\n\nEncapsulates all data preparation logic\nMakes data pipeline portable and reproducible\nSimplifies sharing data pipelines between projects\n\n\n\n\nThe Lightning Trainer handles the training loop and validation:\nfrom pytorch_lightning import Trainer\n\n# Create model and data module\nmodel = MNISTModel()\ndata_module = MNISTDataModule()\n\n# Initialize trainer\ntrainer = Trainer(\n    max_epochs=10,\n    accelerator=\"auto\",  # Automatically use available GPU\n    devices=\"auto\",\n    logger=True,         # Use TensorBoard logger by default\n)\n\n# Train model\ntrainer.fit(model, datamodule=data_module)\n\n# Test model\ntrainer.test(model, datamodule=data_module)\nThe Trainer automatically handles:\n\nEpoch and batch iteration\nOptimizer steps\nLogging metrics\nHardware acceleration (CPU, GPU, TPU)\nEarly stopping\nCheckpointing\nMulti-GPU training\n\n\n\ntrainer = Trainer(\n    max_epochs=10,                    # Maximum number of epochs\n    min_epochs=1,                     # Minimum number of epochs\n    accelerator=\"cpu\",                # Use CPU acceleration\n    devices=1,                        # Use 1 CPUs\n    precision=\"16-mixed\",             # Use mixed precision for faster training\n    gradient_clip_val=0.5,            # Clip gradients\n    accumulate_grad_batches=4,        # Accumulate gradients over 4 batches\n    log_every_n_steps=50,             # Log metrics every 50 steps\n    val_check_interval=0.25,          # Run validation 4 times per epoch\n    fast_dev_run=False,               # Debug mode (only run a few batches)\n    deterministic=True,               # Make training deterministic\n)\n\n\n\n\nCallbacks add functionality to the training loop without modifying the core code:\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n\n# Save the best model based on validation accuracy\ncheckpoint_callback = ModelCheckpoint(\n    monitor='val_acc',\n    dirpath='./checkpoints',\n    filename='mnist-{epoch:02d}-{val_acc:.2f}',\n    save_top_k=3,\n    mode='max',\n)\n\n# Stop training when validation loss stops improving\nearly_stop_callback = EarlyStopping(\n    monitor='val_loss',\n    patience=3,\n    verbose=True,\n    mode='min'\n)\n\n# Monitor learning rate\nlr_monitor = LearningRateMonitor(logging_interval='step')\n\n# Initialize trainer with callbacks\ntrainer = Trainer(\n    max_epochs=10,\n    callbacks=[checkpoint_callback, early_stop_callback, lr_monitor]\n)\n\n\nYou can create custom callbacks by extending the base Callback class:\nfrom pytorch_lightning.callbacks import Callback\n\nclass PrintingCallback(Callback):\n    def on_train_start(self, trainer, pl_module):\n        print(\"Training has started!\")\n        \n    def on_train_end(self, trainer, pl_module):\n        print(\"Training has finished!\")\n        \n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        if batch_idx % 100 == 0:\n            print(f\"Batch {batch_idx} completed\")\n\n\n\n\nLightning supports various loggers:\nfrom pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n\n# TensorBoard logger\ntensorboard_logger = TensorBoardLogger(save_dir=\"logs/\", name=\"mnist\")\n\n# Initialize trainer with loggers\ntrainer = Trainer(\n    max_epochs=10,\n    logger=[tensorboard_logger]\n)\nAdding metrics in your model:\ndef training_step(self, batch, batch_idx):\n    x, y = batch\n    logits = self(x)\n    loss = F.cross_entropy(logits, y)\n    \n    # Log scalar values\n    self.log('train_loss', loss)\n    \n    # Log learning rate\n    lr = self.optimizers().param_groups[0]['lr']\n    self.log('learning_rate', lr)\n    \n    # Log histograms (on GPU)\n    if batch_idx % 100 == 0:\n        self.logger.experiment.add_histogram('logits', logits, self.global_step)\n    \n    return loss\n\n\n\nLightning makes distributed training simple:\n# Single GPU\ntrainer = Trainer(accelerator=\"gpu\", devices=1)\n\n# Multiple GPUs (DDP strategy automatically selected)\ntrainer = Trainer(accelerator=\"gpu\", devices=4)\n\n# Specific GPU indices\ntrainer = Trainer(accelerator=\"gpu\", devices=[0, 1, 3])\n\n# TPU cores\ntrainer = Trainer(accelerator=\"tpu\", devices=8)\n\n# Advanced distributed training\ntrainer = Trainer(\n    accelerator=\"gpu\",\n    devices=4,\n    strategy=\"ddp\",  # Distributed Data Parallel\n    num_nodes=2      # Use 2 machines\n)\nOther distribution strategies:\n\nddp_spawn: Similar to DDP but uses spawn for multiprocessing\ndeepspeed: For very large models using DeepSpeed\nfsdp: Fully Sharded Data Parallel for huge models\n\n\n\n\nLightning integrates well with optimization libraries:\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.tuner import Tuner\n\n# Create model\nmodel = MNISTModel()\ndata_module = MNISTDataModule()\n\n# Create trainer\ntrainer = Trainer(max_epochs=10)\n\n# Use Lightning's Tuner for auto-scaling batch size\ntuner = Tuner(trainer)\ntuner.scale_batch_size(model, datamodule=data_module)\n\n# Find optimal learning rate\nlr_finder = tuner.lr_find(model, datamodule=data_module)\nmodel.learning_rate = lr_finder.suggestion()\n\n# Train with optimized parameters\ntrainer.fit(model, datamodule=data_module)\n\n\n\nSaving and loading model checkpoints:\n# Save checkpoints during training\ncheckpoint_callback = ModelCheckpoint(\n    dirpath='./checkpoints',\n    filename='{epoch}-{val_loss:.2f}',\n    monitor='val_loss',\n    save_top_k=3,\n    mode='min'\n)\n\ntrainer = Trainer(\n    max_epochs=10,\n    callbacks=[checkpoint_callback]\n)\n\ntrainer.fit(model, datamodule=data_module)\n\n# Path to best checkpoint\nbest_model_path = checkpoint_callback.best_model_path\n\n# Load checkpoint into model\nmodel = MNISTModel.load_from_checkpoint(best_model_path)\n\n# Continue training from checkpoint\ntrainer.fit(model, datamodule=data_module)\nCheckpointing for production:\n# Save model in production-ready format\ntrainer.save_checkpoint(\"model.ckpt\")\n\n# Extract state dict for production\ncheckpoint = torch.load(\"model.ckpt\")\nmodel_state_dict = checkpoint[\"state_dict\"]\n\n# Save just the PyTorch model for production\nmodel = MNISTModel()\nmodel.load_state_dict(model_state_dict)\ntorch.save(model.state_dict(), \"production_model.pt\")\n\n\n\nConverting Lightning models to production:\n# Option 1: Use the Lightning model directly\nmodel = MNISTModel.load_from_checkpoint(\"model.ckpt\")\nmodel.eval()\nmodel.freeze()  # Freeze the model parameters\n\n# Make predictions\nwith torch.no_grad():\n    x = torch.randn(1, 1, 28, 28)\n    y_hat = model(x)\n\n# Option 2: Extract the PyTorch model\nclass ProductionModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(28 * 28, 128)\n        self.layer_2 = nn.Linear(128, 10)\n    \n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.layer_1(x))\n        x = self.layer_2(x)\n        return x\n\n# Load weights from Lightning model\nlightning_model = MNISTModel.load_from_checkpoint(\"model.ckpt\")\nproduction_model = ProductionModel()\n\n# Copy weights\nproduction_model.layer_1.weight.data = lightning_model.layer_1.weight.data\nproduction_model.layer_1.bias.data = lightning_model.layer_1.bias.data\nproduction_model.layer_2.weight.data = lightning_model.layer_2.weight.data\nproduction_model.layer_2.bias.data = lightning_model.layer_2.bias.data\n\n# Save production model\ntorch.save(production_model.state_dict(), \"production_model.pt\")\n\n# Export to ONNX\ndummy_input = torch.randn(1, 1, 28, 28)\ntorch.onnx.export(\n    production_model,\n    dummy_input,\n    \"model.onnx\",\n    export_params=True,\n    opset_version=11,\n    input_names=['input'],\n    output_names=['output']\n)\n\n\n\n\n\nA well-organized Lightning project structure:\nproject/\nâ”œâ”€â”€ configs/              # Configuration files\nâ”œâ”€â”€ data/                 # Data files\nâ”œâ”€â”€ lightning_logs/       # Generated logs\nâ”œâ”€â”€ models/               # Model definitions\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â””â”€â”€ mnist_model.py    # LightningModule\nâ”œâ”€â”€ data_modules/         # Data modules\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â””â”€â”€ mnist_data.py     # LightningDataModule\nâ”œâ”€â”€ callbacks/            # Custom callbacks\nâ”œâ”€â”€ utils/                # Utility functions\nâ”œâ”€â”€ main.py               # Training script\nâ””â”€â”€ README.md\n\n\n\nLightning provides a CLI for running experiments from config files:\n# main.py\nfrom pytorch_lightning.cli import LightningCLI\n\ndef cli_main():\n    # Create CLI with LightningModule and LightningDataModule\n    cli = LightningCLI(\n        MNISTModel,\n        MNISTDataModule,\n        save_config_callback=None,\n    )\n\nif __name__ == \"__main__\":\n    cli_main()\nRun with:\npython main.py fit --config configs/mnist.yaml\nExample config file (mnist.yaml):\nmodel:\n  learning_rate: 0.001\n  hidden_size: 128\ndata:\n  data_dir: ./data\n  batch_size: 64\ntrainer:\n  max_epochs: 10\n  accelerator: gpu\n  devices: 1\n\n\n\nLightning includes tools to profile your code:\nfrom pytorch_lightning.profilers import PyTorchProfiler, SimpleProfiler\n\n# PyTorch Profiler\nprofiler = PyTorchProfiler(\n    on_trace_ready=torch.profiler.tensorboard_trace_handler(\"logs/profiler\"),\n    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2)\n)\n\n# Simple Profiler\n# profiler = SimpleProfiler()\nmodel = MNISTModel()\ndata_module = MNISTDataModule()\ntrainer = Trainer(\n    max_epochs=5,\n    profiler=profiler\n)\n\ntrainer.fit(model, datamodule=data_module)\n\n\n\n\n\ntrainer = Trainer(\n    accumulate_grad_batches=4  # Accumulate over 4 batches\n)\n\n\n\ndef configure_optimizers(self):\n    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, \n        mode='min', \n        factor=0.1, \n        patience=5\n    )\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\n            \"scheduler\": scheduler,\n            \"monitor\": \"val_loss\",\n            \"interval\": \"epoch\",\n            \"frequency\": 1\n        }\n    }\n\n\n\ntrainer = Trainer(\n    precision=\"16-mixed\"  # Use 16-bit mixed precision\n)\n\n\n\nclass TransferLearningModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        # Load pretrained model\n        self.feature_extractor = torchvision.models.resnet18(pretrained=True)\n        \n        # Freeze feature extractor\n        for param in self.feature_extractor.parameters():\n            param.requires_grad = False\n            \n        # Replace final layer\n        num_features = self.feature_extractor.fc.in_features\n        self.feature_extractor.fc = nn.Linear(num_features, 10)\n        \n    def unfreeze_features(self):\n        # Unfreeze model after some training\n        for param in self.feature_extractor.parameters():\n            param.requires_grad = True\n\n\n\n\n\nPyTorch Lightning provides an organized framework that separates research code from engineering boilerplate, making deep learning projects easier to develop, share, and scale. Itâ€™s especially valuable for research projects that need to be reproducible and scalable across different hardware configurations.\nFor further information:\n\nPyTorch Lightning Documentation\nLightning GitHub Repository\nLightning Tutorials"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#table-of-contents",
    "href": "posts/pytorch-lightning/index.html#table-of-contents",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Introduction to PyTorch Lightning\nInstallation\nBasic Structure: The LightningModule\nDataModules\nTraining with Trainer\nCallbacks\nLogging\nDistributed Training\nHyperparameter Tuning\nModel Checkpointing\nProduction Deployment\nBest Practices"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#introduction-to-pytorch-lightning",
    "href": "posts/pytorch-lightning/index.html#introduction-to-pytorch-lightning",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "PyTorch Lightning separates research code from engineering code, making models more:\n\nReproducible: The same code works across different hardware\nReadable: Standard project structure makes collaboration easier\nScalable: Train on CPUs, GPUs, TPUs or clusters with no code changes\n\nLightning helps you focus on the science by handling the engineering details."
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#installation",
    "href": "posts/pytorch-lightning/index.html#installation",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "pip install pytorch-lightning\nFor the latest features, you can install from the source:\npip install git+https://github.com/Lightning-AI/lightning.git"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#basic-structure-the-lightningmodule",
    "href": "posts/pytorch-lightning/index.html#basic-structure-the-lightningmodule",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "The core component in Lightning is the LightningModule, which organizes your PyTorch code into a standardized structure:\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MNISTModel(pl.LightningModule):\n    def __init__(self, lr=0.001):\n        super().__init__()\n        self.save_hyperparameters()  # Saves learning_rate to hparams\n        self.layer_1 = nn.Linear(28 * 28, 128)\n        self.layer_2 = nn.Linear(128, 10)\n        self.lr = lr\n        \n    def forward(self, x):\n        batch_size, channels, width, height = x.size()\n        x = x.view(batch_size, -1)\n        x = self.layer_1(x)\n        x = F.relu(x)\n        x = self.layer_2(x)\n        return x\n        \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        self.log('train_loss', loss)\n        return loss\n        \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        self.log('val_loss', loss)\n        self.log('val_acc', acc)\n        \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        self.log('test_loss', loss)\n        self.log('test_acc', acc)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer\nThis basic structure includes:\n\nModel architecture (__init__ and forward)\nTraining logic (training_step)\nValidation logic (validation_step)\nTest logic (test_step)\nOptimization setup (configure_optimizers)"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#datamodules",
    "href": "posts/pytorch-lightning/index.html#datamodules",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Lightningâ€™s LightningDataModule encapsulates all data-related logic:\nfrom pytorch_lightning import LightningDataModule\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\n\nclass MNISTDataModule(LightningDataModule):\n    def __init__(self, data_dir='./data', batch_size=32):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n        \n    def prepare_data(self):\n        # Download data if needed\n        MNIST(self.data_dir, train=True, download=True)\n        MNIST(self.data_dir, train=False, download=True)\n        \n    def setup(self, stage=None):\n        # Assign train/val/test datasets\n        if stage == 'fit' or stage is None:\n            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n            \n        if stage == 'test' or stage is None:\n            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n            \n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=self.batch_size)\n        \n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n        \n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=self.batch_size)\nBenefits of LightningDataModule:\n\nEncapsulates all data preparation logic\nMakes data pipeline portable and reproducible\nSimplifies sharing data pipelines between projects"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#training-with-trainer",
    "href": "posts/pytorch-lightning/index.html#training-with-trainer",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "The Lightning Trainer handles the training loop and validation:\nfrom pytorch_lightning import Trainer\n\n# Create model and data module\nmodel = MNISTModel()\ndata_module = MNISTDataModule()\n\n# Initialize trainer\ntrainer = Trainer(\n    max_epochs=10,\n    accelerator=\"auto\",  # Automatically use available GPU\n    devices=\"auto\",\n    logger=True,         # Use TensorBoard logger by default\n)\n\n# Train model\ntrainer.fit(model, datamodule=data_module)\n\n# Test model\ntrainer.test(model, datamodule=data_module)\nThe Trainer automatically handles:\n\nEpoch and batch iteration\nOptimizer steps\nLogging metrics\nHardware acceleration (CPU, GPU, TPU)\nEarly stopping\nCheckpointing\nMulti-GPU training\n\n\n\ntrainer = Trainer(\n    max_epochs=10,                    # Maximum number of epochs\n    min_epochs=1,                     # Minimum number of epochs\n    accelerator=\"cpu\",                # Use CPU acceleration\n    devices=1,                        # Use 1 CPUs\n    precision=\"16-mixed\",             # Use mixed precision for faster training\n    gradient_clip_val=0.5,            # Clip gradients\n    accumulate_grad_batches=4,        # Accumulate gradients over 4 batches\n    log_every_n_steps=50,             # Log metrics every 50 steps\n    val_check_interval=0.25,          # Run validation 4 times per epoch\n    fast_dev_run=False,               # Debug mode (only run a few batches)\n    deterministic=True,               # Make training deterministic\n)"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#callbacks",
    "href": "posts/pytorch-lightning/index.html#callbacks",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Callbacks add functionality to the training loop without modifying the core code:\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n\n# Save the best model based on validation accuracy\ncheckpoint_callback = ModelCheckpoint(\n    monitor='val_acc',\n    dirpath='./checkpoints',\n    filename='mnist-{epoch:02d}-{val_acc:.2f}',\n    save_top_k=3,\n    mode='max',\n)\n\n# Stop training when validation loss stops improving\nearly_stop_callback = EarlyStopping(\n    monitor='val_loss',\n    patience=3,\n    verbose=True,\n    mode='min'\n)\n\n# Monitor learning rate\nlr_monitor = LearningRateMonitor(logging_interval='step')\n\n# Initialize trainer with callbacks\ntrainer = Trainer(\n    max_epochs=10,\n    callbacks=[checkpoint_callback, early_stop_callback, lr_monitor]\n)\n\n\nYou can create custom callbacks by extending the base Callback class:\nfrom pytorch_lightning.callbacks import Callback\n\nclass PrintingCallback(Callback):\n    def on_train_start(self, trainer, pl_module):\n        print(\"Training has started!\")\n        \n    def on_train_end(self, trainer, pl_module):\n        print(\"Training has finished!\")\n        \n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        if batch_idx % 100 == 0:\n            print(f\"Batch {batch_idx} completed\")"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#logging",
    "href": "posts/pytorch-lightning/index.html#logging",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Lightning supports various loggers:\nfrom pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n\n# TensorBoard logger\ntensorboard_logger = TensorBoardLogger(save_dir=\"logs/\", name=\"mnist\")\n\n# Initialize trainer with loggers\ntrainer = Trainer(\n    max_epochs=10,\n    logger=[tensorboard_logger]\n)\nAdding metrics in your model:\ndef training_step(self, batch, batch_idx):\n    x, y = batch\n    logits = self(x)\n    loss = F.cross_entropy(logits, y)\n    \n    # Log scalar values\n    self.log('train_loss', loss)\n    \n    # Log learning rate\n    lr = self.optimizers().param_groups[0]['lr']\n    self.log('learning_rate', lr)\n    \n    # Log histograms (on GPU)\n    if batch_idx % 100 == 0:\n        self.logger.experiment.add_histogram('logits', logits, self.global_step)\n    \n    return loss"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#distributed-training",
    "href": "posts/pytorch-lightning/index.html#distributed-training",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Lightning makes distributed training simple:\n# Single GPU\ntrainer = Trainer(accelerator=\"gpu\", devices=1)\n\n# Multiple GPUs (DDP strategy automatically selected)\ntrainer = Trainer(accelerator=\"gpu\", devices=4)\n\n# Specific GPU indices\ntrainer = Trainer(accelerator=\"gpu\", devices=[0, 1, 3])\n\n# TPU cores\ntrainer = Trainer(accelerator=\"tpu\", devices=8)\n\n# Advanced distributed training\ntrainer = Trainer(\n    accelerator=\"gpu\",\n    devices=4,\n    strategy=\"ddp\",  # Distributed Data Parallel\n    num_nodes=2      # Use 2 machines\n)\nOther distribution strategies:\n\nddp_spawn: Similar to DDP but uses spawn for multiprocessing\ndeepspeed: For very large models using DeepSpeed\nfsdp: Fully Sharded Data Parallel for huge models"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#hyperparameter-tuning",
    "href": "posts/pytorch-lightning/index.html#hyperparameter-tuning",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Lightning integrates well with optimization libraries:\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.tuner import Tuner\n\n# Create model\nmodel = MNISTModel()\ndata_module = MNISTDataModule()\n\n# Create trainer\ntrainer = Trainer(max_epochs=10)\n\n# Use Lightning's Tuner for auto-scaling batch size\ntuner = Tuner(trainer)\ntuner.scale_batch_size(model, datamodule=data_module)\n\n# Find optimal learning rate\nlr_finder = tuner.lr_find(model, datamodule=data_module)\nmodel.learning_rate = lr_finder.suggestion()\n\n# Train with optimized parameters\ntrainer.fit(model, datamodule=data_module)"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#model-checkpointing",
    "href": "posts/pytorch-lightning/index.html#model-checkpointing",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Saving and loading model checkpoints:\n# Save checkpoints during training\ncheckpoint_callback = ModelCheckpoint(\n    dirpath='./checkpoints',\n    filename='{epoch}-{val_loss:.2f}',\n    monitor='val_loss',\n    save_top_k=3,\n    mode='min'\n)\n\ntrainer = Trainer(\n    max_epochs=10,\n    callbacks=[checkpoint_callback]\n)\n\ntrainer.fit(model, datamodule=data_module)\n\n# Path to best checkpoint\nbest_model_path = checkpoint_callback.best_model_path\n\n# Load checkpoint into model\nmodel = MNISTModel.load_from_checkpoint(best_model_path)\n\n# Continue training from checkpoint\ntrainer.fit(model, datamodule=data_module)\nCheckpointing for production:\n# Save model in production-ready format\ntrainer.save_checkpoint(\"model.ckpt\")\n\n# Extract state dict for production\ncheckpoint = torch.load(\"model.ckpt\")\nmodel_state_dict = checkpoint[\"state_dict\"]\n\n# Save just the PyTorch model for production\nmodel = MNISTModel()\nmodel.load_state_dict(model_state_dict)\ntorch.save(model.state_dict(), \"production_model.pt\")"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#production-deployment",
    "href": "posts/pytorch-lightning/index.html#production-deployment",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "Converting Lightning models to production:\n# Option 1: Use the Lightning model directly\nmodel = MNISTModel.load_from_checkpoint(\"model.ckpt\")\nmodel.eval()\nmodel.freeze()  # Freeze the model parameters\n\n# Make predictions\nwith torch.no_grad():\n    x = torch.randn(1, 1, 28, 28)\n    y_hat = model(x)\n\n# Option 2: Extract the PyTorch model\nclass ProductionModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(28 * 28, 128)\n        self.layer_2 = nn.Linear(128, 10)\n    \n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.layer_1(x))\n        x = self.layer_2(x)\n        return x\n\n# Load weights from Lightning model\nlightning_model = MNISTModel.load_from_checkpoint(\"model.ckpt\")\nproduction_model = ProductionModel()\n\n# Copy weights\nproduction_model.layer_1.weight.data = lightning_model.layer_1.weight.data\nproduction_model.layer_1.bias.data = lightning_model.layer_1.bias.data\nproduction_model.layer_2.weight.data = lightning_model.layer_2.weight.data\nproduction_model.layer_2.bias.data = lightning_model.layer_2.bias.data\n\n# Save production model\ntorch.save(production_model.state_dict(), \"production_model.pt\")\n\n# Export to ONNX\ndummy_input = torch.randn(1, 1, 28, 28)\ntorch.onnx.export(\n    production_model,\n    dummy_input,\n    \"model.onnx\",\n    export_params=True,\n    opset_version=11,\n    input_names=['input'],\n    output_names=['output']\n)"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#best-practices",
    "href": "posts/pytorch-lightning/index.html#best-practices",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "A well-organized Lightning project structure:\nproject/\nâ”œâ”€â”€ configs/              # Configuration files\nâ”œâ”€â”€ data/                 # Data files\nâ”œâ”€â”€ lightning_logs/       # Generated logs\nâ”œâ”€â”€ models/               # Model definitions\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â””â”€â”€ mnist_model.py    # LightningModule\nâ”œâ”€â”€ data_modules/         # Data modules\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â””â”€â”€ mnist_data.py     # LightningDataModule\nâ”œâ”€â”€ callbacks/            # Custom callbacks\nâ”œâ”€â”€ utils/                # Utility functions\nâ”œâ”€â”€ main.py               # Training script\nâ””â”€â”€ README.md\n\n\n\nLightning provides a CLI for running experiments from config files:\n# main.py\nfrom pytorch_lightning.cli import LightningCLI\n\ndef cli_main():\n    # Create CLI with LightningModule and LightningDataModule\n    cli = LightningCLI(\n        MNISTModel,\n        MNISTDataModule,\n        save_config_callback=None,\n    )\n\nif __name__ == \"__main__\":\n    cli_main()\nRun with:\npython main.py fit --config configs/mnist.yaml\nExample config file (mnist.yaml):\nmodel:\n  learning_rate: 0.001\n  hidden_size: 128\ndata:\n  data_dir: ./data\n  batch_size: 64\ntrainer:\n  max_epochs: 10\n  accelerator: gpu\n  devices: 1\n\n\n\nLightning includes tools to profile your code:\nfrom pytorch_lightning.profilers import PyTorchProfiler, SimpleProfiler\n\n# PyTorch Profiler\nprofiler = PyTorchProfiler(\n    on_trace_ready=torch.profiler.tensorboard_trace_handler(\"logs/profiler\"),\n    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2)\n)\n\n# Simple Profiler\n# profiler = SimpleProfiler()\nmodel = MNISTModel()\ndata_module = MNISTDataModule()\ntrainer = Trainer(\n    max_epochs=5,\n    profiler=profiler\n)\n\ntrainer.fit(model, datamodule=data_module)\n\n\n\n\n\ntrainer = Trainer(\n    accumulate_grad_batches=4  # Accumulate over 4 batches\n)\n\n\n\ndef configure_optimizers(self):\n    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, \n        mode='min', \n        factor=0.1, \n        patience=5\n    )\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\n            \"scheduler\": scheduler,\n            \"monitor\": \"val_loss\",\n            \"interval\": \"epoch\",\n            \"frequency\": 1\n        }\n    }\n\n\n\ntrainer = Trainer(\n    precision=\"16-mixed\"  # Use 16-bit mixed precision\n)\n\n\n\nclass TransferLearningModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        # Load pretrained model\n        self.feature_extractor = torchvision.models.resnet18(pretrained=True)\n        \n        # Freeze feature extractor\n        for param in self.feature_extractor.parameters():\n            param.requires_grad = False\n            \n        # Replace final layer\n        num_features = self.feature_extractor.fc.in_features\n        self.feature_extractor.fc = nn.Linear(num_features, 10)\n        \n    def unfreeze_features(self):\n        # Unfreeze model after some training\n        for param in self.feature_extractor.parameters():\n            param.requires_grad = True"
  },
  {
    "objectID": "posts/pytorch-lightning/index.html#conclusion",
    "href": "posts/pytorch-lightning/index.html#conclusion",
    "title": "PyTorch Lightning: A Comprehensive Guide",
    "section": "",
    "text": "PyTorch Lightning provides an organized framework that separates research code from engineering boilerplate, making deep learning projects easier to develop, share, and scale. Itâ€™s especially valuable for research projects that need to be reproducible and scalable across different hardware configurations.\nFor further information:\n\nPyTorch Lightning Documentation\nLightning GitHub Repository\nLightning Tutorials"
  },
  {
    "objectID": "posts/dino-v2-explained/index.html",
    "href": "posts/dino-v2-explained/index.html",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "In 2023, Meta AI Research unveiled DINOv2 (Self-Distillation with No Labels v2), a breakthrough in self-supervised visual learning that produces remarkably versatile and robust visual features. This article provides a detailed exploration of DINOv2â€™s architecture and training methodology, explaining how it achieves state-of-the-art performance across diverse visual tasks without task-specific supervision.\n\n\n\nAt the heart of DINOv2 is the Vision Transformer (ViT) architecture, which has proven highly effective for computer vision tasks. DINOv2 specifically uses:\n\n\n\nViT-S/14: Small model (22M parameters)\nViT-B/14: Base model (87M parameters)\n\nViT-L/14: Large model (304M parameters)\nViT-g/14: Giant model (1.1B parameters)\n\nThe â€œ/14â€ indicates a patch size of 14Ã—14 pixels. These patches are how images are tokenized before being processed by the transformer.\n\n\n\nDINOv2 incorporates several architectural improvements over the original DINO:\n\nImproved Layer Normalization: Uses a modified version of layer normalization that enhances stability during training at scale.\nSwiGLU Activation: Replaces standard ReLU or GELU activations with SwiGLU, which improves representation quality.\nRegister Tokens: Additional learnable tokens (alongside the [CLS] token) that capture different aspects of image information.\nAttention Bias: Incorporates relative position embeddings through attention biases instead of absolute positional encodings.\nPost-Normalization: Places the layer normalization after the multi-head attention and feed-forward blocks rather than before them.\n\n\n\n\n\nDINOv2â€™s training methodology centers around self-distillation, where a model essentially teaches itself. This is implemented through a student-teacher framework:\n\n\n\nStudent Network: The network being trained, updated via backpropagation\nTeacher Network: An exponential moving average (EMA) of the studentâ€™s parameters\nBoth networks share the same architecture but different parameters\n\nThis approach creates a moving target that continuously evolves as training progresses, preventing trivial solutions where the network collapses to outputting the same representation for all inputs.\n\n\n\nA key component of DINOv2â€™s training is its sophisticated multi-crop approach:\n\nGlobal Views: Two large crops covering significant portions of the image (224Ã—224 pixels)\nLocal Views: Multiple smaller crops capturing image details (96Ã—96 pixels)\n\nThe student network processes both global and local views, while the teacher network only processes global views. This forces the model to learn both global context and local details.\n\n\n\nThe training objective is a cross-entropy loss that encourages the studentâ€™s output distribution for local views to match the teacherâ€™s output distribution for global views of the same image. Mathematically:\nL = H(Pt(g), Ps(l))\nWhere:\n\nH is the cross-entropy\nPt(g) is the teacherâ€™s prediction on global views\nPs(l) is the studentâ€™s prediction on local views\n\nThe teacherâ€™s outputs are sharpened using a temperature parameter that gradually decreases throughout training, making the targets increasingly focused on specific features.\n\n\n\n\nDINOv2â€™s impressive performance comes not just from architecture but from meticulous data preparation:\n\n\nThe researchers curated a high-quality dataset of 142 million images from publicly available sources, with careful filtering to remove:\n\nDuplicate images\nLow-quality content\nInappropriate material\nText-heavy images\nHuman faces\n\n\n\n\nDuring training, DINOv2 employs a robust augmentation strategy:\n\nRandom resized cropping: Different sized views of the same image\nRandom horizontal flips: Mirroring images horizontally\nColor jittering: Altering brightness, contrast, saturation, and hue\nGaussian blur: Adding controlled blur to some views\nSolarization: Inverting pixels above a threshold (applied selectively)\n\nThese augmentations create diverse views while preserving the semantic content, forcing the model to learn invariance to these transformations.\n\n\n\n\nTraining a model of DINOv2â€™s scale requires sophisticated distributed computing approaches:\n\n\n\nOptimizer: AdamW with a cosine learning rate schedule\nGradient Accumulation: Used to handle effectively larger batch sizes\nMixed Precision: FP16 calculations to speed up training\nSharding: Model parameters distributed across multiple GPUs\n\n\n\n\nDINOv2 uses enormous effective batch sizes (up to 65,536 images) by leveraging distributed training across hundreds of GPUs. This large batch size is crucial for learning high-quality representations.\n\n\n\n\nTo prevent representation collapse and ensure diverse, meaningful features, DINOv2 employs:\n\nCentering: Ensuring the average output across the batch remains close to zero\nSharpening: Gradually decreasing the temperature parameter of the teacherâ€™s softmax\nDALL-E VAE Integration: Using a pre-trained DALL-E VAE to improve representation quality\nWeight Decay: Applied differently to various components of the model\n\n\n\n\nAfter training, DINOv2 can be used in different ways:\n\n\n\n[CLS] Token: The class token representation serves as a global image descriptor\nRegister Tokens: Multiple specialized tokens that capture different aspects of the image\nPatch Tokens: Local features corresponding to specific regions of the image\n\n\n\n\nThe researchers also created smaller, distilled versions of DINOv2 that maintain much of the performance while requiring significantly fewer computational resources for deployment.\n\n\n\n\nDINOv2 represents a remarkable achievement in self-supervised visual learning. Its sophisticated architecture and training methodology enable it to learn general-purpose visual features that transfer exceptionally well across diverse tasks. The careful balance of architectural innovations, data curation, and training techniques creates a visual representation system that approaches the versatility and power that weâ€™ve seen in large language models.\nThe success of DINOv2 highlights how self-supervised learning can leverage vast amounts of unlabeled data to create foundation models for computer vision that may eventually eliminate the need for task-specific supervised training in many applications."
  },
  {
    "objectID": "posts/dino-v2-explained/index.html#architectural-foundation-vision-transformers",
    "href": "posts/dino-v2-explained/index.html#architectural-foundation-vision-transformers",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "At the heart of DINOv2 is the Vision Transformer (ViT) architecture, which has proven highly effective for computer vision tasks. DINOv2 specifically uses:\n\n\n\nViT-S/14: Small model (22M parameters)\nViT-B/14: Base model (87M parameters)\n\nViT-L/14: Large model (304M parameters)\nViT-g/14: Giant model (1.1B parameters)\n\nThe â€œ/14â€ indicates a patch size of 14Ã—14 pixels. These patches are how images are tokenized before being processed by the transformer.\n\n\n\nDINOv2 incorporates several architectural improvements over the original DINO:\n\nImproved Layer Normalization: Uses a modified version of layer normalization that enhances stability during training at scale.\nSwiGLU Activation: Replaces standard ReLU or GELU activations with SwiGLU, which improves representation quality.\nRegister Tokens: Additional learnable tokens (alongside the [CLS] token) that capture different aspects of image information.\nAttention Bias: Incorporates relative position embeddings through attention biases instead of absolute positional encodings.\nPost-Normalization: Places the layer normalization after the multi-head attention and feed-forward blocks rather than before them."
  },
  {
    "objectID": "posts/dino-v2-explained/index.html#training-methodology-self-distillation-framework",
    "href": "posts/dino-v2-explained/index.html#training-methodology-self-distillation-framework",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "DINOv2â€™s training methodology centers around self-distillation, where a model essentially teaches itself. This is implemented through a student-teacher framework:\n\n\n\nStudent Network: The network being trained, updated via backpropagation\nTeacher Network: An exponential moving average (EMA) of the studentâ€™s parameters\nBoth networks share the same architecture but different parameters\n\nThis approach creates a moving target that continuously evolves as training progresses, preventing trivial solutions where the network collapses to outputting the same representation for all inputs.\n\n\n\nA key component of DINOv2â€™s training is its sophisticated multi-crop approach:\n\nGlobal Views: Two large crops covering significant portions of the image (224Ã—224 pixels)\nLocal Views: Multiple smaller crops capturing image details (96Ã—96 pixels)\n\nThe student network processes both global and local views, while the teacher network only processes global views. This forces the model to learn both global context and local details.\n\n\n\nThe training objective is a cross-entropy loss that encourages the studentâ€™s output distribution for local views to match the teacherâ€™s output distribution for global views of the same image. Mathematically:\nL = H(Pt(g), Ps(l))\nWhere:\n\nH is the cross-entropy\nPt(g) is the teacherâ€™s prediction on global views\nPs(l) is the studentâ€™s prediction on local views\n\nThe teacherâ€™s outputs are sharpened using a temperature parameter that gradually decreases throughout training, making the targets increasingly focused on specific features."
  },
  {
    "objectID": "posts/dino-v2-explained/index.html#data-curation-and-processing",
    "href": "posts/dino-v2-explained/index.html#data-curation-and-processing",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "DINOv2â€™s impressive performance comes not just from architecture but from meticulous data preparation:\n\n\nThe researchers curated a high-quality dataset of 142 million images from publicly available sources, with careful filtering to remove:\n\nDuplicate images\nLow-quality content\nInappropriate material\nText-heavy images\nHuman faces\n\n\n\n\nDuring training, DINOv2 employs a robust augmentation strategy:\n\nRandom resized cropping: Different sized views of the same image\nRandom horizontal flips: Mirroring images horizontally\nColor jittering: Altering brightness, contrast, saturation, and hue\nGaussian blur: Adding controlled blur to some views\nSolarization: Inverting pixels above a threshold (applied selectively)\n\nThese augmentations create diverse views while preserving the semantic content, forcing the model to learn invariance to these transformations."
  },
  {
    "objectID": "posts/dino-v2-explained/index.html#distributed-training-strategy",
    "href": "posts/dino-v2-explained/index.html#distributed-training-strategy",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "Training a model of DINOv2â€™s scale requires sophisticated distributed computing approaches:\n\n\n\nOptimizer: AdamW with a cosine learning rate schedule\nGradient Accumulation: Used to handle effectively larger batch sizes\nMixed Precision: FP16 calculations to speed up training\nSharding: Model parameters distributed across multiple GPUs\n\n\n\n\nDINOv2 uses enormous effective batch sizes (up to 65,536 images) by leveraging distributed training across hundreds of GPUs. This large batch size is crucial for learning high-quality representations."
  },
  {
    "objectID": "posts/dino-v2-explained/index.html#regularization-techniques",
    "href": "posts/dino-v2-explained/index.html#regularization-techniques",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "To prevent representation collapse and ensure diverse, meaningful features, DINOv2 employs:\n\nCentering: Ensuring the average output across the batch remains close to zero\nSharpening: Gradually decreasing the temperature parameter of the teacherâ€™s softmax\nDALL-E VAE Integration: Using a pre-trained DALL-E VAE to improve representation quality\nWeight Decay: Applied differently to various components of the model"
  },
  {
    "objectID": "posts/dino-v2-explained/index.html#feature-extraction-and-deployment",
    "href": "posts/dino-v2-explained/index.html#feature-extraction-and-deployment",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "After training, DINOv2 can be used in different ways:\n\n\n\n[CLS] Token: The class token representation serves as a global image descriptor\nRegister Tokens: Multiple specialized tokens that capture different aspects of the image\nPatch Tokens: Local features corresponding to specific regions of the image\n\n\n\n\nThe researchers also created smaller, distilled versions of DINOv2 that maintain much of the performance while requiring significantly fewer computational resources for deployment."
  },
  {
    "objectID": "posts/dino-v2-explained/index.html#conclusion",
    "href": "posts/dino-v2-explained/index.html#conclusion",
    "title": "DINOv2: A Deep Dive into Architecture and Training",
    "section": "",
    "text": "DINOv2 represents a remarkable achievement in self-supervised visual learning. Its sophisticated architecture and training methodology enable it to learn general-purpose visual features that transfer exceptionally well across diverse tasks. The careful balance of architectural innovations, data curation, and training techniques creates a visual representation system that approaches the versatility and power that weâ€™ve seen in large language models.\nThe success of DINOv2 highlights how self-supervised learning can leverage vast amounts of unlabeled data to create foundation models for computer vision that may eventually eliminate the need for task-specific supervised training in many applications."
  },
  {
    "objectID": "posts/hugging-face-accelerate/index.html",
    "href": "posts/hugging-face-accelerate/index.html",
    "title": "Hugging Face Accelerate Code Guide",
    "section": "",
    "text": "Iâ€™ve created a comprehensive code guide for Hugging Face Accelerate that covers everything from basic setup to advanced features like DeepSpeed integration.\n\n\n\nInstallation and Setup\nBasic Concepts\nSimple Training Loop\nAdvanced Features\nMulti-GPU Training\nMixed Precision Training\nGradient Accumulation\nDeepSpeed Integration\nTroubleshooting\n\n\n\n\n\n\npip install accelerate\n\n\n\nRun the configuration wizard to set up your training environment:\naccelerate config\nOr create a config file programmatically:\nfrom accelerate import Accelerator\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config(mixed_precision=\"fp16\")  # or \"bf16\", \"no\"\n\n\n\n\n\n\nThe Accelerator is the main class that handles device placement, gradient synchronization, and other distributed training concerns.\nfrom accelerate import Accelerator\n\n# Initialize accelerator\naccelerator = Accelerator()\n\n# Key properties\ndevice = accelerator.device\nis_main_process = accelerator.is_main_process\nnum_processes = accelerator.num_processes\n\n\n\nAccelerate automatically handles device placement:\n# Manual device placement (old way)\nmodel = model.to(device)\nbatch = {k: v.to(device) for k, v in batch.items()}\n\n# Accelerate way (automatic)\nmodel, optimizer, dataloader = accelerator.prepare(model, optimizer, dataloader)\n# No need to move batch to device - accelerate handles it\n\n\n\n\n\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom accelerate import Accelerator\nfrom transformers import AutoModel, AutoTokenizer, AdamW\n\ndef train_model():\n    # Initialize accelerator\n    accelerator = Accelerator()\n    \n    # Load model and tokenizer\n    model = AutoModel.from_pretrained(\"bert-base-uncased\")\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n    \n    # Create optimizer\n    optimizer = AdamW(model.parameters(), lr=5e-5)\n    \n    # Create dataloader (your dataset here)\n    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    \n    # Prepare everything with accelerator\n    model, optimizer, train_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader\n    )\n    \n    # Training loop\n    model.train()\n    for epoch in range(3):\n        for batch in train_dataloader:\n            # Forward pass\n            outputs = model(**batch)\n            loss = outputs.loss\n            \n            # Backward pass\n            accelerator.backward(loss)\n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # Print loss (only on main process)\n            if accelerator.is_main_process:\n                print(f\"Loss: {loss.item():.4f}\")\n\nif __name__ == \"__main__\":\n    train_model()\n\n\n\n# Single GPU\npython train.py\n\n# Multiple GPUs\naccelerate launch --num_processes=2 train.py\n\n# With config file\naccelerate launch --config_file config.yaml train.py\n\n\n\n\n\n\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\n\n# Initialize with logging\naccelerator = Accelerator(log_with=\"tensorboard\", project_dir=\"./logs\")\n\n# Get logger\nlogger = get_logger(__name__)\n\n# Start tracking\naccelerator.init_trackers(\"my_experiment\")\n\n# Log metrics\naccelerator.log({\"train_loss\": loss.item(), \"epoch\": epoch})\n\n# End tracking\naccelerator.end_training()\n\n\n\n# Save model\naccelerator.save_model(model, \"path/to/save\")\n\n# Or save state dict\naccelerator.save(model.state_dict(), \"model.pt\")\n\n# Load model\naccelerator.load_state(\"model.pt\")\n\n# Save complete training state\naccelerator.save_state(\"checkpoint_dir\")\n\n# Load complete training state\naccelerator.load_state(\"checkpoint_dir\")\n\n\n\ndef evaluate_model(model, eval_dataloader, accelerator):\n    model.eval()\n    total_loss = 0\n    total_samples = 0\n    \n    with torch.no_grad():\n        for batch in eval_dataloader:\n            outputs = model(**batch)\n            loss = outputs.loss\n            \n            # Gather losses from all processes\n            gathered_loss = accelerator.gather(loss)\n            total_loss += gathered_loss.sum().item()\n            total_samples += gathered_loss.shape[0]\n    \n    avg_loss = total_loss / total_samples\n    return avg_loss\n\n\n\n\n\n\nfrom accelerate import Accelerator\n\ndef train_multi_gpu():\n    accelerator = Accelerator()\n    \n    # Model will be replicated across GPUs\n    model = MyModel()\n    optimizer = torch.optim.Adam(model.parameters())\n    \n    # Prepare for multi-GPU\n    model, optimizer, train_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader\n    )\n    \n    # Training loop remains the same\n    for batch in train_dataloader:\n        outputs = model(**batch)\n        loss = outputs.loss\n        \n        # Accelerate handles gradient synchronization\n        accelerator.backward(loss)\n        optimizer.step()\n        optimizer.zero_grad()\n\n\n\n# Launch on 4 GPUs\naccelerate launch --num_processes=4 --multi_gpu train.py\n\n# Launch with specific GPUs\nCUDA_VISIBLE_DEVICES=0,1,3 accelerate launch --num_processes=3 train.py\n\n# Launch on multiple nodes\naccelerate launch --num_processes=8 --num_machines=2 --main_process_ip=192.168.1.1 train.py\n\n\n\n\n\n\n# Enable mixed precision in config or during initialization\naccelerator = Accelerator(mixed_precision=\"fp16\")  # or \"bf16\"\n\n# Training loop remains exactly the same\nfor batch in train_dataloader:\n    outputs = model(**batch)\n    loss = outputs.loss\n    \n    # Accelerate handles scaling automatically\n    accelerator.backward(loss)\n    optimizer.step()\n    optimizer.zero_grad()\n\n\n\n# Access the scaler if needed\nif accelerator.mixed_precision == \"fp16\":\n    scaler = accelerator.scaler\n    \n    # Manual scaling (usually not needed)\n    scaled_loss = scaler.scale(loss)\n    scaled_loss.backward()\n    scaler.step(optimizer)\n    scaler.update()\n\n\n\n\n\n\naccelerator = Accelerator(gradient_accumulation_steps=4)\n\nfor batch in train_dataloader:\n    # Use accumulate context manager\n    with accelerator.accumulate(model):\n        outputs = model(**batch)\n        loss = outputs.loss\n        \n        accelerator.backward(loss)\n        optimizer.step()\n        optimizer.zero_grad()\n\n\n\ndef train_with_dynamic_accumulation():\n    accumulation_steps = 2\n    \n    for i, batch in enumerate(train_dataloader):\n        outputs = model(**batch)\n        loss = outputs.loss / accumulation_steps  # Scale loss\n        \n        accelerator.backward(loss)\n        \n        if (i + 1) % accumulation_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n\n\n\n\n\n\nCreate a DeepSpeed config file (ds_config.json):\n{\n    \"train_batch_size\": 32,\n    \"gradient_accumulation_steps\": 1,\n    \"optimizer\": {\n        \"type\": \"Adam\",\n        \"params\": {\n            \"lr\": 5e-5\n        }\n    },\n    \"fp16\": {\n        \"enabled\": true\n    },\n    \"zero_optimization\": {\n        \"stage\": 2\n    }\n}\n\n\n\nfrom accelerate import Accelerator\n\n# Initialize with DeepSpeed\naccelerator = Accelerator(deepspeed_plugin=\"ds_config.json\")\n\n# Or programmatically\nfrom accelerate import DeepSpeedPlugin\n\nds_plugin = DeepSpeedPlugin(\n    gradient_accumulation_steps=4,\n    zero_stage=2,\n    offload_optimizer_device=\"cpu\"\n)\naccelerator = Accelerator(deepspeed_plugin=ds_plugin)\n\n# Training code remains the same\nmodel, optimizer = accelerator.prepare(model, optimizer)\n\n\n\naccelerate launch --config_file ds_config.yaml train.py\n\n\n\n\n\n\n\n\n# Clear cache regularly\nif accelerator.is_main_process:\n    torch.cuda.empty_cache()\n\n# Use gradient checkpointing\nmodel.gradient_checkpointing_enable()\n\n# Reduce batch size or increase gradient accumulation\n\n\n\n# Wait for all processes\naccelerator.wait_for_everyone()\n\n# Gather data from all processes\nall_losses = accelerator.gather(loss)\n\n# Reduce across processes\navg_loss = accelerator.reduce(loss, reduction=\"mean\")\n\n\n\n# Enable debug mode\naccelerator = Accelerator(debug=True)\n\n# Check if running in distributed mode\nif accelerator.distributed_type != \"NO\":\n    print(f\"Running on {accelerator.num_processes} processes\")\n\n# Print only on main process\naccelerator.print(\"This will only print once\")\n\n\n\n\n\nUse appropriate batch sizes: Larger batch sizes generally improve GPU utilization\nEnable mixed precision: Use fp16 or bf16 for faster training\nGradient accumulation: Simulate larger batch sizes without memory issues\nDataLoader optimization: Use num_workers and pin_memory=True\nCompile models: Use torch.compile() for PyTorch 2.0+\n\n# Optimized setup\naccelerator = Accelerator(\n    mixed_precision=\"bf16\",\n    gradient_accumulation_steps=4\n)\n\n# Compile model (PyTorch 2.0+)\nmodel = torch.compile(model)\n\n# Optimized DataLoader\ntrain_dataloader = DataLoader(\n    dataset,\n    batch_size=32,\n    num_workers=4,\n    pin_memory=True,\n    shuffle=True\n)\n\n\n\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\nfrom accelerate import Accelerator\nfrom datasets import load_dataset\n\ndef main():\n    # Initialize\n    accelerator = Accelerator(mixed_precision=\"fp16\")\n    \n    # Load data\n    dataset = load_dataset(\"imdb\")\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n    \n    def tokenize_function(examples):\n        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\")\n    \n    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n    train_dataset = tokenized_datasets[\"train\"].with_format(\"torch\")\n    \n    # Model and optimizer\n    model = AutoModelForSequenceClassification.from_pretrained(\n        \"bert-base-uncased\", num_labels=2\n    )\n    optimizer = AdamW(model.parameters(), lr=5e-5)\n    \n    # DataLoader\n    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    \n    # Prepare everything\n    model, optimizer, train_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader\n    )\n    \n    # Training loop\n    num_epochs = 3\n    model.train()\n    \n    for epoch in range(num_epochs):\n        total_loss = 0\n        for step, batch in enumerate(train_dataloader):\n            outputs = model(**batch)\n            loss = outputs.loss\n            \n            accelerator.backward(loss)\n            optimizer.step()\n            optimizer.zero_grad()\n            \n            total_loss += loss.item()\n            \n            if step % 100 == 0 and accelerator.is_main_process:\n                print(f\"Epoch {epoch}, Step {step}, Loss: {loss.item():.4f}\")\n        \n        if accelerator.is_main_process:\n            avg_loss = total_loss / len(train_dataloader)\n            print(f\"Epoch {epoch} completed. Average loss: {avg_loss:.4f}\")\n    \n    # Save model\n    accelerator.wait_for_everyone()\n    unwrapped_model = accelerator.unwrap_model(model)\n    unwrapped_model.save_pretrained(\"./fine_tuned_bert\")\n    \n    if accelerator.is_main_process:\n        tokenizer.save_pretrained(\"./fine_tuned_bert\")\n\nif __name__ == \"__main__\":\n    main()\nThis guide covers the essential aspects of using Hugging Face Accelerate for distributed training. The library abstracts away much of the complexity while providing fine-grained control when needed."
  },
  {
    "objectID": "posts/hugging-face-accelerate/index.html#table-of-contents",
    "href": "posts/hugging-face-accelerate/index.html#table-of-contents",
    "title": "Hugging Face Accelerate Code Guide",
    "section": "",
    "text": "Installation and Setup\nBasic Concepts\nSimple Training Loop\nAdvanced Features\nMulti-GPU Training\nMixed Precision Training\nGradient Accumulation\nDeepSpeed Integration\nTroubleshooting"
  },
  {
    "objectID": "posts/hugging-face-accelerate/index.html#installation-and-setup",
    "href": "posts/hugging-face-accelerate/index.html#installation-and-setup",
    "title": "Hugging Face Accelerate Code Guide",
    "section": "",
    "text": "pip install accelerate\n\n\n\nRun the configuration wizard to set up your training environment:\naccelerate config\nOr create a config file programmatically:\nfrom accelerate import Accelerator\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config(mixed_precision=\"fp16\")  # or \"bf16\", \"no\""
  },
  {
    "objectID": "posts/hugging-face-accelerate/index.html#basic-concepts",
    "href": "posts/hugging-face-accelerate/index.html#basic-concepts",
    "title": "Hugging Face Accelerate Code Guide",
    "section": "",
    "text": "The Accelerator is the main class that handles device placement, gradient synchronization, and other distributed training concerns.\nfrom accelerate import Accelerator\n\n# Initialize accelerator\naccelerator = Accelerator()\n\n# Key properties\ndevice = accelerator.device\nis_main_process = accelerator.is_main_process\nnum_processes = accelerator.num_processes\n\n\n\nAccelerate automatically handles device placement:\n# Manual device placement (old way)\nmodel = model.to(device)\nbatch = {k: v.to(device) for k, v in batch.items()}\n\n# Accelerate way (automatic)\nmodel, optimizer, dataloader = accelerator.prepare(model, optimizer, dataloader)\n# No need to move batch to device - accelerate handles it"
  },
  {
    "objectID": "posts/hugging-face-accelerate/index.html#simple-training-loop",
    "href": "posts/hugging-face-accelerate/index.html#simple-training-loop",
    "title": "Hugging Face Accelerate Code Guide",
    "section": "",
    "text": "import torch\nfrom torch.utils.data import DataLoader\nfrom accelerate import Accelerator\nfrom transformers import AutoModel, AutoTokenizer, AdamW\n\ndef train_model():\n    # Initialize accelerator\n    accelerator = Accelerator()\n    \n    # Load model and tokenizer\n    model = AutoModel.from_pretrained(\"bert-base-uncased\")\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n    \n    # Create optimizer\n    optimizer = AdamW(model.parameters(), lr=5e-5)\n    \n    # Create dataloader (your dataset here)\n    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    \n    # Prepare everything with accelerator\n    model, optimizer, train_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader\n    )\n    \n    # Training loop\n    model.train()\n    for epoch in range(3):\n        for batch in train_dataloader:\n            # Forward pass\n            outputs = model(**batch)\n            loss = outputs.loss\n            \n            # Backward pass\n            accelerator.backward(loss)\n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # Print loss (only on main process)\n            if accelerator.is_main_process:\n                print(f\"Loss: {loss.item():.4f}\")\n\nif __name__ == \"__main__\":\n    train_model()\n\n\n\n# Single GPU\npython train.py\n\n# Multiple GPUs\naccelerate launch --num_processes=2 train.py\n\n# With config file\naccelerate launch --config_file config.yaml train.py"
  },
  {
    "objectID": "posts/hugging-face-accelerate/index.html#advanced-features",
    "href": "posts/hugging-face-accelerate/index.html#advanced-features",
    "title": "Hugging Face Accelerate Code Guide",
    "section": "",
    "text": "from accelerate import Accelerator\nfrom accelerate.logging import get_logger\n\n# Initialize with logging\naccelerator = Accelerator(log_with=\"tensorboard\", project_dir=\"./logs\")\n\n# Get logger\nlogger = get_logger(__name__)\n\n# Start tracking\naccelerator.init_trackers(\"my_experiment\")\n\n# Log metrics\naccelerator.log({\"train_loss\": loss.item(), \"epoch\": epoch})\n\n# End tracking\naccelerator.end_training()\n\n\n\n# Save model\naccelerator.save_model(model, \"path/to/save\")\n\n# Or save state dict\naccelerator.save(model.state_dict(), \"model.pt\")\n\n# Load model\naccelerator.load_state(\"model.pt\")\n\n# Save complete training state\naccelerator.save_state(\"checkpoint_dir\")\n\n# Load complete training state\naccelerator.load_state(\"checkpoint_dir\")\n\n\n\ndef evaluate_model(model, eval_dataloader, accelerator):\n    model.eval()\n    total_loss = 0\n    total_samples = 0\n    \n    with torch.no_grad():\n        for batch in eval_dataloader:\n            outputs = model(**batch)\n            loss = outputs.loss\n            \n            # Gather losses from all processes\n            gathered_loss = accelerator.gather(loss)\n            total_loss += gathered_loss.sum().item()\n            total_samples += gathered_loss.shape[0]\n    \n    avg_loss = total_loss / total_samples\n    return avg_loss"
  },
  {
    "objectID": "posts/hugging-face-accelerate/index.html#multi-gpu-training",
    "href": "posts/hugging-face-accelerate/index.html#multi-gpu-training",
    "title": "Hugging Face Accelerate Code Guide",
    "section": "",
    "text": "from accelerate import Accelerator\n\ndef train_multi_gpu():\n    accelerator = Accelerator()\n    \n    # Model will be replicated across GPUs\n    model = MyModel()\n    optimizer = torch.optim.Adam(model.parameters())\n    \n    # Prepare for multi-GPU\n    model, optimizer, train_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader\n    )\n    \n    # Training loop remains the same\n    for batch in train_dataloader:\n        outputs = model(**batch)\n        loss = outputs.loss\n        \n        # Accelerate handles gradient synchronization\n        accelerator.backward(loss)\n        optimizer.step()\n        optimizer.zero_grad()\n\n\n\n# Launch on 4 GPUs\naccelerate launch --num_processes=4 --multi_gpu train.py\n\n# Launch with specific GPUs\nCUDA_VISIBLE_DEVICES=0,1,3 accelerate launch --num_processes=3 train.py\n\n# Launch on multiple nodes\naccelerate launch --num_processes=8 --num_machines=2 --main_process_ip=192.168.1.1 train.py"
  },
  {
    "objectID": "posts/hugging-face-accelerate/index.html#mixed-precision-training",
    "href": "posts/hugging-face-accelerate/index.html#mixed-precision-training",
    "title": "Hugging Face Accelerate Code Guide",
    "section": "",
    "text": "# Enable mixed precision in config or during initialization\naccelerator = Accelerator(mixed_precision=\"fp16\")  # or \"bf16\"\n\n# Training loop remains exactly the same\nfor batch in train_dataloader:\n    outputs = model(**batch)\n    loss = outputs.loss\n    \n    # Accelerate handles scaling automatically\n    accelerator.backward(loss)\n    optimizer.step()\n    optimizer.zero_grad()\n\n\n\n# Access the scaler if needed\nif accelerator.mixed_precision == \"fp16\":\n    scaler = accelerator.scaler\n    \n    # Manual scaling (usually not needed)\n    scaled_loss = scaler.scale(loss)\n    scaled_loss.backward()\n    scaler.step(optimizer)\n    scaler.update()"
  },
  {
    "objectID": "posts/hugging-face-accelerate/index.html#gradient-accumulation",
    "href": "posts/hugging-face-accelerate/index.html#gradient-accumulation",
    "title": "Hugging Face Accelerate Code Guide",
    "section": "",
    "text": "accelerator = Accelerator(gradient_accumulation_steps=4)\n\nfor batch in train_dataloader:\n    # Use accumulate context manager\n    with accelerator.accumulate(model):\n        outputs = model(**batch)\n        loss = outputs.loss\n        \n        accelerator.backward(loss)\n        optimizer.step()\n        optimizer.zero_grad()\n\n\n\ndef train_with_dynamic_accumulation():\n    accumulation_steps = 2\n    \n    for i, batch in enumerate(train_dataloader):\n        outputs = model(**batch)\n        loss = outputs.loss / accumulation_steps  # Scale loss\n        \n        accelerator.backward(loss)\n        \n        if (i + 1) % accumulation_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()"
  },
  {
    "objectID": "posts/hugging-face-accelerate/index.html#deepspeed-integration",
    "href": "posts/hugging-face-accelerate/index.html#deepspeed-integration",
    "title": "Hugging Face Accelerate Code Guide",
    "section": "",
    "text": "Create a DeepSpeed config file (ds_config.json):\n{\n    \"train_batch_size\": 32,\n    \"gradient_accumulation_steps\": 1,\n    \"optimizer\": {\n        \"type\": \"Adam\",\n        \"params\": {\n            \"lr\": 5e-5\n        }\n    },\n    \"fp16\": {\n        \"enabled\": true\n    },\n    \"zero_optimization\": {\n        \"stage\": 2\n    }\n}\n\n\n\nfrom accelerate import Accelerator\n\n# Initialize with DeepSpeed\naccelerator = Accelerator(deepspeed_plugin=\"ds_config.json\")\n\n# Or programmatically\nfrom accelerate import DeepSpeedPlugin\n\nds_plugin = DeepSpeedPlugin(\n    gradient_accumulation_steps=4,\n    zero_stage=2,\n    offload_optimizer_device=\"cpu\"\n)\naccelerator = Accelerator(deepspeed_plugin=ds_plugin)\n\n# Training code remains the same\nmodel, optimizer = accelerator.prepare(model, optimizer)\n\n\n\naccelerate launch --config_file ds_config.yaml train.py"
  },
  {
    "objectID": "posts/hugging-face-accelerate/index.html#troubleshooting",
    "href": "posts/hugging-face-accelerate/index.html#troubleshooting",
    "title": "Hugging Face Accelerate Code Guide",
    "section": "",
    "text": "# Clear cache regularly\nif accelerator.is_main_process:\n    torch.cuda.empty_cache()\n\n# Use gradient checkpointing\nmodel.gradient_checkpointing_enable()\n\n# Reduce batch size or increase gradient accumulation\n\n\n\n# Wait for all processes\naccelerator.wait_for_everyone()\n\n# Gather data from all processes\nall_losses = accelerator.gather(loss)\n\n# Reduce across processes\navg_loss = accelerator.reduce(loss, reduction=\"mean\")\n\n\n\n# Enable debug mode\naccelerator = Accelerator(debug=True)\n\n# Check if running in distributed mode\nif accelerator.distributed_type != \"NO\":\n    print(f\"Running on {accelerator.num_processes} processes\")\n\n# Print only on main process\naccelerator.print(\"This will only print once\")\n\n\n\n\n\nUse appropriate batch sizes: Larger batch sizes generally improve GPU utilization\nEnable mixed precision: Use fp16 or bf16 for faster training\nGradient accumulation: Simulate larger batch sizes without memory issues\nDataLoader optimization: Use num_workers and pin_memory=True\nCompile models: Use torch.compile() for PyTorch 2.0+\n\n# Optimized setup\naccelerator = Accelerator(\n    mixed_precision=\"bf16\",\n    gradient_accumulation_steps=4\n)\n\n# Compile model (PyTorch 2.0+)\nmodel = torch.compile(model)\n\n# Optimized DataLoader\ntrain_dataloader = DataLoader(\n    dataset,\n    batch_size=32,\n    num_workers=4,\n    pin_memory=True,\n    shuffle=True\n)"
  },
  {
    "objectID": "posts/hugging-face-accelerate/index.html#complete-example-bert-fine-tuning",
    "href": "posts/hugging-face-accelerate/index.html#complete-example-bert-fine-tuning",
    "title": "Hugging Face Accelerate Code Guide",
    "section": "",
    "text": "import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\nfrom accelerate import Accelerator\nfrom datasets import load_dataset\n\ndef main():\n    # Initialize\n    accelerator = Accelerator(mixed_precision=\"fp16\")\n    \n    # Load data\n    dataset = load_dataset(\"imdb\")\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n    \n    def tokenize_function(examples):\n        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\")\n    \n    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n    train_dataset = tokenized_datasets[\"train\"].with_format(\"torch\")\n    \n    # Model and optimizer\n    model = AutoModelForSequenceClassification.from_pretrained(\n        \"bert-base-uncased\", num_labels=2\n    )\n    optimizer = AdamW(model.parameters(), lr=5e-5)\n    \n    # DataLoader\n    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    \n    # Prepare everything\n    model, optimizer, train_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader\n    )\n    \n    # Training loop\n    num_epochs = 3\n    model.train()\n    \n    for epoch in range(num_epochs):\n        total_loss = 0\n        for step, batch in enumerate(train_dataloader):\n            outputs = model(**batch)\n            loss = outputs.loss\n            \n            accelerator.backward(loss)\n            optimizer.step()\n            optimizer.zero_grad()\n            \n            total_loss += loss.item()\n            \n            if step % 100 == 0 and accelerator.is_main_process:\n                print(f\"Epoch {epoch}, Step {step}, Loss: {loss.item():.4f}\")\n        \n        if accelerator.is_main_process:\n            avg_loss = total_loss / len(train_dataloader)\n            print(f\"Epoch {epoch} completed. Average loss: {avg_loss:.4f}\")\n    \n    # Save model\n    accelerator.wait_for_everyone()\n    unwrapped_model = accelerator.unwrap_model(model)\n    unwrapped_model.save_pretrained(\"./fine_tuned_bert\")\n    \n    if accelerator.is_main_process:\n        tokenizer.save_pretrained(\"./fine_tuned_bert\")\n\nif __name__ == \"__main__\":\n    main()\nThis guide covers the essential aspects of using Hugging Face Accelerate for distributed training. The library abstracts away much of the complexity while providing fine-grained control when needed."
  },
  {
    "objectID": "posts/student-teacher-vanilla/index.html",
    "href": "posts/student-teacher-vanilla/index.html",
    "title": "Student-Teacher Network Training Guide in PyTorch",
    "section": "",
    "text": "Student-teacher networks, also known as knowledge distillation, involve training a smaller â€œstudentâ€ model to mimic the behavior of a larger, pre-trained â€œteacherâ€ model. This technique helps compress large models while maintaining performance.\n\n\n\n\n\nThe student learns from both:\n\nHard targets: Original ground truth labels\nSoft targets: Teacherâ€™s probability distributions (softened with temperature)\n\n\n\n\nHigher temperature values create softer probability distributions, making it easier for the student to learn from the teacherâ€™s uncertainty.\n\n\n\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nimport numpy as np\n\n\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n\n\nclass TeacherNetwork(nn.Module):\n    \"\"\"Large teacher network (e.g., ResNet-50 equivalent)\"\"\"\n    def __init__(self, num_classes=10):\n        super(TeacherNetwork, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            \n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, 3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            \n            nn.Conv2d(128, 256, 3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, 3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(256, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n\n\nclass StudentNetwork(nn.Module):\n    \"\"\"Smaller student network\"\"\"\n    def __init__(self, num_classes=10):\n        super(StudentNetwork, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            \n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            \n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(128, 64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64, num_classes)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n\n\nclass DistillationLoss(nn.Module):\n    \"\"\"\n    Knowledge Distillation Loss combining:\n    1. Cross-entropy loss with true labels\n    2. KL divergence loss with teacher predictions\n    \"\"\"\n    def __init__(self, alpha=0.7, temperature=4.0):\n        super(DistillationLoss, self).__init__()\n        self.alpha = alpha  # Weight for distillation loss\n        self.temperature = temperature\n        self.ce_loss = nn.CrossEntropyLoss()\n        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n    \n    def forward(self, student_logits, teacher_logits, labels):\n        # Cross-entropy loss with true labels\n        ce_loss = self.ce_loss(student_logits, labels)\n        \n        # Soft targets from teacher\n        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)\n        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=1)\n        \n        # KL divergence loss\n        kl_loss = self.kl_loss(student_log_probs, teacher_probs) * (self.temperature ** 2)\n        \n        # Combined loss\n        total_loss = (1 - self.alpha) * ce_loss + self.alpha * kl_loss\n        \n        return total_loss, ce_loss, kl_loss\n\n\n\ndef load_data(batch_size=128):\n    \"\"\"Load CIFAR-10 dataset\"\"\"\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n    ])\n    \n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n    ])\n    \n    train_dataset = torchvision.datasets.CIFAR10(\n        root='./data', train=True, download=True, transform=transform_train\n    )\n    test_dataset = torchvision.datasets.CIFAR10(\n        root='./data', train=False, download=True, transform=transform_test\n    )\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    \n    return train_loader, test_loader\n\n\n\ndef train_teacher(model, train_loader, test_loader, epochs=50, lr=0.001):\n    \"\"\"Train the teacher network\"\"\"\n    print(\"Training Teacher Network...\")\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n    \n    model.train()\n    best_acc = 0.0\n    \n    for epoch in range(epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        progress_bar = tqdm(train_loader, desc=f'Teacher Epoch {epoch+1}/{epochs}')\n        for batch_idx, (inputs, targets) in enumerate(progress_bar):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n            \n            progress_bar.set_postfix({\n                'Loss': f'{running_loss/(batch_idx+1):.4f}',\n                'Acc': f'{100.*correct/total:.2f}%'\n            })\n        \n        # Evaluate\n        test_acc = evaluate_model(model, test_loader)\n        print(f'Teacher Epoch {epoch+1}: Train Acc: {100.*correct/total:.2f}%, Test Acc: {test_acc:.2f}%')\n        \n        if test_acc &gt; best_acc:\n            best_acc = test_acc\n            torch.save(model.state_dict(), 'teacher_best.pth')\n        \n        scheduler.step()\n    \n    print(f'Teacher training completed. Best accuracy: {best_acc:.2f}%')\n    return model\n\n\n\ndef train_student(student, teacher, train_loader, test_loader, epochs=100, lr=0.001):\n    \"\"\"Train the student network using knowledge distillation\"\"\"\n    print(\"Training Student Network with Knowledge Distillation...\")\n    \n    distillation_loss = DistillationLoss(alpha=0.7, temperature=4.0)\n    optimizer = optim.Adam(student.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n    \n    teacher.eval()  # Teacher in evaluation mode\n    student.train()\n    best_acc = 0.0\n    \n    for epoch in range(epochs):\n        running_loss = 0.0\n        running_ce_loss = 0.0\n        running_kl_loss = 0.0\n        correct = 0\n        total = 0\n        \n        progress_bar = tqdm(train_loader, desc=f'Student Epoch {epoch+1}/{epochs}')\n        for batch_idx, (inputs, targets) in enumerate(progress_bar):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Get predictions from both networks\n            with torch.no_grad():\n                teacher_logits = teacher(inputs)\n            \n            student_logits = student(inputs)\n            \n            # Calculate distillation loss\n            total_loss, ce_loss, kl_loss = distillation_loss(\n                student_logits, teacher_logits, targets\n            )\n            \n            total_loss.backward()\n            optimizer.step()\n            \n            # Statistics\n            running_loss += total_loss.item()\n            running_ce_loss += ce_loss.item()\n            running_kl_loss += kl_loss.item()\n            \n            _, predicted = student_logits.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n            \n            progress_bar.set_postfix({\n                'Loss': f'{running_loss/(batch_idx+1):.4f}',\n                'CE': f'{running_ce_loss/(batch_idx+1):.4f}',\n                'KL': f'{running_kl_loss/(batch_idx+1):.4f}',\n                'Acc': f'{100.*correct/total:.2f}%'\n            })\n        \n        # Evaluate\n        test_acc = evaluate_model(student, test_loader)\n        print(f'Student Epoch {epoch+1}: Train Acc: {100.*correct/total:.2f}%, Test Acc: {test_acc:.2f}%')\n        \n        if test_acc &gt; best_acc:\n            best_acc = test_acc\n            torch.save(student.state_dict(), 'student_best.pth')\n        \n        scheduler.step()\n    \n    print(f'Student training completed. Best accuracy: {best_acc:.2f}%')\n    return student\n\n\n\ndef train_student_baseline(student, train_loader, test_loader, epochs=100, lr=0.001):\n    \"\"\"Train student without distillation (baseline comparison)\"\"\"\n    print(\"Training Student Network (Baseline - No Distillation)...\")\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(student.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n    \n    student.train()\n    best_acc = 0.0\n    \n    for epoch in range(epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        progress_bar = tqdm(train_loader, desc=f'Baseline Epoch {epoch+1}/{epochs}')\n        for batch_idx, (inputs, targets) in enumerate(progress_bar):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            outputs = student(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n            \n            progress_bar.set_postfix({\n                'Loss': f'{running_loss/(batch_idx+1):.4f}',\n                'Acc': f'{100.*correct/total:.2f}%'\n            })\n        \n        # Evaluate\n        test_acc = evaluate_model(student, test_loader)\n        print(f'Baseline Epoch {epoch+1}: Train Acc: {100.*correct/total:.2f}%, Test Acc: {test_acc:.2f}%')\n        \n        if test_acc &gt; best_acc:\n            best_acc = test_acc\n            torch.save(student.state_dict(), 'student_baseline_best.pth')\n        \n        scheduler.step()\n    \n    print(f'Baseline training completed. Best accuracy: {best_acc:.2f}%')\n    return student\n\n\n\ndef evaluate_model(model, test_loader):\n    \"\"\"Evaluate model accuracy\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    \n    accuracy = 100. * correct / total\n    model.train()\n    return accuracy\n\n\n\ndef count_parameters(model):\n    \"\"\"Count trainable parameters\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\n\n# Load data\ntrain_loader, test_loader = load_data(batch_size=128)\n\n# Initialize networks\nteacher = TeacherNetwork(num_classes=10).to(device)\nstudent_distilled = StudentNetwork(num_classes=10).to(device)\nstudent_baseline = StudentNetwork(num_classes=10).to(device)\n\nprint(f\"Teacher parameters: {count_parameters(teacher):,}\")\nprint(f\"Student parameters: {count_parameters(student_distilled):,}\")\nprint(f\"Compression ratio: {count_parameters(teacher) / count_parameters(student_distilled):.1f}x\")\n\n# Train teacher (or load pre-trained)\ntry:\n    teacher.load_state_dict(torch.load('teacher_best.pth'))\n    print(\"Loaded pre-trained teacher model\")\nexcept FileNotFoundError:\n    print(\"Training teacher from scratch...\")\n    teacher = train_teacher(teacher, train_loader, test_loader, epochs=50)\n\nteacher_acc = evaluate_model(teacher, test_loader)\nprint(f\"Teacher accuracy: {teacher_acc:.2f}%\")\n\n# Train student with knowledge distillation\nstudent_distilled = train_student(\n    student_distilled, teacher, train_loader, test_loader, epochs=100\n)\n\n# Train student baseline (without distillation)\nstudent_baseline = train_student_baseline(\n    student_baseline, train_loader, test_loader, epochs=100\n)\n\n# Final evaluation\ndistilled_acc = evaluate_model(student_distilled, test_loader)\nbaseline_acc = evaluate_model(student_baseline, test_loader)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"FINAL RESULTS\")\nprint(\"=\"*50)\nprint(f\"Teacher accuracy:           {teacher_acc:.2f}%\")\nprint(f\"Student (distilled):        {distilled_acc:.2f}%\")\nprint(f\"Student (baseline):         {baseline_acc:.2f}%\")\nprint(f\"Distillation improvement:   {distilled_acc - baseline_acc:.2f}%\")\nprint(f\"Parameters reduction:       {count_parameters(teacher) / count_parameters(student_distilled):.1f}x\")\n\n\n\n\n\n\nclass FeatureDistillationLoss(nn.Module):\n    \"\"\"Distillation using intermediate feature maps\"\"\"\n    def __init__(self, alpha=0.5, beta=0.3, temperature=4.0):\n        super().__init__()\n        self.alpha = alpha      # Weight for prediction distillation\n        self.beta = beta        # Weight for feature distillation\n        self.temperature = temperature\n        self.ce_loss = nn.CrossEntropyLoss()\n        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n        self.mse_loss = nn.MSELoss()\n    \n    def forward(self, student_logits, teacher_logits, student_features, teacher_features, labels):\n        # Standard distillation loss\n        ce_loss = self.ce_loss(student_logits, labels)\n        \n        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)\n        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=1)\n        kl_loss = self.kl_loss(student_log_probs, teacher_probs) * (self.temperature ** 2)\n        \n        # Feature distillation loss\n        feature_loss = self.mse_loss(student_features, teacher_features)\n        \n        total_loss = (1 - self.alpha - self.beta) * ce_loss + self.alpha * kl_loss + self.beta * feature_loss\n        \n        return total_loss, ce_loss, kl_loss, feature_loss\n\n\n\nclass AttentionTransferLoss(nn.Module):\n    \"\"\"Transfer attention maps from teacher to student\"\"\"\n    def __init__(self, p=2):\n        super().__init__()\n        self.p = p\n    \n    def attention_map(self, feature_map):\n        # Compute attention as the L2 norm across channels\n        return torch.norm(feature_map, p=self.p, dim=1, keepdim=True)\n    \n    def forward(self, student_features, teacher_features):\n        student_attention = self.attention_map(student_features)\n        teacher_attention = self.attention_map(teacher_features)\n        \n        # Normalize attention maps\n        student_attention = F.normalize(student_attention.view(student_attention.size(0), -1))\n        teacher_attention = F.normalize(teacher_attention.view(teacher_attention.size(0), -1))\n        \n        return F.mse_loss(student_attention, teacher_attention)\n\n\n\nclass ProgressiveDistillation:\n    \"\"\"Gradually increase distillation weight during training\"\"\"\n    def __init__(self, initial_alpha=0.1, final_alpha=0.9, warmup_epochs=20):\n        self.initial_alpha = initial_alpha\n        self.final_alpha = final_alpha\n        self.warmup_epochs = warmup_epochs\n    \n    def get_alpha(self, epoch):\n        if epoch &lt; self.warmup_epochs:\n            alpha = self.initial_alpha + (self.final_alpha - self.initial_alpha) * (epoch / self.warmup_epochs)\n        else:\n            alpha = self.final_alpha\n        return alpha\n\n\n\n\n\n\n\nLow (1-2): Hard targets, less knowledge transfer\nMedium (3-5): Balanced knowledge transfer (recommended)\nHigh (6-10): Very soft targets, may lose important information\n\n\n\n\n\n0.1-0.3: Focus on ground truth labels\n0.5-0.7: Balanced approach (recommended)\n0.8-0.9: Heavy focus on teacher knowledge\n\n\n\n\n\nStart with same LR as baseline training\nConsider lower LR for student to avoid overfitting to teacher\nUse learning rate scheduling\n\n\n\n\n\n\nTeacher Quality: Ensure teacher model is well-trained and robust\nArchitecture Matching: Student should have similar structure but smaller capacity\nTemperature Tuning: Experiment with different temperature values\nRegularization: Use dropout and weight decay to prevent overfitting\nEvaluation: Compare against baseline student training\nMulti-Teacher: Consider ensemble of teachers for better knowledge transfer\n\n\n\n\n\n\nSolutions:\n\nReduce temperature value\nDecrease alpha (give more weight to ground truth)\nCheck teacher model quality\nEnsure proper normalization\n\n\n\n\nSolutions:\n\nIncrease learning rate\nUse progressive distillation\nWarm up the distillation loss\nCheck gradient flow\n\n\n\n\nSolutions:\n\nAdd regularization\nReduce alpha value\nUse data augmentation\nEarly stopping based on validation loss\n\nThis comprehensive guide provides both theoretical understanding and practical implementation of student-teacher networks in PyTorch, with advanced techniques and troubleshooting tips for successful knowledge distillation."
  },
  {
    "objectID": "posts/student-teacher-vanilla/index.html#overview",
    "href": "posts/student-teacher-vanilla/index.html#overview",
    "title": "Student-Teacher Network Training Guide in PyTorch",
    "section": "",
    "text": "Student-teacher networks, also known as knowledge distillation, involve training a smaller â€œstudentâ€ model to mimic the behavior of a larger, pre-trained â€œteacherâ€ model. This technique helps compress large models while maintaining performance."
  },
  {
    "objectID": "posts/student-teacher-vanilla/index.html#key-concepts",
    "href": "posts/student-teacher-vanilla/index.html#key-concepts",
    "title": "Student-Teacher Network Training Guide in PyTorch",
    "section": "",
    "text": "The student learns from both:\n\nHard targets: Original ground truth labels\nSoft targets: Teacherâ€™s probability distributions (softened with temperature)\n\n\n\n\nHigher temperature values create softer probability distributions, making it easier for the student to learn from the teacherâ€™s uncertainty."
  },
  {
    "objectID": "posts/student-teacher-vanilla/index.html#complete-implementation",
    "href": "posts/student-teacher-vanilla/index.html#complete-implementation",
    "title": "Student-Teacher Network Training Guide in PyTorch",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nimport numpy as np\n\n\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n\n\nclass TeacherNetwork(nn.Module):\n    \"\"\"Large teacher network (e.g., ResNet-50 equivalent)\"\"\"\n    def __init__(self, num_classes=10):\n        super(TeacherNetwork, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            \n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, 3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            \n            nn.Conv2d(128, 256, 3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, 3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(256, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n\n\nclass StudentNetwork(nn.Module):\n    \"\"\"Smaller student network\"\"\"\n    def __init__(self, num_classes=10):\n        super(StudentNetwork, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            \n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            \n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(128, 64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64, num_classes)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n\n\nclass DistillationLoss(nn.Module):\n    \"\"\"\n    Knowledge Distillation Loss combining:\n    1. Cross-entropy loss with true labels\n    2. KL divergence loss with teacher predictions\n    \"\"\"\n    def __init__(self, alpha=0.7, temperature=4.0):\n        super(DistillationLoss, self).__init__()\n        self.alpha = alpha  # Weight for distillation loss\n        self.temperature = temperature\n        self.ce_loss = nn.CrossEntropyLoss()\n        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n    \n    def forward(self, student_logits, teacher_logits, labels):\n        # Cross-entropy loss with true labels\n        ce_loss = self.ce_loss(student_logits, labels)\n        \n        # Soft targets from teacher\n        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)\n        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=1)\n        \n        # KL divergence loss\n        kl_loss = self.kl_loss(student_log_probs, teacher_probs) * (self.temperature ** 2)\n        \n        # Combined loss\n        total_loss = (1 - self.alpha) * ce_loss + self.alpha * kl_loss\n        \n        return total_loss, ce_loss, kl_loss\n\n\n\ndef load_data(batch_size=128):\n    \"\"\"Load CIFAR-10 dataset\"\"\"\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n    ])\n    \n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n    ])\n    \n    train_dataset = torchvision.datasets.CIFAR10(\n        root='./data', train=True, download=True, transform=transform_train\n    )\n    test_dataset = torchvision.datasets.CIFAR10(\n        root='./data', train=False, download=True, transform=transform_test\n    )\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    \n    return train_loader, test_loader\n\n\n\ndef train_teacher(model, train_loader, test_loader, epochs=50, lr=0.001):\n    \"\"\"Train the teacher network\"\"\"\n    print(\"Training Teacher Network...\")\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n    \n    model.train()\n    best_acc = 0.0\n    \n    for epoch in range(epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        progress_bar = tqdm(train_loader, desc=f'Teacher Epoch {epoch+1}/{epochs}')\n        for batch_idx, (inputs, targets) in enumerate(progress_bar):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n            \n            progress_bar.set_postfix({\n                'Loss': f'{running_loss/(batch_idx+1):.4f}',\n                'Acc': f'{100.*correct/total:.2f}%'\n            })\n        \n        # Evaluate\n        test_acc = evaluate_model(model, test_loader)\n        print(f'Teacher Epoch {epoch+1}: Train Acc: {100.*correct/total:.2f}%, Test Acc: {test_acc:.2f}%')\n        \n        if test_acc &gt; best_acc:\n            best_acc = test_acc\n            torch.save(model.state_dict(), 'teacher_best.pth')\n        \n        scheduler.step()\n    \n    print(f'Teacher training completed. Best accuracy: {best_acc:.2f}%')\n    return model\n\n\n\ndef train_student(student, teacher, train_loader, test_loader, epochs=100, lr=0.001):\n    \"\"\"Train the student network using knowledge distillation\"\"\"\n    print(\"Training Student Network with Knowledge Distillation...\")\n    \n    distillation_loss = DistillationLoss(alpha=0.7, temperature=4.0)\n    optimizer = optim.Adam(student.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n    \n    teacher.eval()  # Teacher in evaluation mode\n    student.train()\n    best_acc = 0.0\n    \n    for epoch in range(epochs):\n        running_loss = 0.0\n        running_ce_loss = 0.0\n        running_kl_loss = 0.0\n        correct = 0\n        total = 0\n        \n        progress_bar = tqdm(train_loader, desc=f'Student Epoch {epoch+1}/{epochs}')\n        for batch_idx, (inputs, targets) in enumerate(progress_bar):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Get predictions from both networks\n            with torch.no_grad():\n                teacher_logits = teacher(inputs)\n            \n            student_logits = student(inputs)\n            \n            # Calculate distillation loss\n            total_loss, ce_loss, kl_loss = distillation_loss(\n                student_logits, teacher_logits, targets\n            )\n            \n            total_loss.backward()\n            optimizer.step()\n            \n            # Statistics\n            running_loss += total_loss.item()\n            running_ce_loss += ce_loss.item()\n            running_kl_loss += kl_loss.item()\n            \n            _, predicted = student_logits.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n            \n            progress_bar.set_postfix({\n                'Loss': f'{running_loss/(batch_idx+1):.4f}',\n                'CE': f'{running_ce_loss/(batch_idx+1):.4f}',\n                'KL': f'{running_kl_loss/(batch_idx+1):.4f}',\n                'Acc': f'{100.*correct/total:.2f}%'\n            })\n        \n        # Evaluate\n        test_acc = evaluate_model(student, test_loader)\n        print(f'Student Epoch {epoch+1}: Train Acc: {100.*correct/total:.2f}%, Test Acc: {test_acc:.2f}%')\n        \n        if test_acc &gt; best_acc:\n            best_acc = test_acc\n            torch.save(student.state_dict(), 'student_best.pth')\n        \n        scheduler.step()\n    \n    print(f'Student training completed. Best accuracy: {best_acc:.2f}%')\n    return student\n\n\n\ndef train_student_baseline(student, train_loader, test_loader, epochs=100, lr=0.001):\n    \"\"\"Train student without distillation (baseline comparison)\"\"\"\n    print(\"Training Student Network (Baseline - No Distillation)...\")\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(student.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n    \n    student.train()\n    best_acc = 0.0\n    \n    for epoch in range(epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        progress_bar = tqdm(train_loader, desc=f'Baseline Epoch {epoch+1}/{epochs}')\n        for batch_idx, (inputs, targets) in enumerate(progress_bar):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            outputs = student(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n            \n            progress_bar.set_postfix({\n                'Loss': f'{running_loss/(batch_idx+1):.4f}',\n                'Acc': f'{100.*correct/total:.2f}%'\n            })\n        \n        # Evaluate\n        test_acc = evaluate_model(student, test_loader)\n        print(f'Baseline Epoch {epoch+1}: Train Acc: {100.*correct/total:.2f}%, Test Acc: {test_acc:.2f}%')\n        \n        if test_acc &gt; best_acc:\n            best_acc = test_acc\n            torch.save(student.state_dict(), 'student_baseline_best.pth')\n        \n        scheduler.step()\n    \n    print(f'Baseline training completed. Best accuracy: {best_acc:.2f}%')\n    return student\n\n\n\ndef evaluate_model(model, test_loader):\n    \"\"\"Evaluate model accuracy\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    \n    accuracy = 100. * correct / total\n    model.train()\n    return accuracy\n\n\n\ndef count_parameters(model):\n    \"\"\"Count trainable parameters\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\n\n# Load data\ntrain_loader, test_loader = load_data(batch_size=128)\n\n# Initialize networks\nteacher = TeacherNetwork(num_classes=10).to(device)\nstudent_distilled = StudentNetwork(num_classes=10).to(device)\nstudent_baseline = StudentNetwork(num_classes=10).to(device)\n\nprint(f\"Teacher parameters: {count_parameters(teacher):,}\")\nprint(f\"Student parameters: {count_parameters(student_distilled):,}\")\nprint(f\"Compression ratio: {count_parameters(teacher) / count_parameters(student_distilled):.1f}x\")\n\n# Train teacher (or load pre-trained)\ntry:\n    teacher.load_state_dict(torch.load('teacher_best.pth'))\n    print(\"Loaded pre-trained teacher model\")\nexcept FileNotFoundError:\n    print(\"Training teacher from scratch...\")\n    teacher = train_teacher(teacher, train_loader, test_loader, epochs=50)\n\nteacher_acc = evaluate_model(teacher, test_loader)\nprint(f\"Teacher accuracy: {teacher_acc:.2f}%\")\n\n# Train student with knowledge distillation\nstudent_distilled = train_student(\n    student_distilled, teacher, train_loader, test_loader, epochs=100\n)\n\n# Train student baseline (without distillation)\nstudent_baseline = train_student_baseline(\n    student_baseline, train_loader, test_loader, epochs=100\n)\n\n# Final evaluation\ndistilled_acc = evaluate_model(student_distilled, test_loader)\nbaseline_acc = evaluate_model(student_baseline, test_loader)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"FINAL RESULTS\")\nprint(\"=\"*50)\nprint(f\"Teacher accuracy:           {teacher_acc:.2f}%\")\nprint(f\"Student (distilled):        {distilled_acc:.2f}%\")\nprint(f\"Student (baseline):         {baseline_acc:.2f}%\")\nprint(f\"Distillation improvement:   {distilled_acc - baseline_acc:.2f}%\")\nprint(f\"Parameters reduction:       {count_parameters(teacher) / count_parameters(student_distilled):.1f}x\")"
  },
  {
    "objectID": "posts/student-teacher-vanilla/index.html#advanced-techniques",
    "href": "posts/student-teacher-vanilla/index.html#advanced-techniques",
    "title": "Student-Teacher Network Training Guide in PyTorch",
    "section": "",
    "text": "class FeatureDistillationLoss(nn.Module):\n    \"\"\"Distillation using intermediate feature maps\"\"\"\n    def __init__(self, alpha=0.5, beta=0.3, temperature=4.0):\n        super().__init__()\n        self.alpha = alpha      # Weight for prediction distillation\n        self.beta = beta        # Weight for feature distillation\n        self.temperature = temperature\n        self.ce_loss = nn.CrossEntropyLoss()\n        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n        self.mse_loss = nn.MSELoss()\n    \n    def forward(self, student_logits, teacher_logits, student_features, teacher_features, labels):\n        # Standard distillation loss\n        ce_loss = self.ce_loss(student_logits, labels)\n        \n        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)\n        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=1)\n        kl_loss = self.kl_loss(student_log_probs, teacher_probs) * (self.temperature ** 2)\n        \n        # Feature distillation loss\n        feature_loss = self.mse_loss(student_features, teacher_features)\n        \n        total_loss = (1 - self.alpha - self.beta) * ce_loss + self.alpha * kl_loss + self.beta * feature_loss\n        \n        return total_loss, ce_loss, kl_loss, feature_loss\n\n\n\nclass AttentionTransferLoss(nn.Module):\n    \"\"\"Transfer attention maps from teacher to student\"\"\"\n    def __init__(self, p=2):\n        super().__init__()\n        self.p = p\n    \n    def attention_map(self, feature_map):\n        # Compute attention as the L2 norm across channels\n        return torch.norm(feature_map, p=self.p, dim=1, keepdim=True)\n    \n    def forward(self, student_features, teacher_features):\n        student_attention = self.attention_map(student_features)\n        teacher_attention = self.attention_map(teacher_features)\n        \n        # Normalize attention maps\n        student_attention = F.normalize(student_attention.view(student_attention.size(0), -1))\n        teacher_attention = F.normalize(teacher_attention.view(teacher_attention.size(0), -1))\n        \n        return F.mse_loss(student_attention, teacher_attention)\n\n\n\nclass ProgressiveDistillation:\n    \"\"\"Gradually increase distillation weight during training\"\"\"\n    def __init__(self, initial_alpha=0.1, final_alpha=0.9, warmup_epochs=20):\n        self.initial_alpha = initial_alpha\n        self.final_alpha = final_alpha\n        self.warmup_epochs = warmup_epochs\n    \n    def get_alpha(self, epoch):\n        if epoch &lt; self.warmup_epochs:\n            alpha = self.initial_alpha + (self.final_alpha - self.initial_alpha) * (epoch / self.warmup_epochs)\n        else:\n            alpha = self.final_alpha\n        return alpha"
  },
  {
    "objectID": "posts/student-teacher-vanilla/index.html#hyperparameter-guidelines",
    "href": "posts/student-teacher-vanilla/index.html#hyperparameter-guidelines",
    "title": "Student-Teacher Network Training Guide in PyTorch",
    "section": "",
    "text": "Low (1-2): Hard targets, less knowledge transfer\nMedium (3-5): Balanced knowledge transfer (recommended)\nHigh (6-10): Very soft targets, may lose important information\n\n\n\n\n\n0.1-0.3: Focus on ground truth labels\n0.5-0.7: Balanced approach (recommended)\n0.8-0.9: Heavy focus on teacher knowledge\n\n\n\n\n\nStart with same LR as baseline training\nConsider lower LR for student to avoid overfitting to teacher\nUse learning rate scheduling"
  },
  {
    "objectID": "posts/student-teacher-vanilla/index.html#best-practices",
    "href": "posts/student-teacher-vanilla/index.html#best-practices",
    "title": "Student-Teacher Network Training Guide in PyTorch",
    "section": "",
    "text": "Teacher Quality: Ensure teacher model is well-trained and robust\nArchitecture Matching: Student should have similar structure but smaller capacity\nTemperature Tuning: Experiment with different temperature values\nRegularization: Use dropout and weight decay to prevent overfitting\nEvaluation: Compare against baseline student training\nMulti-Teacher: Consider ensemble of teachers for better knowledge transfer"
  },
  {
    "objectID": "posts/student-teacher-vanilla/index.html#common-issues-and-solutions",
    "href": "posts/student-teacher-vanilla/index.html#common-issues-and-solutions",
    "title": "Student-Teacher Network Training Guide in PyTorch",
    "section": "",
    "text": "Solutions:\n\nReduce temperature value\nDecrease alpha (give more weight to ground truth)\nCheck teacher model quality\nEnsure proper normalization\n\n\n\n\nSolutions:\n\nIncrease learning rate\nUse progressive distillation\nWarm up the distillation loss\nCheck gradient flow\n\n\n\n\nSolutions:\n\nAdd regularization\nReduce alpha value\nUse data augmentation\nEarly stopping based on validation loss\n\nThis comprehensive guide provides both theoretical understanding and practical implementation of student-teacher networks in PyTorch, with advanced techniques and troubleshooting tips for successful knowledge distillation."
  },
  {
    "objectID": "posts/accelerate-vs-fabric/index.html",
    "href": "posts/accelerate-vs-fabric/index.html",
    "title": "Hugging Face Accelerate vs PyTorch Lightning Fabric: A Deep Dive Comparison",
    "section": "",
    "text": "When youâ€™re working with deep learning models that need to scale across multiple GPUs or even multiple machines, youâ€™ll quickly encounter the complexity of distributed training. Two libraries have emerged as popular solutions to simplify this challenge: Hugging Face Accelerate and PyTorch Lightning Fabric. While both aim to make distributed training more accessible, they take fundamentally different approaches to solving the problem.\nThink of these libraries as two different philosophies for handling the complexity of scaling machine learning workloads. Accelerate acts like a careful translator, taking your existing PyTorch code and automatically adapting it for distributed environments with minimal changes. Lightning Fabric, on the other hand, functions more like a structured framework that provides you with powerful tools and patterns, but asks you to organize your code in specific ways to unlock its full potential.\n\n\nHugging Face Accelerate was born from a simple but powerful idea: most researchers and practitioners already have working PyTorch code, and they shouldnâ€™t need to rewrite everything just to scale it up. The libraryâ€™s design philosophy centers around minimal code changes. You can take a training loop that works on a single GPU and, with just a few additional lines, make it work across multiple GPUs, TPUs, or even different machines.\nThe beauty of Accelerate lies in its transparency. When you wrap your model, optimizer, and data loader with Accelerateâ€™s prepare function, the library handles the complex orchestration of distributed training behind the scenes. Your core training logic remains largely unchanged, which means you can focus on your model architecture and training strategies rather than wrestling with distributed computing concepts.\nLightning Fabric approaches the problem from a different angle. Rather than trying to be invisible, Fabric provides you with a set of powerful abstractions and tools that make distributed training not just possible, but elegant. Itâ€™s part of the broader PyTorch Lightning ecosystem, which has always emphasized best practices and reproducible research. Fabric gives you fine-grained control over the training process while still handling the low-level distributed computing details.\n\n\n\nWhen youâ€™re starting with Accelerate, the learning curve feels remarkably gentle. Letâ€™s imagine you have a standard PyTorch training loop. To make it work with Accelerate, you typically need to make just a few key changes: initialize an Accelerator object, wrap your model and optimizer with the prepare method, and replace your loss.backward() call with accelerator.backward(loss). The rest of your code can remain exactly as it was.\nThis approach has profound implications for how teams adopt distributed training. Junior developers can start using distributed training without needing to understand concepts like gradient synchronization, device placement, or communication backends. More experienced practitioners can gradually learn these concepts while their code continues to work.\nLightning Fabric requires a bit more upfront learning, but this investment pays dividends in terms of flexibility and control. Fabric encourages you to structure your code using its abstractions, which might feel unfamiliar at first but lead to more maintainable and scalable codebases. Youâ€™ll work with Fabricâ€™s strategy system for distributed training, its device management for handling different hardware, and its logging integrations for experiment tracking.\nThe key insight here is that Fabricâ€™s slightly steeper learning curve comes with corresponding benefits. Once you understand Fabricâ€™s patterns, youâ€™ll find it easier to implement complex training scenarios, debug distributed issues, and maintain consistency across different experiments.\n\n\n\nBoth libraries are built on top of PyTorchâ€™s native distributed training capabilities, so their fundamental performance characteristics are quite similar. However, they differ in how they expose optimization opportunities to you as a developer.\nAccelerate shines in its simplicity for standard use cases. The library automatically handles many optimization decisions for you, such as choosing appropriate communication backends and managing memory efficiently across devices. For many common scenarios, particularly when training transformer models, Accelerateâ€™s automatic optimizations work excellently out of the box.\nHowever, this automation can sometimes work against you when you need fine-grained control. If youâ€™re implementing custom gradient accumulation strategies, working with unusual model architectures, or need to optimize communication patterns for your specific hardware setup, Accelerateâ€™s abstractions might feel limiting.\nLightning Fabric provides more explicit control over these optimization decisions. You can choose specific distributed strategies, customize how gradients are synchronized, and implement sophisticated mixed-precision training schemes. This control comes at the cost of needing to understand what these choices mean, but it enables you to squeeze every bit of performance out of your hardware.\n\n\n\n\n\nfrom accelerate import Accelerator\nimport torch\nfrom torch.utils.data import DataLoader\n\n# Initialize accelerator - handles device placement and distributed setup\naccelerator = Accelerator()\n\n# Your existing model, optimizer, and data loader\nmodel = YourModel()\noptimizer = torch.optim.AdamW(model.parameters())\ntrain_dataloader = DataLoader(dataset, batch_size=32)\n\n# Prepare everything for distributed training - this is the key step\nmodel, optimizer, train_dataloader = accelerator.prepare(\n    model, optimizer, train_dataloader\n)\n\n# Your training loop stays almost identical\nfor batch in train_dataloader:\n    optimizer.zero_grad()\n    \n    # Forward pass works exactly as before\n    outputs = model(**batch)\n    loss = outputs.loss\n    \n    # Use accelerator.backward instead of loss.backward()\n    accelerator.backward(loss)\n    \n    optimizer.step()\n    \n    # Logging works seamlessly across all processes\n    accelerator.log({\"loss\": loss.item()})\n\n\n\nfrom lightning.fabric import Fabric\nimport torch\nfrom torch.utils.data import DataLoader\n\n# Initialize Fabric with explicit strategy choices\nfabric = Fabric(accelerator=\"gpu\", devices=4, strategy=\"ddp\")\nfabric.launch()\n\n# Setup model and optimizer\nmodel = YourModel()\noptimizer = torch.optim.AdamW(model.parameters())\n\n# Setup for distributed training - more explicit control\nmodel, optimizer = fabric.setup(model, optimizer)\ntrain_dataloader = fabric.setup_dataloaders(DataLoader(dataset, batch_size=32))\n\n# Training loop with explicit fabric calls\nfor batch in train_dataloader:\n    optimizer.zero_grad()\n    \n    # Forward pass\n    outputs = model(**batch)\n    loss = outputs.loss\n    \n    # Backward pass with fabric\n    fabric.backward(loss)\n    \n    optimizer.step()\n    \n    # Explicit logging with fabric\n    fabric.log(\"loss\", loss.item())\nThe code examples illustrate a key difference: Accelerate aims to make your existing code work with minimal changes, while Fabric provides more explicit control over the distributed training process.\n\n\n\n\nThe ecosystem story reveals another important distinction between these libraries. Hugging Face Accelerate benefits from its tight integration with the broader Hugging Face ecosystem. If youâ€™re working with transformers, datasets, or other Hugging Face libraries, Accelerate provides seamless interoperability. The library also integrates well with popular experiment tracking tools and supports various hardware configurations out of the box.\nLightning Fabric is part of the comprehensive PyTorch Lightning ecosystem, which includes not just distributed training tools, but also experiment management, hyperparameter optimization, and deployment utilities. This ecosystem approach means that once you invest in learning Fabric, you gain access to a complete toolkit for machine learning research and production.\n\n\n\n\n\nAccelerate provides automatic memory management features that work well for most use cases. The library can automatically handle gradient accumulation, mixed precision training, and even advanced techniques like gradient checkpointing. These features work transparently, requiring minimal configuration from the user.\nLightning Fabric offers more granular control over memory management. You can implement custom gradient accumulation strategies, fine-tune mixed precision settings, and even implement advanced memory optimization techniques like activation checkpointing with precise control over which layers to checkpoint.\n\n\n\nBoth libraries support a wide range of hardware configurations, from single GPUs to multi-node clusters. Accelerate automatically detects your hardware setup and configures itself accordingly, making it particularly easy to move code between different environments without modification.\nFabric provides explicit configuration options for different hardware setups, giving you more control over how your training job utilizes available resources. This can be particularly valuable when working with heterogeneous hardware or when you need to optimize for specific cluster configurations.\n\n\n\n\nThe debugging experience differs significantly between these libraries. Accelerateâ€™s transparent approach means that debugging often feels similar to debugging single-GPU code. When issues arise, theyâ€™re usually related to distributed training concepts rather than library-specific problems.\nLightning Fabric provides more explicit debugging tools and better error messages when distributed training issues occur. The libraryâ€™s structured approach makes it easier to isolate problems and reason about whatâ€™s happening across different processes.\n\n\n\nIn practice, both libraries perform similarly for most common use cases, since theyâ€™re both built on PyTorchâ€™s native distributed training capabilities. The performance differences typically come from how well each libraryâ€™s abstractions match your specific use case.\nAccelerate tends to perform excellently for transformer models and other common architectures, where its built-in optimizations align well with typical usage patterns. Lightning Fabric can sometimes achieve better performance for custom architectures or specialized training procedures, where its fine-grained control allows for targeted optimizations.\n\n\n\nIf youâ€™re currently using single-GPU training and want to scale up, Accelerate offers the smoother migration path. You can often get distributed training working in a matter of hours, then gradually learn more advanced concepts as needed.\nLightning Fabric requires more upfront investment but provides a more sustainable long-term foundation. Teams that choose Fabric often find that the initial learning investment pays off through increased productivity and fewer distributed training issues over time.\n\n\n\nBoth libraries benefit from active, supportive communities. Accelerateâ€™s community is closely tied to the broader Hugging Face ecosystem, with extensive documentation and examples focused on transformer models and NLP applications.\nLightning Fabricâ€™s community is part of the larger PyTorch Lightning ecosystem, with strong representation across different domains of machine learning. The community provides extensive examples for computer vision, NLP, and other domains.\n\n\n\nThe decision between Accelerate and Lightning Fabric should consider several factors beyond just technical capabilities. Team expertise, project timeline, and long-term maintenance requirements all play important roles.\nChoose Accelerate when you need to scale existing code quickly, when your team is new to distributed training, or when youâ€™re working primarily with transformer models. The libraryâ€™s minimal learning curve and automatic optimizations make it an excellent choice for rapid prototyping and iteration.\nChoose Lightning Fabric when you need fine-grained control over training procedures, when youâ€™re implementing custom training algorithms, or when you want to invest in a comprehensive framework that will serve multiple projects. The upfront learning investment is worthwhile for teams building production ML systems or conducting advanced research.\n\n\n\nBoth libraries continue to evolve rapidly, with regular updates that add new features and improve performance. Accelerateâ€™s development is closely tied to advances in the Hugging Face ecosystem, particularly around transformer models and large language models.\nLightning Fabricâ€™s development focuses on providing cutting-edge distributed training capabilities and maintaining compatibility with the latest PyTorch features. The library often serves as a testing ground for new distributed training patterns that later influence the broader ecosystem.\n\n\n\nHugging Face Accelerate and PyTorch Lightning Fabric represent two excellent but philosophically different approaches to distributed training. Accelerate prioritizes simplicity and ease of adoption, making it possible to scale existing code with minimal changes. Lightning Fabric emphasizes flexibility and control, providing powerful tools for teams that need to customize their training procedures.\nNeither choice is inherently better than the other. The right choice depends on your specific needs, team expertise, and project requirements. Both libraries will successfully help you move beyond single-GPU limitations and unlock the full potential of distributed computing for machine learning.\nThe most important step is to start experimenting with distributed training, regardless of which library you choose. Both Accelerate and Fabric provide excellent foundations for learning distributed training concepts and scaling your machine learning workloads effectively."
  },
  {
    "objectID": "posts/accelerate-vs-fabric/index.html#understanding-the-core-philosophy",
    "href": "posts/accelerate-vs-fabric/index.html#understanding-the-core-philosophy",
    "title": "Hugging Face Accelerate vs PyTorch Lightning Fabric: A Deep Dive Comparison",
    "section": "",
    "text": "Hugging Face Accelerate was born from a simple but powerful idea: most researchers and practitioners already have working PyTorch code, and they shouldnâ€™t need to rewrite everything just to scale it up. The libraryâ€™s design philosophy centers around minimal code changes. You can take a training loop that works on a single GPU and, with just a few additional lines, make it work across multiple GPUs, TPUs, or even different machines.\nThe beauty of Accelerate lies in its transparency. When you wrap your model, optimizer, and data loader with Accelerateâ€™s prepare function, the library handles the complex orchestration of distributed training behind the scenes. Your core training logic remains largely unchanged, which means you can focus on your model architecture and training strategies rather than wrestling with distributed computing concepts.\nLightning Fabric approaches the problem from a different angle. Rather than trying to be invisible, Fabric provides you with a set of powerful abstractions and tools that make distributed training not just possible, but elegant. Itâ€™s part of the broader PyTorch Lightning ecosystem, which has always emphasized best practices and reproducible research. Fabric gives you fine-grained control over the training process while still handling the low-level distributed computing details."
  },
  {
    "objectID": "posts/accelerate-vs-fabric/index.html#code-integration-and-learning-curve",
    "href": "posts/accelerate-vs-fabric/index.html#code-integration-and-learning-curve",
    "title": "Hugging Face Accelerate vs PyTorch Lightning Fabric: A Deep Dive Comparison",
    "section": "",
    "text": "When youâ€™re starting with Accelerate, the learning curve feels remarkably gentle. Letâ€™s imagine you have a standard PyTorch training loop. To make it work with Accelerate, you typically need to make just a few key changes: initialize an Accelerator object, wrap your model and optimizer with the prepare method, and replace your loss.backward() call with accelerator.backward(loss). The rest of your code can remain exactly as it was.\nThis approach has profound implications for how teams adopt distributed training. Junior developers can start using distributed training without needing to understand concepts like gradient synchronization, device placement, or communication backends. More experienced practitioners can gradually learn these concepts while their code continues to work.\nLightning Fabric requires a bit more upfront learning, but this investment pays dividends in terms of flexibility and control. Fabric encourages you to structure your code using its abstractions, which might feel unfamiliar at first but lead to more maintainable and scalable codebases. Youâ€™ll work with Fabricâ€™s strategy system for distributed training, its device management for handling different hardware, and its logging integrations for experiment tracking.\nThe key insight here is that Fabricâ€™s slightly steeper learning curve comes with corresponding benefits. Once you understand Fabricâ€™s patterns, youâ€™ll find it easier to implement complex training scenarios, debug distributed issues, and maintain consistency across different experiments."
  },
  {
    "objectID": "posts/accelerate-vs-fabric/index.html#performance-and-optimization-capabilities",
    "href": "posts/accelerate-vs-fabric/index.html#performance-and-optimization-capabilities",
    "title": "Hugging Face Accelerate vs PyTorch Lightning Fabric: A Deep Dive Comparison",
    "section": "",
    "text": "Both libraries are built on top of PyTorchâ€™s native distributed training capabilities, so their fundamental performance characteristics are quite similar. However, they differ in how they expose optimization opportunities to you as a developer.\nAccelerate shines in its simplicity for standard use cases. The library automatically handles many optimization decisions for you, such as choosing appropriate communication backends and managing memory efficiently across devices. For many common scenarios, particularly when training transformer models, Accelerateâ€™s automatic optimizations work excellently out of the box.\nHowever, this automation can sometimes work against you when you need fine-grained control. If youâ€™re implementing custom gradient accumulation strategies, working with unusual model architectures, or need to optimize communication patterns for your specific hardware setup, Accelerateâ€™s abstractions might feel limiting.\nLightning Fabric provides more explicit control over these optimization decisions. You can choose specific distributed strategies, customize how gradients are synchronized, and implement sophisticated mixed-precision training schemes. This control comes at the cost of needing to understand what these choices mean, but it enables you to squeeze every bit of performance out of your hardware."
  },
  {
    "objectID": "posts/accelerate-vs-fabric/index.html#code-examples-and-practical-implementation",
    "href": "posts/accelerate-vs-fabric/index.html#code-examples-and-practical-implementation",
    "title": "Hugging Face Accelerate vs PyTorch Lightning Fabric: A Deep Dive Comparison",
    "section": "",
    "text": "from accelerate import Accelerator\nimport torch\nfrom torch.utils.data import DataLoader\n\n# Initialize accelerator - handles device placement and distributed setup\naccelerator = Accelerator()\n\n# Your existing model, optimizer, and data loader\nmodel = YourModel()\noptimizer = torch.optim.AdamW(model.parameters())\ntrain_dataloader = DataLoader(dataset, batch_size=32)\n\n# Prepare everything for distributed training - this is the key step\nmodel, optimizer, train_dataloader = accelerator.prepare(\n    model, optimizer, train_dataloader\n)\n\n# Your training loop stays almost identical\nfor batch in train_dataloader:\n    optimizer.zero_grad()\n    \n    # Forward pass works exactly as before\n    outputs = model(**batch)\n    loss = outputs.loss\n    \n    # Use accelerator.backward instead of loss.backward()\n    accelerator.backward(loss)\n    \n    optimizer.step()\n    \n    # Logging works seamlessly across all processes\n    accelerator.log({\"loss\": loss.item()})\n\n\n\nfrom lightning.fabric import Fabric\nimport torch\nfrom torch.utils.data import DataLoader\n\n# Initialize Fabric with explicit strategy choices\nfabric = Fabric(accelerator=\"gpu\", devices=4, strategy=\"ddp\")\nfabric.launch()\n\n# Setup model and optimizer\nmodel = YourModel()\noptimizer = torch.optim.AdamW(model.parameters())\n\n# Setup for distributed training - more explicit control\nmodel, optimizer = fabric.setup(model, optimizer)\ntrain_dataloader = fabric.setup_dataloaders(DataLoader(dataset, batch_size=32))\n\n# Training loop with explicit fabric calls\nfor batch in train_dataloader:\n    optimizer.zero_grad()\n    \n    # Forward pass\n    outputs = model(**batch)\n    loss = outputs.loss\n    \n    # Backward pass with fabric\n    fabric.backward(loss)\n    \n    optimizer.step()\n    \n    # Explicit logging with fabric\n    fabric.log(\"loss\", loss.item())\nThe code examples illustrate a key difference: Accelerate aims to make your existing code work with minimal changes, while Fabric provides more explicit control over the distributed training process."
  },
  {
    "objectID": "posts/accelerate-vs-fabric/index.html#ecosystem-integration-and-tooling",
    "href": "posts/accelerate-vs-fabric/index.html#ecosystem-integration-and-tooling",
    "title": "Hugging Face Accelerate vs PyTorch Lightning Fabric: A Deep Dive Comparison",
    "section": "",
    "text": "The ecosystem story reveals another important distinction between these libraries. Hugging Face Accelerate benefits from its tight integration with the broader Hugging Face ecosystem. If youâ€™re working with transformers, datasets, or other Hugging Face libraries, Accelerate provides seamless interoperability. The library also integrates well with popular experiment tracking tools and supports various hardware configurations out of the box.\nLightning Fabric is part of the comprehensive PyTorch Lightning ecosystem, which includes not just distributed training tools, but also experiment management, hyperparameter optimization, and deployment utilities. This ecosystem approach means that once you invest in learning Fabric, you gain access to a complete toolkit for machine learning research and production."
  },
  {
    "objectID": "posts/accelerate-vs-fabric/index.html#advanced-features-and-customization",
    "href": "posts/accelerate-vs-fabric/index.html#advanced-features-and-customization",
    "title": "Hugging Face Accelerate vs PyTorch Lightning Fabric: A Deep Dive Comparison",
    "section": "",
    "text": "Accelerate provides automatic memory management features that work well for most use cases. The library can automatically handle gradient accumulation, mixed precision training, and even advanced techniques like gradient checkpointing. These features work transparently, requiring minimal configuration from the user.\nLightning Fabric offers more granular control over memory management. You can implement custom gradient accumulation strategies, fine-tune mixed precision settings, and even implement advanced memory optimization techniques like activation checkpointing with precise control over which layers to checkpoint.\n\n\n\nBoth libraries support a wide range of hardware configurations, from single GPUs to multi-node clusters. Accelerate automatically detects your hardware setup and configures itself accordingly, making it particularly easy to move code between different environments without modification.\nFabric provides explicit configuration options for different hardware setups, giving you more control over how your training job utilizes available resources. This can be particularly valuable when working with heterogeneous hardware or when you need to optimize for specific cluster configurations."
  },
  {
    "objectID": "posts/accelerate-vs-fabric/index.html#debugging-and-development-experience",
    "href": "posts/accelerate-vs-fabric/index.html#debugging-and-development-experience",
    "title": "Hugging Face Accelerate vs PyTorch Lightning Fabric: A Deep Dive Comparison",
    "section": "",
    "text": "The debugging experience differs significantly between these libraries. Accelerateâ€™s transparent approach means that debugging often feels similar to debugging single-GPU code. When issues arise, theyâ€™re usually related to distributed training concepts rather than library-specific problems.\nLightning Fabric provides more explicit debugging tools and better error messages when distributed training issues occur. The libraryâ€™s structured approach makes it easier to isolate problems and reason about whatâ€™s happening across different processes."
  },
  {
    "objectID": "posts/accelerate-vs-fabric/index.html#performance-benchmarks-and-real-world-usage",
    "href": "posts/accelerate-vs-fabric/index.html#performance-benchmarks-and-real-world-usage",
    "title": "Hugging Face Accelerate vs PyTorch Lightning Fabric: A Deep Dive Comparison",
    "section": "",
    "text": "In practice, both libraries perform similarly for most common use cases, since theyâ€™re both built on PyTorchâ€™s native distributed training capabilities. The performance differences typically come from how well each libraryâ€™s abstractions match your specific use case.\nAccelerate tends to perform excellently for transformer models and other common architectures, where its built-in optimizations align well with typical usage patterns. Lightning Fabric can sometimes achieve better performance for custom architectures or specialized training procedures, where its fine-grained control allows for targeted optimizations."
  },
  {
    "objectID": "posts/accelerate-vs-fabric/index.html#migration-and-adoption-strategies",
    "href": "posts/accelerate-vs-fabric/index.html#migration-and-adoption-strategies",
    "title": "Hugging Face Accelerate vs PyTorch Lightning Fabric: A Deep Dive Comparison",
    "section": "",
    "text": "If youâ€™re currently using single-GPU training and want to scale up, Accelerate offers the smoother migration path. You can often get distributed training working in a matter of hours, then gradually learn more advanced concepts as needed.\nLightning Fabric requires more upfront investment but provides a more sustainable long-term foundation. Teams that choose Fabric often find that the initial learning investment pays off through increased productivity and fewer distributed training issues over time."
  },
  {
    "objectID": "posts/accelerate-vs-fabric/index.html#community-and-support",
    "href": "posts/accelerate-vs-fabric/index.html#community-and-support",
    "title": "Hugging Face Accelerate vs PyTorch Lightning Fabric: A Deep Dive Comparison",
    "section": "",
    "text": "Both libraries benefit from active, supportive communities. Accelerateâ€™s community is closely tied to the broader Hugging Face ecosystem, with extensive documentation and examples focused on transformer models and NLP applications.\nLightning Fabricâ€™s community is part of the larger PyTorch Lightning ecosystem, with strong representation across different domains of machine learning. The community provides extensive examples for computer vision, NLP, and other domains."
  },
  {
    "objectID": "posts/accelerate-vs-fabric/index.html#making-the-right-choice-for-your-team",
    "href": "posts/accelerate-vs-fabric/index.html#making-the-right-choice-for-your-team",
    "title": "Hugging Face Accelerate vs PyTorch Lightning Fabric: A Deep Dive Comparison",
    "section": "",
    "text": "The decision between Accelerate and Lightning Fabric should consider several factors beyond just technical capabilities. Team expertise, project timeline, and long-term maintenance requirements all play important roles.\nChoose Accelerate when you need to scale existing code quickly, when your team is new to distributed training, or when youâ€™re working primarily with transformer models. The libraryâ€™s minimal learning curve and automatic optimizations make it an excellent choice for rapid prototyping and iteration.\nChoose Lightning Fabric when you need fine-grained control over training procedures, when youâ€™re implementing custom training algorithms, or when you want to invest in a comprehensive framework that will serve multiple projects. The upfront learning investment is worthwhile for teams building production ML systems or conducting advanced research."
  },
  {
    "objectID": "posts/accelerate-vs-fabric/index.html#future-considerations",
    "href": "posts/accelerate-vs-fabric/index.html#future-considerations",
    "title": "Hugging Face Accelerate vs PyTorch Lightning Fabric: A Deep Dive Comparison",
    "section": "",
    "text": "Both libraries continue to evolve rapidly, with regular updates that add new features and improve performance. Accelerateâ€™s development is closely tied to advances in the Hugging Face ecosystem, particularly around transformer models and large language models.\nLightning Fabricâ€™s development focuses on providing cutting-edge distributed training capabilities and maintaining compatibility with the latest PyTorch features. The library often serves as a testing ground for new distributed training patterns that later influence the broader ecosystem."
  },
  {
    "objectID": "posts/accelerate-vs-fabric/index.html#conclusion",
    "href": "posts/accelerate-vs-fabric/index.html#conclusion",
    "title": "Hugging Face Accelerate vs PyTorch Lightning Fabric: A Deep Dive Comparison",
    "section": "",
    "text": "Hugging Face Accelerate and PyTorch Lightning Fabric represent two excellent but philosophically different approaches to distributed training. Accelerate prioritizes simplicity and ease of adoption, making it possible to scale existing code with minimal changes. Lightning Fabric emphasizes flexibility and control, providing powerful tools for teams that need to customize their training procedures.\nNeither choice is inherently better than the other. The right choice depends on your specific needs, team expertise, and project requirements. Both libraries will successfully help you move beyond single-GPU limitations and unlock the full potential of distributed computing for machine learning.\nThe most important step is to start experimenting with distributed training, regardless of which library you choose. Both Accelerate and Fabric provide excellent foundations for learning distributed training concepts and scaling your machine learning workloads effectively."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "ðŸ‘ï¸ Welcome to My Computer Vision Blog!\n\nThis is the first post in this blog.\nHello and welcome!\nIâ€™m thrilled to kick off this blog dedicated to exploring the fascinating world of computer vision â€” a field where machines learn to see, interpret, and understand the visual world around us. Whether youâ€™re a seasoned AI researcher, an aspiring developer, or simply curious about how technology can â€œsee,â€ youâ€™ll find something valuable here.\nFrom image processing techniques to deep learning breakthroughs, from real-world applications to hands-on tutorials â€” this blog will cover it all. My goal is to make computer vision approachable, insightful, and exciting for everyone.\nSo, whether youâ€™re here to learn, build, or stay on top of the latest innovations, Iâ€™m glad to have you along for the journey. Letâ€™s dive into the visual future together!\nStay tuned, and letâ€™s make machines see the world like never before. ðŸš€\nCheers,  Krishna"
  },
  {
    "objectID": "posts/pytorch-optimizations/index.html",
    "href": "posts/pytorch-optimizations/index.html",
    "title": "PyTorch Training and Inference Optimization Guide",
    "section": "",
    "text": "The guide includes practical code examples you can directly use in your projects, along with best practices and common pitfalls to avoid. Each section builds upon the previous ones, so you can implement these optimizations incrementally based on your specific needs and performance requirements.\n\n\n\nGeneral Optimization Principles\nTraining Optimizations\nInference Optimizations\nMemory Management\nHardware-Specific Optimizations\nProfiling and Debugging\n\n\n\n\n\n\nimport torch\n\n# Use half precision when possible (reduces memory and increases speed)\nmodel = model.half()  # Convert to float16\n# Or use mixed precision training\nfrom torch.cuda.amp import autocast, GradScaler\n\n# Use appropriate tensor types\nx = torch.tensor(data, dtype=torch.float32)  # Explicit dtype\n\n\n\nfrom torch.utils.data import DataLoader\nimport torch.multiprocessing as mp\n\n# Optimize DataLoader\ntrain_loader = DataLoader(\n    dataset,\n    batch_size=32,\n    shuffle=True,\n    num_workers=4,  # Use multiple workers\n    pin_memory=True,  # Faster GPU transfer\n    persistent_workers=True,  # Keep workers alive\n    prefetch_factor=2  # Prefetch batches\n)\n\n# Use non_blocking transfers\nfor batch in train_loader:\n    data = batch[0].to(device, non_blocking=True)\n    target = batch[1].to(device, non_blocking=True)\n\n\n\n# Avoid unnecessary CPU-GPU transfers\nx = torch.randn(1000, 1000, device='cuda')  # Create directly on GPU\n\n# Use in-place operations when possible\nx.add_(y)  # Instead of x = x + y\nx.mul_(2)  # Instead of x = x * 2\n\n# Batch operations instead of loops\n# Bad\nfor i in range(batch_size):\n    result[i] = model(x[i])\n\n# Good\nresult = model(x)  # Process entire batch\n\n\n\n\n\n\nfrom torch.cuda.amp import autocast, GradScaler\n\nmodel = MyModel().cuda()\noptimizer = torch.optim.Adam(model.parameters())\nscaler = GradScaler()\n\nfor epoch in range(num_epochs):\n    for batch in train_loader:\n        optimizer.zero_grad()\n        \n        # Forward pass with autocast\n        with autocast():\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n        \n        # Backward pass with gradient scaling\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n\n\naccumulation_steps = 4\noptimizer.zero_grad()\n\nfor i, batch in enumerate(train_loader):\n    with autocast():\n        outputs = model(inputs)\n        loss = criterion(outputs, targets) / accumulation_steps\n    \n    scaler.scale(loss).backward()\n    \n    if (i + 1) % accumulation_steps == 0:\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad()\n\n\n\nfrom torch.optim.lr_scheduler import OneCycleLR\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\nscheduler = OneCycleLR(\n    optimizer,\n    max_lr=0.01,\n    epochs=num_epochs,\n    steps_per_epoch=len(train_loader)\n)\n\n# Use scheduler after each batch for OneCycleLR\nfor batch in train_loader:\n    # ... training step ...\n    scheduler.step()\n\n\n\n# Compile model for faster training\nmodel = torch.compile(model)\n\n# Different modes for different use cases\nmodel = torch.compile(model, mode=\"reduce-overhead\")  # For large models\nmodel = torch.compile(model, mode=\"max-autotune\")     # For maximum performance\n\n\n\ndef save_checkpoint(model, optimizer, epoch, loss, filename):\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss,\n    }, filename)\n\ndef load_checkpoint(model, optimizer, filename):\n    checkpoint = torch.load(filename)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    return checkpoint['epoch'], checkpoint['loss']\n\n\n\n\n\n\n# Set model to evaluation mode\nmodel.eval()\n\n# Disable gradient computation\nwith torch.no_grad():\n    outputs = model(inputs)\n\n# Use torch.inference_mode() for even better performance\nwith torch.inference_mode():\n    outputs = model(inputs)\n\n\n\n# Trace the model\nexample_input = torch.randn(1, 3, 224, 224)\ntraced_model = torch.jit.trace(model, example_input)\n\n# Or script the model\nscripted_model = torch.jit.script(model)\n\n# Optimize the scripted model\noptimized_model = torch.jit.optimize_for_inference(scripted_model)\n\n# Save and load\ntorch.jit.save(optimized_model, \"optimized_model.pt\")\nloaded_model = torch.jit.load(\"optimized_model.pt\")\n\n\n\nimport torch.quantization as quant\n\n# Post-training quantization\nmodel.eval()\nquantized_model = torch.quantization.quantize_dynamic(\n    model, {torch.nn.Linear}, dtype=torch.qint8\n)\n\n# Quantization-aware training\nmodel.train()\nmodel.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\ntorch.quantization.prepare_qat(model, inplace=True)\n\n# Train the model...\n\n# Convert to quantized model\nquantized_model = torch.quantization.convert(model, inplace=False)\n\n\n\ndef batch_inference(model, data_loader, device):\n    model.eval()\n    results = []\n    \n    with torch.inference_mode():\n        for batch in data_loader:\n            inputs = batch.to(device, non_blocking=True)\n            outputs = model(inputs)\n            results.append(outputs.cpu())\n    \n    return torch.cat(results, dim=0)\n\n\n\n\n\n\n# Clear unnecessary variables\ndel intermediate_results\ntorch.cuda.empty_cache()  # Free GPU memory\n\n# Use gradient checkpointing for large models\nfrom torch.utils.checkpoint import checkpoint\n\nclass MyModel(nn.Module):\n    def forward(self, x):\n        # Use checkpointing for memory-intensive layers\n        x = checkpoint(self.expensive_layer, x)\n        return x\n\n\n\ndef print_memory_usage():\n    if torch.cuda.is_available():\n        print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n        print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n\n# Monitor during training\nfor epoch in range(num_epochs):\n    for batch in train_loader:\n        # ... training code ...\n        if batch_idx % 100 == 0:\n            print_memory_usage()\n\n\n\nclass MemoryEfficientDataset(torch.utils.data.Dataset):\n    def __init__(self, data_paths):\n        self.data_paths = data_paths\n    \n    def __getitem__(self, idx):\n        # Load data on-demand instead of keeping in memory\n        data = self.load_data(self.data_paths[idx])\n        return data\n    \n    def __len__(self):\n        return len(self.data_paths)\n\n\n\n\n\n\n# Set optimal GPU settings\ntorch.backends.cudnn.benchmark = True  # For fixed input sizes\ntorch.backends.cudnn.deterministic = False  # For reproducibility (slower)\n\n# Use multiple GPUs\nif torch.cuda.device_count() &gt; 1:\n    model = nn.DataParallel(model)\n\n# Or use DistributedDataParallel for better performance\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nmodel = DDP(model, device_ids=[local_rank])\n\n\n\n# Set number of threads\ntorch.set_num_threads(4)\n\n# Use Intel MKL-DNN optimizations\ntorch.backends.mkldnn.enabled = True\n\n\n\n# Use Metal Performance Shaders on Apple Silicon\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n    model = model.to(device)\n\n\n\n\n\n\nfrom torch.profiler import profile, record_function, ProfilerActivity\n\nwith profile(\n    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n    record_shapes=True,\n    profile_memory=True,\n    with_stack=True\n) as prof:\n    for batch in train_loader:\n        with record_function(\"forward\"):\n            outputs = model(inputs)\n        with record_function(\"backward\"):\n            loss.backward()\n        with record_function(\"optimizer\"):\n            optimizer.step()\n\n# Save trace for tensorboard\nprof.export_chrome_trace(\"trace.json\")\n\n\n\n# Profile memory usage\nwith profile(profile_memory=True) as prof:\n    model(inputs)\n\nprint(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))\n\n\n\nimport time\n\ndef benchmark_model(model, input_tensor, num_runs=100):\n    model.eval()\n    \n    # Warmup\n    with torch.no_grad():\n        for _ in range(10):\n            _ = model(input_tensor)\n    \n    # Benchmark\n    torch.cuda.synchronize()\n    start_time = time.time()\n    \n    with torch.no_grad():\n        for _ in range(num_runs):\n            _ = model(input_tensor)\n    \n    torch.cuda.synchronize()\n    end_time = time.time()\n    \n    avg_time = (end_time - start_time) / num_runs\n    print(f\"Average inference time: {avg_time*1000:.2f} ms\")\n\n\n\n\n\nAlways profile first - Identify bottlenecks before optimizing\nUse mixed precision - Significant speedup with minimal accuracy loss\nOptimize data loading - Use multiple workers and pin memory\nBatch operations - Avoid loops over individual samples\nModel compilation - Use torch.compile() for PyTorch 2.0+\nMemory management - Monitor and optimize memory usage\nHardware utilization - Use all available compute resources\nQuantization for inference - Reduce model size and increase speed\nTorchScript for production - Better performance and deployment options\nRegular checkpointing - Save training progress and enable resumption\n\n\n\n\n\nMoving tensors between CPU and GPU unnecessarily\nUsing small batch sizes that underutilize hardware\nNot using torch.no_grad() during inference\nCreating tensors in loops instead of batching\nNot clearing variables and calling torch.cuda.empty_cache()\nUsing synchronous operations when asynchronous would work\nNot leveraging built-in optimized functions"
  },
  {
    "objectID": "posts/pytorch-optimizations/index.html#table-of-contents",
    "href": "posts/pytorch-optimizations/index.html#table-of-contents",
    "title": "PyTorch Training and Inference Optimization Guide",
    "section": "",
    "text": "General Optimization Principles\nTraining Optimizations\nInference Optimizations\nMemory Management\nHardware-Specific Optimizations\nProfiling and Debugging"
  },
  {
    "objectID": "posts/pytorch-optimizations/index.html#general-optimization-principles",
    "href": "posts/pytorch-optimizations/index.html#general-optimization-principles",
    "title": "PyTorch Training and Inference Optimization Guide",
    "section": "",
    "text": "import torch\n\n# Use half precision when possible (reduces memory and increases speed)\nmodel = model.half()  # Convert to float16\n# Or use mixed precision training\nfrom torch.cuda.amp import autocast, GradScaler\n\n# Use appropriate tensor types\nx = torch.tensor(data, dtype=torch.float32)  # Explicit dtype\n\n\n\nfrom torch.utils.data import DataLoader\nimport torch.multiprocessing as mp\n\n# Optimize DataLoader\ntrain_loader = DataLoader(\n    dataset,\n    batch_size=32,\n    shuffle=True,\n    num_workers=4,  # Use multiple workers\n    pin_memory=True,  # Faster GPU transfer\n    persistent_workers=True,  # Keep workers alive\n    prefetch_factor=2  # Prefetch batches\n)\n\n# Use non_blocking transfers\nfor batch in train_loader:\n    data = batch[0].to(device, non_blocking=True)\n    target = batch[1].to(device, non_blocking=True)\n\n\n\n# Avoid unnecessary CPU-GPU transfers\nx = torch.randn(1000, 1000, device='cuda')  # Create directly on GPU\n\n# Use in-place operations when possible\nx.add_(y)  # Instead of x = x + y\nx.mul_(2)  # Instead of x = x * 2\n\n# Batch operations instead of loops\n# Bad\nfor i in range(batch_size):\n    result[i] = model(x[i])\n\n# Good\nresult = model(x)  # Process entire batch"
  },
  {
    "objectID": "posts/pytorch-optimizations/index.html#training-optimizations",
    "href": "posts/pytorch-optimizations/index.html#training-optimizations",
    "title": "PyTorch Training and Inference Optimization Guide",
    "section": "",
    "text": "from torch.cuda.amp import autocast, GradScaler\n\nmodel = MyModel().cuda()\noptimizer = torch.optim.Adam(model.parameters())\nscaler = GradScaler()\n\nfor epoch in range(num_epochs):\n    for batch in train_loader:\n        optimizer.zero_grad()\n        \n        # Forward pass with autocast\n        with autocast():\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n        \n        # Backward pass with gradient scaling\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n\n\naccumulation_steps = 4\noptimizer.zero_grad()\n\nfor i, batch in enumerate(train_loader):\n    with autocast():\n        outputs = model(inputs)\n        loss = criterion(outputs, targets) / accumulation_steps\n    \n    scaler.scale(loss).backward()\n    \n    if (i + 1) % accumulation_steps == 0:\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad()\n\n\n\nfrom torch.optim.lr_scheduler import OneCycleLR\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\nscheduler = OneCycleLR(\n    optimizer,\n    max_lr=0.01,\n    epochs=num_epochs,\n    steps_per_epoch=len(train_loader)\n)\n\n# Use scheduler after each batch for OneCycleLR\nfor batch in train_loader:\n    # ... training step ...\n    scheduler.step()\n\n\n\n# Compile model for faster training\nmodel = torch.compile(model)\n\n# Different modes for different use cases\nmodel = torch.compile(model, mode=\"reduce-overhead\")  # For large models\nmodel = torch.compile(model, mode=\"max-autotune\")     # For maximum performance\n\n\n\ndef save_checkpoint(model, optimizer, epoch, loss, filename):\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss,\n    }, filename)\n\ndef load_checkpoint(model, optimizer, filename):\n    checkpoint = torch.load(filename)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    return checkpoint['epoch'], checkpoint['loss']"
  },
  {
    "objectID": "posts/pytorch-optimizations/index.html#inference-optimizations",
    "href": "posts/pytorch-optimizations/index.html#inference-optimizations",
    "title": "PyTorch Training and Inference Optimization Guide",
    "section": "",
    "text": "# Set model to evaluation mode\nmodel.eval()\n\n# Disable gradient computation\nwith torch.no_grad():\n    outputs = model(inputs)\n\n# Use torch.inference_mode() for even better performance\nwith torch.inference_mode():\n    outputs = model(inputs)\n\n\n\n# Trace the model\nexample_input = torch.randn(1, 3, 224, 224)\ntraced_model = torch.jit.trace(model, example_input)\n\n# Or script the model\nscripted_model = torch.jit.script(model)\n\n# Optimize the scripted model\noptimized_model = torch.jit.optimize_for_inference(scripted_model)\n\n# Save and load\ntorch.jit.save(optimized_model, \"optimized_model.pt\")\nloaded_model = torch.jit.load(\"optimized_model.pt\")\n\n\n\nimport torch.quantization as quant\n\n# Post-training quantization\nmodel.eval()\nquantized_model = torch.quantization.quantize_dynamic(\n    model, {torch.nn.Linear}, dtype=torch.qint8\n)\n\n# Quantization-aware training\nmodel.train()\nmodel.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\ntorch.quantization.prepare_qat(model, inplace=True)\n\n# Train the model...\n\n# Convert to quantized model\nquantized_model = torch.quantization.convert(model, inplace=False)\n\n\n\ndef batch_inference(model, data_loader, device):\n    model.eval()\n    results = []\n    \n    with torch.inference_mode():\n        for batch in data_loader:\n            inputs = batch.to(device, non_blocking=True)\n            outputs = model(inputs)\n            results.append(outputs.cpu())\n    \n    return torch.cat(results, dim=0)"
  },
  {
    "objectID": "posts/pytorch-optimizations/index.html#memory-management",
    "href": "posts/pytorch-optimizations/index.html#memory-management",
    "title": "PyTorch Training and Inference Optimization Guide",
    "section": "",
    "text": "# Clear unnecessary variables\ndel intermediate_results\ntorch.cuda.empty_cache()  # Free GPU memory\n\n# Use gradient checkpointing for large models\nfrom torch.utils.checkpoint import checkpoint\n\nclass MyModel(nn.Module):\n    def forward(self, x):\n        # Use checkpointing for memory-intensive layers\n        x = checkpoint(self.expensive_layer, x)\n        return x\n\n\n\ndef print_memory_usage():\n    if torch.cuda.is_available():\n        print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n        print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n\n# Monitor during training\nfor epoch in range(num_epochs):\n    for batch in train_loader:\n        # ... training code ...\n        if batch_idx % 100 == 0:\n            print_memory_usage()\n\n\n\nclass MemoryEfficientDataset(torch.utils.data.Dataset):\n    def __init__(self, data_paths):\n        self.data_paths = data_paths\n    \n    def __getitem__(self, idx):\n        # Load data on-demand instead of keeping in memory\n        data = self.load_data(self.data_paths[idx])\n        return data\n    \n    def __len__(self):\n        return len(self.data_paths)"
  },
  {
    "objectID": "posts/pytorch-optimizations/index.html#hardware-specific-optimizations",
    "href": "posts/pytorch-optimizations/index.html#hardware-specific-optimizations",
    "title": "PyTorch Training and Inference Optimization Guide",
    "section": "",
    "text": "# Set optimal GPU settings\ntorch.backends.cudnn.benchmark = True  # For fixed input sizes\ntorch.backends.cudnn.deterministic = False  # For reproducibility (slower)\n\n# Use multiple GPUs\nif torch.cuda.device_count() &gt; 1:\n    model = nn.DataParallel(model)\n\n# Or use DistributedDataParallel for better performance\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nmodel = DDP(model, device_ids=[local_rank])\n\n\n\n# Set number of threads\ntorch.set_num_threads(4)\n\n# Use Intel MKL-DNN optimizations\ntorch.backends.mkldnn.enabled = True\n\n\n\n# Use Metal Performance Shaders on Apple Silicon\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n    model = model.to(device)"
  },
  {
    "objectID": "posts/pytorch-optimizations/index.html#profiling-and-debugging",
    "href": "posts/pytorch-optimizations/index.html#profiling-and-debugging",
    "title": "PyTorch Training and Inference Optimization Guide",
    "section": "",
    "text": "from torch.profiler import profile, record_function, ProfilerActivity\n\nwith profile(\n    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n    record_shapes=True,\n    profile_memory=True,\n    with_stack=True\n) as prof:\n    for batch in train_loader:\n        with record_function(\"forward\"):\n            outputs = model(inputs)\n        with record_function(\"backward\"):\n            loss.backward()\n        with record_function(\"optimizer\"):\n            optimizer.step()\n\n# Save trace for tensorboard\nprof.export_chrome_trace(\"trace.json\")\n\n\n\n# Profile memory usage\nwith profile(profile_memory=True) as prof:\n    model(inputs)\n\nprint(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))\n\n\n\nimport time\n\ndef benchmark_model(model, input_tensor, num_runs=100):\n    model.eval()\n    \n    # Warmup\n    with torch.no_grad():\n        for _ in range(10):\n            _ = model(input_tensor)\n    \n    # Benchmark\n    torch.cuda.synchronize()\n    start_time = time.time()\n    \n    with torch.no_grad():\n        for _ in range(num_runs):\n            _ = model(input_tensor)\n    \n    torch.cuda.synchronize()\n    end_time = time.time()\n    \n    avg_time = (end_time - start_time) / num_runs\n    print(f\"Average inference time: {avg_time*1000:.2f} ms\")"
  },
  {
    "objectID": "posts/pytorch-optimizations/index.html#best-practices-summary",
    "href": "posts/pytorch-optimizations/index.html#best-practices-summary",
    "title": "PyTorch Training and Inference Optimization Guide",
    "section": "",
    "text": "Always profile first - Identify bottlenecks before optimizing\nUse mixed precision - Significant speedup with minimal accuracy loss\nOptimize data loading - Use multiple workers and pin memory\nBatch operations - Avoid loops over individual samples\nModel compilation - Use torch.compile() for PyTorch 2.0+\nMemory management - Monitor and optimize memory usage\nHardware utilization - Use all available compute resources\nQuantization for inference - Reduce model size and increase speed\nTorchScript for production - Better performance and deployment options\nRegular checkpointing - Save training progress and enable resumption"
  },
  {
    "objectID": "posts/pytorch-optimizations/index.html#common-pitfalls-to-avoid",
    "href": "posts/pytorch-optimizations/index.html#common-pitfalls-to-avoid",
    "title": "PyTorch Training and Inference Optimization Guide",
    "section": "",
    "text": "Moving tensors between CPU and GPU unnecessarily\nUsing small batch sizes that underutilize hardware\nNot using torch.no_grad() during inference\nCreating tensors in loops instead of batching\nNot clearing variables and calling torch.cuda.empty_cache()\nUsing synchronous operations when asynchronous would work\nNot leveraging built-in optimized functions"
  },
  {
    "objectID": "posts/influence-selection/index.html",
    "href": "posts/influence-selection/index.html",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Active learning is a machine learning paradigm where the algorithm can interactively query an oracle (typically a human annotator) to label new data points. The key idea is to select the most informative samples to be labeled, reducing the overall labeling effort while maintaining or improving model performance. This guide focuses on influence selection methods used in active learning strategies.\n\n\n\n\nFundamentals of Active Learning\nInfluence Selection Strategies\nUncertainty-Based Methods\nDiversity-Based Methods\nExpected Model Change\nExpected Error Reduction\nInfluence Functions\nQuery-by-Committee\nImplementation Considerations\nEvaluation Metrics\nPractical Examples\nAdvanced Topics\n\n\n\n\n\n\nThe typical active learning process follows these steps:\n\nStart with a small labeled dataset and a large unlabeled pool\nTrain an initial model on the labeled data\nApply an influence selection strategy to choose informative samples from the unlabeled pool\nGet annotations for the selected samples\nAdd the newly labeled samples to the training set\nRetrain the model and repeat steps 3-6 until a stopping condition is met\n\n\n\n\n\nPool-based: The learner has access to a pool of unlabeled data and selects the most informative samples\nStream-based: Samples arrive sequentially, and the learner must decide on-the-fly whether to request labels\n\n\n\n\n\nInfluence selection is about identifying which unlabeled samples would be most beneficial to label next. Here are the main strategies:\n\n\n\nThese methods select samples that the model is most uncertain about.\n\n\n\ndef least_confidence(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    \n    # Get the confidence values for the most probable class\n    confidences = np.max(probabilities, axis=1)\n    \n    # Select the k samples with the lowest confidence\n    least_confident_indices = np.argsort(confidences)[:k]\n    \n    return unlabeled_pool[least_confident_indices]\n\n\n\n\nSelects samples with the smallest margin between the two most likely classes:\n\ndef margin_sampling(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    \n    # Sort the probabilities in descending order\n    sorted_probs = np.sort(probabilities, axis=1)[:, ::-1]\n    \n    # Calculate the margin between the first and second most probable classes\n    margins = sorted_probs[:, 0] - sorted_probs[:, 1]\n    \n    # Select the k samples with the smallest margins\n    smallest_margin_indices = np.argsort(margins)[:k]\n    \n    return unlabeled_pool[smallest_margin_indices]\n\n\n\n\nSelects samples with the highest predictive entropy:\n\ndef entropy_sampling(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    \n    # Calculate entropy for each sample\n    entropies = -np.sum(probabilities * np.log(probabilities + 1e-10), axis=1)\n    \n    # Select the k samples with the highest entropy\n    highest_entropy_indices = np.argsort(entropies)[::-1][:k]\n    \n    return unlabeled_pool[highest_entropy_indices]\n\n\n\n\nFor Bayesian models, BALD selects samples that maximize the mutual information between predictions and model parameters:\n\ndef bald_sampling(bayesian_model, unlabeled_pool, k, n_samples=100):\n    # Get multiple predictions by sampling from the model's posterior\n    probs_samples = []\n    for _ in range(n_samples):\n        probs = bayesian_model.predict_proba(unlabeled_pool)\n        probs_samples.append(probs)\n    \n    # Stack into a 3D array: (samples, data points, classes)\n    probs_samples = np.stack(probs_samples)\n    \n    # Calculate the average probability across all samples\n    mean_probs = np.mean(probs_samples, axis=0)\n    \n    # Calculate the entropy of the average prediction\n    entropy_mean = -np.sum(mean_probs * np.log(mean_probs + 1e-10), axis=1)\n    \n    # Calculate the average entropy across all samples\n    entropy_samples = -np.sum(probs_samples * np.log(probs_samples + 1e-10), axis=2)\n    mean_entropy = np.mean(entropy_samples, axis=0)\n    \n    # Mutual information = entropy of the mean - mean of entropies\n    bald_scores = entropy_mean - mean_entropy\n    \n    # Select the k samples with the highest BALD scores\n    highest_bald_indices = np.argsort(bald_scores)[::-1][:k]\n    \n    return unlabeled_pool[highest_bald_indices]\n\n\n\n\n\nThese methods aim to select a diverse set of examples to ensure broad coverage of the input space.\n\n\n\ndef clustering_based_sampling(unlabeled_pool, k, n_clusters=None):\n    if n_clusters is None:\n        n_clusters = k\n    \n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(unlabeled_pool)\n    \n    # Get the cluster centers and distances to each point\n    centers = kmeans.cluster_centers_\n    distances = kmeans.transform(unlabeled_pool)  # Distance to each cluster center\n    \n    # Select one sample from each cluster (closest to the center)\n    selected_indices = []\n    for i in range(n_clusters):\n        # Get the samples in this cluster\n        cluster_samples = np.where(kmeans.labels_ == i)[0]\n        \n        # Find the sample closest to the center\n        closest_sample = cluster_samples[np.argmin(distances[cluster_samples, i])]\n        selected_indices.append(closest_sample)\n    \n    # If we need more samples than clusters, fill with the most uncertain samples\n    if k &gt; n_clusters:\n        # Implementation depends on uncertainty measure\n        pass\n    \n    return unlabeled_pool[selected_indices[:k]]\n\n\n\n\nThe core-set approach aims to select a subset of data that best represents the whole dataset:\n\ndef core_set_sampling(labeled_pool, unlabeled_pool, k):\n    # Combine labeled and unlabeled data for distance calculations\n    all_data = np.vstack((labeled_pool, unlabeled_pool))\n    \n    # Compute pairwise distances\n    distances = pairwise_distances(all_data)\n    \n    # Split distances into labeled-unlabeled and unlabeled-unlabeled\n    n_labeled = labeled_pool.shape[0]\n    dist_labeled_unlabeled = distances[:n_labeled, n_labeled:]\n    \n    # For each unlabeled sample, find the minimum distance to any labeled sample\n    min_distances = np.min(dist_labeled_unlabeled, axis=0)\n    \n    # Select the k samples with the largest minimum distances\n    farthest_indices = np.argsort(min_distances)[::-1][:k]\n    \n    return unlabeled_pool[farthest_indices]\n\n\n\n\n\nThe Expected Model Change (EMC) method selects samples that would cause the greatest change in the model if they were labeled:\n\ndef expected_model_change(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    n_classes = probabilities.shape[1]\n    \n    # Calculate expected gradient length for each sample\n    expected_changes = []\n    for i, x in enumerate(unlabeled_pool):\n        # Calculate expected gradient length across all possible labels\n        change = 0\n        for c in range(n_classes):\n            # For each possible class, calculate the gradient if this was the true label\n            x_expanded = x.reshape(1, -1)\n            # Here we would compute the gradient of the model with respect to the sample\n            # For simplicity, we use a placeholder\n            gradient = compute_gradient(model, x_expanded, c)\n            norm_gradient = np.linalg.norm(gradient)\n            \n            # Weight by the probability of this class\n            change += probabilities[i, c] * norm_gradient\n        \n        expected_changes.append(change)\n    \n    # Select the k samples with the highest expected change\n    highest_change_indices = np.argsort(expected_changes)[::-1][:k]\n    \n    return unlabeled_pool[highest_change_indices]\n\nNote: The compute_gradient function would need to be implemented based on the specific model being used.\n\n\n\nThe Expected Error Reduction method selects samples that, when labeled, would minimally reduce the modelâ€™s expected error:\n\ndef expected_error_reduction(model, unlabeled_pool, unlabeled_pool_remaining, k):\n    # Predict probabilities for all remaining unlabeled data\n    current_probs = model.predict_proba(unlabeled_pool_remaining)\n    current_entropy = -np.sum(current_probs * np.log(current_probs + 1e-10), axis=1)\n    \n    expected_error_reductions = []\n    \n    # For each sample in the unlabeled pool we're considering\n    for i, x in enumerate(unlabeled_pool):\n        # Predict probabilities for this sample\n        probs = model.predict_proba(x.reshape(1, -1))[0]\n        \n        # Calculate the expected error reduction for each possible label\n        error_reduction = 0\n        for c in range(len(probs)):\n            # Create a hypothetical new model with this labeled sample\n            # For simplicity, we use a placeholder function\n            hypothetical_model = train_with_additional_sample(model, x, c)\n            \n            # Get new probabilities with this model\n            new_probs = hypothetical_model.predict_proba(unlabeled_pool_remaining)\n            new_entropy = -np.sum(new_probs * np.log(new_probs + 1e-10), axis=1)\n            \n            # Expected entropy reduction\n            reduction = np.sum(current_entropy - new_entropy)\n            \n            # Weight by the probability of this class\n            error_reduction += probs[c] * reduction\n        \n        expected_error_reductions.append(error_reduction)\n    \n    # Select the k samples with the highest expected error reduction\n    highest_reduction_indices = np.argsort(expected_error_reductions)[::-1][:k]\n    \n    return unlabeled_pool[highest_reduction_indices]\n\nNote: The train_with_additional_sample function would need to be implemented based on the specific model being used.\n\n\n\nInfluence functions approximate the effect of adding or removing a training example without retraining the model:\n\ndef influence_function_sampling(model, unlabeled_pool, labeled_pool, k, labels):\n    influences = []\n    \n    # For each unlabeled sample\n    for x_u in unlabeled_pool:\n        # Calculate the influence of adding this sample to the training set\n        influence = calculate_influence(model, x_u, labeled_pool, labels)\n        influences.append(influence)\n    \n    # Select the k samples with the highest influence\n    highest_influence_indices = np.argsort(influences)[::-1][:k]\n    \n    return unlabeled_pool[highest_influence_indices]\n\nNote: The calculate_influence function would need to be implemented based on the specific model and influence metric being used.\n\n\n\nQuery-by-Committee (QBC) methods train multiple models (a committee) and select samples where they disagree:\n\ndef query_by_committee(committee_models, unlabeled_pool, k):\n    # Get predictions from all committee members\n    all_predictions = []\n    for model in committee_models:\n        preds = model.predict(unlabeled_pool)\n        all_predictions.append(preds)\n    \n    # Stack predictions into a 2D array (committee members, data points)\n    all_predictions = np.stack(all_predictions)\n    \n    # Calculate disagreement (e.g., using vote entropy)\n    disagreements = []\n    for i in range(unlabeled_pool.shape[0]):\n        # Count votes for each class\n        votes = np.bincount(all_predictions[:, i])\n        # Normalize to get probabilities\n        vote_probs = votes / len(committee_models)\n        # Calculate entropy\n        entropy = -np.sum(vote_probs * np.log2(vote_probs + 1e-10))\n        disagreements.append(entropy)\n    \n    # Select the k samples with the highest disagreement\n    highest_disagreement_indices = np.argsort(disagreements)[::-1][:k]\n    \n    return unlabeled_pool[highest_disagreement_indices]\n\n\n\n\n\n\nIn practice, itâ€™s often more efficient to select multiple samples at once. However, simply selecting the top-k samples may lead to redundancy. Consider using:\n\nGreedy Selection with Diversity: Select one sample at a time, then update the diversity metrics to avoid selecting similar samples.\n\n\ndef batch_selection_with_diversity(model, unlabeled_pool, k, lambda_diversity=0.5):\n    selected_indices = []\n    remaining_indices = list(range(len(unlabeled_pool)))\n    \n    # Calculate uncertainty scores for all samples\n    probabilities = model.predict_proba(unlabeled_pool)\n    entropies = -np.sum(probabilities * np.log(probabilities + 1e-10), axis=1)\n    \n    # Calculate distance matrix for diversity\n    distance_matrix = pairwise_distances(unlabeled_pool)\n    \n    for _ in range(k):\n        if not remaining_indices:\n            break\n        \n        scores = np.zeros(len(remaining_indices))\n        \n        # Calculate uncertainty scores\n        uncertainty_scores = entropies[remaining_indices]\n        \n        # Calculate diversity scores (if we have already selected some samples)\n        if selected_indices:\n            # For each remaining sample, calculate the minimum distance to any selected sample\n            diversity_scores = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n        else:\n            diversity_scores = np.zeros(len(remaining_indices))\n        \n        # Normalize scores\n        uncertainty_scores = (uncertainty_scores - np.min(uncertainty_scores)) / (np.max(uncertainty_scores) - np.min(uncertainty_scores) + 1e-10)\n        if selected_indices:\n            diversity_scores = (diversity_scores - np.min(diversity_scores)) / (np.max(diversity_scores) - np.min(diversity_scores) + 1e-10)\n        \n        # Combine scores\n        scores = (1 - lambda_diversity) * uncertainty_scores + lambda_diversity * diversity_scores\n        \n        # Select the sample with the highest score\n        best_idx = np.argmax(scores)\n        selected_idx = remaining_indices[best_idx]\n        \n        # Add to selected and remove from remaining\n        selected_indices.append(selected_idx)\n        remaining_indices.remove(selected_idx)\n    \n    return unlabeled_pool[selected_indices]\n\n\nSubmodular Function Maximization: Use a submodular function to ensure diversity in the selected batch.\n\n\n\n\nActive learning can inadvertently reinforce class imbalance. Consider:\n\nStratified Sampling: Ensure representation from all classes.\nHybrid Approaches: Combine uncertainty-based and density-based methods.\nDiversity Constraints: Explicitly enforce diversity in feature space.\n\n\n\n\nSome methods (like expected error reduction) can be computationally expensive. Consider:\n\nSubsample the Unlabeled Pool: Only consider a random subset for selection.\nPre-compute Embeddings: Use a fixed feature extractor to pre-compute embeddings.\nApproximate Methods: Use approximations for expensive operations.\n\n\n\n\n\n\n\nPlot model performance vs.Â number of labeled samples:\n\ndef plot_learning_curve(model_factory, X_train, y_train, X_test, y_test, \n                        active_learning_strategy, initial_size=10, \n                        batch_size=10, n_iterations=20):\n    # Initialize with a small labeled set\n    labeled_indices = np.random.choice(len(X_train), initial_size, replace=False)\n    unlabeled_indices = np.setdiff1d(np.arange(len(X_train)), labeled_indices)\n    \n    performance = []\n    \n    for i in range(n_iterations):\n        # Create a fresh model\n        model = model_factory()\n        \n        # Train on the currently labeled data\n        model.fit(X_train[labeled_indices], y_train[labeled_indices])\n        \n        # Evaluate on the test set\n        score = model.score(X_test, y_test)\n        performance.append((len(labeled_indices), score))\n        \n        # Select the next batch of samples\n        if len(unlabeled_indices) &gt; 0:\n            # Use the specified active learning strategy\n            selected_indices = active_learning_strategy(\n                model, X_train[unlabeled_indices], batch_size\n            )\n            \n            # Map back to original indices\n            selected_original_indices = unlabeled_indices[selected_indices]\n            \n            # Update labeled and unlabeled indices\n            labeled_indices = np.append(labeled_indices, selected_original_indices)\n            unlabeled_indices = np.setdiff1d(unlabeled_indices, selected_original_indices)\n    \n    # Plot the learning curve\n    counts, scores = zip(*performance)\n    plt.figure(figsize=(10, 6))\n    plt.plot(counts, scores, 'o-')\n    plt.xlabel('Number of labeled samples')\n    plt.ylabel('Model accuracy')\n    plt.title('Active Learning Performance')\n    plt.grid(True)\n    \n    return performance\n\n\n\n\nAlways compare your active learning strategy with random sampling as a baseline.\n\n\n\nCalculate how many annotations you saved compared to using the entire dataset.\n\n\n\n\n\n\n\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n# Load data\nmnist = fetch_openml('mnist_784', version=1, cache=True)\nX, y = mnist['data'], mnist['target']\n\n# Split into train and test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Initially, only a small portion is labeled\ninitial_size = 100\nlabeled_indices = np.random.choice(len(X_train), initial_size, replace=False)\nunlabeled_indices = np.setdiff1d(np.arange(len(X_train)), labeled_indices)\n\n# Tracking performance\nactive_learning_performance = []\nrandom_sampling_performance = []\n\n# Active learning loop\nfor i in range(10):  # 10 iterations\n    # Train a model on the currently labeled data\n    model = RandomForestClassifier(n_estimators=50, random_state=42)\n    model.fit(X_train.iloc[labeled_indices], y_train.iloc[labeled_indices])\n    \n    # Evaluate on the test set\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    active_learning_performance.append((len(labeled_indices), accuracy))\n    \n    print(f\"Iteration {i+1}: {len(labeled_indices)} labeled samples, \"\n          f\"accuracy: {accuracy:.4f}\")\n    \n    # Select 100 new samples using entropy sampling\n    if len(unlabeled_indices) &gt; 0:\n        # Predict probabilities for each unlabeled sample\n        probs = model.predict_proba(X_train.iloc[unlabeled_indices])\n        \n        # Calculate entropy\n        entropies = -np.sum(probs * np.log(probs + 1e-10), axis=1)\n        \n        # Select samples with the highest entropy\n        top_indices = np.argsort(entropies)[::-1][:100]\n        \n        # Update labeled and unlabeled indices\n        selected_indices = unlabeled_indices[top_indices]\n        labeled_indices = np.append(labeled_indices, selected_indices)\n        unlabeled_indices = np.setdiff1d(unlabeled_indices, selected_indices)\n\n# Plot learning curve\ncounts, scores = zip(*active_learning_performance)\nplt.figure(figsize=(10, 6))\nplt.plot(counts, scores, 'o-', label='Active Learning')\nplt.xlabel('Number of labeled samples')\nplt.ylabel('Model accuracy')\nplt.title('Active Learning Performance')\nplt.grid(True)\nplt.legend()\nplt.show()\n\nIteration 1: 100 labeled samples, accuracy: 0.6917\nIteration 2: 200 labeled samples, accuracy: 0.7290\nIteration 3: 300 labeled samples, accuracy: 0.7716\nIteration 4: 400 labeled samples, accuracy: 0.7817\nIteration 5: 500 labeled samples, accuracy: 0.8239\nIteration 6: 600 labeled samples, accuracy: 0.8227\nIteration 7: 700 labeled samples, accuracy: 0.8282\nIteration 8: 800 labeled samples, accuracy: 0.8435\nIteration 9: 900 labeled samples, accuracy: 0.8454\nIteration 10: 1000 labeled samples, accuracy: 0.8549\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.datasets import fetch_20newsgroups\n\n# Load data\ncategories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\ntwenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\ntwenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n\n# Feature extraction\nvectorizer = TfidfVectorizer(stop_words='english')\nX_train = vectorizer.fit_transform(twenty_train.data)\nX_test = vectorizer.transform(twenty_test.data)\ny_train = twenty_train.target\ny_test = twenty_test.target\n\n# Initially, only a small portion is labeled\ninitial_size = 20\nlabeled_indices = np.random.choice(len(X_train.toarray()), initial_size, replace=False)\nunlabeled_indices = np.setdiff1d(np.arange(len(X_train.toarray())), labeled_indices)\n\n# Create a committee of models\nmodels = [\n    ('nb', MultinomialNB()),\n    ('svm', SVC(kernel='linear', probability=True)),\n    ('svm2', SVC(kernel='rbf', probability=True))\n]\n\n# Active learning loop\nfor i in range(10):  # 10 iterations\n    # Train each model on the currently labeled data\n    committee_models = []\n    for name, model in models:\n        model.fit(X_train[labeled_indices], y_train[labeled_indices])\n        committee_models.append(model)\n    \n    # Evaluate using the VotingClassifier\n    voting_clf = VotingClassifier(estimators=models, voting='soft')\n    voting_clf.fit(X_train[labeled_indices], y_train[labeled_indices])\n    \n    accuracy = voting_clf.score(X_test, y_test)\n    print(f\"Iteration {i+1}: {len(labeled_indices)} labeled samples, \"\n          f\"accuracy: {accuracy:.4f}\")\n    \n    # Select 10 new samples using Query-by-Committee\n    if len(unlabeled_indices) &gt; 0:\n        # Get predictions from all committee members\n        all_predictions = []\n        for model in committee_models:\n            preds = model.predict(X_train[unlabeled_indices])\n            all_predictions.append(preds)\n        \n        # Calculate vote entropy\n        vote_entropies = []\n        all_predictions = np.array(all_predictions)\n        for i in range(len(unlabeled_indices)):\n            # Count votes for each class\n            votes = np.bincount(all_predictions[:, i], minlength=len(categories))\n            # Normalize to get probabilities\n            vote_probs = votes / len(committee_models)\n            # Calculate entropy\n            entropy = -np.sum(vote_probs * np.log2(vote_probs + 1e-10))\n            vote_entropies.append(entropy)\n        \n        # Select samples with the highest vote entropy\n        top_indices = np.argsort(vote_entropies)[::-1][:10]\n        \n        # Update labeled and unlabeled indices\n        selected_indices = unlabeled_indices[top_indices]\n        labeled_indices = np.append(labeled_indices, selected_indices)\n        unlabeled_indices = np.setdiff1d(unlabeled_indices, selected_indices)\n\nIteration 1: 20 labeled samples, accuracy: 0.2636\nIteration 2: 30 labeled samples, accuracy: 0.3442\nIteration 3: 40 labeled samples, accuracy: 0.3435\nIteration 4: 50 labeled samples, accuracy: 0.4634\nIteration 5: 60 labeled samples, accuracy: 0.5386\nIteration 6: 70 labeled samples, accuracy: 0.5499\nIteration 7: 80 labeled samples, accuracy: 0.6119\nIteration 8: 90 labeled samples, accuracy: 0.6784\nIteration 9: 100 labeled samples, accuracy: 0.7583\nIteration 10: 110 labeled samples, accuracy: 0.7730\n\n\n\n\n\n\n\n\nCombining transfer learning with active learning can be powerful:\n\nUse pre-trained models as feature extractors.\nApply active learning on the feature space.\nFine-tune the model on the selected samples.\n\n\n\n\nSpecial considerations for deep learning models:\n\nUncertainty Estimation: Use dropout or ensemble methods for better uncertainty estimation.\nBatch Normalization: Be careful with batch normalization layers when retraining.\nData Augmentation: Apply data augmentation to increase the effective size of the labeled pool.\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Subset\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\n# Define a simple CNN\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x, dropout=True):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        if dropout:\n            x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        if dropout:\n            x = self.dropout2(x)\n        x = self.fc2(x)\n        return x\n\n# MC Dropout for uncertainty estimation\ndef mc_dropout_uncertainty(model, data_loader, n_samples=10):\n    model.eval()\n    all_probs = []\n    \n    with torch.no_grad():\n        for _ in range(n_samples):\n            batch_probs = []\n            for data, _ in data_loader:\n                output = model(data, dropout=True)\n                probs = F.softmax(output, dim=1)\n                batch_probs.append(probs)\n            \n            # Concatenate batch probabilities\n            all_probs.append(torch.cat(batch_probs))\n    \n    # Stack along a new dimension\n    all_probs = torch.stack(all_probs)\n    \n    # Calculate the mean probabilities\n    mean_probs = torch.mean(all_probs, dim=0)\n    \n    # Calculate entropy of the mean prediction\n    entropy = -torch.sum(mean_probs * torch.log(mean_probs + 1e-10), dim=1)\n    \n    return entropy.numpy()\n\n\n\n\nLeverage both labeled and unlabeled data during training:\n\nSelf-Training: Use model predictions on unlabeled data as pseudo-labels.\nCo-Training: Train multiple models and use their predictions to teach each other.\nConsistency Regularization: Enforce consistent predictions across different perturbations.\n\n\ndef semi_supervised_active_learning(labeled_X, labeled_y, unlabeled_X, model, confidence_threshold=0.95):\n    # Train model on labeled data\n    model.fit(labeled_X, labeled_y)\n    \n    # Predict on unlabeled data\n    probabilities = model.predict_proba(unlabeled_X)\n    max_probs = np.max(probabilities, axis=1)\n    \n    # Get high confidence predictions\n    confident_indices = np.where(max_probs &gt;= confidence_threshold)[0]\n    \n    # Get pseudo-labels for confident predictions\n    pseudo_labels = model.predict(unlabeled_X[confident_indices])\n    \n    # Train on combined dataset\n    combined_X = np.vstack([labeled_X, unlabeled_X[confident_indices]])\n    combined_y = np.concatenate([labeled_y, pseudo_labels])\n    \n    model.fit(combined_X, combined_y)\n    \n    return model, confident_indices\n\n\n\n\nWhen labeled data from the target domain is scarce, active learning can help select the most informative samples:\n\nDomain Discrepancy Measures: Select samples that minimize domain discrepancy.\nAdversarial Selection: Select samples that the domain discriminator is most uncertain about.\nFeature Space Alignment: Select samples that help align feature spaces between domains.\n\n\n\n\n\nAnnotation Interface Design: Make the annotation process intuitive and efficient.\nCognitive Load Management: Group similar samples to reduce cognitive switching.\nExplanations: Provide model explanations to help annotators understand the current modelâ€™s decisions.\nQuality Control: Incorporate mechanisms to detect and correct annotation errors.\n\n\n\n\n\nActive learning provides a powerful framework for efficiently building machine learning models with limited labeled data. By selecting the most informative samples for annotation, active learning can significantly reduce the labeling effort while maintaining high model performance.\nThe key to successful active learning is choosing the right influence selection strategy for your specific problem and data characteristics. Consider the following when designing your active learning pipeline:\n\nData Characteristics: Dense vs.Â sparse data, balanced vs.Â imbalanced classes, feature distribution.\nModel Type: Linear models, tree-based models, deep learning models.\nComputational Resources: Available memory and processing power.\nAnnotation Budget: Number of samples that can be labeled.\nTask Complexity: Classification vs.Â regression, number of classes, difficulty of the task.\n\nBy carefully considering these factors and implementing the appropriate influence selection methods, you can build high-performance models with minimal annotation effort."
  },
  {
    "objectID": "posts/influence-selection/index.html#introduction",
    "href": "posts/influence-selection/index.html#introduction",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Active learning is a machine learning paradigm where the algorithm can interactively query an oracle (typically a human annotator) to label new data points. The key idea is to select the most informative samples to be labeled, reducing the overall labeling effort while maintaining or improving model performance. This guide focuses on influence selection methods used in active learning strategies."
  },
  {
    "objectID": "posts/influence-selection/index.html#table-of-contents",
    "href": "posts/influence-selection/index.html#table-of-contents",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Fundamentals of Active Learning\nInfluence Selection Strategies\nUncertainty-Based Methods\nDiversity-Based Methods\nExpected Model Change\nExpected Error Reduction\nInfluence Functions\nQuery-by-Committee\nImplementation Considerations\nEvaluation Metrics\nPractical Examples\nAdvanced Topics"
  },
  {
    "objectID": "posts/influence-selection/index.html#fundamentals-of-active-learning",
    "href": "posts/influence-selection/index.html#fundamentals-of-active-learning",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "The typical active learning process follows these steps:\n\nStart with a small labeled dataset and a large unlabeled pool\nTrain an initial model on the labeled data\nApply an influence selection strategy to choose informative samples from the unlabeled pool\nGet annotations for the selected samples\nAdd the newly labeled samples to the training set\nRetrain the model and repeat steps 3-6 until a stopping condition is met\n\n\n\n\n\nPool-based: The learner has access to a pool of unlabeled data and selects the most informative samples\nStream-based: Samples arrive sequentially, and the learner must decide on-the-fly whether to request labels"
  },
  {
    "objectID": "posts/influence-selection/index.html#influence-selection-strategies",
    "href": "posts/influence-selection/index.html#influence-selection-strategies",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Influence selection is about identifying which unlabeled samples would be most beneficial to label next. Here are the main strategies:"
  },
  {
    "objectID": "posts/influence-selection/index.html#uncertainty-based-methods",
    "href": "posts/influence-selection/index.html#uncertainty-based-methods",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "These methods select samples that the model is most uncertain about.\n\n\n\ndef least_confidence(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    \n    # Get the confidence values for the most probable class\n    confidences = np.max(probabilities, axis=1)\n    \n    # Select the k samples with the lowest confidence\n    least_confident_indices = np.argsort(confidences)[:k]\n    \n    return unlabeled_pool[least_confident_indices]\n\n\n\n\nSelects samples with the smallest margin between the two most likely classes:\n\ndef margin_sampling(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    \n    # Sort the probabilities in descending order\n    sorted_probs = np.sort(probabilities, axis=1)[:, ::-1]\n    \n    # Calculate the margin between the first and second most probable classes\n    margins = sorted_probs[:, 0] - sorted_probs[:, 1]\n    \n    # Select the k samples with the smallest margins\n    smallest_margin_indices = np.argsort(margins)[:k]\n    \n    return unlabeled_pool[smallest_margin_indices]\n\n\n\n\nSelects samples with the highest predictive entropy:\n\ndef entropy_sampling(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    \n    # Calculate entropy for each sample\n    entropies = -np.sum(probabilities * np.log(probabilities + 1e-10), axis=1)\n    \n    # Select the k samples with the highest entropy\n    highest_entropy_indices = np.argsort(entropies)[::-1][:k]\n    \n    return unlabeled_pool[highest_entropy_indices]\n\n\n\n\nFor Bayesian models, BALD selects samples that maximize the mutual information between predictions and model parameters:\n\ndef bald_sampling(bayesian_model, unlabeled_pool, k, n_samples=100):\n    # Get multiple predictions by sampling from the model's posterior\n    probs_samples = []\n    for _ in range(n_samples):\n        probs = bayesian_model.predict_proba(unlabeled_pool)\n        probs_samples.append(probs)\n    \n    # Stack into a 3D array: (samples, data points, classes)\n    probs_samples = np.stack(probs_samples)\n    \n    # Calculate the average probability across all samples\n    mean_probs = np.mean(probs_samples, axis=0)\n    \n    # Calculate the entropy of the average prediction\n    entropy_mean = -np.sum(mean_probs * np.log(mean_probs + 1e-10), axis=1)\n    \n    # Calculate the average entropy across all samples\n    entropy_samples = -np.sum(probs_samples * np.log(probs_samples + 1e-10), axis=2)\n    mean_entropy = np.mean(entropy_samples, axis=0)\n    \n    # Mutual information = entropy of the mean - mean of entropies\n    bald_scores = entropy_mean - mean_entropy\n    \n    # Select the k samples with the highest BALD scores\n    highest_bald_indices = np.argsort(bald_scores)[::-1][:k]\n    \n    return unlabeled_pool[highest_bald_indices]"
  },
  {
    "objectID": "posts/influence-selection/index.html#diversity-based-methods",
    "href": "posts/influence-selection/index.html#diversity-based-methods",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "These methods aim to select a diverse set of examples to ensure broad coverage of the input space.\n\n\n\ndef clustering_based_sampling(unlabeled_pool, k, n_clusters=None):\n    if n_clusters is None:\n        n_clusters = k\n    \n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(unlabeled_pool)\n    \n    # Get the cluster centers and distances to each point\n    centers = kmeans.cluster_centers_\n    distances = kmeans.transform(unlabeled_pool)  # Distance to each cluster center\n    \n    # Select one sample from each cluster (closest to the center)\n    selected_indices = []\n    for i in range(n_clusters):\n        # Get the samples in this cluster\n        cluster_samples = np.where(kmeans.labels_ == i)[0]\n        \n        # Find the sample closest to the center\n        closest_sample = cluster_samples[np.argmin(distances[cluster_samples, i])]\n        selected_indices.append(closest_sample)\n    \n    # If we need more samples than clusters, fill with the most uncertain samples\n    if k &gt; n_clusters:\n        # Implementation depends on uncertainty measure\n        pass\n    \n    return unlabeled_pool[selected_indices[:k]]\n\n\n\n\nThe core-set approach aims to select a subset of data that best represents the whole dataset:\n\ndef core_set_sampling(labeled_pool, unlabeled_pool, k):\n    # Combine labeled and unlabeled data for distance calculations\n    all_data = np.vstack((labeled_pool, unlabeled_pool))\n    \n    # Compute pairwise distances\n    distances = pairwise_distances(all_data)\n    \n    # Split distances into labeled-unlabeled and unlabeled-unlabeled\n    n_labeled = labeled_pool.shape[0]\n    dist_labeled_unlabeled = distances[:n_labeled, n_labeled:]\n    \n    # For each unlabeled sample, find the minimum distance to any labeled sample\n    min_distances = np.min(dist_labeled_unlabeled, axis=0)\n    \n    # Select the k samples with the largest minimum distances\n    farthest_indices = np.argsort(min_distances)[::-1][:k]\n    \n    return unlabeled_pool[farthest_indices]"
  },
  {
    "objectID": "posts/influence-selection/index.html#expected-model-change",
    "href": "posts/influence-selection/index.html#expected-model-change",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "The Expected Model Change (EMC) method selects samples that would cause the greatest change in the model if they were labeled:\n\ndef expected_model_change(model, unlabeled_pool, k):\n    # Predict probabilities for each sample in the unlabeled pool\n    probabilities = model.predict_proba(unlabeled_pool)\n    n_classes = probabilities.shape[1]\n    \n    # Calculate expected gradient length for each sample\n    expected_changes = []\n    for i, x in enumerate(unlabeled_pool):\n        # Calculate expected gradient length across all possible labels\n        change = 0\n        for c in range(n_classes):\n            # For each possible class, calculate the gradient if this was the true label\n            x_expanded = x.reshape(1, -1)\n            # Here we would compute the gradient of the model with respect to the sample\n            # For simplicity, we use a placeholder\n            gradient = compute_gradient(model, x_expanded, c)\n            norm_gradient = np.linalg.norm(gradient)\n            \n            # Weight by the probability of this class\n            change += probabilities[i, c] * norm_gradient\n        \n        expected_changes.append(change)\n    \n    # Select the k samples with the highest expected change\n    highest_change_indices = np.argsort(expected_changes)[::-1][:k]\n    \n    return unlabeled_pool[highest_change_indices]\n\nNote: The compute_gradient function would need to be implemented based on the specific model being used."
  },
  {
    "objectID": "posts/influence-selection/index.html#expected-error-reduction",
    "href": "posts/influence-selection/index.html#expected-error-reduction",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "The Expected Error Reduction method selects samples that, when labeled, would minimally reduce the modelâ€™s expected error:\n\ndef expected_error_reduction(model, unlabeled_pool, unlabeled_pool_remaining, k):\n    # Predict probabilities for all remaining unlabeled data\n    current_probs = model.predict_proba(unlabeled_pool_remaining)\n    current_entropy = -np.sum(current_probs * np.log(current_probs + 1e-10), axis=1)\n    \n    expected_error_reductions = []\n    \n    # For each sample in the unlabeled pool we're considering\n    for i, x in enumerate(unlabeled_pool):\n        # Predict probabilities for this sample\n        probs = model.predict_proba(x.reshape(1, -1))[0]\n        \n        # Calculate the expected error reduction for each possible label\n        error_reduction = 0\n        for c in range(len(probs)):\n            # Create a hypothetical new model with this labeled sample\n            # For simplicity, we use a placeholder function\n            hypothetical_model = train_with_additional_sample(model, x, c)\n            \n            # Get new probabilities with this model\n            new_probs = hypothetical_model.predict_proba(unlabeled_pool_remaining)\n            new_entropy = -np.sum(new_probs * np.log(new_probs + 1e-10), axis=1)\n            \n            # Expected entropy reduction\n            reduction = np.sum(current_entropy - new_entropy)\n            \n            # Weight by the probability of this class\n            error_reduction += probs[c] * reduction\n        \n        expected_error_reductions.append(error_reduction)\n    \n    # Select the k samples with the highest expected error reduction\n    highest_reduction_indices = np.argsort(expected_error_reductions)[::-1][:k]\n    \n    return unlabeled_pool[highest_reduction_indices]\n\nNote: The train_with_additional_sample function would need to be implemented based on the specific model being used."
  },
  {
    "objectID": "posts/influence-selection/index.html#influence-functions",
    "href": "posts/influence-selection/index.html#influence-functions",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Influence functions approximate the effect of adding or removing a training example without retraining the model:\n\ndef influence_function_sampling(model, unlabeled_pool, labeled_pool, k, labels):\n    influences = []\n    \n    # For each unlabeled sample\n    for x_u in unlabeled_pool:\n        # Calculate the influence of adding this sample to the training set\n        influence = calculate_influence(model, x_u, labeled_pool, labels)\n        influences.append(influence)\n    \n    # Select the k samples with the highest influence\n    highest_influence_indices = np.argsort(influences)[::-1][:k]\n    \n    return unlabeled_pool[highest_influence_indices]\n\nNote: The calculate_influence function would need to be implemented based on the specific model and influence metric being used."
  },
  {
    "objectID": "posts/influence-selection/index.html#query-by-committee",
    "href": "posts/influence-selection/index.html#query-by-committee",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Query-by-Committee (QBC) methods train multiple models (a committee) and select samples where they disagree:\n\ndef query_by_committee(committee_models, unlabeled_pool, k):\n    # Get predictions from all committee members\n    all_predictions = []\n    for model in committee_models:\n        preds = model.predict(unlabeled_pool)\n        all_predictions.append(preds)\n    \n    # Stack predictions into a 2D array (committee members, data points)\n    all_predictions = np.stack(all_predictions)\n    \n    # Calculate disagreement (e.g., using vote entropy)\n    disagreements = []\n    for i in range(unlabeled_pool.shape[0]):\n        # Count votes for each class\n        votes = np.bincount(all_predictions[:, i])\n        # Normalize to get probabilities\n        vote_probs = votes / len(committee_models)\n        # Calculate entropy\n        entropy = -np.sum(vote_probs * np.log2(vote_probs + 1e-10))\n        disagreements.append(entropy)\n    \n    # Select the k samples with the highest disagreement\n    highest_disagreement_indices = np.argsort(disagreements)[::-1][:k]\n    \n    return unlabeled_pool[highest_disagreement_indices]"
  },
  {
    "objectID": "posts/influence-selection/index.html#implementation-considerations",
    "href": "posts/influence-selection/index.html#implementation-considerations",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "In practice, itâ€™s often more efficient to select multiple samples at once. However, simply selecting the top-k samples may lead to redundancy. Consider using:\n\nGreedy Selection with Diversity: Select one sample at a time, then update the diversity metrics to avoid selecting similar samples.\n\n\ndef batch_selection_with_diversity(model, unlabeled_pool, k, lambda_diversity=0.5):\n    selected_indices = []\n    remaining_indices = list(range(len(unlabeled_pool)))\n    \n    # Calculate uncertainty scores for all samples\n    probabilities = model.predict_proba(unlabeled_pool)\n    entropies = -np.sum(probabilities * np.log(probabilities + 1e-10), axis=1)\n    \n    # Calculate distance matrix for diversity\n    distance_matrix = pairwise_distances(unlabeled_pool)\n    \n    for _ in range(k):\n        if not remaining_indices:\n            break\n        \n        scores = np.zeros(len(remaining_indices))\n        \n        # Calculate uncertainty scores\n        uncertainty_scores = entropies[remaining_indices]\n        \n        # Calculate diversity scores (if we have already selected some samples)\n        if selected_indices:\n            # For each remaining sample, calculate the minimum distance to any selected sample\n            diversity_scores = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n        else:\n            diversity_scores = np.zeros(len(remaining_indices))\n        \n        # Normalize scores\n        uncertainty_scores = (uncertainty_scores - np.min(uncertainty_scores)) / (np.max(uncertainty_scores) - np.min(uncertainty_scores) + 1e-10)\n        if selected_indices:\n            diversity_scores = (diversity_scores - np.min(diversity_scores)) / (np.max(diversity_scores) - np.min(diversity_scores) + 1e-10)\n        \n        # Combine scores\n        scores = (1 - lambda_diversity) * uncertainty_scores + lambda_diversity * diversity_scores\n        \n        # Select the sample with the highest score\n        best_idx = np.argmax(scores)\n        selected_idx = remaining_indices[best_idx]\n        \n        # Add to selected and remove from remaining\n        selected_indices.append(selected_idx)\n        remaining_indices.remove(selected_idx)\n    \n    return unlabeled_pool[selected_indices]\n\n\nSubmodular Function Maximization: Use a submodular function to ensure diversity in the selected batch.\n\n\n\n\nActive learning can inadvertently reinforce class imbalance. Consider:\n\nStratified Sampling: Ensure representation from all classes.\nHybrid Approaches: Combine uncertainty-based and density-based methods.\nDiversity Constraints: Explicitly enforce diversity in feature space.\n\n\n\n\nSome methods (like expected error reduction) can be computationally expensive. Consider:\n\nSubsample the Unlabeled Pool: Only consider a random subset for selection.\nPre-compute Embeddings: Use a fixed feature extractor to pre-compute embeddings.\nApproximate Methods: Use approximations for expensive operations."
  },
  {
    "objectID": "posts/influence-selection/index.html#evaluation-metrics",
    "href": "posts/influence-selection/index.html#evaluation-metrics",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Plot model performance vs.Â number of labeled samples:\n\ndef plot_learning_curve(model_factory, X_train, y_train, X_test, y_test, \n                        active_learning_strategy, initial_size=10, \n                        batch_size=10, n_iterations=20):\n    # Initialize with a small labeled set\n    labeled_indices = np.random.choice(len(X_train), initial_size, replace=False)\n    unlabeled_indices = np.setdiff1d(np.arange(len(X_train)), labeled_indices)\n    \n    performance = []\n    \n    for i in range(n_iterations):\n        # Create a fresh model\n        model = model_factory()\n        \n        # Train on the currently labeled data\n        model.fit(X_train[labeled_indices], y_train[labeled_indices])\n        \n        # Evaluate on the test set\n        score = model.score(X_test, y_test)\n        performance.append((len(labeled_indices), score))\n        \n        # Select the next batch of samples\n        if len(unlabeled_indices) &gt; 0:\n            # Use the specified active learning strategy\n            selected_indices = active_learning_strategy(\n                model, X_train[unlabeled_indices], batch_size\n            )\n            \n            # Map back to original indices\n            selected_original_indices = unlabeled_indices[selected_indices]\n            \n            # Update labeled and unlabeled indices\n            labeled_indices = np.append(labeled_indices, selected_original_indices)\n            unlabeled_indices = np.setdiff1d(unlabeled_indices, selected_original_indices)\n    \n    # Plot the learning curve\n    counts, scores = zip(*performance)\n    plt.figure(figsize=(10, 6))\n    plt.plot(counts, scores, 'o-')\n    plt.xlabel('Number of labeled samples')\n    plt.ylabel('Model accuracy')\n    plt.title('Active Learning Performance')\n    plt.grid(True)\n    \n    return performance\n\n\n\n\nAlways compare your active learning strategy with random sampling as a baseline.\n\n\n\nCalculate how many annotations you saved compared to using the entire dataset."
  },
  {
    "objectID": "posts/influence-selection/index.html#practical-examples",
    "href": "posts/influence-selection/index.html#practical-examples",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "import numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n# Load data\nmnist = fetch_openml('mnist_784', version=1, cache=True)\nX, y = mnist['data'], mnist['target']\n\n# Split into train and test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Initially, only a small portion is labeled\ninitial_size = 100\nlabeled_indices = np.random.choice(len(X_train), initial_size, replace=False)\nunlabeled_indices = np.setdiff1d(np.arange(len(X_train)), labeled_indices)\n\n# Tracking performance\nactive_learning_performance = []\nrandom_sampling_performance = []\n\n# Active learning loop\nfor i in range(10):  # 10 iterations\n    # Train a model on the currently labeled data\n    model = RandomForestClassifier(n_estimators=50, random_state=42)\n    model.fit(X_train.iloc[labeled_indices], y_train.iloc[labeled_indices])\n    \n    # Evaluate on the test set\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    active_learning_performance.append((len(labeled_indices), accuracy))\n    \n    print(f\"Iteration {i+1}: {len(labeled_indices)} labeled samples, \"\n          f\"accuracy: {accuracy:.4f}\")\n    \n    # Select 100 new samples using entropy sampling\n    if len(unlabeled_indices) &gt; 0:\n        # Predict probabilities for each unlabeled sample\n        probs = model.predict_proba(X_train.iloc[unlabeled_indices])\n        \n        # Calculate entropy\n        entropies = -np.sum(probs * np.log(probs + 1e-10), axis=1)\n        \n        # Select samples with the highest entropy\n        top_indices = np.argsort(entropies)[::-1][:100]\n        \n        # Update labeled and unlabeled indices\n        selected_indices = unlabeled_indices[top_indices]\n        labeled_indices = np.append(labeled_indices, selected_indices)\n        unlabeled_indices = np.setdiff1d(unlabeled_indices, selected_indices)\n\n# Plot learning curve\ncounts, scores = zip(*active_learning_performance)\nplt.figure(figsize=(10, 6))\nplt.plot(counts, scores, 'o-', label='Active Learning')\nplt.xlabel('Number of labeled samples')\nplt.ylabel('Model accuracy')\nplt.title('Active Learning Performance')\nplt.grid(True)\nplt.legend()\nplt.show()\n\nIteration 1: 100 labeled samples, accuracy: 0.6917\nIteration 2: 200 labeled samples, accuracy: 0.7290\nIteration 3: 300 labeled samples, accuracy: 0.7716\nIteration 4: 400 labeled samples, accuracy: 0.7817\nIteration 5: 500 labeled samples, accuracy: 0.8239\nIteration 6: 600 labeled samples, accuracy: 0.8227\nIteration 7: 700 labeled samples, accuracy: 0.8282\nIteration 8: 800 labeled samples, accuracy: 0.8435\nIteration 9: 900 labeled samples, accuracy: 0.8454\nIteration 10: 1000 labeled samples, accuracy: 0.8549\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.datasets import fetch_20newsgroups\n\n# Load data\ncategories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\ntwenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\ntwenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n\n# Feature extraction\nvectorizer = TfidfVectorizer(stop_words='english')\nX_train = vectorizer.fit_transform(twenty_train.data)\nX_test = vectorizer.transform(twenty_test.data)\ny_train = twenty_train.target\ny_test = twenty_test.target\n\n# Initially, only a small portion is labeled\ninitial_size = 20\nlabeled_indices = np.random.choice(len(X_train.toarray()), initial_size, replace=False)\nunlabeled_indices = np.setdiff1d(np.arange(len(X_train.toarray())), labeled_indices)\n\n# Create a committee of models\nmodels = [\n    ('nb', MultinomialNB()),\n    ('svm', SVC(kernel='linear', probability=True)),\n    ('svm2', SVC(kernel='rbf', probability=True))\n]\n\n# Active learning loop\nfor i in range(10):  # 10 iterations\n    # Train each model on the currently labeled data\n    committee_models = []\n    for name, model in models:\n        model.fit(X_train[labeled_indices], y_train[labeled_indices])\n        committee_models.append(model)\n    \n    # Evaluate using the VotingClassifier\n    voting_clf = VotingClassifier(estimators=models, voting='soft')\n    voting_clf.fit(X_train[labeled_indices], y_train[labeled_indices])\n    \n    accuracy = voting_clf.score(X_test, y_test)\n    print(f\"Iteration {i+1}: {len(labeled_indices)} labeled samples, \"\n          f\"accuracy: {accuracy:.4f}\")\n    \n    # Select 10 new samples using Query-by-Committee\n    if len(unlabeled_indices) &gt; 0:\n        # Get predictions from all committee members\n        all_predictions = []\n        for model in committee_models:\n            preds = model.predict(X_train[unlabeled_indices])\n            all_predictions.append(preds)\n        \n        # Calculate vote entropy\n        vote_entropies = []\n        all_predictions = np.array(all_predictions)\n        for i in range(len(unlabeled_indices)):\n            # Count votes for each class\n            votes = np.bincount(all_predictions[:, i], minlength=len(categories))\n            # Normalize to get probabilities\n            vote_probs = votes / len(committee_models)\n            # Calculate entropy\n            entropy = -np.sum(vote_probs * np.log2(vote_probs + 1e-10))\n            vote_entropies.append(entropy)\n        \n        # Select samples with the highest vote entropy\n        top_indices = np.argsort(vote_entropies)[::-1][:10]\n        \n        # Update labeled and unlabeled indices\n        selected_indices = unlabeled_indices[top_indices]\n        labeled_indices = np.append(labeled_indices, selected_indices)\n        unlabeled_indices = np.setdiff1d(unlabeled_indices, selected_indices)\n\nIteration 1: 20 labeled samples, accuracy: 0.2636\nIteration 2: 30 labeled samples, accuracy: 0.3442\nIteration 3: 40 labeled samples, accuracy: 0.3435\nIteration 4: 50 labeled samples, accuracy: 0.4634\nIteration 5: 60 labeled samples, accuracy: 0.5386\nIteration 6: 70 labeled samples, accuracy: 0.5499\nIteration 7: 80 labeled samples, accuracy: 0.6119\nIteration 8: 90 labeled samples, accuracy: 0.6784\nIteration 9: 100 labeled samples, accuracy: 0.7583\nIteration 10: 110 labeled samples, accuracy: 0.7730"
  },
  {
    "objectID": "posts/influence-selection/index.html#advanced-topics",
    "href": "posts/influence-selection/index.html#advanced-topics",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Combining transfer learning with active learning can be powerful:\n\nUse pre-trained models as feature extractors.\nApply active learning on the feature space.\nFine-tune the model on the selected samples.\n\n\n\n\nSpecial considerations for deep learning models:\n\nUncertainty Estimation: Use dropout or ensemble methods for better uncertainty estimation.\nBatch Normalization: Be careful with batch normalization layers when retraining.\nData Augmentation: Apply data augmentation to increase the effective size of the labeled pool.\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Subset\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\n# Define a simple CNN\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x, dropout=True):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        if dropout:\n            x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        if dropout:\n            x = self.dropout2(x)\n        x = self.fc2(x)\n        return x\n\n# MC Dropout for uncertainty estimation\ndef mc_dropout_uncertainty(model, data_loader, n_samples=10):\n    model.eval()\n    all_probs = []\n    \n    with torch.no_grad():\n        for _ in range(n_samples):\n            batch_probs = []\n            for data, _ in data_loader:\n                output = model(data, dropout=True)\n                probs = F.softmax(output, dim=1)\n                batch_probs.append(probs)\n            \n            # Concatenate batch probabilities\n            all_probs.append(torch.cat(batch_probs))\n    \n    # Stack along a new dimension\n    all_probs = torch.stack(all_probs)\n    \n    # Calculate the mean probabilities\n    mean_probs = torch.mean(all_probs, dim=0)\n    \n    # Calculate entropy of the mean prediction\n    entropy = -torch.sum(mean_probs * torch.log(mean_probs + 1e-10), dim=1)\n    \n    return entropy.numpy()\n\n\n\n\nLeverage both labeled and unlabeled data during training:\n\nSelf-Training: Use model predictions on unlabeled data as pseudo-labels.\nCo-Training: Train multiple models and use their predictions to teach each other.\nConsistency Regularization: Enforce consistent predictions across different perturbations.\n\n\ndef semi_supervised_active_learning(labeled_X, labeled_y, unlabeled_X, model, confidence_threshold=0.95):\n    # Train model on labeled data\n    model.fit(labeled_X, labeled_y)\n    \n    # Predict on unlabeled data\n    probabilities = model.predict_proba(unlabeled_X)\n    max_probs = np.max(probabilities, axis=1)\n    \n    # Get high confidence predictions\n    confident_indices = np.where(max_probs &gt;= confidence_threshold)[0]\n    \n    # Get pseudo-labels for confident predictions\n    pseudo_labels = model.predict(unlabeled_X[confident_indices])\n    \n    # Train on combined dataset\n    combined_X = np.vstack([labeled_X, unlabeled_X[confident_indices]])\n    combined_y = np.concatenate([labeled_y, pseudo_labels])\n    \n    model.fit(combined_X, combined_y)\n    \n    return model, confident_indices\n\n\n\n\nWhen labeled data from the target domain is scarce, active learning can help select the most informative samples:\n\nDomain Discrepancy Measures: Select samples that minimize domain discrepancy.\nAdversarial Selection: Select samples that the domain discriminator is most uncertain about.\nFeature Space Alignment: Select samples that help align feature spaces between domains.\n\n\n\n\n\nAnnotation Interface Design: Make the annotation process intuitive and efficient.\nCognitive Load Management: Group similar samples to reduce cognitive switching.\nExplanations: Provide model explanations to help annotators understand the current modelâ€™s decisions.\nQuality Control: Incorporate mechanisms to detect and correct annotation errors."
  },
  {
    "objectID": "posts/influence-selection/index.html#conclusion",
    "href": "posts/influence-selection/index.html#conclusion",
    "title": "Active Learning Influence Selection: A Comprehensive Guide",
    "section": "",
    "text": "Active learning provides a powerful framework for efficiently building machine learning models with limited labeled data. By selecting the most informative samples for annotation, active learning can significantly reduce the labeling effort while maintaining high model performance.\nThe key to successful active learning is choosing the right influence selection strategy for your specific problem and data characteristics. Consider the following when designing your active learning pipeline:\n\nData Characteristics: Dense vs.Â sparse data, balanced vs.Â imbalanced classes, feature distribution.\nModel Type: Linear models, tree-based models, deep learning models.\nComputational Resources: Available memory and processing power.\nAnnotation Budget: Number of samples that can be labeled.\nTask Complexity: Classification vs.Â regression, number of classes, difficulty of the task.\n\nBy carefully considering these factors and implementing the appropriate influence selection methods, you can build high-performance models with minimal annotation effort."
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html",
    "href": "posts/vision-transformers-explained/index.html",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "Vision Transformers (ViTs) represent a paradigm shift in computer vision, adapting the transformer architecture that revolutionized natural language processing for image classification and other visual tasks. Instead of relying on convolutional neural networks (CNNs), ViTs treat images as sequences of patches, applying the self-attention mechanism to understand spatial relationships and visual features.\n\n\n\nTraditional computer vision relied heavily on Convolutional Neural Networks (CNNs), which process images through layers of convolutions that detect local features like edges, textures, and patterns. While effective, CNNs have limitations in capturing long-range dependencies across an image due to their local receptive fields.\nTransformers, originally designed for language tasks, excel at modeling long-range dependencies through self-attention mechanisms. The key insight behind Vision Transformers was asking: â€œWhat if we could apply this powerful attention mechanism to images?â€\n\n\n\nThe fundamental innovation of ViTs lies in treating images as sequences of patches rather than pixel grids. Hereâ€™s how this transformation works:\n\n\n\nPatch Division: An input image (typically 224Ã—224 pixels) is divided into fixed-size patches (commonly 16Ã—16 pixels), resulting in a sequence of patches\nLinear Projection: Each patch is flattened into a vector and linearly projected to create patch embeddings\nPosition Encoding: Since transformers donâ€™t inherently understand spatial relationships, positional encodings are added to maintain spatial information\nClassification Token: A special learnable [CLS] token is prepended to the sequence, similar to BERTâ€™s approach\n\n\n\n\nFor an image of size HÃ—WÃ—C divided into patches of size PÃ—P:\n\nNumber of patches: N = (HÃ—W)/PÂ²\nEach patch becomes a vector of size PÂ²Ã—C\nAfter linear projection: embedding dimension D\n\n\n\n\n\n\n\nThe patch embedding layer converts image patches into token embeddings that the transformer can process. This involves:\n\nReshaping patches into vectors\nLinear transformation to desired embedding dimension\nAdding positional encodings\n\n\n\n\nThe core of ViT consists of standard transformer encoder blocks, each containing:\n\nMulti-Head Self-Attention (MSA): Allows patches to attend to all other patches\nLayer Normalization: Applied before both attention and MLP layers\nMulti-Layer Perceptron (MLP): Two-layer feedforward network with GELU activation\nResidual Connections: Skip connections around both attention and MLP blocks\n\n\n\n\nThe final component extracts the [CLS] tokenâ€™s representation and passes it through:\n\nLayer normalization\nLinear classifier to produce class predictions\n\n\n\n\n\nThe self-attention mechanism in ViTs operates differently from CNNs:\n\n\n\nEach patch can attend to every other patch in the image\nAttention weights reveal which parts of the image are most relevant for classification\nThis enables modeling of long-range spatial dependencies\n\n\n\n\nUnlike CNNs that build up receptive fields gradually, ViTs have global receptive fields from the first layer, allowing immediate access to information across the entire image.\n\n\n\n\n\n\nVision Transformers typically require large amounts of training data to perform well:\n\nPre-training: Often trained on large datasets like ImageNet-21k or JFT-300M\nFine-tuning: Then adapted to specific tasks with smaller datasets\nData Efficiency: ViTs can be less data-efficient than CNNs when training from scratch\n\n\n\n\n\nInitialization: Careful weight initialization is crucial\nLearning Rate: Often requires different learning rates for different components\nRegularization: Techniques like dropout and weight decay are important\nWarmup: Learning rate warmup is commonly used\n\n\n\n\n\n\n\n\nViT-B/16, ViT-L/16, ViT-H/14: Different model sizes with varying patch sizes\nDeiT (Data-efficient ViT): Improved training strategies for smaller datasets\nSwin Transformer: Hierarchical vision transformer with shifted windows\nCaiT: Class-Attention in Image Transformers with separate class attention\n\n\n\n\n\nHierarchical Processing: Multi-scale feature extraction\nLocal Attention: Restricting attention to local neighborhoods\nHybrid Models: Combining CNN features with transformer processing\n\n\n\n\n\n\n\n\nLong-range Dependencies: Natural ability to model global relationships\nInterpretability: Attention maps provide insights into model decisions\nScalability: Performance improves with larger models and datasets\nTransfer Learning: Excellent pre-trained representations\nArchitectural Simplicity: Unified architecture for various vision tasks\n\n\n\n\n\nState-of-the-art results on image classification\nStrong performance on object detection and segmentation when adapted\nExcellent transfer learning capabilities across domains\n\n\n\n\n\n\n\n\nData Hunger: Requires large datasets for optimal performance\nComputational Cost: High memory and compute requirements\nInductive Bias: Lacks CNNâ€™s built-in spatial inductive biases\nSmall Dataset Performance: Can underperform CNNs on limited data\n\n\n\n\n\nImproving data efficiency\nReducing computational requirements\nBetter integration of spatial inductive biases\nHybrid CNN-Transformer architectures\n\n\n\n\n\n\n\n\nObject Detection: DETR (Detection Transformer) applies transformers to detection\nSemantic Segmentation: Segmentation transformers for pixel-level predictions\nImage Generation: Vision transformers in generative models\nVideo Analysis: Extending to temporal sequences\n\n\n\n\n\nVision-Language Models: CLIP and similar models combining vision and text\nVisual Question Answering: Integrating visual and textual understanding\nImage Captioning: Generating descriptions from visual content\n\n\n\n\n\n\n\nChoose ViT variants based on:\n\nAvailable computational resources\nDataset size and characteristics\nRequired inference speed\nTarget accuracy requirements\n\n\n\n\n\nUse pre-trained models when possible\nApply appropriate data augmentation\nConsider knowledge distillation for smaller models\nMonitor for overfitting, especially on smaller datasets\n\n\n\n\n\nUse mixed precision training to reduce memory usage\nImplement gradient checkpointing for large models\nConsider model parallelism for very large architectures\nApply appropriate regularization techniques\n\n\n\n\n\n\n\n\nEfficiency Improvements: Making ViTs more computationally efficient\nArchitecture Search: Automated design of vision transformer architectures\nSelf-Supervised Learning: Reducing dependence on labeled data\nUnified Architectures: Single models handling multiple vision tasks\n\n\n\n\n\nReal-time vision applications\nMobile and edge deployment\nScientific imaging and medical applications\nAutonomous systems and robotics\n\n\n\n\n\nVision Transformers represent a fundamental shift in computer vision, demonstrating that the transformer architectureâ€™s success in NLP can extend to visual tasks. While they present challenges in terms of data requirements and computational cost, their ability to model long-range dependencies and achieve state-of-the-art performance makes them a crucial tool in modern computer vision.\nThe field continues to evolve rapidly, with ongoing research addressing current limitations while exploring new applications. As the technology matures, we can expect ViTs to become increasingly practical for a wider range of real-world applications, potentially reshaping how we approach visual understanding tasks.\nUnderstanding Vision Transformers is essential for anyone working in modern computer vision, as they represent not just a new model architecture, but a new way of thinking about how machines can understand and process visual information."
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#introduction",
    "href": "posts/vision-transformers-explained/index.html#introduction",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "Vision Transformers (ViTs) represent a paradigm shift in computer vision, adapting the transformer architecture that revolutionized natural language processing for image classification and other visual tasks. Instead of relying on convolutional neural networks (CNNs), ViTs treat images as sequences of patches, applying the self-attention mechanism to understand spatial relationships and visual features."
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#background-from-cnns-to-transformers",
    "href": "posts/vision-transformers-explained/index.html#background-from-cnns-to-transformers",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "Traditional computer vision relied heavily on Convolutional Neural Networks (CNNs), which process images through layers of convolutions that detect local features like edges, textures, and patterns. While effective, CNNs have limitations in capturing long-range dependencies across an image due to their local receptive fields.\nTransformers, originally designed for language tasks, excel at modeling long-range dependencies through self-attention mechanisms. The key insight behind Vision Transformers was asking: â€œWhat if we could apply this powerful attention mechanism to images?â€"
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#core-concept-images-as-sequences",
    "href": "posts/vision-transformers-explained/index.html#core-concept-images-as-sequences",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "The fundamental innovation of ViTs lies in treating images as sequences of patches rather than pixel grids. Hereâ€™s how this transformation works:\n\n\n\nPatch Division: An input image (typically 224Ã—224 pixels) is divided into fixed-size patches (commonly 16Ã—16 pixels), resulting in a sequence of patches\nLinear Projection: Each patch is flattened into a vector and linearly projected to create patch embeddings\nPosition Encoding: Since transformers donâ€™t inherently understand spatial relationships, positional encodings are added to maintain spatial information\nClassification Token: A special learnable [CLS] token is prepended to the sequence, similar to BERTâ€™s approach\n\n\n\n\nFor an image of size HÃ—WÃ—C divided into patches of size PÃ—P:\n\nNumber of patches: N = (HÃ—W)/PÂ²\nEach patch becomes a vector of size PÂ²Ã—C\nAfter linear projection: embedding dimension D"
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#architecture-components",
    "href": "posts/vision-transformers-explained/index.html#architecture-components",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "The patch embedding layer converts image patches into token embeddings that the transformer can process. This involves:\n\nReshaping patches into vectors\nLinear transformation to desired embedding dimension\nAdding positional encodings\n\n\n\n\nThe core of ViT consists of standard transformer encoder blocks, each containing:\n\nMulti-Head Self-Attention (MSA): Allows patches to attend to all other patches\nLayer Normalization: Applied before both attention and MLP layers\nMulti-Layer Perceptron (MLP): Two-layer feedforward network with GELU activation\nResidual Connections: Skip connections around both attention and MLP blocks\n\n\n\n\nThe final component extracts the [CLS] tokenâ€™s representation and passes it through:\n\nLayer normalization\nLinear classifier to produce class predictions"
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#self-attention-in-vision",
    "href": "posts/vision-transformers-explained/index.html#self-attention-in-vision",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "The self-attention mechanism in ViTs operates differently from CNNs:\n\n\n\nEach patch can attend to every other patch in the image\nAttention weights reveal which parts of the image are most relevant for classification\nThis enables modeling of long-range spatial dependencies\n\n\n\n\nUnlike CNNs that build up receptive fields gradually, ViTs have global receptive fields from the first layer, allowing immediate access to information across the entire image."
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#training-considerations",
    "href": "posts/vision-transformers-explained/index.html#training-considerations",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "Vision Transformers typically require large amounts of training data to perform well:\n\nPre-training: Often trained on large datasets like ImageNet-21k or JFT-300M\nFine-tuning: Then adapted to specific tasks with smaller datasets\nData Efficiency: ViTs can be less data-efficient than CNNs when training from scratch\n\n\n\n\n\nInitialization: Careful weight initialization is crucial\nLearning Rate: Often requires different learning rates for different components\nRegularization: Techniques like dropout and weight decay are important\nWarmup: Learning rate warmup is commonly used"
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#variants-and-improvements",
    "href": "posts/vision-transformers-explained/index.html#variants-and-improvements",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "ViT-B/16, ViT-L/16, ViT-H/14: Different model sizes with varying patch sizes\nDeiT (Data-efficient ViT): Improved training strategies for smaller datasets\nSwin Transformer: Hierarchical vision transformer with shifted windows\nCaiT: Class-Attention in Image Transformers with separate class attention\n\n\n\n\n\nHierarchical Processing: Multi-scale feature extraction\nLocal Attention: Restricting attention to local neighborhoods\nHybrid Models: Combining CNN features with transformer processing"
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#advantages-of-vision-transformers",
    "href": "posts/vision-transformers-explained/index.html#advantages-of-vision-transformers",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "Long-range Dependencies: Natural ability to model global relationships\nInterpretability: Attention maps provide insights into model decisions\nScalability: Performance improves with larger models and datasets\nTransfer Learning: Excellent pre-trained representations\nArchitectural Simplicity: Unified architecture for various vision tasks\n\n\n\n\n\nState-of-the-art results on image classification\nStrong performance on object detection and segmentation when adapted\nExcellent transfer learning capabilities across domains"
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#limitations-and-challenges",
    "href": "posts/vision-transformers-explained/index.html#limitations-and-challenges",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "Data Hunger: Requires large datasets for optimal performance\nComputational Cost: High memory and compute requirements\nInductive Bias: Lacks CNNâ€™s built-in spatial inductive biases\nSmall Dataset Performance: Can underperform CNNs on limited data\n\n\n\n\n\nImproving data efficiency\nReducing computational requirements\nBetter integration of spatial inductive biases\nHybrid CNN-Transformer architectures"
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#applications-beyond-classification",
    "href": "posts/vision-transformers-explained/index.html#applications-beyond-classification",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "Object Detection: DETR (Detection Transformer) applies transformers to detection\nSemantic Segmentation: Segmentation transformers for pixel-level predictions\nImage Generation: Vision transformers in generative models\nVideo Analysis: Extending to temporal sequences\n\n\n\n\n\nVision-Language Models: CLIP and similar models combining vision and text\nVisual Question Answering: Integrating visual and textual understanding\nImage Captioning: Generating descriptions from visual content"
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#implementation-considerations",
    "href": "posts/vision-transformers-explained/index.html#implementation-considerations",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "Choose ViT variants based on:\n\nAvailable computational resources\nDataset size and characteristics\nRequired inference speed\nTarget accuracy requirements\n\n\n\n\n\nUse pre-trained models when possible\nApply appropriate data augmentation\nConsider knowledge distillation for smaller models\nMonitor for overfitting, especially on smaller datasets\n\n\n\n\n\nUse mixed precision training to reduce memory usage\nImplement gradient checkpointing for large models\nConsider model parallelism for very large architectures\nApply appropriate regularization techniques"
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#future-directions",
    "href": "posts/vision-transformers-explained/index.html#future-directions",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "Efficiency Improvements: Making ViTs more computationally efficient\nArchitecture Search: Automated design of vision transformer architectures\nSelf-Supervised Learning: Reducing dependence on labeled data\nUnified Architectures: Single models handling multiple vision tasks\n\n\n\n\n\nReal-time vision applications\nMobile and edge deployment\nScientific imaging and medical applications\nAutonomous systems and robotics"
  },
  {
    "objectID": "posts/vision-transformers-explained/index.html#conclusion",
    "href": "posts/vision-transformers-explained/index.html#conclusion",
    "title": "Vision Transformers (ViT): A Simple Guide",
    "section": "",
    "text": "Vision Transformers represent a fundamental shift in computer vision, demonstrating that the transformer architectureâ€™s success in NLP can extend to visual tasks. While they present challenges in terms of data requirements and computational cost, their ability to model long-range dependencies and achieve state-of-the-art performance makes them a crucial tool in modern computer vision.\nThe field continues to evolve rapidly, with ongoing research addressing current limitations while exploring new applications. As the technology matures, we can expect ViTs to become increasingly practical for a wider range of real-world applications, potentially reshaping how we approach visual understanding tasks.\nUnderstanding Vision Transformers is essential for anyone working in modern computer vision, as they represent not just a new model architecture, but a new way of thinking about how machines can understand and process visual information."
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html",
    "href": "posts/albumentations-vs-torchvision/index.html",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "This guide compares two popular image augmentation libraries for PyTorch:\n\nTorchVision Transforms: Built-in PyTorch library for basic image transformations\nAlbumentations: Fast, flexible library with advanced augmentation techniques\n\n\n\n\n# TorchVision (comes with PyTorch)\npip install torch torchvision\n\n# Albumentations\npip install albumentations\n\n\n\n\nimport torch\nimport torchvision.transforms as T\nfrom torchvision.transforms import functional as TF\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nTorchVision\nAlbumentations\n\n\n\n\nInput Format\nPIL Image, Tensor\nNumPy array (OpenCV format)\n\n\nPerformance\nModerate\nFast (optimized)\n\n\nAugmentation Variety\nBasic to intermediate\nExtensive advanced options\n\n\nBounding Box Support\nLimited\nExcellent\n\n\nSegmentation Masks\nBasic\nAdvanced\n\n\nKeypoint Support\nNo\nYes\n\n\nProbability Control\nLimited\nFine-grained\n\n\n\n\n\n\n\n\n\n# TorchVision approach\ndef load_image_torchvision(path):\n    return Image.open(path).convert('RGB')\n\n# Albumentations approach  \ndef load_image_albumentations(path):\n    image = cv2.imread(path)\n    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# Example usage\nimg_path = \"cat.jpg\"\ntorch_img = load_image_torchvision(img_path)\nalbu_img = load_image_albumentations(img_path)\n\n\n\n\n\n\n# TorchVision transforms\ntorchvision_transform = T.Compose([\n    T.Resize((224, 224)),\n    T.RandomHorizontalFlip(p=0.5),\n    T.RandomRotation(degrees=15),\n    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Albumentations equivalent\nalbumentations_transform = A.Compose([\n    A.Resize(224, 224),\n    A.HorizontalFlip(p=0.5),\n    A.Rotate(limit=15, p=1.0),\n    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=1.0),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2()\n])\n\n# Apply transforms\ntorch_result = torchvision_transform(torch_img)\nalbu_result = albumentations_transform(image=albu_img)['image']\n\n\n\n\n\n\n\n# Advanced geometric transformations\nadvanced_geometric = A.Compose([\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30, p=0.8),\n    A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.3),\n    A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3),\n    A.OpticalDistortion(distort_limit=0.5, shift_limit=0.5, p=0.3)\n])\n\n# Weather and lighting effects\nweather_effects = A.Compose([\n    A.RandomRain(slant_lower=-10, slant_upper=10, drop_length=20, p=0.3),\n    A.RandomSnow(snow_point_lower=0.1, snow_point_upper=0.3, p=0.3),\n    A.RandomFog(fog_coef_lower=0.3, fog_coef_upper=1, p=0.3),\n    A.RandomSunFlare(flare_roi=(0, 0, 1, 0.5), angle_lower=0, p=0.3)\n])\n\n# Noise and blur effects\nnoise_blur = A.Compose([\n    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n    A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=0.3),\n    A.MotionBlur(blur_limit=7, p=0.3),\n    A.MedianBlur(blur_limit=7, p=0.3),\n    A.Blur(blur_limit=7, p=0.3)\n])\n\n\n\nimport torchvision.transforms.v2 as T2\n\n# TorchVision v2 with better functionality\ntorchvision_v2_transform = T2.Compose([\n    T2.Resize((224, 224)),\n    T2.RandomHorizontalFlip(p=0.5),\n    T2.RandomChoice([\n        T2.ColorJitter(brightness=0.3),\n        T2.ColorJitter(contrast=0.3),\n        T2.ColorJitter(saturation=0.3)\n    ]),\n    T2.RandomApply([T2.GaussianBlur(kernel_size=3)], p=0.3),\n    T2.ToTensor(),\n    T2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n\n\n\n\n\n# Define bounding boxes in Pascal VOC format (x_min, y_min, x_max, y_max)\nbboxes = [[50, 50, 150, 150, 'person'], [200, 100, 300, 200, 'car']]\n\nbbox_transform = A.Compose([\n    A.Resize(416, 416),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.3),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),\n], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n\n# Apply transform\ntransformed = bbox_transform(\n    image=image, \n    bboxes=[[50, 50, 150, 150], [200, 100, 300, 200]], \n    class_labels=['person', 'car']\n)\n\ntransformed_image = transformed['image']\ntransformed_bboxes = transformed['bboxes']\ntransformed_labels = transformed['class_labels']\n\n\n\n# TorchVision v2 has some bbox support\nimport torchvision.transforms.v2 as T2\n\nbbox_torchvision = T2.Compose([\n    T2.Resize((416, 416)),\n    T2.RandomHorizontalFlip(p=0.5),\n    T2.ToTensor()\n])\n\n# Requires manual handling of bounding boxes\n# Less intuitive than Albumentations\n\n\n\n\n\n\n# Segmentation mask handling\nsegmentation_transform = A.Compose([\n    A.Resize(512, 512),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.3),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),\n    A.Normalize(),\n    ToTensorV2()\n])\n\n# Apply to image and mask simultaneously\nresult = segmentation_transform(image=image, mask=mask)\ntransformed_image = result['image']\ntransformed_mask = result['mask']\n\n\n\n# TorchVision requires separate handling\ndef apply_transform_to_mask(transform, image, mask):\n    # Manual synchronization needed\n    seed = torch.randint(0, 2**32, size=(1,)).item()\n    \n    torch.manual_seed(seed)\n    transformed_image = transform(image)\n    \n    torch.manual_seed(seed)\n    # Apply only geometric transforms to mask\n    mask_transform = T.Compose([\n        T.Resize((512, 512)),\n        T.RandomHorizontalFlip(p=0.5),\n        T.ToTensor()\n    ])\n    transformed_mask = mask_transform(mask)\n    \n    return transformed_image, transformed_mask\n\n\n\n\n\nimport time\n\ndef benchmark_transforms(image, iterations=1000):\n    # TorchVision timing\n    start_time = time.time()\n    for _ in range(iterations):\n        _ = torchvision_transform(image.copy())\n    torch_time = time.time() - start_time\n    \n    # Convert to numpy for Albumentations\n    np_image = np.array(image)\n    \n    # Albumentations timing\n    start_time = time.time()\n    for _ in range(iterations):\n        _ = albumentations_transform(image=np_image.copy())\n    albu_time = time.time() - start_time\n    \n    print(f\"TorchVision: {torch_time:.3f}s ({iterations} iterations)\")\n    print(f\"Albumentations: {albu_time:.3f}s ({iterations} iterations)\")\n    print(f\"Speedup: {torch_time/albu_time:.2f}x\")\n\n# Run benchmark\nbenchmark_transforms(torch_img)\n\nTorchVision: 38.434s (1000 iterations)\nAlbumentations: 3.152s (1000 iterations)\nSpeedup: 12.19x\n\n\n\n\n\n\n\ndef create_training_pipeline():\n    return A.Compose([\n        # Geometric transformations\n        A.OneOf([\n            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30),\n            A.ElasticTransform(alpha=1, sigma=50),\n            A.GridDistortion(num_steps=5, distort_limit=0.3),\n        ], p=0.5),\n        \n        # Color augmentations\n        A.OneOf([\n            A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20),\n            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n        ], p=0.8),\n        \n        # Noise and blur\n        A.OneOf([\n            A.GaussNoise(var_limit=(10, 50)),\n            A.ISONoise(),\n            A.MultiplicativeNoise(),\n        ], p=0.3),\n        \n        A.OneOf([\n            A.MotionBlur(blur_limit=5),\n            A.MedianBlur(blur_limit=5),\n            A.GaussianBlur(blur_limit=5),\n        ], p=0.3),\n        \n        # Final processing\n        A.Resize(224, 224),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n\ndef create_validation_pipeline():\n    return A.Compose([\n        A.Resize(224, 224),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n\n\n\ndef create_simple_training_pipeline():\n    return T.Compose([\n        T.Resize((224, 224)),\n        T.RandomHorizontalFlip(p=0.5),\n        T.RandomRotation(degrees=15),\n        T.RandomApply([T.ColorJitter(0.3, 0.3, 0.3, 0.1)], p=0.8),\n        T.RandomApply([T.GaussianBlur(kernel_size=3)], p=0.3),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\ndef create_simple_validation_pipeline():\n    return T.Compose([\n        T.Resize((224, 224)),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n\n\n\n\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CustomDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Load image for Albumentations (OpenCV format)\n        image = cv2.imread(self.image_paths[idx])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.transform:\n            transformed = self.transform(image=image)\n            image = transformed['image']\n        \n        return image, self.labels[idx]\n\n# Usage\ntrain_dataset = CustomDataset(\n    image_paths=train_paths,\n    labels=train_labels,\n    transform=create_training_pipeline()\n)\n\n\n\nclass TorchVisionDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Load image for TorchVision (PIL format)\n        image = Image.open(self.image_paths[idx]).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, self.labels[idx]\n\n# Usage\ntrain_dataset = TorchVisionDataset(\n    image_paths=train_paths,\n    labels=train_labels,\n    transform=create_simple_training_pipeline()\n)\n\n\n\n\n\n\n\nWorking with object detection or segmentation tasks\nNeed advanced augmentation techniques (weather effects, distortions)\nPerformance is critical (processing large datasets)\nWorking with bounding boxes or keypoints\nNeed fine-grained control over augmentation probabilities\nDealing with medical or satellite imagery\n\n\n\n\n\nBuilding simple image classification models\nWorking within pure PyTorch ecosystem\nNeed basic augmentations only\nPrototyping quickly\nFollowing PyTorch tutorials or established workflows\nWorking with pre-trained models that expect specific preprocessing\n\n\n\n\n\n\n\n# Use ReplayCompose for debugging\nreplay_transform = A.ReplayCompose([\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.3),\n])\n\nresult = replay_transform(image=image)\ntransformed_image = result['image']\nreplay_data = result['replay']\n\n# Apply same transforms to another image\nresult2 = A.ReplayCompose.replay(replay_data, image=another_image)\n\n# Efficient bbox handling\nbbox_params = A.BboxParams(\n    format='pascal_voc',\n    min_area=1024,  # Filter out small boxes\n    min_visibility=0.3,  # Filter out mostly occluded boxes\n    label_fields=['class_labels']\n)\n\n\n\n# Use functional API for custom control\ndef custom_transform(image):\n    if torch.rand(1) &lt; 0.5:\n        image = TF.hflip(image)\n    \n    # Apply rotation with custom logic\n    angle = torch.randint(-30, 30, (1,)).item()\n    image = TF.rotate(image, angle)\n    \n    return image\n\n# Combine with standard transforms\ncombined_transform = T.Compose([\n    T.Lambda(custom_transform),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n\n\n\nBoth libraries have their strengths:\nAlbumentations excels in:\n\nAdvanced augmentation techniques\nPerformance optimization\nComputer vision tasks beyond classification\nProfessional production environments\n\nTorchVision is ideal for:\n\nSimple classification tasks\nLearning and prototyping\nTight PyTorch integration\nBasic augmentation needs\n\nChoose based on your specific requirements, with Albumentations being the go-to choice for advanced computer vision projects and TorchVision for simpler classification tasks."
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#overview",
    "href": "posts/albumentations-vs-torchvision/index.html#overview",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "This guide compares two popular image augmentation libraries for PyTorch:\n\nTorchVision Transforms: Built-in PyTorch library for basic image transformations\nAlbumentations: Fast, flexible library with advanced augmentation techniques"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#installation",
    "href": "posts/albumentations-vs-torchvision/index.html#installation",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "# TorchVision (comes with PyTorch)\npip install torch torchvision\n\n# Albumentations\npip install albumentations"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#basic-setup-and-imports",
    "href": "posts/albumentations-vs-torchvision/index.html#basic-setup-and-imports",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "import torch\nimport torchvision.transforms as T\nfrom torchvision.transforms import functional as TF\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#key-differences-at-a-glance",
    "href": "posts/albumentations-vs-torchvision/index.html#key-differences-at-a-glance",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "Feature\nTorchVision\nAlbumentations\n\n\n\n\nInput Format\nPIL Image, Tensor\nNumPy array (OpenCV format)\n\n\nPerformance\nModerate\nFast (optimized)\n\n\nAugmentation Variety\nBasic to intermediate\nExtensive advanced options\n\n\nBounding Box Support\nLimited\nExcellent\n\n\nSegmentation Masks\nBasic\nAdvanced\n\n\nKeypoint Support\nNo\nYes\n\n\nProbability Control\nLimited\nFine-grained"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#basic-transformations-comparison",
    "href": "posts/albumentations-vs-torchvision/index.html#basic-transformations-comparison",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "# TorchVision approach\ndef load_image_torchvision(path):\n    return Image.open(path).convert('RGB')\n\n# Albumentations approach  \ndef load_image_albumentations(path):\n    image = cv2.imread(path)\n    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# Example usage\nimg_path = \"cat.jpg\"\ntorch_img = load_image_torchvision(img_path)\nalbu_img = load_image_albumentations(img_path)\n\n\n\n\n\n\n# TorchVision transforms\ntorchvision_transform = T.Compose([\n    T.Resize((224, 224)),\n    T.RandomHorizontalFlip(p=0.5),\n    T.RandomRotation(degrees=15),\n    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Albumentations equivalent\nalbumentations_transform = A.Compose([\n    A.Resize(224, 224),\n    A.HorizontalFlip(p=0.5),\n    A.Rotate(limit=15, p=1.0),\n    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=1.0),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2()\n])\n\n# Apply transforms\ntorch_result = torchvision_transform(torch_img)\nalbu_result = albumentations_transform(image=albu_img)['image']"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#advanced-augmentations",
    "href": "posts/albumentations-vs-torchvision/index.html#advanced-augmentations",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "# Advanced geometric transformations\nadvanced_geometric = A.Compose([\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30, p=0.8),\n    A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.3),\n    A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3),\n    A.OpticalDistortion(distort_limit=0.5, shift_limit=0.5, p=0.3)\n])\n\n# Weather and lighting effects\nweather_effects = A.Compose([\n    A.RandomRain(slant_lower=-10, slant_upper=10, drop_length=20, p=0.3),\n    A.RandomSnow(snow_point_lower=0.1, snow_point_upper=0.3, p=0.3),\n    A.RandomFog(fog_coef_lower=0.3, fog_coef_upper=1, p=0.3),\n    A.RandomSunFlare(flare_roi=(0, 0, 1, 0.5), angle_lower=0, p=0.3)\n])\n\n# Noise and blur effects\nnoise_blur = A.Compose([\n    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n    A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=0.3),\n    A.MotionBlur(blur_limit=7, p=0.3),\n    A.MedianBlur(blur_limit=7, p=0.3),\n    A.Blur(blur_limit=7, p=0.3)\n])\n\n\n\nimport torchvision.transforms.v2 as T2\n\n# TorchVision v2 with better functionality\ntorchvision_v2_transform = T2.Compose([\n    T2.Resize((224, 224)),\n    T2.RandomHorizontalFlip(p=0.5),\n    T2.RandomChoice([\n        T2.ColorJitter(brightness=0.3),\n        T2.ColorJitter(contrast=0.3),\n        T2.ColorJitter(saturation=0.3)\n    ]),\n    T2.RandomApply([T2.GaussianBlur(kernel_size=3)], p=0.3),\n    T2.ToTensor(),\n    T2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#working-with-bounding-boxes",
    "href": "posts/albumentations-vs-torchvision/index.html#working-with-bounding-boxes",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "# Define bounding boxes in Pascal VOC format (x_min, y_min, x_max, y_max)\nbboxes = [[50, 50, 150, 150, 'person'], [200, 100, 300, 200, 'car']]\n\nbbox_transform = A.Compose([\n    A.Resize(416, 416),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.3),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),\n], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n\n# Apply transform\ntransformed = bbox_transform(\n    image=image, \n    bboxes=[[50, 50, 150, 150], [200, 100, 300, 200]], \n    class_labels=['person', 'car']\n)\n\ntransformed_image = transformed['image']\ntransformed_bboxes = transformed['bboxes']\ntransformed_labels = transformed['class_labels']\n\n\n\n# TorchVision v2 has some bbox support\nimport torchvision.transforms.v2 as T2\n\nbbox_torchvision = T2.Compose([\n    T2.Resize((416, 416)),\n    T2.RandomHorizontalFlip(p=0.5),\n    T2.ToTensor()\n])\n\n# Requires manual handling of bounding boxes\n# Less intuitive than Albumentations"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#working-with-segmentation-masks",
    "href": "posts/albumentations-vs-torchvision/index.html#working-with-segmentation-masks",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "# Segmentation mask handling\nsegmentation_transform = A.Compose([\n    A.Resize(512, 512),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.3),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),\n    A.Normalize(),\n    ToTensorV2()\n])\n\n# Apply to image and mask simultaneously\nresult = segmentation_transform(image=image, mask=mask)\ntransformed_image = result['image']\ntransformed_mask = result['mask']\n\n\n\n# TorchVision requires separate handling\ndef apply_transform_to_mask(transform, image, mask):\n    # Manual synchronization needed\n    seed = torch.randint(0, 2**32, size=(1,)).item()\n    \n    torch.manual_seed(seed)\n    transformed_image = transform(image)\n    \n    torch.manual_seed(seed)\n    # Apply only geometric transforms to mask\n    mask_transform = T.Compose([\n        T.Resize((512, 512)),\n        T.RandomHorizontalFlip(p=0.5),\n        T.ToTensor()\n    ])\n    transformed_mask = mask_transform(mask)\n    \n    return transformed_image, transformed_mask"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#performance-comparison",
    "href": "posts/albumentations-vs-torchvision/index.html#performance-comparison",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "import time\n\ndef benchmark_transforms(image, iterations=1000):\n    # TorchVision timing\n    start_time = time.time()\n    for _ in range(iterations):\n        _ = torchvision_transform(image.copy())\n    torch_time = time.time() - start_time\n    \n    # Convert to numpy for Albumentations\n    np_image = np.array(image)\n    \n    # Albumentations timing\n    start_time = time.time()\n    for _ in range(iterations):\n        _ = albumentations_transform(image=np_image.copy())\n    albu_time = time.time() - start_time\n    \n    print(f\"TorchVision: {torch_time:.3f}s ({iterations} iterations)\")\n    print(f\"Albumentations: {albu_time:.3f}s ({iterations} iterations)\")\n    print(f\"Speedup: {torch_time/albu_time:.2f}x\")\n\n# Run benchmark\nbenchmark_transforms(torch_img)\n\nTorchVision: 38.434s (1000 iterations)\nAlbumentations: 3.152s (1000 iterations)\nSpeedup: 12.19x"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#custom-pipeline-examples",
    "href": "posts/albumentations-vs-torchvision/index.html#custom-pipeline-examples",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "def create_training_pipeline():\n    return A.Compose([\n        # Geometric transformations\n        A.OneOf([\n            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30),\n            A.ElasticTransform(alpha=1, sigma=50),\n            A.GridDistortion(num_steps=5, distort_limit=0.3),\n        ], p=0.5),\n        \n        # Color augmentations\n        A.OneOf([\n            A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20),\n            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n        ], p=0.8),\n        \n        # Noise and blur\n        A.OneOf([\n            A.GaussNoise(var_limit=(10, 50)),\n            A.ISONoise(),\n            A.MultiplicativeNoise(),\n        ], p=0.3),\n        \n        A.OneOf([\n            A.MotionBlur(blur_limit=5),\n            A.MedianBlur(blur_limit=5),\n            A.GaussianBlur(blur_limit=5),\n        ], p=0.3),\n        \n        # Final processing\n        A.Resize(224, 224),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n\ndef create_validation_pipeline():\n    return A.Compose([\n        A.Resize(224, 224),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n\n\n\ndef create_simple_training_pipeline():\n    return T.Compose([\n        T.Resize((224, 224)),\n        T.RandomHorizontalFlip(p=0.5),\n        T.RandomRotation(degrees=15),\n        T.RandomApply([T.ColorJitter(0.3, 0.3, 0.3, 0.1)], p=0.8),\n        T.RandomApply([T.GaussianBlur(kernel_size=3)], p=0.3),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\ndef create_simple_validation_pipeline():\n    return T.Compose([\n        T.Resize((224, 224)),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#dataset-integration",
    "href": "posts/albumentations-vs-torchvision/index.html#dataset-integration",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "from torch.utils.data import Dataset, DataLoader\n\nclass CustomDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Load image for Albumentations (OpenCV format)\n        image = cv2.imread(self.image_paths[idx])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.transform:\n            transformed = self.transform(image=image)\n            image = transformed['image']\n        \n        return image, self.labels[idx]\n\n# Usage\ntrain_dataset = CustomDataset(\n    image_paths=train_paths,\n    labels=train_labels,\n    transform=create_training_pipeline()\n)\n\n\n\nclass TorchVisionDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Load image for TorchVision (PIL format)\n        image = Image.open(self.image_paths[idx]).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, self.labels[idx]\n\n# Usage\ntrain_dataset = TorchVisionDataset(\n    image_paths=train_paths,\n    labels=train_labels,\n    transform=create_simple_training_pipeline()\n)"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#when-to-use-which-library",
    "href": "posts/albumentations-vs-torchvision/index.html#when-to-use-which-library",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "Working with object detection or segmentation tasks\nNeed advanced augmentation techniques (weather effects, distortions)\nPerformance is critical (processing large datasets)\nWorking with bounding boxes or keypoints\nNeed fine-grained control over augmentation probabilities\nDealing with medical or satellite imagery\n\n\n\n\n\nBuilding simple image classification models\nWorking within pure PyTorch ecosystem\nNeed basic augmentations only\nPrototyping quickly\nFollowing PyTorch tutorials or established workflows\nWorking with pre-trained models that expect specific preprocessing"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#best-practices",
    "href": "posts/albumentations-vs-torchvision/index.html#best-practices",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "# Use ReplayCompose for debugging\nreplay_transform = A.ReplayCompose([\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.3),\n])\n\nresult = replay_transform(image=image)\ntransformed_image = result['image']\nreplay_data = result['replay']\n\n# Apply same transforms to another image\nresult2 = A.ReplayCompose.replay(replay_data, image=another_image)\n\n# Efficient bbox handling\nbbox_params = A.BboxParams(\n    format='pascal_voc',\n    min_area=1024,  # Filter out small boxes\n    min_visibility=0.3,  # Filter out mostly occluded boxes\n    label_fields=['class_labels']\n)\n\n\n\n# Use functional API for custom control\ndef custom_transform(image):\n    if torch.rand(1) &lt; 0.5:\n        image = TF.hflip(image)\n    \n    # Apply rotation with custom logic\n    angle = torch.randint(-30, 30, (1,)).item()\n    image = TF.rotate(image, angle)\n    \n    return image\n\n# Combine with standard transforms\ncombined_transform = T.Compose([\n    T.Lambda(custom_transform),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])"
  },
  {
    "objectID": "posts/albumentations-vs-torchvision/index.html#conclusion",
    "href": "posts/albumentations-vs-torchvision/index.html#conclusion",
    "title": "Albumentations vs TorchVision Transforms: Complete Code Guide",
    "section": "",
    "text": "Both libraries have their strengths:\nAlbumentations excels in:\n\nAdvanced augmentation techniques\nPerformance optimization\nComputer vision tasks beyond classification\nProfessional production environments\n\nTorchVision is ideal for:\n\nSimple classification tasks\nLearning and prototyping\nTight PyTorch integration\nBasic augmentation needs\n\nChoose based on your specific requirements, with Albumentations being the go-to choice for advanced computer vision projects and TorchVision for simpler classification tasks."
  },
  {
    "objectID": "posts/mlflow-pytorch/index.html",
    "href": "posts/mlflow-pytorch/index.html",
    "title": "MLflow for PyTorch - Complete Guide",
    "section": "",
    "text": "MLflow is an open-source platform for managing the machine learning lifecycle, including experimentation, reproducibility, deployment, and model registry. This guide covers how to integrate MLflow with PyTorch for comprehensive ML workflow management.\n\n\n\nInstallation and Setup\nBasic MLflow Concepts\nExperiment Tracking\nModel Logging\nModel Registry\nModel Deployment\nAdvanced Features\nBest Practices\n\n\n\n\n\n\npip install mlflow\npip install torch torchvision\n\n\n\nmlflow ui\nThis starts the MLflow UI at http://localhost:5000\n\n\n\nimport mlflow\nimport mlflow.pytorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\n# Set tracking URI (optional - defaults to local)\nmlflow.set_tracking_uri(\"http://localhost:5000\")\n\n# Set experiment name\nmlflow.set_experiment(\"pytorch_experiments\")\n\n\n\n\n\nExperiment: A collection of runs for a particular task\nRun: A single execution of your ML code\nArtifact: Files generated during a run (models, plots, data)\nMetric: Numerical values tracked over time\nParameter: Input configurations for your run\n\n\n\n\n\n\nimport mlflow\n\nwith mlflow.start_run():\n    # Your training code here\n    mlflow.log_param(\"learning_rate\", 0.001)\n    mlflow.log_metric(\"accuracy\", 0.95)\n    mlflow.log_artifact(\"model.pth\")\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport mlflow\nimport mlflow.pytorch\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\nclass SimpleNet(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(SimpleNet, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n\ndef train_model():\n    # Hyperparameters\n    input_size = 784\n    hidden_size = 128\n    num_classes = 10\n    learning_rate = 0.001\n    batch_size = 64\n    num_epochs = 10\n    \n    # Start MLflow run\n    with mlflow.start_run():\n        # Log hyperparameters\n        mlflow.log_param(\"input_size\", input_size)\n        mlflow.log_param(\"hidden_size\", hidden_size)\n        mlflow.log_param(\"num_classes\", num_classes)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n        mlflow.log_param(\"batch_size\", batch_size)\n        mlflow.log_param(\"num_epochs\", num_epochs)\n        \n        # Initialize model\n        model = SimpleNet(input_size, hidden_size, num_classes)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n        \n        # Training loop\n        for epoch in range(num_epochs):\n            running_loss = 0.0\n            correct = 0\n            total = 0\n            \n            # Simulate training data\n            for i in range(100):  # 100 batches\n                # Generate dummy data\n                inputs = torch.randn(batch_size, input_size)\n                labels = torch.randint(0, num_classes, (batch_size,))\n                \n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n                \n                running_loss += loss.item()\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n            \n            # Calculate metrics\n            epoch_loss = running_loss / 100\n            epoch_acc = 100 * correct / total\n            \n            # Log metrics\n            mlflow.log_metric(\"loss\", epoch_loss, step=epoch)\n            mlflow.log_metric(\"accuracy\", epoch_acc, step=epoch)\n            \n            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n        \n        # Log model\n        mlflow.pytorch.log_model(model, \"model\")\n        \n        # Log additional artifacts\n        torch.save(model.state_dict(), \"model_state_dict.pth\")\n        mlflow.log_artifact(\"model_state_dict.pth\")\n\n# Run training\ntrain_model()\n\n\n\n\n\n\n\n\n# Log the entire model\nmlflow.pytorch.log_model(model, \"complete_model\")\n\n\n\n# Save and log state dict\ntorch.save(model.state_dict(), \"model_state_dict.pth\")\nmlflow.log_artifact(\"model_state_dict.pth\")\n\n\n\n# Log model with custom code for loading\nmlflow.pytorch.log_model(\n    model, \n    \"model\",\n    code_paths=[\"model_definition.py\"]  # Include custom model definition\n)\n\n\n\nimport mlflow.pytorch\n\n# Create conda environment specification\nconda_env = {\n    'channels': ['defaults', 'pytorch'],\n    'dependencies': [\n        'python=3.8',\n        'pytorch',\n        'torchvision',\n        {'pip': ['mlflow']}\n    ],\n    'name': 'pytorch_env'\n}\n\nmlflow.pytorch.log_model(\n    model,\n    \"model\",\n    conda_env=conda_env\n)\n\n\n\n\n\n\n\n# Register model during logging\nmlflow.pytorch.log_model(\n    model, \n    \"model\",\n    registered_model_name=\"MyPyTorchModel\"\n)\n\n# Or register existing run\nmodel_uri = \"runs:/your_run_id/model\"\nmlflow.register_model(model_uri, \"MyPyTorchModel\")\n\n\n\nfrom mlflow.tracking import MlflowClient\n\nclient = MlflowClient()\n\n# Transition model to different stages\nclient.transition_model_version_stage(\n    name=\"MyPyTorchModel\",\n    version=1,\n    stage=\"Production\"\n)\n\n# Get model by stage\nmodel_version = client.get_latest_versions(\n    \"MyPyTorchModel\", \n    stages=[\"Production\"]\n)[0]\n\n\n\n# Load model from registry\nmodel = mlflow.pytorch.load_model(\n    model_uri=f\"models:/MyPyTorchModel/Production\"\n)\n\n# Or load specific version\nmodel = mlflow.pytorch.load_model(\n    model_uri=f\"models:/MyPyTorchModel/1\"\n)\n\n\n\n\n\n\n# Serve model locally\n# Run in terminal:\n# mlflow models serve -m models:/MyPyTorchModel/Production -p 1234\n\n\n\nimport requests\nimport json\n\n# Prepare data\ndata = {\n    \"inputs\": [[1.0, 2.0, 3.0, 4.0]]  # Your input features\n}\n\n# Make prediction request\nresponse = requests.post(\n    \"http://localhost:1234/invocations\",\n    headers={\"Content-Type\": \"application/json\"},\n    data=json.dumps(data)\n)\n\npredictions = response.json()\nprint(predictions)\n\n\n\n# Build Docker image\nmlflow models build-docker -m models:/MyPyTorchModel/Production -n my-pytorch-model\n\n# Run Docker container\ndocker run -p 8080:8080 my-pytorch-model\n\n\n\n\n\n\nimport mlflow.pyfunc\n\nclass PyTorchModelWrapper(mlflow.pyfunc.PythonModel):\n    def __init__(self, model):\n        self.model = model\n    \n    def predict(self, context, model_input):\n        # Custom prediction logic\n        with torch.no_grad():\n            tensor_input = torch.FloatTensor(model_input.values)\n            predictions = self.model(tensor_input)\n            return predictions.numpy()\n\n# Log custom model\nwrapped_model = PyTorchModelWrapper(model)\nmlflow.pyfunc.log_model(\n    \"custom_model\", \n    python_model=wrapped_model\n)\n\n\n\n# Enable automatic logging\nmlflow.pytorch.autolog()\n\n# Your training code - metrics and models are logged automatically\nwith mlflow.start_run():\n    # Training happens here\n    pass\n\n\n\nimport itertools\n\n# Define hyperparameter grid\nparam_grid = {\n    'learning_rate': [0.001, 0.01, 0.1],\n    'hidden_size': [64, 128, 256],\n    'batch_size': [32, 64, 128]\n}\n\n# Run experiments\nfor params in [dict(zip(param_grid.keys(), v)) \n               for v in itertools.product(*param_grid.values())]:\n    with mlflow.start_run():\n        # Log parameters\n        for key, value in params.items():\n            mlflow.log_param(key, value)\n        \n        # Train model with these parameters\n        model = train_with_params(params)\n        \n        # Log results\n        mlflow.log_metric(\"final_accuracy\", accuracy)\n        mlflow.pytorch.log_model(model, \"model\")\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Create and log plots\ndef log_training_plots(train_losses, val_losses):\n    plt.figure(figsize=(10, 6))\n    plt.plot(train_losses, label='Training Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Training and Validation Loss')\n    plt.savefig('loss_plot.png')\n    mlflow.log_artifact('loss_plot.png')\n    plt.close()\n\n# Log confusion matrix\ndef log_confusion_matrix(y_true, y_pred, class_names):\n    from sklearn.metrics import confusion_matrix\n    import seaborn as sns\n    \n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix')\n    plt.savefig('confusion_matrix.png')\n    mlflow.log_artifact('confusion_matrix.png')\n    plt.close()\n\n\n\n\n\n\n# Use descriptive experiment names\nmlflow.set_experiment(\"image_classification_resnet\")\n\n# Use run names for specific configurations\nwith mlflow.start_run(run_name=\"resnet50_adam_lr001\"):\n    pass\n\n\n\ndef comprehensive_logging(model, optimizer, criterion, config):\n    # Log hyperparameters\n    mlflow.log_params(config)\n    \n    # Log model architecture info\n    total_params = sum(p.numel() for p in model.parameters())\n    mlflow.log_param(\"total_parameters\", total_params)\n    mlflow.log_param(\"model_architecture\", str(model))\n    \n    # Log optimizer info\n    mlflow.log_param(\"optimizer\", type(optimizer).__name__)\n    mlflow.log_param(\"criterion\", type(criterion).__name__)\n    \n    # Log system info\n    mlflow.log_param(\"cuda_available\", torch.cuda.is_available())\n    if torch.cuda.is_available():\n        mlflow.log_param(\"gpu_name\", torch.cuda.get_device_name(0))\n\n\n\ndef safe_mlflow_run(training_function, **kwargs):\n    try:\n        with mlflow.start_run():\n            result = training_function(**kwargs)\n            mlflow.log_param(\"status\", \"success\")\n            return result\n    except Exception as e:\n        mlflow.log_param(\"status\", \"failed\")\n        mlflow.log_param(\"error\", str(e))\n        raise e\n\n\n\ndef compare_models():\n    # Get experiment\n    experiment = mlflow.get_experiment_by_name(\"pytorch_experiments\")\n    runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n    \n    # Sort by accuracy\n    best_runs = runs.sort_values(\"metrics.accuracy\", ascending=False)\n    \n    print(\"Top 5 models by accuracy:\")\n    print(best_runs[[\"run_id\", \"metrics.accuracy\", \"params.learning_rate\"]].head())\n\n\n\ndef load_model_safely(model_uri):\n    try:\n        model = mlflow.pytorch.load_model(model_uri)\n        model.eval()  # Set to evaluation mode\n        return model\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\n        return None\n\n# Usage\nmodel = load_model_safely(\"models:/MyPyTorchModel/Production\")\nif model:\n    # Use model for inference\n    pass\n\n\n\n\nMLflow provides a comprehensive solution for managing PyTorch ML workflows:\n\nExperiment Tracking: Log parameters, metrics, and artifacts\nModel Management: Version and organize your models\nModel Registry: Centralized model store with lifecycle management\n\nDeployment: Easy model serving and deployment options\nReproducibility: Track everything needed to reproduce experiments\n\nStart with basic experiment tracking, then gradually adopt more advanced features like the model registry and deployment capabilities as your ML workflow matures."
  },
  {
    "objectID": "posts/mlflow-pytorch/index.html#table-of-contents",
    "href": "posts/mlflow-pytorch/index.html#table-of-contents",
    "title": "MLflow for PyTorch - Complete Guide",
    "section": "",
    "text": "Installation and Setup\nBasic MLflow Concepts\nExperiment Tracking\nModel Logging\nModel Registry\nModel Deployment\nAdvanced Features\nBest Practices"
  },
  {
    "objectID": "posts/mlflow-pytorch/index.html#installation-and-setup",
    "href": "posts/mlflow-pytorch/index.html#installation-and-setup",
    "title": "MLflow for PyTorch - Complete Guide",
    "section": "",
    "text": "pip install mlflow\npip install torch torchvision\n\n\n\nmlflow ui\nThis starts the MLflow UI at http://localhost:5000\n\n\n\nimport mlflow\nimport mlflow.pytorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\n# Set tracking URI (optional - defaults to local)\nmlflow.set_tracking_uri(\"http://localhost:5000\")\n\n# Set experiment name\nmlflow.set_experiment(\"pytorch_experiments\")"
  },
  {
    "objectID": "posts/mlflow-pytorch/index.html#basic-mlflow-concepts",
    "href": "posts/mlflow-pytorch/index.html#basic-mlflow-concepts",
    "title": "MLflow for PyTorch - Complete Guide",
    "section": "",
    "text": "Experiment: A collection of runs for a particular task\nRun: A single execution of your ML code\nArtifact: Files generated during a run (models, plots, data)\nMetric: Numerical values tracked over time\nParameter: Input configurations for your run"
  },
  {
    "objectID": "posts/mlflow-pytorch/index.html#experiment-tracking",
    "href": "posts/mlflow-pytorch/index.html#experiment-tracking",
    "title": "MLflow for PyTorch - Complete Guide",
    "section": "",
    "text": "import mlflow\n\nwith mlflow.start_run():\n    # Your training code here\n    mlflow.log_param(\"learning_rate\", 0.001)\n    mlflow.log_metric(\"accuracy\", 0.95)\n    mlflow.log_artifact(\"model.pth\")\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport mlflow\nimport mlflow.pytorch\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\nclass SimpleNet(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(SimpleNet, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n\ndef train_model():\n    # Hyperparameters\n    input_size = 784\n    hidden_size = 128\n    num_classes = 10\n    learning_rate = 0.001\n    batch_size = 64\n    num_epochs = 10\n    \n    # Start MLflow run\n    with mlflow.start_run():\n        # Log hyperparameters\n        mlflow.log_param(\"input_size\", input_size)\n        mlflow.log_param(\"hidden_size\", hidden_size)\n        mlflow.log_param(\"num_classes\", num_classes)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n        mlflow.log_param(\"batch_size\", batch_size)\n        mlflow.log_param(\"num_epochs\", num_epochs)\n        \n        # Initialize model\n        model = SimpleNet(input_size, hidden_size, num_classes)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n        \n        # Training loop\n        for epoch in range(num_epochs):\n            running_loss = 0.0\n            correct = 0\n            total = 0\n            \n            # Simulate training data\n            for i in range(100):  # 100 batches\n                # Generate dummy data\n                inputs = torch.randn(batch_size, input_size)\n                labels = torch.randint(0, num_classes, (batch_size,))\n                \n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n                \n                running_loss += loss.item()\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n            \n            # Calculate metrics\n            epoch_loss = running_loss / 100\n            epoch_acc = 100 * correct / total\n            \n            # Log metrics\n            mlflow.log_metric(\"loss\", epoch_loss, step=epoch)\n            mlflow.log_metric(\"accuracy\", epoch_acc, step=epoch)\n            \n            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n        \n        # Log model\n        mlflow.pytorch.log_model(model, \"model\")\n        \n        # Log additional artifacts\n        torch.save(model.state_dict(), \"model_state_dict.pth\")\n        mlflow.log_artifact(\"model_state_dict.pth\")\n\n# Run training\ntrain_model()"
  },
  {
    "objectID": "posts/mlflow-pytorch/index.html#model-logging",
    "href": "posts/mlflow-pytorch/index.html#model-logging",
    "title": "MLflow for PyTorch - Complete Guide",
    "section": "",
    "text": "# Log the entire model\nmlflow.pytorch.log_model(model, \"complete_model\")\n\n\n\n# Save and log state dict\ntorch.save(model.state_dict(), \"model_state_dict.pth\")\nmlflow.log_artifact(\"model_state_dict.pth\")\n\n\n\n# Log model with custom code for loading\nmlflow.pytorch.log_model(\n    model, \n    \"model\",\n    code_paths=[\"model_definition.py\"]  # Include custom model definition\n)\n\n\n\nimport mlflow.pytorch\n\n# Create conda environment specification\nconda_env = {\n    'channels': ['defaults', 'pytorch'],\n    'dependencies': [\n        'python=3.8',\n        'pytorch',\n        'torchvision',\n        {'pip': ['mlflow']}\n    ],\n    'name': 'pytorch_env'\n}\n\nmlflow.pytorch.log_model(\n    model,\n    \"model\",\n    conda_env=conda_env\n)"
  },
  {
    "objectID": "posts/mlflow-pytorch/index.html#model-registry",
    "href": "posts/mlflow-pytorch/index.html#model-registry",
    "title": "MLflow for PyTorch - Complete Guide",
    "section": "",
    "text": "# Register model during logging\nmlflow.pytorch.log_model(\n    model, \n    \"model\",\n    registered_model_name=\"MyPyTorchModel\"\n)\n\n# Or register existing run\nmodel_uri = \"runs:/your_run_id/model\"\nmlflow.register_model(model_uri, \"MyPyTorchModel\")\n\n\n\nfrom mlflow.tracking import MlflowClient\n\nclient = MlflowClient()\n\n# Transition model to different stages\nclient.transition_model_version_stage(\n    name=\"MyPyTorchModel\",\n    version=1,\n    stage=\"Production\"\n)\n\n# Get model by stage\nmodel_version = client.get_latest_versions(\n    \"MyPyTorchModel\", \n    stages=[\"Production\"]\n)[0]\n\n\n\n# Load model from registry\nmodel = mlflow.pytorch.load_model(\n    model_uri=f\"models:/MyPyTorchModel/Production\"\n)\n\n# Or load specific version\nmodel = mlflow.pytorch.load_model(\n    model_uri=f\"models:/MyPyTorchModel/1\"\n)"
  },
  {
    "objectID": "posts/mlflow-pytorch/index.html#model-deployment",
    "href": "posts/mlflow-pytorch/index.html#model-deployment",
    "title": "MLflow for PyTorch - Complete Guide",
    "section": "",
    "text": "# Serve model locally\n# Run in terminal:\n# mlflow models serve -m models:/MyPyTorchModel/Production -p 1234\n\n\n\nimport requests\nimport json\n\n# Prepare data\ndata = {\n    \"inputs\": [[1.0, 2.0, 3.0, 4.0]]  # Your input features\n}\n\n# Make prediction request\nresponse = requests.post(\n    \"http://localhost:1234/invocations\",\n    headers={\"Content-Type\": \"application/json\"},\n    data=json.dumps(data)\n)\n\npredictions = response.json()\nprint(predictions)\n\n\n\n# Build Docker image\nmlflow models build-docker -m models:/MyPyTorchModel/Production -n my-pytorch-model\n\n# Run Docker container\ndocker run -p 8080:8080 my-pytorch-model"
  },
  {
    "objectID": "posts/mlflow-pytorch/index.html#advanced-features",
    "href": "posts/mlflow-pytorch/index.html#advanced-features",
    "title": "MLflow for PyTorch - Complete Guide",
    "section": "",
    "text": "import mlflow.pyfunc\n\nclass PyTorchModelWrapper(mlflow.pyfunc.PythonModel):\n    def __init__(self, model):\n        self.model = model\n    \n    def predict(self, context, model_input):\n        # Custom prediction logic\n        with torch.no_grad():\n            tensor_input = torch.FloatTensor(model_input.values)\n            predictions = self.model(tensor_input)\n            return predictions.numpy()\n\n# Log custom model\nwrapped_model = PyTorchModelWrapper(model)\nmlflow.pyfunc.log_model(\n    \"custom_model\", \n    python_model=wrapped_model\n)\n\n\n\n# Enable automatic logging\nmlflow.pytorch.autolog()\n\n# Your training code - metrics and models are logged automatically\nwith mlflow.start_run():\n    # Training happens here\n    pass\n\n\n\nimport itertools\n\n# Define hyperparameter grid\nparam_grid = {\n    'learning_rate': [0.001, 0.01, 0.1],\n    'hidden_size': [64, 128, 256],\n    'batch_size': [32, 64, 128]\n}\n\n# Run experiments\nfor params in [dict(zip(param_grid.keys(), v)) \n               for v in itertools.product(*param_grid.values())]:\n    with mlflow.start_run():\n        # Log parameters\n        for key, value in params.items():\n            mlflow.log_param(key, value)\n        \n        # Train model with these parameters\n        model = train_with_params(params)\n        \n        # Log results\n        mlflow.log_metric(\"final_accuracy\", accuracy)\n        mlflow.pytorch.log_model(model, \"model\")\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Create and log plots\ndef log_training_plots(train_losses, val_losses):\n    plt.figure(figsize=(10, 6))\n    plt.plot(train_losses, label='Training Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Training and Validation Loss')\n    plt.savefig('loss_plot.png')\n    mlflow.log_artifact('loss_plot.png')\n    plt.close()\n\n# Log confusion matrix\ndef log_confusion_matrix(y_true, y_pred, class_names):\n    from sklearn.metrics import confusion_matrix\n    import seaborn as sns\n    \n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix')\n    plt.savefig('confusion_matrix.png')\n    mlflow.log_artifact('confusion_matrix.png')\n    plt.close()"
  },
  {
    "objectID": "posts/mlflow-pytorch/index.html#best-practices",
    "href": "posts/mlflow-pytorch/index.html#best-practices",
    "title": "MLflow for PyTorch - Complete Guide",
    "section": "",
    "text": "# Use descriptive experiment names\nmlflow.set_experiment(\"image_classification_resnet\")\n\n# Use run names for specific configurations\nwith mlflow.start_run(run_name=\"resnet50_adam_lr001\"):\n    pass\n\n\n\ndef comprehensive_logging(model, optimizer, criterion, config):\n    # Log hyperparameters\n    mlflow.log_params(config)\n    \n    # Log model architecture info\n    total_params = sum(p.numel() for p in model.parameters())\n    mlflow.log_param(\"total_parameters\", total_params)\n    mlflow.log_param(\"model_architecture\", str(model))\n    \n    # Log optimizer info\n    mlflow.log_param(\"optimizer\", type(optimizer).__name__)\n    mlflow.log_param(\"criterion\", type(criterion).__name__)\n    \n    # Log system info\n    mlflow.log_param(\"cuda_available\", torch.cuda.is_available())\n    if torch.cuda.is_available():\n        mlflow.log_param(\"gpu_name\", torch.cuda.get_device_name(0))\n\n\n\ndef safe_mlflow_run(training_function, **kwargs):\n    try:\n        with mlflow.start_run():\n            result = training_function(**kwargs)\n            mlflow.log_param(\"status\", \"success\")\n            return result\n    except Exception as e:\n        mlflow.log_param(\"status\", \"failed\")\n        mlflow.log_param(\"error\", str(e))\n        raise e\n\n\n\ndef compare_models():\n    # Get experiment\n    experiment = mlflow.get_experiment_by_name(\"pytorch_experiments\")\n    runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n    \n    # Sort by accuracy\n    best_runs = runs.sort_values(\"metrics.accuracy\", ascending=False)\n    \n    print(\"Top 5 models by accuracy:\")\n    print(best_runs[[\"run_id\", \"metrics.accuracy\", \"params.learning_rate\"]].head())\n\n\n\ndef load_model_safely(model_uri):\n    try:\n        model = mlflow.pytorch.load_model(model_uri)\n        model.eval()  # Set to evaluation mode\n        return model\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\n        return None\n\n# Usage\nmodel = load_model_safely(\"models:/MyPyTorchModel/Production\")\nif model:\n    # Use model for inference\n    pass"
  },
  {
    "objectID": "posts/mlflow-pytorch/index.html#summary",
    "href": "posts/mlflow-pytorch/index.html#summary",
    "title": "MLflow for PyTorch - Complete Guide",
    "section": "",
    "text": "MLflow provides a comprehensive solution for managing PyTorch ML workflows:\n\nExperiment Tracking: Log parameters, metrics, and artifacts\nModel Management: Version and organize your models\nModel Registry: Centralized model store with lifecycle management\n\nDeployment: Easy model serving and deployment options\nReproducibility: Track everything needed to reproduce experiments\n\nStart with basic experiment tracking, then gradually adopt more advanced features like the model registry and deployment capabilities as your ML workflow matures."
  },
  {
    "objectID": "posts/pandas-to-polars/index.html",
    "href": "posts/pandas-to-polars/index.html",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "As datasets grow in size and complexity, performance and efficiency become critical in data processing. While Pandas has long been the go-to library for data manipulation in Python, it can struggle with speed and memory usage, especially on large datasets. Polars, a newer DataFrame library written in Rust, offers a faster, more memory-efficient alternative with support for lazy evaluation and multi-threading.\nThis guide explores how to convert Pandas DataFrames to Polars, and highlights key differences in syntax, performance, and functionality. Whether youâ€™re looking to speed up your data workflows or just exploring modern tools, understanding the transition from Pandas to Polars is a valuable step.\n\n\n\nInstallation and Setup\nCreating DataFrames\nBasic Operations\nFiltering Data\nGrouping and Aggregation\nJoining/Merging DataFrames\nHandling Missing Values\nString Operations\nTime Series Operations\nPerformance Comparison\nAPI Philosophy Differences\nMigration Guide\n\n\n\n\n\n\n\n# Import pandas\nimport pandas as pd\n\n\n\n\n\n# Import polars\nimport polars as pl\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\n\n# Create DataFrame from dictionary\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'age': [25, 30, 35, 40],\n    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n}\ndf_pd = pd.DataFrame(data)\nprint(df_pd)\n\n      name  age         city\n0    Alice   25     New York\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n3    David   40      Houston\n\n\n\n\n\n\nimport polars as pl\n\n# Create DataFrame from dictionary\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'age': [25, 30, 35, 40],\n    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n}\ndf_pl = pl.DataFrame(data)\nprint(df_pl)\n\nshape: (4, 3)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ name    â”† age â”† city        â”‚\nâ”‚ ---     â”† --- â”† ---         â”‚\nâ”‚ str     â”† i64 â”† str         â”‚\nâ•žâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ Alice   â”† 25  â”† New York    â”‚\nâ”‚ Bob     â”† 30  â”† Los Angeles â”‚\nâ”‚ Charlie â”† 35  â”† Chicago     â”‚\nâ”‚ David   â”† 40  â”† Houston     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\n\n\n\n\n\n\n\n# Select a single column (returns Series)\nseries = df_pd['name']\n\n# Select multiple columns\ndf_subset = df_pd[['name', 'age']]\n\n\n\n\n\n# Select a single column (returns Series)\nseries = df_pl['name']\n# Alternative method\nseries = df_pl.select(pl.col('name')).to_series()\n\n# Select multiple columns\ndf_subset = df_pl.select(['name', 'age'])\n# Alternative method\ndf_subset = df_pl.select(pl.col(['name', 'age']))\n\n\n\n\n\n\n\n\n# Add a new column\ndf_pd['is_adult'] = df_pd['age'] &gt;= 18\n\n# Using assign (creates a new DataFrame)\ndf_pd = df_pd.assign(age_squared=df_pd['age'] ** 2)\n\n\n\n\n\n# Add a new column\ndf_pl = df_pl.with_columns(\n    pl.when(pl.col('age') &gt;= 18).then(True).otherwise(False).alias('is_adult')\n)\n\n# Creating derived columns \ndf_pl = df_pl.with_columns(\n    (pl.col('age') ** 2).alias('age_squared')\n)\n\n# Multiple columns at once\ndf_pl = df_pl.with_columns([\n    pl.col('age').is_null().alias('age_is_null'),\n    (pl.col('age') * 2).alias('age_doubled')\n])\n\n\n\n\n\n\n\n\n# Get summary statistics\nsummary = df_pd.describe()\n\n# Individual statistics\nmean_age = df_pd['age'].mean()\nmedian_age = df_pd['age'].median()\nmin_age = df_pd['age'].min()\nmax_age = df_pd['age'].max()\n\n\n\n\n\n# Get summary statistics\nsummary = df_pl.describe()\n\n# Individual statistics\nmean_age = df_pl.select(pl.col('age').mean()).item()\nmedian_age = df_pl.select(pl.col('age').median()).item()\nmin_age = df_pl.select(pl.col('age').min()).item()\nmax_age = df_pl.select(pl.col('age').max()).item()\n\n\n\n\n\n\n\n\n\n\n\n# Filter rows\nadults = df_pd[df_pd['age'] &gt;= 18]\n\n# Multiple conditions\nfiltered = df_pd[(df_pd['age'] &gt; 30) & (df_pd['city'] == 'Chicago')]\n\n\n\n\n\n# Filter rows\nadults = df_pl.filter(pl.col('age') &gt;= 18)\n\n# Multiple conditions\nfiltered = df_pl.filter((pl.col('age') &gt; 30) & (pl.col('city') == 'Chicago'))\n\n\n\n\n\n\n\n\n# Filter with OR conditions\ndf_filtered = df_pd[(df_pd['city'] == 'New York') | (df_pd['city'] == 'Chicago')]\n\n# Using isin\ncities = ['New York', 'Chicago']\ndf_filtered = df_pd[df_pd['city'].isin(cities)]\n\n# String contains\ndf_filtered = df_pd[df_pd['name'].str.contains('li')]\n\n\n\n\n\n# Filter with OR conditions\ndf_filtered = df_pl.filter((pl.col('city') == 'New York') | (pl.col('city') == 'Chicago'))\n\n# Using is_in\ncities = ['New York', 'Chicago']\ndf_filtered = df_pl.filter(pl.col('city').is_in(cities))\n\n# String contains\ndf_filtered = df_pl.filter(pl.col('name').str.contains('li'))\n\n\n\n\n\n\n\n\n\n\n\n# Group by one column and aggregate\ncity_stats = df_pd.groupby('city').agg({\n    'age': ['mean', 'min', 'max', 'count']\n})\n\n# Reset index for flat DataFrame\ncity_stats = city_stats.reset_index()\n\n\n\n\n\n# Group by one column and aggregate\ncity_stats = df_pl.group_by('city').agg([\n    pl.col('age').mean().alias('age_mean'),\n    pl.col('age').min().alias('age_min'),\n    pl.col('age').max().alias('age_max'),\n    pl.col('age').count().alias('age_count')\n])\n\n\n\n\n\n\n\n\n\n\n\n# Create another DataFrame\nemployee_data = {\n    'emp_id': [1, 2, 3, 4],\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'dept': ['HR', 'IT', 'Finance', 'IT']\n}\nemployee_df_pd = pd.DataFrame(employee_data)\n\nsalary_data = {\n    'emp_id': [1, 2, 3, 5],\n    'salary': [50000, 60000, 70000, 80000]\n}\nsalary_df_pd = pd.DataFrame(salary_data)\n\n# Inner join\nmerged_df = employee_df_pd.merge(\n    salary_df_pd,\n    on='emp_id',\n    how='inner'\n)\n\n\n\n\n\n# Create another DataFrame\nemployee_data = {\n    'emp_id': [1, 2, 3, 4],\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'dept': ['HR', 'IT', 'Finance', 'IT']\n}\nemployee_df_pl = pl.DataFrame(employee_data)\n\nsalary_data = {\n    'emp_id': [1, 2, 3, 5],\n    'salary': [50000, 60000, 70000, 80000]\n}\nsalary_df_pl = pl.DataFrame(salary_data)\n\n# Inner join\nmerged_df = employee_df_pl.join(\n    salary_df_pl,\n    on='emp_id',\n    how='inner'\n)\n\n\n\n\n\n\n\n\n# Left join\nleft_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='left')\n\n# Right join\nright_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='right')\n\n# Outer join\nouter_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='outer')\n\n\n\n\n\n# Left join\nleft_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='left')\n\n# Right join\nright_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='right')\n\n# Outer join\nouter_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='full')\n\n\n\n\n\n\n\n\n\n\n\n# Check for missing values\nmissing_count = df_pd.isnull().sum()\n\n# Check if any column has missing values\nhas_missing = df_pd.isnull().any().any()\n\n\n\n\n\n# Check for missing values\nmissing_count = df_pl.null_count()\n\n# Check if specific column has missing values\nhas_missing = df_pl.select(pl.col('age').is_null().any()).item()\n\n\n\n\n\n\n\n\n# Drop rows with any missing values\ndf_pd_clean = df_pd.dropna()\n\n# Fill missing values\ndf_pd_filled = df_pd.fillna({\n    'age': 0,\n    'city': 'Unknown'\n})\n\n# Forward fill\ndf_pd_ffill = df_pd.ffill()\n\n\n\n\n\n# Drop rows with any missing values\ndf_pl_clean = df_pl.drop_nulls()\n\n# Fill missing values\ndf_pl_filled = df_pl.with_columns([\n    pl.col('age').fill_null(0),\n    pl.col('city').fill_null('Unknown')\n])\n\n# Forward fill\ndf_pl_ffill = df_pl.with_columns([\n    pl.col('age').fill_null(strategy='forward'),\n    pl.col('city').fill_null(strategy='forward')\n])\n\n\n\n\n\n\n\n\n\n\n\n# Convert to uppercase\ndf_pd['name_upper'] = df_pd['name'].str.upper()\n\n# Get string length\ndf_pd['name_length'] = df_pd['name'].str.len()\n\n# Extract substring\ndf_pd['name_first_char'] = df_pd['name'].str[0]\n\n# Replace substrings\ndf_pd['city_replaced'] = df_pd['city'].str.replace('New', 'Old')\n\n\n\n\n\n# Convert to uppercase\ndf_pl = df_pl.with_columns(pl.col('name').str.to_uppercase().alias('name_upper'))\n\n# Get string length\ndf_pl = df_pl.with_columns(pl.col('name').str.len_chars().alias('name_length'))\n\n# Extract substring \ndf_pl = df_pl.with_columns(pl.col('name').str.slice(0, 1).alias('name_first_char'))\n\n# Replace substrings\ndf_pl = df_pl.with_columns(pl.col('city').str.replace('New', 'Old').alias('city_replaced'))\n\n\n\n\n\n\n\n\n# Split string\ndf_pd['first_word'] = df_pd['city'].str.split(' ').str[0]\n\n# Pattern matching\nhas_new = df_pd['city'].str.contains('New')\n\n# Extract with regex\ndf_pd['extracted'] = df_pd['city'].str.extract(r'(\\w+)\\s')\n\n\n\n\n\n# Split string\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.split(' ').list.get(0).alias('first_word')\n)\n\n# Pattern matching\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.contains('New').alias('has_new')\n)\n\n# Extract with regex\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.extract(r'(\\w+)\\s').alias('extracted')\n)\n\n\n\n\n\n\n\n\n\n\n\n# Create DataFrame with dates\ndates_pd = pd.DataFrame({\n    'date_str': ['2023-01-01', '2023-02-15', '2023-03-30']\n})\n\n# Parse dates\ndates_pd['date'] = pd.to_datetime(dates_pd['date_str'])\n\n# Extract components\ndates_pd['year'] = dates_pd['date'].dt.year\ndates_pd['month'] = dates_pd['date'].dt.month\ndates_pd['day'] = dates_pd['date'].dt.day\ndates_pd['weekday'] = dates_pd['date'].dt.day_name()\n\n\n\n\n\n# Create DataFrame with dates\ndates_pl = pl.DataFrame({\n    'date_str': ['2023-01-01', '2023-02-15', '2023-03-30']\n})\n\n# Parse dates\ndates_pl = dates_pl.with_columns(\n    pl.col('date_str').str.strptime(pl.Datetime, '%Y-%m-%d').alias('date')\n)\n\n# Extract components\ndates_pl = dates_pl.with_columns([\n    pl.col('date').dt.year().alias('year'),\n    pl.col('date').dt.month().alias('month'),\n    pl.col('date').dt.day().alias('day'),\n    pl.col('date').dt.weekday().replace_strict({\n        0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', \n        3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'\n    }, default=\"unknown\").alias('weekday')\n])\n\n\n\n\n\n\n\n\n# Add days\ndates_pd['next_week'] = dates_pd['date'] + pd.Timedelta(days=7)\n\n# Date difference\ndate_range = pd.date_range(start='2023-01-01', end='2023-01-10')\ndf_dates = pd.DataFrame({'date': date_range})\ndf_dates['days_since_start'] = (df_dates['date'] - df_dates['date'].min()).dt.days\n\n\n\n\n\n# Add days\ndates_pl = dates_pl.with_columns(\n    (pl.col('date') + pl.duration(days=7)).alias('next_week')\n)\n\n# Date difference\ndate_range = pd.date_range(start='2023-01-01', end='2023-01-10')  # Using pandas to generate range\ndf_dates = pl.DataFrame({'date': date_range})\ndf_dates = df_dates.with_columns(\n    (pl.col('date') - pl.col('date').min()).dt.total_days().alias('days_since_start')\n)\n\n\n\n\n\n\nThis section demonstrates performance differences between pandas and polars for a large dataset operation.\n\nimport pandas as pd\nimport polars as pl\nimport time\nimport numpy as np\n\n# Generate a large dataset (10 million rows)\nn = 10_000_000\ndata = {\n    'id': np.arange(n),\n    'value': np.random.randn(n),\n    'group': np.random.choice(['A', 'B', 'C', 'D'], n)\n}\n\n# Convert to pandas DataFrame\ndf_pd = pd.DataFrame(data)\n\n# Convert to polars DataFrame\ndf_pl = pl.DataFrame(data)\n\n# Benchmark: Group by and calculate mean, min, max\nprint(\"Running pandas groupby...\")\nstart = time.time()\nresult_pd = df_pd.groupby('group').agg({\n    'value': ['mean', 'min', 'max', 'count']\n})\npd_time = time.time() - start\nprint(f\"Pandas time: {pd_time:.4f} seconds\")\n\nprint(\"Running polars groupby...\")\nstart = time.time()\nresult_pl = df_pl.group_by('group').agg([\n    pl.col('value').mean().alias('value_mean'),\n    pl.col('value').min().alias('value_min'),\n    pl.col('value').max().alias('value_max'),\n    pl.col('value').count().alias('value_count')\n])\npl_time = time.time() - start\nprint(f\"Polars time: {pl_time:.4f} seconds\")\n\nprint(f\"Polars is {pd_time / pl_time:.2f}x faster\")\n\nRunning pandas groupby...\nPandas time: 0.2320 seconds\nRunning polars groupby...\nPolars time: 0.0545 seconds\nPolars is 4.26x faster\n\n\nTypically, for operations like this, Polars will be 3-10x faster than pandas, especially as data sizes increase. The performance gap widens further with more complex operations that can benefit from query optimization.\n\n\n\nPandas and Polars differ in several fundamental aspects:\n\n\nPandas uses eager execution by default:\nPolars supports both eager and lazy execution:\n\n\n\nPandas often uses assignment operations:\n\n# Many pandas operations use in-place assignment\npd_df['new_col'] = pd_df['new_col'] * 2\npd_df['new_col'] = pd_df['new_col'].fillna(0)\n\n# Some operations return new DataFrames\npd_df = pd_df.sort_values('new_col')\n\nPolars consistently uses method chaining:\n\n# All operations return new DataFrames and can be chained\npl_df = (pl_df\n    .with_columns((pl.col('new_col') * 2).alias('new_col'))\n    .with_columns(pl.col('new_col').fill_null(0))\n    .sort('new_col')\n)\n\n\n\n\nPandas directly references columns:\n\npd_df['result'] = pd_df['age'] + pd_df['new_col']\nfiltered = pd_df[pd_df['age'] &gt; pd_df['age'].mean()]\n\nPolars uses an expression API:\n\npl_df = pl_df.with_columns(\n    (pl.col('age') + pl.col('new_col')).alias('result')\n)\nfiltered = pl_df.filter(pl.col('age') &gt; pl.col('age').mean())\n\n\n\n\n\nIf youâ€™re transitioning from pandas to polars, here are key mappings between common operations:\n\n\n\n\n\n\n\n\nOperation\nPandas\nPolars\n\n\n\n\nRead CSV\npd.read_csv('file.csv')\npl.read_csv('file.csv')\n\n\nSelect columns\ndf[['col1', 'col2']]\ndf.select(['col1', 'col2'])\n\n\nAdd column\ndf['new'] = df['col1'] * 2\ndf.with_columns((pl.col('col1') * 2).alias('new'))\n\n\nFilter rows\ndf[df['col'] &gt; 5]\ndf.filter(pl.col('col') &gt; 5)\n\n\nSort\ndf.sort_values('col')\ndf.sort('col')\n\n\nGroup by\ndf.groupby('col').agg({'val': 'sum'})\ndf.group_by('col').agg(pl.col('val').sum())\n\n\nJoin\ndf1.merge(df2, on='key')\ndf1.join(df2, on='key')\n\n\nFill NA\ndf.fillna(0)\ndf.fill_null(0)\n\n\nDrop NA\ndf.dropna()\ndf.drop_nulls()\n\n\nRename\ndf.rename(columns={'a': 'b'})\ndf.rename({'a': 'b'})\n\n\nUnique values\ndf['col'].unique()\ndf.select(pl.col('col').unique())\n\n\nValue counts\ndf['col'].value_counts()\ndf.group_by('col').count()\n\n\n\n\n\n\nThink in expressions: Use pl.col() to reference columns in operations\nEmbrace method chaining: String operations together instead of intermediate variables\nTry lazy execution: For complex operations, use pl.scan_csv() and lazy operations\nUse with_columns(): Instead of direct assignment, use with_columns for adding/modifying columns\nLearn the expression functions: Many operations like string manipulation use different syntax\n\n\n\n\nDespite Polarsâ€™ advantages, pandas might still be preferred when:\n\nWorking with existing codebases heavily dependent on pandas\nUsing specialized libraries that only support pandas (some visualization tools)\nDealing with very small datasets where performance isnâ€™t critical\nUsing pandas-specific features without polars equivalents\nWorking with time series data that benefits from pandasâ€™ specialized functionality\n\n\n\n\n\nPolars offers significant performance improvements and a more consistent API compared to pandas, particularly for large datasets and complex operations. While the syntax differences require some adjustment, the benefits in speed and memory efficiency make it a compelling choice for modern data analysis workflows.\nBoth libraries have their place in the Python data ecosystem. Pandas remains the more mature option with broader ecosystem compatibility, while Polars represents the future of high-performance data processing. For new projects dealing with large datasets, Polars is increasingly becoming the recommended choice."
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#table-of-contents",
    "href": "posts/pandas-to-polars/index.html#table-of-contents",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "Installation and Setup\nCreating DataFrames\nBasic Operations\nFiltering Data\nGrouping and Aggregation\nJoining/Merging DataFrames\nHandling Missing Values\nString Operations\nTime Series Operations\nPerformance Comparison\nAPI Philosophy Differences\nMigration Guide"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#installation-and-setup",
    "href": "posts/pandas-to-polars/index.html#installation-and-setup",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Import pandas\nimport pandas as pd\n\n\n\n\n\n# Import polars\nimport polars as pl"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#creating-dataframes",
    "href": "posts/pandas-to-polars/index.html#creating-dataframes",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "import pandas as pd\n\n# Create DataFrame from dictionary\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'age': [25, 30, 35, 40],\n    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n}\ndf_pd = pd.DataFrame(data)\nprint(df_pd)\n\n      name  age         city\n0    Alice   25     New York\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n3    David   40      Houston\n\n\n\n\n\n\nimport polars as pl\n\n# Create DataFrame from dictionary\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'age': [25, 30, 35, 40],\n    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n}\ndf_pl = pl.DataFrame(data)\nprint(df_pl)\n\nshape: (4, 3)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ name    â”† age â”† city        â”‚\nâ”‚ ---     â”† --- â”† ---         â”‚\nâ”‚ str     â”† i64 â”† str         â”‚\nâ•žâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ Alice   â”† 25  â”† New York    â”‚\nâ”‚ Bob     â”† 30  â”† Los Angeles â”‚\nâ”‚ Charlie â”† 35  â”† Chicago     â”‚\nâ”‚ David   â”† 40  â”† Houston     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#basic-operations",
    "href": "posts/pandas-to-polars/index.html#basic-operations",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Select a single column (returns Series)\nseries = df_pd['name']\n\n# Select multiple columns\ndf_subset = df_pd[['name', 'age']]\n\n\n\n\n\n# Select a single column (returns Series)\nseries = df_pl['name']\n# Alternative method\nseries = df_pl.select(pl.col('name')).to_series()\n\n# Select multiple columns\ndf_subset = df_pl.select(['name', 'age'])\n# Alternative method\ndf_subset = df_pl.select(pl.col(['name', 'age']))\n\n\n\n\n\n\n\n\n# Add a new column\ndf_pd['is_adult'] = df_pd['age'] &gt;= 18\n\n# Using assign (creates a new DataFrame)\ndf_pd = df_pd.assign(age_squared=df_pd['age'] ** 2)\n\n\n\n\n\n# Add a new column\ndf_pl = df_pl.with_columns(\n    pl.when(pl.col('age') &gt;= 18).then(True).otherwise(False).alias('is_adult')\n)\n\n# Creating derived columns \ndf_pl = df_pl.with_columns(\n    (pl.col('age') ** 2).alias('age_squared')\n)\n\n# Multiple columns at once\ndf_pl = df_pl.with_columns([\n    pl.col('age').is_null().alias('age_is_null'),\n    (pl.col('age') * 2).alias('age_doubled')\n])\n\n\n\n\n\n\n\n\n# Get summary statistics\nsummary = df_pd.describe()\n\n# Individual statistics\nmean_age = df_pd['age'].mean()\nmedian_age = df_pd['age'].median()\nmin_age = df_pd['age'].min()\nmax_age = df_pd['age'].max()\n\n\n\n\n\n# Get summary statistics\nsummary = df_pl.describe()\n\n# Individual statistics\nmean_age = df_pl.select(pl.col('age').mean()).item()\nmedian_age = df_pl.select(pl.col('age').median()).item()\nmin_age = df_pl.select(pl.col('age').min()).item()\nmax_age = df_pl.select(pl.col('age').max()).item()"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#filtering-data",
    "href": "posts/pandas-to-polars/index.html#filtering-data",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Filter rows\nadults = df_pd[df_pd['age'] &gt;= 18]\n\n# Multiple conditions\nfiltered = df_pd[(df_pd['age'] &gt; 30) & (df_pd['city'] == 'Chicago')]\n\n\n\n\n\n# Filter rows\nadults = df_pl.filter(pl.col('age') &gt;= 18)\n\n# Multiple conditions\nfiltered = df_pl.filter((pl.col('age') &gt; 30) & (pl.col('city') == 'Chicago'))\n\n\n\n\n\n\n\n\n# Filter with OR conditions\ndf_filtered = df_pd[(df_pd['city'] == 'New York') | (df_pd['city'] == 'Chicago')]\n\n# Using isin\ncities = ['New York', 'Chicago']\ndf_filtered = df_pd[df_pd['city'].isin(cities)]\n\n# String contains\ndf_filtered = df_pd[df_pd['name'].str.contains('li')]\n\n\n\n\n\n# Filter with OR conditions\ndf_filtered = df_pl.filter((pl.col('city') == 'New York') | (pl.col('city') == 'Chicago'))\n\n# Using is_in\ncities = ['New York', 'Chicago']\ndf_filtered = df_pl.filter(pl.col('city').is_in(cities))\n\n# String contains\ndf_filtered = df_pl.filter(pl.col('name').str.contains('li'))"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#grouping-and-aggregation",
    "href": "posts/pandas-to-polars/index.html#grouping-and-aggregation",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Group by one column and aggregate\ncity_stats = df_pd.groupby('city').agg({\n    'age': ['mean', 'min', 'max', 'count']\n})\n\n# Reset index for flat DataFrame\ncity_stats = city_stats.reset_index()\n\n\n\n\n\n# Group by one column and aggregate\ncity_stats = df_pl.group_by('city').agg([\n    pl.col('age').mean().alias('age_mean'),\n    pl.col('age').min().alias('age_min'),\n    pl.col('age').max().alias('age_max'),\n    pl.col('age').count().alias('age_count')\n])"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#joiningmerging-dataframes",
    "href": "posts/pandas-to-polars/index.html#joiningmerging-dataframes",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Create another DataFrame\nemployee_data = {\n    'emp_id': [1, 2, 3, 4],\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'dept': ['HR', 'IT', 'Finance', 'IT']\n}\nemployee_df_pd = pd.DataFrame(employee_data)\n\nsalary_data = {\n    'emp_id': [1, 2, 3, 5],\n    'salary': [50000, 60000, 70000, 80000]\n}\nsalary_df_pd = pd.DataFrame(salary_data)\n\n# Inner join\nmerged_df = employee_df_pd.merge(\n    salary_df_pd,\n    on='emp_id',\n    how='inner'\n)\n\n\n\n\n\n# Create another DataFrame\nemployee_data = {\n    'emp_id': [1, 2, 3, 4],\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'dept': ['HR', 'IT', 'Finance', 'IT']\n}\nemployee_df_pl = pl.DataFrame(employee_data)\n\nsalary_data = {\n    'emp_id': [1, 2, 3, 5],\n    'salary': [50000, 60000, 70000, 80000]\n}\nsalary_df_pl = pl.DataFrame(salary_data)\n\n# Inner join\nmerged_df = employee_df_pl.join(\n    salary_df_pl,\n    on='emp_id',\n    how='inner'\n)\n\n\n\n\n\n\n\n\n# Left join\nleft_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='left')\n\n# Right join\nright_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='right')\n\n# Outer join\nouter_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='outer')\n\n\n\n\n\n# Left join\nleft_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='left')\n\n# Right join\nright_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='right')\n\n# Outer join\nouter_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='full')"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#handling-missing-values",
    "href": "posts/pandas-to-polars/index.html#handling-missing-values",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Check for missing values\nmissing_count = df_pd.isnull().sum()\n\n# Check if any column has missing values\nhas_missing = df_pd.isnull().any().any()\n\n\n\n\n\n# Check for missing values\nmissing_count = df_pl.null_count()\n\n# Check if specific column has missing values\nhas_missing = df_pl.select(pl.col('age').is_null().any()).item()\n\n\n\n\n\n\n\n\n# Drop rows with any missing values\ndf_pd_clean = df_pd.dropna()\n\n# Fill missing values\ndf_pd_filled = df_pd.fillna({\n    'age': 0,\n    'city': 'Unknown'\n})\n\n# Forward fill\ndf_pd_ffill = df_pd.ffill()\n\n\n\n\n\n# Drop rows with any missing values\ndf_pl_clean = df_pl.drop_nulls()\n\n# Fill missing values\ndf_pl_filled = df_pl.with_columns([\n    pl.col('age').fill_null(0),\n    pl.col('city').fill_null('Unknown')\n])\n\n# Forward fill\ndf_pl_ffill = df_pl.with_columns([\n    pl.col('age').fill_null(strategy='forward'),\n    pl.col('city').fill_null(strategy='forward')\n])"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#string-operations",
    "href": "posts/pandas-to-polars/index.html#string-operations",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Convert to uppercase\ndf_pd['name_upper'] = df_pd['name'].str.upper()\n\n# Get string length\ndf_pd['name_length'] = df_pd['name'].str.len()\n\n# Extract substring\ndf_pd['name_first_char'] = df_pd['name'].str[0]\n\n# Replace substrings\ndf_pd['city_replaced'] = df_pd['city'].str.replace('New', 'Old')\n\n\n\n\n\n# Convert to uppercase\ndf_pl = df_pl.with_columns(pl.col('name').str.to_uppercase().alias('name_upper'))\n\n# Get string length\ndf_pl = df_pl.with_columns(pl.col('name').str.len_chars().alias('name_length'))\n\n# Extract substring \ndf_pl = df_pl.with_columns(pl.col('name').str.slice(0, 1).alias('name_first_char'))\n\n# Replace substrings\ndf_pl = df_pl.with_columns(pl.col('city').str.replace('New', 'Old').alias('city_replaced'))\n\n\n\n\n\n\n\n\n# Split string\ndf_pd['first_word'] = df_pd['city'].str.split(' ').str[0]\n\n# Pattern matching\nhas_new = df_pd['city'].str.contains('New')\n\n# Extract with regex\ndf_pd['extracted'] = df_pd['city'].str.extract(r'(\\w+)\\s')\n\n\n\n\n\n# Split string\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.split(' ').list.get(0).alias('first_word')\n)\n\n# Pattern matching\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.contains('New').alias('has_new')\n)\n\n# Extract with regex\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.extract(r'(\\w+)\\s').alias('extracted')\n)"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#time-series-operations",
    "href": "posts/pandas-to-polars/index.html#time-series-operations",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Create DataFrame with dates\ndates_pd = pd.DataFrame({\n    'date_str': ['2023-01-01', '2023-02-15', '2023-03-30']\n})\n\n# Parse dates\ndates_pd['date'] = pd.to_datetime(dates_pd['date_str'])\n\n# Extract components\ndates_pd['year'] = dates_pd['date'].dt.year\ndates_pd['month'] = dates_pd['date'].dt.month\ndates_pd['day'] = dates_pd['date'].dt.day\ndates_pd['weekday'] = dates_pd['date'].dt.day_name()\n\n\n\n\n\n# Create DataFrame with dates\ndates_pl = pl.DataFrame({\n    'date_str': ['2023-01-01', '2023-02-15', '2023-03-30']\n})\n\n# Parse dates\ndates_pl = dates_pl.with_columns(\n    pl.col('date_str').str.strptime(pl.Datetime, '%Y-%m-%d').alias('date')\n)\n\n# Extract components\ndates_pl = dates_pl.with_columns([\n    pl.col('date').dt.year().alias('year'),\n    pl.col('date').dt.month().alias('month'),\n    pl.col('date').dt.day().alias('day'),\n    pl.col('date').dt.weekday().replace_strict({\n        0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', \n        3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'\n    }, default=\"unknown\").alias('weekday')\n])\n\n\n\n\n\n\n\n\n# Add days\ndates_pd['next_week'] = dates_pd['date'] + pd.Timedelta(days=7)\n\n# Date difference\ndate_range = pd.date_range(start='2023-01-01', end='2023-01-10')\ndf_dates = pd.DataFrame({'date': date_range})\ndf_dates['days_since_start'] = (df_dates['date'] - df_dates['date'].min()).dt.days\n\n\n\n\n\n# Add days\ndates_pl = dates_pl.with_columns(\n    (pl.col('date') + pl.duration(days=7)).alias('next_week')\n)\n\n# Date difference\ndate_range = pd.date_range(start='2023-01-01', end='2023-01-10')  # Using pandas to generate range\ndf_dates = pl.DataFrame({'date': date_range})\ndf_dates = df_dates.with_columns(\n    (pl.col('date') - pl.col('date').min()).dt.total_days().alias('days_since_start')\n)"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#performance-comparison",
    "href": "posts/pandas-to-polars/index.html#performance-comparison",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "This section demonstrates performance differences between pandas and polars for a large dataset operation.\n\nimport pandas as pd\nimport polars as pl\nimport time\nimport numpy as np\n\n# Generate a large dataset (10 million rows)\nn = 10_000_000\ndata = {\n    'id': np.arange(n),\n    'value': np.random.randn(n),\n    'group': np.random.choice(['A', 'B', 'C', 'D'], n)\n}\n\n# Convert to pandas DataFrame\ndf_pd = pd.DataFrame(data)\n\n# Convert to polars DataFrame\ndf_pl = pl.DataFrame(data)\n\n# Benchmark: Group by and calculate mean, min, max\nprint(\"Running pandas groupby...\")\nstart = time.time()\nresult_pd = df_pd.groupby('group').agg({\n    'value': ['mean', 'min', 'max', 'count']\n})\npd_time = time.time() - start\nprint(f\"Pandas time: {pd_time:.4f} seconds\")\n\nprint(\"Running polars groupby...\")\nstart = time.time()\nresult_pl = df_pl.group_by('group').agg([\n    pl.col('value').mean().alias('value_mean'),\n    pl.col('value').min().alias('value_min'),\n    pl.col('value').max().alias('value_max'),\n    pl.col('value').count().alias('value_count')\n])\npl_time = time.time() - start\nprint(f\"Polars time: {pl_time:.4f} seconds\")\n\nprint(f\"Polars is {pd_time / pl_time:.2f}x faster\")\n\nRunning pandas groupby...\nPandas time: 0.2320 seconds\nRunning polars groupby...\nPolars time: 0.0545 seconds\nPolars is 4.26x faster\n\n\nTypically, for operations like this, Polars will be 3-10x faster than pandas, especially as data sizes increase. The performance gap widens further with more complex operations that can benefit from query optimization."
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#api-philosophy-differences",
    "href": "posts/pandas-to-polars/index.html#api-philosophy-differences",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "Pandas and Polars differ in several fundamental aspects:\n\n\nPandas uses eager execution by default:\nPolars supports both eager and lazy execution:\n\n\n\nPandas often uses assignment operations:\n\n# Many pandas operations use in-place assignment\npd_df['new_col'] = pd_df['new_col'] * 2\npd_df['new_col'] = pd_df['new_col'].fillna(0)\n\n# Some operations return new DataFrames\npd_df = pd_df.sort_values('new_col')\n\nPolars consistently uses method chaining:\n\n# All operations return new DataFrames and can be chained\npl_df = (pl_df\n    .with_columns((pl.col('new_col') * 2).alias('new_col'))\n    .with_columns(pl.col('new_col').fill_null(0))\n    .sort('new_col')\n)\n\n\n\n\nPandas directly references columns:\n\npd_df['result'] = pd_df['age'] + pd_df['new_col']\nfiltered = pd_df[pd_df['age'] &gt; pd_df['age'].mean()]\n\nPolars uses an expression API:\n\npl_df = pl_df.with_columns(\n    (pl.col('age') + pl.col('new_col')).alias('result')\n)\nfiltered = pl_df.filter(pl.col('age') &gt; pl.col('age').mean())"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#migration-guide",
    "href": "posts/pandas-to-polars/index.html#migration-guide",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "If youâ€™re transitioning from pandas to polars, here are key mappings between common operations:\n\n\n\n\n\n\n\n\nOperation\nPandas\nPolars\n\n\n\n\nRead CSV\npd.read_csv('file.csv')\npl.read_csv('file.csv')\n\n\nSelect columns\ndf[['col1', 'col2']]\ndf.select(['col1', 'col2'])\n\n\nAdd column\ndf['new'] = df['col1'] * 2\ndf.with_columns((pl.col('col1') * 2).alias('new'))\n\n\nFilter rows\ndf[df['col'] &gt; 5]\ndf.filter(pl.col('col') &gt; 5)\n\n\nSort\ndf.sort_values('col')\ndf.sort('col')\n\n\nGroup by\ndf.groupby('col').agg({'val': 'sum'})\ndf.group_by('col').agg(pl.col('val').sum())\n\n\nJoin\ndf1.merge(df2, on='key')\ndf1.join(df2, on='key')\n\n\nFill NA\ndf.fillna(0)\ndf.fill_null(0)\n\n\nDrop NA\ndf.dropna()\ndf.drop_nulls()\n\n\nRename\ndf.rename(columns={'a': 'b'})\ndf.rename({'a': 'b'})\n\n\nUnique values\ndf['col'].unique()\ndf.select(pl.col('col').unique())\n\n\nValue counts\ndf['col'].value_counts()\ndf.group_by('col').count()\n\n\n\n\n\n\nThink in expressions: Use pl.col() to reference columns in operations\nEmbrace method chaining: String operations together instead of intermediate variables\nTry lazy execution: For complex operations, use pl.scan_csv() and lazy operations\nUse with_columns(): Instead of direct assignment, use with_columns for adding/modifying columns\nLearn the expression functions: Many operations like string manipulation use different syntax\n\n\n\n\nDespite Polarsâ€™ advantages, pandas might still be preferred when:\n\nWorking with existing codebases heavily dependent on pandas\nUsing specialized libraries that only support pandas (some visualization tools)\nDealing with very small datasets where performance isnâ€™t critical\nUsing pandas-specific features without polars equivalents\nWorking with time series data that benefits from pandasâ€™ specialized functionality"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#conclusion",
    "href": "posts/pandas-to-polars/index.html#conclusion",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "Polars offers significant performance improvements and a more consistent API compared to pandas, particularly for large datasets and complex operations. While the syntax differences require some adjustment, the benefits in speed and memory efficiency make it a compelling choice for modern data analysis workflows.\nBoth libraries have their place in the Python data ecosystem. Pandas remains the more mature option with broader ecosystem compatibility, while Polars represents the future of high-performance data processing. For new projects dealing with large datasets, Polars is increasingly becoming the recommended choice."
  },
  {
    "objectID": "posts/kubeflow-pytorch/index.html",
    "href": "posts/kubeflow-pytorch/index.html",
    "title": "Kubeflow Deep Learning Guide with PyTorch",
    "section": "",
    "text": "Introduction\nPrerequisites\nSetting Up Your Environment\nCreating PyTorch Training Jobs\nDistributed Training\nHyperparameter Tuning with Katib\nModel Serving with KServe\nComplete Pipeline Example\nBest Practices\n\n\n\n\nKubeflow is a machine learning toolkit for Kubernetes that makes deployments of ML workflows on Kubernetes simple, portable, and scalable. This guide focuses on using Kubeflow with PyTorch for deep learning tasks.\n\n\n\nTraining Operator: For distributed training jobs\nKatib: For hyperparameter tuning and neural architecture search\nKServe: For model serving and inference\nPipelines: For ML workflow orchestration\nNotebooks: For interactive development\n\n\n\n\n\nBefore starting, ensure you have:\n\nKubernetes cluster with Kubeflow installed\nkubectl configured to access your cluster\nDocker for building container images\nBasic knowledge of PyTorch and Kubernetes\n\n\n\n\n\n\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: pytorch-training\n\n\n\nCreate a Dockerfile for your PyTorch environment:\nFROM pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime\n\nWORKDIR /app\n\n# Install additional dependencies\nRUN pip install --no-cache-dir \\\n    torchvision \\\n    tensorboard \\\n    scikit-learn \\\n    pandas \\\n    numpy \\\n    matplotlib \\\n    seaborn\n\n# Copy your training code\nCOPY . /app/\n\n# Set the default command\nCMD [\"python\", \"train.py\"]\n\n\n\n\n\n\nCreate a basic PyTorch training script (train.py):\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport argparse\nimport os\n\nclass SimpleNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(SimpleNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, 3, 1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, 3, 1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Dropout(0.25),\n            nn.Flatten(),\n            nn.Linear(9216, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, num_classes)\n        )\n    \n    def forward(self, x):\n        return self.features(x)\n\ndef train_epoch(model, device, train_loader, optimizer, criterion, epoch):\n    model.train()\n    total_loss = 0\n    correct = 0\n    \n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        pred = output.argmax(dim=1, keepdim=True)\n        correct += pred.eq(target.view_as(pred)).sum().item()\n        \n        if batch_idx % 100 == 0:\n            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] '\n                  f'Loss: {loss.item():.6f}')\n    \n    accuracy = 100. * correct / len(train_loader.dataset)\n    avg_loss = total_loss / len(train_loader)\n    print(f'Train Epoch: {epoch}, Average Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n    return avg_loss, accuracy\n\ndef test(model, device, test_loader, criterion):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    \n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += criterion(output, target).item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n    \n    test_loss /= len(test_loader)\n    accuracy = 100. * correct / len(test_loader.dataset)\n    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n    return test_loss, accuracy\n\ndef main():\n    parser = argparse.ArgumentParser(description='PyTorch MNIST Training')\n    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                        help='input batch size for testing (default: 1000)')\n    parser.add_argument('--epochs', type=int, default=10, metavar='N',\n                        help='number of epochs to train (default: 10)')\n    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n                        help='learning rate (default: 0.01)')\n    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n                        help='SGD momentum (default: 0.5)')\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='disables CUDA training')\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\n    parser.add_argument('--model-dir', type=str, default='/tmp/model',\n                        help='directory to save the model')\n    \n    args = parser.parse_args()\n    \n    torch.manual_seed(args.seed)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n    \n    # Data loading\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n    \n    train_dataset = datasets.MNIST('/tmp/data', train=True, download=True, transform=transform)\n    test_dataset = datasets.MNIST('/tmp/data', train=False, transform=transform)\n    \n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False)\n    \n    # Model, loss, and optimizer\n    model = SimpleNet().to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n    \n    # Training loop\n    for epoch in range(1, args.epochs + 1):\n        train_loss, train_acc = train_epoch(model, device, train_loader, optimizer, criterion, epoch)\n        test_loss, test_acc = test(model, device, test_loader, criterion)\n    \n    # Save model\n    os.makedirs(args.model_dir, exist_ok=True)\n    torch.save(model.state_dict(), f'{args.model_dir}/model.pth')\n    print(f'Model saved to {args.model_dir}/model.pth')\n\nif __name__ == '__main__':\n    main()\n\n\n\nCreate a pytorchjob.yaml file:\napiVersion: kubeflow.org/v1\nkind: PyTorchJob\nmetadata:\n  name: pytorch-mnist-training\n  namespace: pytorch-training\nspec:\n  pytorchReplicaSpecs:\n    Master:\n      replicas: 1\n      restartPolicy: OnFailure\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n        spec:\n          containers:\n          - name: pytorch\n            image: your-registry/pytorch-mnist:latest\n            imagePullPolicy: Always\n            command:\n            - python\n            - train.py\n            args:\n            - --epochs=20\n            - --batch-size=64\n            - --lr=0.01\n            - --model-dir=/mnt/model\n            resources:\n              requests:\n                memory: \"2Gi\"\n                cpu: \"1\"\n              limits:\n                memory: \"4Gi\"\n                cpu: \"2\"\n                nvidia.com/gpu: \"1\"\n            volumeMounts:\n            - name: model-storage\n              mountPath: /mnt/model\n          volumes:\n          - name: model-storage\n            persistentVolumeClaim:\n              claimName: model-pvc\n\n\n\n\nFor distributed training across multiple GPUs or nodes:\n\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nimport os\n\ndef setup(rank, world_size):\n    \"\"\"Initialize the distributed environment.\"\"\"\n    os.environ['MASTER_ADDR'] = os.environ.get('MASTER_ADDR', 'localhost')\n    os.environ['MASTER_PORT'] = os.environ.get('MASTER_PORT', '12355')\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    \"\"\"Clean up the distributed environment.\"\"\"\n    dist.destroy_process_group()\n\ndef train_distributed(rank, world_size, args):\n    setup(rank, world_size)\n    \n    device = torch.device(f\"cuda:{rank}\")\n    torch.cuda.set_device(device)\n    \n    # Create model and move to GPU\n    model = SimpleNet().to(device)\n    model = DDP(model, device_ids=[rank])\n    \n    # Create distributed sampler\n    train_sampler = DistributedSampler(train_dataset, \n                                       num_replicas=world_size, \n                                       rank=rank)\n    \n    train_loader = DataLoader(train_dataset, \n                              batch_size=args.batch_size,\n                              sampler=train_sampler,\n                              pin_memory=True)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=args.lr)\n    \n    # Training loop\n    for epoch in range(args.epochs):\n        train_sampler.set_epoch(epoch)\n        \n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n            \n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n            \n            if rank == 0 and batch_idx % 100 == 0:\n                print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}\")\n    \n    # Save model only on rank 0\n    if rank == 0:\n        torch.save(model.module.state_dict(), f'{args.model_dir}/distributed_model.pth')\n    \n    cleanup()\n\n\n\napiVersion: kubeflow.org/v1\nkind: PyTorchJob\nmetadata:\n  name: pytorch-distributed-training\n  namespace: pytorch-training\nspec:\n  pytorchReplicaSpecs:\n    Master:\n      replicas: 1\n      restartPolicy: OnFailure\n      template:\n        spec:\n          containers:\n          - name: pytorch\n            image: your-registry/pytorch-distributed:latest\n            command:\n            - python\n            - distributed_train.py\n            args:\n            - --epochs=50\n            - --batch-size=32\n            resources:\n              limits:\n                nvidia.com/gpu: \"1\"\n    Worker:\n      replicas: 3\n      restartPolicy: OnFailure\n      template:\n        spec:\n          containers:\n          - name: pytorch\n            image: your-registry/pytorch-distributed:latest\n            command:\n            - python\n            - distributed_train.py\n            args:\n            - --epochs=50\n            - --batch-size=32\n            resources:\n              limits:\n                nvidia.com/gpu: \"1\"\n\n\n\n\n\n\nCreate a katib-experiment.yaml:\napiVersion: kubeflow.org/v1beta1\nkind: Experiment\nmetadata:\n  name: pytorch-hyperparameter-tuning\n  namespace: pytorch-training\nspec:\n  algorithm:\n    algorithmName: random\n  objective:\n    type: maximize\n    goal: 0.95\n    objectiveMetricName: accuracy\n  parameters:\n  - name: lr\n    parameterType: double\n    feasibleSpace:\n      min: \"0.001\"\n      max: \"0.1\"\n  - name: batch-size\n    parameterType: int\n    feasibleSpace:\n      min: \"16\"\n      max: \"128\"\n  - name: momentum\n    parameterType: double\n    feasibleSpace:\n      min: \"0.1\"\n      max: \"0.9\"\n  trialTemplate:\n    primaryContainerName: training-container\n    trialSpec:\n      apiVersion: kubeflow.org/v1\n      kind: PyTorchJob\n      spec:\n        pytorchReplicaSpecs:\n          Master:\n            replicas: 1\n            restartPolicy: OnFailure\n            template:\n              spec:\n                containers:\n                - name: training-container\n                  image: your-registry/pytorch-katib:latest\n                  command:\n                  - python\n                  - train_with_metrics.py\n                  args:\n                  - --lr=${trialParameters.lr}\n                  - --batch-size=${trialParameters.batch-size}\n                  - --momentum=${trialParameters.momentum}\n                  - --epochs=10\n  parallelTrialCount: 3\n  maxTrialCount: 20\n  maxFailedTrialCount: 3\n\n\n\n# train_with_metrics.py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport argparse\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--lr', type=float, default=0.01)\n    parser.add_argument('--batch-size', type=int, default=64)\n    parser.add_argument('--momentum', type=float, default=0.5)\n    parser.add_argument('--epochs', type=int, default=10)\n    args = parser.parse_args()\n    \n    # Setup device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Data loading\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n    \n    train_dataset = datasets.MNIST('/tmp/data', train=True, download=True, transform=transform)\n    test_dataset = datasets.MNIST('/tmp/data', train=False, transform=transform)\n    \n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n    \n    # Model\n    model = SimpleNet().to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n    \n    # Training\n    for epoch in range(args.epochs):\n        model.train()\n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n    \n    # Evaluation\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            outputs = model(data)\n            _, predicted = torch.max(outputs.data, 1)\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n    \n    accuracy = correct / total\n    \n    # Print metrics for Katib (important format)\n    print(f\"accuracy={accuracy:.4f}\")\n    print(f\"loss={1-accuracy:.4f}\")\n\nif __name__ == '__main__':\n    main()\n\n\n\n\n\n\nFirst, create a custom predictor (predictor.py):\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms\nimport kserve\nfrom typing import Dict\nimport numpy as np\nfrom PIL import Image\nimport io\nimport base64\n\nclass SimpleNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(SimpleNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, 3, 1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, 3, 1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Dropout(0.25),\n            nn.Flatten(),\n            nn.Linear(9216, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, num_classes)\n        )\n    \n    def forward(self, x):\n        return self.features(x)\n\nclass PyTorchMNISTPredictor(kserve.Model):\n    def __init__(self, name: str):\n        super().__init__(name)\n        self.name = name\n        self.model = None\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n        self.ready = False\n\n    def load(self):\n        self.model = SimpleNet()\n        self.model.load_state_dict(torch.load('/mnt/models/model.pth', map_location='cpu'))\n        self.model.eval()\n        self.ready = True\n\n    def predict(self, payload: Dict) -&gt; Dict:\n        if not self.ready:\n            raise RuntimeError(\"Model not loaded\")\n        \n        # Decode base64 image\n        image_data = base64.b64decode(payload[\"instances\"][0][\"image\"])\n        image = Image.open(io.BytesIO(image_data)).convert('L')\n        \n        # Preprocess\n        input_tensor = self.transform(image).unsqueeze(0)\n        \n        # Predict\n        with torch.no_grad():\n            output = self.model(input_tensor)\n            probabilities = torch.softmax(output, dim=1)\n            predicted_class = torch.argmax(probabilities, dim=1).item()\n            confidence = probabilities[0][predicted_class].item()\n        \n        return {\n            \"predictions\": [{\n                \"class\": predicted_class,\n                \"confidence\": confidence,\n                \"probabilities\": probabilities[0].tolist()\n            }]\n        }\n\nif __name__ == \"__main__\":\n    model = PyTorchMNISTPredictor(\"pytorch-mnist-predictor\")\n    model.load()\n    kserve.ModelServer().start([model])\n\n\n\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: pytorch-mnist-predictor\n  namespace: pytorch-training\nspec:\n  predictor:\n    containers:\n    - name: kserve-container\n      image: your-registry/pytorch-predictor:latest\n      ports:\n      - containerPort: 8080\n        protocol: TCP\n      volumeMounts:\n      - name: model-storage\n        mountPath: /mnt/models\n      resources:\n        requests:\n          cpu: \"100m\"\n          memory: \"1Gi\"\n        limits:\n          cpu: \"1\"\n          memory: \"2Gi\"\n    volumes:\n    - name: model-storage\n      persistentVolumeClaim:\n        claimName: model-pvc\n\n\n\n\n\n\nimport kfp\nfrom kfp import dsl\nfrom kfp.components import create_component_from_func\n\ndef preprocess_data_op():\n    return dsl.ContainerOp(\n        name='preprocess-data',\n        image='your-registry/data-preprocessing:latest',\n        command=['python', 'preprocess.py'],\n        file_outputs={'dataset_path': '/tmp/dataset_path.txt'}\n    )\n\ndef train_model_op(dataset_path, lr: float = 0.01, batch_size: int = 64):\n    return dsl.ContainerOp(\n        name='train-model',\n        image='your-registry/pytorch-training:latest',\n        command=['python', 'train.py'],\n        arguments=[\n            '--data-path', dataset_path,\n            '--lr', lr,\n            '--batch-size', batch_size,\n            '--model-dir', '/tmp/model'\n        ],\n        file_outputs={'model_path': '/tmp/model_path.txt'}\n    )\n\ndef evaluate_model_op(model_path, dataset_path):\n    return dsl.ContainerOp(\n        name='evaluate-model',\n        image='your-registry/pytorch-evaluation:latest',\n        command=['python', 'evaluate.py'],\n        arguments=[\n            '--model-path', model_path,\n            '--data-path', dataset_path\n        ],\n        file_outputs={'metrics': '/tmp/metrics.json'}\n    )\n\ndef deploy_model_op(model_path):\n    return dsl.ContainerOp(\n        name='deploy-model',\n        image='your-registry/model-deployment:latest',\n        command=['python', 'deploy.py'],\n        arguments=['--model-path', model_path]\n    )\n\n@dsl.pipeline(\n    name='PyTorch Training Pipeline',\n    description='Complete PyTorch training and deployment pipeline'\n)\ndef pytorch_training_pipeline(\n    lr: float = 0.01,\n    batch_size: int = 64,\n    epochs: int = 10\n):\n    # Data preprocessing\n    preprocess_task = preprocess_data_op()\n    \n    # Model training\n    train_task = train_model_op(\n        dataset_path=preprocess_task.outputs['dataset_path'],\n        lr=lr,\n        batch_size=batch_size\n    )\n    \n    # Model evaluation\n    evaluate_task = evaluate_model_op(\n        model_path=train_task.outputs['model_path'],\n        dataset_path=preprocess_task.outputs['dataset_path']\n    )\n    \n    # Conditional deployment based on accuracy\n    with dsl.Condition(evaluate_task.outputs['metrics'], '&gt;', '0.9'):\n        deploy_task = deploy_model_op(\n            model_path=train_task.outputs['model_path']\n        )\n\n# Compile and run the pipeline\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pytorch_training_pipeline, 'pytorch_pipeline.yaml')\n\n\n\n\n\n\n\nAlways specify resource requests and limits\nUse GPU resources efficiently with proper scheduling\nImplement proper cleanup procedures\n\n\n\n\n\nUse persistent volumes for model storage\nImplement data versioning\nUse distributed storage for large datasets\n\n\n\n\n\nImplement comprehensive logging\nUse metrics collection for model performance\nSet up alerts for training failures\n\n\n\n\n\nUse proper RBAC configurations\nSecure container images\nImplement secrets management for sensitive data\n\n\n\n\n\nDesign for horizontal scaling\nUse distributed training for large models\nImplement efficient data loading pipelines\n\n\n\n\n\nTag and version your models\nImplement A/B testing for model deployments\nUse model registries for tracking\n\n\n\n\n\nImplement robust error handling in training scripts\nUse appropriate restart policies\nSet up proper monitoring and alerting\n\nThis guide provides a comprehensive foundation for using Kubeflow with PyTorch for deep learning workflows. Adapt the examples to your specific use cases and requirements."
  },
  {
    "objectID": "posts/kubeflow-pytorch/index.html#table-of-contents",
    "href": "posts/kubeflow-pytorch/index.html#table-of-contents",
    "title": "Kubeflow Deep Learning Guide with PyTorch",
    "section": "",
    "text": "Introduction\nPrerequisites\nSetting Up Your Environment\nCreating PyTorch Training Jobs\nDistributed Training\nHyperparameter Tuning with Katib\nModel Serving with KServe\nComplete Pipeline Example\nBest Practices"
  },
  {
    "objectID": "posts/kubeflow-pytorch/index.html#introduction",
    "href": "posts/kubeflow-pytorch/index.html#introduction",
    "title": "Kubeflow Deep Learning Guide with PyTorch",
    "section": "",
    "text": "Kubeflow is a machine learning toolkit for Kubernetes that makes deployments of ML workflows on Kubernetes simple, portable, and scalable. This guide focuses on using Kubeflow with PyTorch for deep learning tasks.\n\n\n\nTraining Operator: For distributed training jobs\nKatib: For hyperparameter tuning and neural architecture search\nKServe: For model serving and inference\nPipelines: For ML workflow orchestration\nNotebooks: For interactive development"
  },
  {
    "objectID": "posts/kubeflow-pytorch/index.html#prerequisites",
    "href": "posts/kubeflow-pytorch/index.html#prerequisites",
    "title": "Kubeflow Deep Learning Guide with PyTorch",
    "section": "",
    "text": "Before starting, ensure you have:\n\nKubernetes cluster with Kubeflow installed\nkubectl configured to access your cluster\nDocker for building container images\nBasic knowledge of PyTorch and Kubernetes"
  },
  {
    "objectID": "posts/kubeflow-pytorch/index.html#setting-up-your-environment",
    "href": "posts/kubeflow-pytorch/index.html#setting-up-your-environment",
    "title": "Kubeflow Deep Learning Guide with PyTorch",
    "section": "",
    "text": "apiVersion: v1\nkind: Namespace\nmetadata:\n  name: pytorch-training\n\n\n\nCreate a Dockerfile for your PyTorch environment:\nFROM pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime\n\nWORKDIR /app\n\n# Install additional dependencies\nRUN pip install --no-cache-dir \\\n    torchvision \\\n    tensorboard \\\n    scikit-learn \\\n    pandas \\\n    numpy \\\n    matplotlib \\\n    seaborn\n\n# Copy your training code\nCOPY . /app/\n\n# Set the default command\nCMD [\"python\", \"train.py\"]"
  },
  {
    "objectID": "posts/kubeflow-pytorch/index.html#creating-pytorch-training-jobs",
    "href": "posts/kubeflow-pytorch/index.html#creating-pytorch-training-jobs",
    "title": "Kubeflow Deep Learning Guide with PyTorch",
    "section": "",
    "text": "Create a basic PyTorch training script (train.py):\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport argparse\nimport os\n\nclass SimpleNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(SimpleNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, 3, 1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, 3, 1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Dropout(0.25),\n            nn.Flatten(),\n            nn.Linear(9216, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, num_classes)\n        )\n    \n    def forward(self, x):\n        return self.features(x)\n\ndef train_epoch(model, device, train_loader, optimizer, criterion, epoch):\n    model.train()\n    total_loss = 0\n    correct = 0\n    \n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        pred = output.argmax(dim=1, keepdim=True)\n        correct += pred.eq(target.view_as(pred)).sum().item()\n        \n        if batch_idx % 100 == 0:\n            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] '\n                  f'Loss: {loss.item():.6f}')\n    \n    accuracy = 100. * correct / len(train_loader.dataset)\n    avg_loss = total_loss / len(train_loader)\n    print(f'Train Epoch: {epoch}, Average Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n    return avg_loss, accuracy\n\ndef test(model, device, test_loader, criterion):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    \n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += criterion(output, target).item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n    \n    test_loss /= len(test_loader)\n    accuracy = 100. * correct / len(test_loader.dataset)\n    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n    return test_loss, accuracy\n\ndef main():\n    parser = argparse.ArgumentParser(description='PyTorch MNIST Training')\n    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                        help='input batch size for testing (default: 1000)')\n    parser.add_argument('--epochs', type=int, default=10, metavar='N',\n                        help='number of epochs to train (default: 10)')\n    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n                        help='learning rate (default: 0.01)')\n    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n                        help='SGD momentum (default: 0.5)')\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='disables CUDA training')\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\n    parser.add_argument('--model-dir', type=str, default='/tmp/model',\n                        help='directory to save the model')\n    \n    args = parser.parse_args()\n    \n    torch.manual_seed(args.seed)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n    \n    # Data loading\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n    \n    train_dataset = datasets.MNIST('/tmp/data', train=True, download=True, transform=transform)\n    test_dataset = datasets.MNIST('/tmp/data', train=False, transform=transform)\n    \n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False)\n    \n    # Model, loss, and optimizer\n    model = SimpleNet().to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n    \n    # Training loop\n    for epoch in range(1, args.epochs + 1):\n        train_loss, train_acc = train_epoch(model, device, train_loader, optimizer, criterion, epoch)\n        test_loss, test_acc = test(model, device, test_loader, criterion)\n    \n    # Save model\n    os.makedirs(args.model_dir, exist_ok=True)\n    torch.save(model.state_dict(), f'{args.model_dir}/model.pth')\n    print(f'Model saved to {args.model_dir}/model.pth')\n\nif __name__ == '__main__':\n    main()\n\n\n\nCreate a pytorchjob.yaml file:\napiVersion: kubeflow.org/v1\nkind: PyTorchJob\nmetadata:\n  name: pytorch-mnist-training\n  namespace: pytorch-training\nspec:\n  pytorchReplicaSpecs:\n    Master:\n      replicas: 1\n      restartPolicy: OnFailure\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n        spec:\n          containers:\n          - name: pytorch\n            image: your-registry/pytorch-mnist:latest\n            imagePullPolicy: Always\n            command:\n            - python\n            - train.py\n            args:\n            - --epochs=20\n            - --batch-size=64\n            - --lr=0.01\n            - --model-dir=/mnt/model\n            resources:\n              requests:\n                memory: \"2Gi\"\n                cpu: \"1\"\n              limits:\n                memory: \"4Gi\"\n                cpu: \"2\"\n                nvidia.com/gpu: \"1\"\n            volumeMounts:\n            - name: model-storage\n              mountPath: /mnt/model\n          volumes:\n          - name: model-storage\n            persistentVolumeClaim:\n              claimName: model-pvc"
  },
  {
    "objectID": "posts/kubeflow-pytorch/index.html#distributed-training",
    "href": "posts/kubeflow-pytorch/index.html#distributed-training",
    "title": "Kubeflow Deep Learning Guide with PyTorch",
    "section": "",
    "text": "For distributed training across multiple GPUs or nodes:\n\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nimport os\n\ndef setup(rank, world_size):\n    \"\"\"Initialize the distributed environment.\"\"\"\n    os.environ['MASTER_ADDR'] = os.environ.get('MASTER_ADDR', 'localhost')\n    os.environ['MASTER_PORT'] = os.environ.get('MASTER_PORT', '12355')\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    \"\"\"Clean up the distributed environment.\"\"\"\n    dist.destroy_process_group()\n\ndef train_distributed(rank, world_size, args):\n    setup(rank, world_size)\n    \n    device = torch.device(f\"cuda:{rank}\")\n    torch.cuda.set_device(device)\n    \n    # Create model and move to GPU\n    model = SimpleNet().to(device)\n    model = DDP(model, device_ids=[rank])\n    \n    # Create distributed sampler\n    train_sampler = DistributedSampler(train_dataset, \n                                       num_replicas=world_size, \n                                       rank=rank)\n    \n    train_loader = DataLoader(train_dataset, \n                              batch_size=args.batch_size,\n                              sampler=train_sampler,\n                              pin_memory=True)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=args.lr)\n    \n    # Training loop\n    for epoch in range(args.epochs):\n        train_sampler.set_epoch(epoch)\n        \n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n            \n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n            \n            if rank == 0 and batch_idx % 100 == 0:\n                print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}\")\n    \n    # Save model only on rank 0\n    if rank == 0:\n        torch.save(model.module.state_dict(), f'{args.model_dir}/distributed_model.pth')\n    \n    cleanup()\n\n\n\napiVersion: kubeflow.org/v1\nkind: PyTorchJob\nmetadata:\n  name: pytorch-distributed-training\n  namespace: pytorch-training\nspec:\n  pytorchReplicaSpecs:\n    Master:\n      replicas: 1\n      restartPolicy: OnFailure\n      template:\n        spec:\n          containers:\n          - name: pytorch\n            image: your-registry/pytorch-distributed:latest\n            command:\n            - python\n            - distributed_train.py\n            args:\n            - --epochs=50\n            - --batch-size=32\n            resources:\n              limits:\n                nvidia.com/gpu: \"1\"\n    Worker:\n      replicas: 3\n      restartPolicy: OnFailure\n      template:\n        spec:\n          containers:\n          - name: pytorch\n            image: your-registry/pytorch-distributed:latest\n            command:\n            - python\n            - distributed_train.py\n            args:\n            - --epochs=50\n            - --batch-size=32\n            resources:\n              limits:\n                nvidia.com/gpu: \"1\""
  },
  {
    "objectID": "posts/kubeflow-pytorch/index.html#hyperparameter-tuning-with-katib",
    "href": "posts/kubeflow-pytorch/index.html#hyperparameter-tuning-with-katib",
    "title": "Kubeflow Deep Learning Guide with PyTorch",
    "section": "",
    "text": "Create a katib-experiment.yaml:\napiVersion: kubeflow.org/v1beta1\nkind: Experiment\nmetadata:\n  name: pytorch-hyperparameter-tuning\n  namespace: pytorch-training\nspec:\n  algorithm:\n    algorithmName: random\n  objective:\n    type: maximize\n    goal: 0.95\n    objectiveMetricName: accuracy\n  parameters:\n  - name: lr\n    parameterType: double\n    feasibleSpace:\n      min: \"0.001\"\n      max: \"0.1\"\n  - name: batch-size\n    parameterType: int\n    feasibleSpace:\n      min: \"16\"\n      max: \"128\"\n  - name: momentum\n    parameterType: double\n    feasibleSpace:\n      min: \"0.1\"\n      max: \"0.9\"\n  trialTemplate:\n    primaryContainerName: training-container\n    trialSpec:\n      apiVersion: kubeflow.org/v1\n      kind: PyTorchJob\n      spec:\n        pytorchReplicaSpecs:\n          Master:\n            replicas: 1\n            restartPolicy: OnFailure\n            template:\n              spec:\n                containers:\n                - name: training-container\n                  image: your-registry/pytorch-katib:latest\n                  command:\n                  - python\n                  - train_with_metrics.py\n                  args:\n                  - --lr=${trialParameters.lr}\n                  - --batch-size=${trialParameters.batch-size}\n                  - --momentum=${trialParameters.momentum}\n                  - --epochs=10\n  parallelTrialCount: 3\n  maxTrialCount: 20\n  maxFailedTrialCount: 3\n\n\n\n# train_with_metrics.py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport argparse\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--lr', type=float, default=0.01)\n    parser.add_argument('--batch-size', type=int, default=64)\n    parser.add_argument('--momentum', type=float, default=0.5)\n    parser.add_argument('--epochs', type=int, default=10)\n    args = parser.parse_args()\n    \n    # Setup device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Data loading\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n    \n    train_dataset = datasets.MNIST('/tmp/data', train=True, download=True, transform=transform)\n    test_dataset = datasets.MNIST('/tmp/data', train=False, transform=transform)\n    \n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n    \n    # Model\n    model = SimpleNet().to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n    \n    # Training\n    for epoch in range(args.epochs):\n        model.train()\n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n    \n    # Evaluation\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            outputs = model(data)\n            _, predicted = torch.max(outputs.data, 1)\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n    \n    accuracy = correct / total\n    \n    # Print metrics for Katib (important format)\n    print(f\"accuracy={accuracy:.4f}\")\n    print(f\"loss={1-accuracy:.4f}\")\n\nif __name__ == '__main__':\n    main()"
  },
  {
    "objectID": "posts/kubeflow-pytorch/index.html#model-serving-with-kserve",
    "href": "posts/kubeflow-pytorch/index.html#model-serving-with-kserve",
    "title": "Kubeflow Deep Learning Guide with PyTorch",
    "section": "",
    "text": "First, create a custom predictor (predictor.py):\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms\nimport kserve\nfrom typing import Dict\nimport numpy as np\nfrom PIL import Image\nimport io\nimport base64\n\nclass SimpleNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(SimpleNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, 3, 1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, 3, 1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Dropout(0.25),\n            nn.Flatten(),\n            nn.Linear(9216, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, num_classes)\n        )\n    \n    def forward(self, x):\n        return self.features(x)\n\nclass PyTorchMNISTPredictor(kserve.Model):\n    def __init__(self, name: str):\n        super().__init__(name)\n        self.name = name\n        self.model = None\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n        self.ready = False\n\n    def load(self):\n        self.model = SimpleNet()\n        self.model.load_state_dict(torch.load('/mnt/models/model.pth', map_location='cpu'))\n        self.model.eval()\n        self.ready = True\n\n    def predict(self, payload: Dict) -&gt; Dict:\n        if not self.ready:\n            raise RuntimeError(\"Model not loaded\")\n        \n        # Decode base64 image\n        image_data = base64.b64decode(payload[\"instances\"][0][\"image\"])\n        image = Image.open(io.BytesIO(image_data)).convert('L')\n        \n        # Preprocess\n        input_tensor = self.transform(image).unsqueeze(0)\n        \n        # Predict\n        with torch.no_grad():\n            output = self.model(input_tensor)\n            probabilities = torch.softmax(output, dim=1)\n            predicted_class = torch.argmax(probabilities, dim=1).item()\n            confidence = probabilities[0][predicted_class].item()\n        \n        return {\n            \"predictions\": [{\n                \"class\": predicted_class,\n                \"confidence\": confidence,\n                \"probabilities\": probabilities[0].tolist()\n            }]\n        }\n\nif __name__ == \"__main__\":\n    model = PyTorchMNISTPredictor(\"pytorch-mnist-predictor\")\n    model.load()\n    kserve.ModelServer().start([model])\n\n\n\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: pytorch-mnist-predictor\n  namespace: pytorch-training\nspec:\n  predictor:\n    containers:\n    - name: kserve-container\n      image: your-registry/pytorch-predictor:latest\n      ports:\n      - containerPort: 8080\n        protocol: TCP\n      volumeMounts:\n      - name: model-storage\n        mountPath: /mnt/models\n      resources:\n        requests:\n          cpu: \"100m\"\n          memory: \"1Gi\"\n        limits:\n          cpu: \"1\"\n          memory: \"2Gi\"\n    volumes:\n    - name: model-storage\n      persistentVolumeClaim:\n        claimName: model-pvc"
  },
  {
    "objectID": "posts/kubeflow-pytorch/index.html#complete-pipeline-example",
    "href": "posts/kubeflow-pytorch/index.html#complete-pipeline-example",
    "title": "Kubeflow Deep Learning Guide with PyTorch",
    "section": "",
    "text": "import kfp\nfrom kfp import dsl\nfrom kfp.components import create_component_from_func\n\ndef preprocess_data_op():\n    return dsl.ContainerOp(\n        name='preprocess-data',\n        image='your-registry/data-preprocessing:latest',\n        command=['python', 'preprocess.py'],\n        file_outputs={'dataset_path': '/tmp/dataset_path.txt'}\n    )\n\ndef train_model_op(dataset_path, lr: float = 0.01, batch_size: int = 64):\n    return dsl.ContainerOp(\n        name='train-model',\n        image='your-registry/pytorch-training:latest',\n        command=['python', 'train.py'],\n        arguments=[\n            '--data-path', dataset_path,\n            '--lr', lr,\n            '--batch-size', batch_size,\n            '--model-dir', '/tmp/model'\n        ],\n        file_outputs={'model_path': '/tmp/model_path.txt'}\n    )\n\ndef evaluate_model_op(model_path, dataset_path):\n    return dsl.ContainerOp(\n        name='evaluate-model',\n        image='your-registry/pytorch-evaluation:latest',\n        command=['python', 'evaluate.py'],\n        arguments=[\n            '--model-path', model_path,\n            '--data-path', dataset_path\n        ],\n        file_outputs={'metrics': '/tmp/metrics.json'}\n    )\n\ndef deploy_model_op(model_path):\n    return dsl.ContainerOp(\n        name='deploy-model',\n        image='your-registry/model-deployment:latest',\n        command=['python', 'deploy.py'],\n        arguments=['--model-path', model_path]\n    )\n\n@dsl.pipeline(\n    name='PyTorch Training Pipeline',\n    description='Complete PyTorch training and deployment pipeline'\n)\ndef pytorch_training_pipeline(\n    lr: float = 0.01,\n    batch_size: int = 64,\n    epochs: int = 10\n):\n    # Data preprocessing\n    preprocess_task = preprocess_data_op()\n    \n    # Model training\n    train_task = train_model_op(\n        dataset_path=preprocess_task.outputs['dataset_path'],\n        lr=lr,\n        batch_size=batch_size\n    )\n    \n    # Model evaluation\n    evaluate_task = evaluate_model_op(\n        model_path=train_task.outputs['model_path'],\n        dataset_path=preprocess_task.outputs['dataset_path']\n    )\n    \n    # Conditional deployment based on accuracy\n    with dsl.Condition(evaluate_task.outputs['metrics'], '&gt;', '0.9'):\n        deploy_task = deploy_model_op(\n            model_path=train_task.outputs['model_path']\n        )\n\n# Compile and run the pipeline\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pytorch_training_pipeline, 'pytorch_pipeline.yaml')"
  },
  {
    "objectID": "posts/kubeflow-pytorch/index.html#best-practices",
    "href": "posts/kubeflow-pytorch/index.html#best-practices",
    "title": "Kubeflow Deep Learning Guide with PyTorch",
    "section": "",
    "text": "Always specify resource requests and limits\nUse GPU resources efficiently with proper scheduling\nImplement proper cleanup procedures\n\n\n\n\n\nUse persistent volumes for model storage\nImplement data versioning\nUse distributed storage for large datasets\n\n\n\n\n\nImplement comprehensive logging\nUse metrics collection for model performance\nSet up alerts for training failures\n\n\n\n\n\nUse proper RBAC configurations\nSecure container images\nImplement secrets management for sensitive data\n\n\n\n\n\nDesign for horizontal scaling\nUse distributed training for large models\nImplement efficient data loading pipelines\n\n\n\n\n\nTag and version your models\nImplement A/B testing for model deployments\nUse model registries for tracking\n\n\n\n\n\nImplement robust error handling in training scripts\nUse appropriate restart policies\nSet up proper monitoring and alerting\n\nThis guide provides a comprehensive foundation for using Kubeflow with PyTorch for deep learning workflows. Adapt the examples to your specific use cases and requirements."
  },
  {
    "objectID": "posts/mobilenet-deployment/index.html",
    "href": "posts/mobilenet-deployment/index.html",
    "title": "MobileNetV2 PyTorch Docker Deployment Guide",
    "section": "",
    "text": "This guide walks you through deploying a pre-trained MobileNetV2 model using PyTorch and Docker, creating a REST API for image classification.\n\n\nmobilenetv2-pytorch-docker/\nâ”œâ”€â”€ app/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ main.py\nâ”‚   â”œâ”€â”€ model_handler.py\nâ”‚   â””â”€â”€ utils.py\nâ”œâ”€â”€ requirements.txt\nâ”œâ”€â”€ Dockerfile\nâ”œâ”€â”€ docker-compose.yml\nâ”œâ”€â”€ .dockerignore\nâ””â”€â”€ README.md\n\n\n\n\n\nfrom fastapi import FastAPI, File, UploadFile, HTTPException\nfrom fastapi.responses import JSONResponse\nimport uvicorn\nimport io\nfrom PIL import Image\nimport numpy as np\nfrom .model_handler import MobileNetV2Handler\nfrom .utils import preprocess_image, decode_predictions\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\napp = FastAPI(\n    title=\"MobileNetV2 PyTorch Image Classification API\",\n    description=\"Deploy MobileNetV2 using PyTorch for image classification\",\n    version=\"1.0.0\"\n)\n\n# Initialize model handler\nmodel_handler = MobileNetV2Handler()\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    \"\"\"Load model on startup\"\"\"\n    try:\n        model_handler.load_model()\n        logger.info(\"Model loaded successfully\")\n    except Exception as e:\n        logger.error(f\"Failed to load model: {e}\")\n        raise\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"MobileNetV2 PyTorch Classification API\", \"status\": \"running\"}\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\", \"model_loaded\": model_handler.is_loaded()}\n\n@app.post(\"/predict\")\nasync def predict(file: UploadFile = File(...)):\n    \"\"\"\n    Predict image class using MobileNetV2\n    \"\"\"\n    if not file.content_type.startswith(\"image/\"):\n        raise HTTPException(status_code=400, detail=\"File must be an image\")\n    \n    try:\n        # Read and preprocess image\n        image_data = await file.read()\n        image = Image.open(io.BytesIO(image_data))\n        \n        if image.mode != 'RGB':\n            image = image.convert('RGB')\n        \n        # Preprocess for MobileNetV2\n        processed_image = preprocess_image(image)\n        \n        # Make prediction\n        predictions = model_handler.predict(processed_image)\n        \n        # Decode predictions\n        decoded_predictions = decode_predictions(predictions, top=5)\n        \n        return JSONResponse(content={\n            \"predictions\": decoded_predictions,\n            \"success\": True\n        })\n        \n    except Exception as e:\n        logger.error(f\"Prediction error: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Prediction failed: {str(e)}\")\n\n@app.post(\"/batch_predict\")\nasync def batch_predict(files: list[UploadFile] = File(...)):\n    \"\"\"\n    Batch prediction for multiple images\n    \"\"\"\n    if len(files) &gt; 10:  # Limit batch size\n        raise HTTPException(status_code=400, detail=\"Maximum 10 images allowed per batch\")\n    \n    results = []\n    \n    for file in files:\n        if not file.content_type.startswith(\"image/\"):\n            results.append({\n                \"filename\": file.filename,\n                \"error\": \"File must be an image\"\n            })\n            continue\n        \n        try:\n            image_data = await file.read()\n            image = Image.open(io.BytesIO(image_data))\n            \n            if image.mode != 'RGB':\n                image = image.convert('RGB')\n            \n            processed_image = preprocess_image(image)\n            predictions = model_handler.predict(processed_image)\n            decoded_predictions = decode_predictions(predictions, top=3)\n            \n            results.append({\n                \"filename\": file.filename,\n                \"predictions\": decoded_predictions,\n                \"success\": True\n            })\n            \n        except Exception as e:\n            results.append({\n                \"filename\": file.filename,\n                \"error\": str(e),\n                \"success\": False\n            })\n    \n    return JSONResponse(content={\"results\": results})\n\n@app.get(\"/model_info\")\nasync def model_info():\n    \"\"\"Get model information\"\"\"\n    return {\n        \"model_name\": \"MobileNetV2\",\n        \"framework\": \"PyTorch\",\n        \"input_size\": [224, 224],\n        \"num_classes\": 1000,\n        \"pretrained\": True\n    }\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\n\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models\nimport numpy as np\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass MobileNetV2Handler:\n    def __init__(self):\n        self.model = None\n        self.device = None\n        self._loaded = False\n        \n    def load_model(self):\n        \"\"\"Load pre-trained MobileNetV2 model\"\"\"\n        try:\n            logger.info(\"Loading MobileNetV2 PyTorch model...\")\n            \n            # Determine device (CPU/GPU)\n            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n            logger.info(f\"Using device: {self.device}\")\n            \n            # Load pre-trained MobileNetV2\n            self.model = models.mobilenet_v2(pretrained=True)\n            self.model.eval()  # Set to evaluation mode\n            self.model.to(self.device)\n            \n            # Warm up the model with a dummy prediction\n            dummy_input = torch.randn(1, 3, 224, 224).to(self.device)\n            with torch.no_grad():\n                _ = self.model(dummy_input)\n            \n            self._loaded = True\n            logger.info(\"Model loaded and warmed up successfully\")\n            \n        except Exception as e:\n            logger.error(f\"Failed to load model: {e}\")\n            raise\n    \n    def predict(self, image_tensor):\n        \"\"\"Make prediction on preprocessed image tensor\"\"\"\n        if not self._loaded:\n            raise RuntimeError(\"Model not loaded\")\n        \n        try:\n            # Ensure tensor is on correct device\n            if isinstance(image_tensor, np.ndarray):\n                image_tensor = torch.from_numpy(image_tensor)\n            \n            image_tensor = image_tensor.to(self.device)\n            \n            # Ensure batch dimension\n            if len(image_tensor.shape) == 3:\n                image_tensor = image_tensor.unsqueeze(0)\n            \n            # Make prediction\n            with torch.no_grad():\n                outputs = self.model(image_tensor)\n                # Apply softmax to get probabilities\n                probabilities = torch.nn.functional.softmax(outputs, dim=1)\n            \n            return probabilities.cpu().numpy()\n            \n        except Exception as e:\n            logger.error(f\"Prediction failed: {e}\")\n            raise\n    \n    def predict_batch(self, image_tensors):\n        \"\"\"Make batch predictions\"\"\"\n        if not self._loaded:\n            raise RuntimeError(\"Model not loaded\")\n        \n        try:\n            # Convert to tensor if numpy array\n            if isinstance(image_tensors, np.ndarray):\n                image_tensors = torch.from_numpy(image_tensors)\n            \n            image_tensors = image_tensors.to(self.device)\n            \n            # Make batch prediction\n            with torch.no_grad():\n                outputs = self.model(image_tensors)\n                probabilities = torch.nn.functional.softmax(outputs, dim=1)\n            \n            return probabilities.cpu().numpy()\n            \n        except Exception as e:\n            logger.error(f\"Batch prediction failed: {e}\")\n            raise\n    \n    def is_loaded(self):\n        \"\"\"Check if model is loaded\"\"\"\n        return self._loaded\n    \n    def get_device(self):\n        \"\"\"Get current device\"\"\"\n        return str(self.device) if self.device else \"not initialized\"\n\n\n\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom torchvision import transforms\nimport json\nimport os\nimport requests\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# ImageNet class labels\nIMAGENET_CLASSES_URL = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n\ndef get_imagenet_classes():\n    \"\"\"Download and cache ImageNet class labels\"\"\"\n    try:\n        if os.path.exists(\"imagenet_classes.txt\"):\n            with open(\"imagenet_classes.txt\", \"r\") as f:\n                classes = [line.strip() for line in f.readlines()]\n        else:\n            logger.info(\"Downloading ImageNet class labels...\")\n            response = requests.get(IMAGENET_CLASSES_URL)\n            classes = response.text.strip().split('\\n')\n            \n            # Cache the classes\n            with open(\"imagenet_classes.txt\", \"w\") as f:\n                for class_name in classes:\n                    f.write(f\"{class_name}\\n\")\n        \n        return classes\n    except Exception as e:\n        logger.warning(f\"Could not load ImageNet classes: {e}\")\n        return [f\"class_{i}\" for i in range(1000)]\n\n# Load ImageNet classes\nIMAGENET_CLASSES = get_imagenet_classes()\n\ndef preprocess_image(image: Image.Image, target_size=(224, 224)):\n    \"\"\"\n    Preprocess image for MobileNetV2 PyTorch model\n    \"\"\"\n    try:\n        # Define transforms\n        transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(target_size),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],  # ImageNet normalization\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n        \n        # Apply transforms\n        image_tensor = transform(image)\n        \n        return image_tensor\n        \n    except Exception as e:\n        raise ValueError(f\"Image preprocessing failed: {e}\")\n\ndef preprocess_batch(images: list, target_size=(224, 224)):\n    \"\"\"\n    Preprocess batch of images\n    \"\"\"\n    try:\n        transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(target_size),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n        \n        batch_tensors = []\n        for image in images:\n            if isinstance(image, str):  # If path\n                image = Image.open(image).convert('RGB')\n            elif not isinstance(image, Image.Image):\n                raise ValueError(\"Invalid image type\")\n            \n            tensor = transform(image)\n            batch_tensors.append(tensor)\n        \n        return torch.stack(batch_tensors)\n        \n    except Exception as e:\n        raise ValueError(f\"Batch preprocessing failed: {e}\")\n\ndef decode_predictions(predictions, top=5):\n    \"\"\"\n    Decode model predictions to human-readable labels\n    \"\"\"\n    try:\n        # Get top predictions\n        if isinstance(predictions, torch.Tensor):\n            predictions = predictions.numpy()\n        \n        # Handle batch predictions (take first sample)\n        if len(predictions.shape) &gt; 1:\n            predictions = predictions[0]\n        \n        # Get top k indices\n        top_indices = np.argsort(predictions)[-top:][::-1]\n        \n        # Format results\n        results = []\n        for idx in top_indices:\n            confidence = float(predictions[idx])\n            class_name = IMAGENET_CLASSES[idx] if idx &lt; len(IMAGENET_CLASSES) else f\"class_{idx}\"\n            \n            results.append({\n                \"class_id\": int(idx),\n                \"class_name\": class_name,\n                \"confidence\": confidence\n            })\n        \n        return results\n        \n    except Exception as e:\n        raise ValueError(f\"Prediction decoding failed: {e}\")\n\ndef validate_image(image_data):\n    \"\"\"\n    Validate image data\n    \"\"\"\n    try:\n        image = Image.open(image_data)\n        return image.format in ['JPEG', 'PNG', 'BMP', 'TIFF', 'WEBP']\n    except:\n        return False\n\ndef tensor_to_numpy(tensor):\n    \"\"\"Convert PyTorch tensor to numpy array\"\"\"\n    if isinstance(tensor, torch.Tensor):\n        return tensor.detach().cpu().numpy()\n    return tensor\n\ndef numpy_to_tensor(array, device='cpu'):\n    \"\"\"Convert numpy array to PyTorch tensor\"\"\"\n    if isinstance(array, np.ndarray):\n        return torch.from_numpy(array).to(device)\n    return array\n\nclass ModelProfiler:\n    \"\"\"Simple profiler for model performance\"\"\"\n    \n    def __init__(self):\n        self.inference_times = []\n        self.preprocessing_times = []\n    \n    def record_inference_time(self, time_ms):\n        self.inference_times.append(time_ms)\n    \n    def record_preprocessing_time(self, time_ms):\n        self.preprocessing_times.append(time_ms)\n    \n    def get_stats(self):\n        if not self.inference_times:\n            return {\"message\": \"No inference data recorded\"}\n        \n        return {\n            \"avg_inference_time_ms\": np.mean(self.inference_times),\n            \"avg_preprocessing_time_ms\": np.mean(self.preprocessing_times) if self.preprocessing_times else 0,\n            \"total_inferences\": len(self.inference_times),\n            \"min_inference_time_ms\": np.min(self.inference_times),\n            \"max_inference_time_ms\": np.max(self.inference_times)\n        }\n\n# Global profiler instance\nprofiler = ModelProfiler()\n\n\n\n# Empty file to make app a Python package\n\n\n\n\n\n\nfastapi==0.104.1\nuvicorn[standard]==0.24.0\ntorch==2.1.0\ntorchvision==0.16.0\nPillow==10.1.0\npython-multipart==0.0.6\nnumpy==1.24.3\nrequests==2.31.0\n\n\n\n# Use official Python runtime as base image\nFROM python:3.11-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    libgl1-mesa-glx \\\n    libglib2.0-0 \\\n    libsm6 \\\n    libxext6 \\\n    libxrender-dev \\\n    libgomp1 \\\n    wget \\\n    curl \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements first for better caching\nCOPY requirements.txt .\n\n# Install PyTorch CPU version (smaller image)\nRUN pip install --no-cache-dir --upgrade pip && \\\n    pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu && \\\n    pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY app/ ./app/\n\n# Create non-root user for security\nRUN useradd --create-home --shell /bin/bash app && \\\n    chown -R app:app /app\nUSER app\n\n# Pre-download ImageNet classes\nRUN python -c \"from app.utils import get_imagenet_classes; get_imagenet_classes()\"\n\n# Expose port\nEXPOSE 8000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=30s --start-period=60s --retries=3 \\\n    CMD curl -f http://localhost:8000/health || exit 1\n\n# Command to run the application\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\n\n\n# Use NVIDIA PyTorch base image\nFROM pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime\n\n# Set working directory\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    curl \\\n    wget \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements first for better caching\nCOPY requirements.txt .\n\n# Install additional dependencies\nRUN pip install --no-cache-dir --upgrade pip && \\\n    pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY app/ ./app/\n\n# Create non-root user for security\nRUN useradd --create-home --shell /bin/bash app && \\\n    chown -R app:app /app\nUSER app\n\n# Pre-download ImageNet classes\nRUN python -c \"from app.utils import get_imagenet_classes; get_imagenet_classes()\"\n\n# Expose port\nEXPOSE 8000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=30s --start-period=60s --retries=3 \\\n    CMD curl -f http://localhost:8000/health || exit 1\n\n# Command to run the application\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\n\n\nversion: '3.8'\n\nservices:\n  mobilenetv2-pytorch-api:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - PYTHONPATH=/app\n      - TORCH_HOME=/app/.torch\n    volumes:\n      - ./logs:/app/logs\n      - torch_cache:/app/.torch\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 60s\n    deploy:\n      resources:\n        limits:\n          memory: 2G\n        reservations:\n          memory: 1G\n\n  # GPU version (uncomment and modify as needed)\n  # mobilenetv2-pytorch-gpu:\n  #   build:\n  #     context: .\n  #     dockerfile: Dockerfile.gpu\n  #   ports:\n  #     - \"8000:8000\"\n  #   environment:\n  #     - PYTHONPATH=/app\n  #     - TORCH_HOME=/app/.torch\n  #     - NVIDIA_VISIBLE_DEVICES=all\n  #   volumes:\n  #     - ./logs:/app/logs\n  #     - torch_cache:/app/.torch\n  #   restart: unless-stopped\n  #   deploy:\n  #     resources:\n  #       reservations:\n  #         devices:\n  #           - driver: nvidia\n  #             count: 1\n  #             capabilities: [gpu]\n\n  # Optional: Add nginx for production\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n    depends_on:\n      - mobilenetv2-pytorch-api\n    restart: unless-stopped\n\nvolumes:\n  torch_cache:\n\n\n\n__pycache__\n*.pyc\n*.pyo\n*.pyd\n.Python\nenv/\npip-log.txt\npip-delete-this-directory.txt\n.git\n.gitignore\nREADME.md\n.pytest_cache\n.coverage\n.nyc_output\nnode_modules\n.DS_Store\n*.log\nlogs/\n*.pth\n*.pt\n.torch/\n\n\n\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    upstream api {\n        server mobilenetv2-pytorch-api:8000;\n    }\n\n    server {\n        listen 80;\n        client_max_body_size 10M;\n\n        location / {\n            proxy_pass http://api;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n            proxy_read_timeout 300;\n            proxy_connect_timeout 300;\n            proxy_send_timeout 300;\n        }\n    }\n}\n\n\n\n\n\n\n# Build the CPU image\ndocker build -t mobilenetv2-pytorch-api .\n\n# Build the GPU image (if you have NVIDIA GPU)\ndocker build -f Dockerfile.gpu -t mobilenetv2-pytorch-gpu .\n\n# Run CPU version\ndocker run -p 8000:8000 mobilenetv2-pytorch-api\n\n# Run GPU version\ndocker run --gpus all -p 8000:8000 mobilenetv2-pytorch-gpu\n\n# Run with environment variables\ndocker run -p 8000:8000 -e TORCH_HOME=/tmp/.torch mobilenetv2-pytorch-api\n\n\n\n# Build and start services\ndocker-compose up --build\n\n# Run in background\ndocker-compose up -d\n\n# View logs\ndocker-compose logs -f mobilenetv2-pytorch-api\n\n# Stop services\ndocker-compose down\n\n\n\n\n\n\n# Health check\ncurl http://localhost:8000/health\n\n# Model info\ncurl http://localhost:8000/model_info\n\n# Single image prediction\ncurl -X POST \"http://localhost:8000/predict\" \\\n     -H \"accept: application/json\" \\\n     -H \"Content-Type: multipart/form-data\" \\\n     -F \"file=@path/to/your/image.jpg\"\n\n# Batch prediction\ncurl -X POST \"http://localhost:8000/batch_predict\" \\\n     -H \"accept: application/json\" \\\n     -H \"Content-Type: multipart/form-data\" \\\n     -F \"files=@image1.jpg\" \\\n     -F \"files=@image2.jpg\"\n\n\n\nimport requests\nimport json\n\n# Single prediction\ndef predict_image(image_path, api_url=\"http://localhost:8000\"):\n    url = f\"{api_url}/predict\"\n    files = {\"file\": open(image_path, \"rb\")}\n    response = requests.post(url, files=files)\n    return response.json()\n\n# Batch prediction\ndef predict_batch(image_paths, api_url=\"http://localhost:8000\"):\n    url = f\"{api_url}/batch_predict\"\n    files = [(\"files\", open(path, \"rb\")) for path in image_paths]\n    response = requests.post(url, files=files)\n    return response.json()\n\n# Usage\nresult = predict_image(\"cat.jpg\")\nprint(json.dumps(result, indent=2))\n\nbatch_result = predict_batch([\"cat.jpg\", \"dog.jpg\"])\nprint(json.dumps(batch_result, indent=2))\n\n\n\n{\n  \"predictions\": [\n    {\n      \"class_id\": 281,\n      \"class_name\": \"tabby\",\n      \"confidence\": 0.8234567\n    },\n    {\n      \"class_id\": 282,\n      \"class_name\": \"tiger_cat\",\n      \"confidence\": 0.1234567\n    }\n  ],\n  \"success\": true\n}\n\n\n\n\n\n\n# Add to model_handler.py for optimization\nimport torch.jit\n\nclass OptimizedMobileNetV2Handler(MobileNetV2Handler):\n    def __init__(self, use_jit=True, use_half_precision=False):\n        super().__init__()\n        self.use_jit = use_jit\n        self.use_half_precision = use_half_precision\n    \n    def load_model(self):\n        super().load_model()\n        \n        if self.use_jit:\n            # TorchScript compilation for faster inference\n            self.model = torch.jit.script(self.model)\n            logger.info(\"Model compiled with TorchScript\")\n        \n        if self.use_half_precision and self.device.type == 'cuda':\n            # Half precision for GPU\n            self.model = self.model.half()\n            logger.info(\"Model converted to half precision\")\n\n\n\n# Multi-stage build for smaller image\nFROM python:3.11-slim as builder\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --user --no-cache-dir -r requirements.txt\n\nFROM python:3.11-slim\n\nWORKDIR /app\nCOPY --from=builder /root/.local /root/.local\nCOPY app/ ./app/\n\n# Make sure scripts in .local are usable\nENV PATH=/root/.local/bin:$PATH\n\nEXPOSE 8000\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\n\n\n\n\n\n# Add to main.py\nimport time\nfrom app.utils import profiler\n\n@app.middleware(\"http\")\nasync def log_requests(request, call_next):\n    start_time = time.time()\n    response = await call_next(request)\n    process_time = (time.time() - start_time) * 1000\n    \n    logger.info(f\"{request.method} {request.url.path} - {process_time:.2f}ms\")\n    \n    if request.url.path == \"/predict\":\n        profiler.record_inference_time(process_time)\n    \n    return response\n\n@app.get(\"/stats\")\nasync def get_stats():\n    \"\"\"Get performance statistics\"\"\"\n    return profiler.get_stats()\n\n\n\n\n\n\n{\n  \"family\": \"mobilenetv2-pytorch-task\",\n  \"networkMode\": \"awsvpc\",\n  \"requiresCompatibilities\": [\"FARGATE\"],\n  \"cpu\": \"1024\",\n  \"memory\": \"2048\",\n  \"containerDefinitions\": [\n    {\n      \"name\": \"mobilenetv2-pytorch-api\",\n      \"image\": \"your-registry/mobilenetv2-pytorch-api:latest\",\n      \"portMappings\": [\n        {\n          \"containerPort\": 8000,\n          \"protocol\": \"tcp\"\n        }\n      ],\n      \"environment\": [\n        {\n          \"name\": \"TORCH_HOME\",\n          \"value\": \"/tmp/.torch\"\n        }\n      ],\n      \"essential\": true,\n      \"logConfiguration\": {\n        \"logDriver\": \"awslogs\",\n        \"options\": {\n          \"awslogs-group\": \"/ecs/mobilenetv2-pytorch\",\n          \"awslogs-region\": \"us-east-1\",\n          \"awslogs-stream-prefix\": \"ecs\"\n        }\n      }\n    }\n  ]\n}\n\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mobilenetv2-pytorch-api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mobilenetv2-pytorch-api\n  template:\n    metadata:\n      labels:\n        app: mobilenetv2-pytorch-api\n    spec:\n      containers:\n      - name: mobilenetv2-pytorch-api\n        image: mobilenetv2-pytorch-api:latest\n        ports:\n        - containerPort: 8000\n        env:\n        - name: TORCH_HOME\n          value: /tmp/.torch\n        resources:\n          requests:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"2Gi\"\n            cpu: \"1000m\"\n        volumeMounts:\n        - name: torch-cache\n          mountPath: /tmp/.torch\n      volumes:\n      - name: torch-cache\n        emptyDir: {}\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mobilenetv2-pytorch-service\nspec:\n  selector:\n    app: mobilenetv2-pytorch-api\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8000\n  type: LoadBalancer\nThis PyTorch-based guide provides the same functionality as the TensorFlow version but uses PyTorchâ€™s ecosystem, including torchvision for pre-trained models, PyTorch transformations for preprocessing, and proper tensor handling throughout the application."
  },
  {
    "objectID": "posts/mobilenet-deployment/index.html#project-structure",
    "href": "posts/mobilenet-deployment/index.html#project-structure",
    "title": "MobileNetV2 PyTorch Docker Deployment Guide",
    "section": "",
    "text": "mobilenetv2-pytorch-docker/\nâ”œâ”€â”€ app/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ main.py\nâ”‚   â”œâ”€â”€ model_handler.py\nâ”‚   â””â”€â”€ utils.py\nâ”œâ”€â”€ requirements.txt\nâ”œâ”€â”€ Dockerfile\nâ”œâ”€â”€ docker-compose.yml\nâ”œâ”€â”€ .dockerignore\nâ””â”€â”€ README.md"
  },
  {
    "objectID": "posts/mobilenet-deployment/index.html#application-code",
    "href": "posts/mobilenet-deployment/index.html#application-code",
    "title": "MobileNetV2 PyTorch Docker Deployment Guide",
    "section": "",
    "text": "from fastapi import FastAPI, File, UploadFile, HTTPException\nfrom fastapi.responses import JSONResponse\nimport uvicorn\nimport io\nfrom PIL import Image\nimport numpy as np\nfrom .model_handler import MobileNetV2Handler\nfrom .utils import preprocess_image, decode_predictions\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\napp = FastAPI(\n    title=\"MobileNetV2 PyTorch Image Classification API\",\n    description=\"Deploy MobileNetV2 using PyTorch for image classification\",\n    version=\"1.0.0\"\n)\n\n# Initialize model handler\nmodel_handler = MobileNetV2Handler()\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    \"\"\"Load model on startup\"\"\"\n    try:\n        model_handler.load_model()\n        logger.info(\"Model loaded successfully\")\n    except Exception as e:\n        logger.error(f\"Failed to load model: {e}\")\n        raise\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"MobileNetV2 PyTorch Classification API\", \"status\": \"running\"}\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\", \"model_loaded\": model_handler.is_loaded()}\n\n@app.post(\"/predict\")\nasync def predict(file: UploadFile = File(...)):\n    \"\"\"\n    Predict image class using MobileNetV2\n    \"\"\"\n    if not file.content_type.startswith(\"image/\"):\n        raise HTTPException(status_code=400, detail=\"File must be an image\")\n    \n    try:\n        # Read and preprocess image\n        image_data = await file.read()\n        image = Image.open(io.BytesIO(image_data))\n        \n        if image.mode != 'RGB':\n            image = image.convert('RGB')\n        \n        # Preprocess for MobileNetV2\n        processed_image = preprocess_image(image)\n        \n        # Make prediction\n        predictions = model_handler.predict(processed_image)\n        \n        # Decode predictions\n        decoded_predictions = decode_predictions(predictions, top=5)\n        \n        return JSONResponse(content={\n            \"predictions\": decoded_predictions,\n            \"success\": True\n        })\n        \n    except Exception as e:\n        logger.error(f\"Prediction error: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Prediction failed: {str(e)}\")\n\n@app.post(\"/batch_predict\")\nasync def batch_predict(files: list[UploadFile] = File(...)):\n    \"\"\"\n    Batch prediction for multiple images\n    \"\"\"\n    if len(files) &gt; 10:  # Limit batch size\n        raise HTTPException(status_code=400, detail=\"Maximum 10 images allowed per batch\")\n    \n    results = []\n    \n    for file in files:\n        if not file.content_type.startswith(\"image/\"):\n            results.append({\n                \"filename\": file.filename,\n                \"error\": \"File must be an image\"\n            })\n            continue\n        \n        try:\n            image_data = await file.read()\n            image = Image.open(io.BytesIO(image_data))\n            \n            if image.mode != 'RGB':\n                image = image.convert('RGB')\n            \n            processed_image = preprocess_image(image)\n            predictions = model_handler.predict(processed_image)\n            decoded_predictions = decode_predictions(predictions, top=3)\n            \n            results.append({\n                \"filename\": file.filename,\n                \"predictions\": decoded_predictions,\n                \"success\": True\n            })\n            \n        except Exception as e:\n            results.append({\n                \"filename\": file.filename,\n                \"error\": str(e),\n                \"success\": False\n            })\n    \n    return JSONResponse(content={\"results\": results})\n\n@app.get(\"/model_info\")\nasync def model_info():\n    \"\"\"Get model information\"\"\"\n    return {\n        \"model_name\": \"MobileNetV2\",\n        \"framework\": \"PyTorch\",\n        \"input_size\": [224, 224],\n        \"num_classes\": 1000,\n        \"pretrained\": True\n    }\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\n\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models\nimport numpy as np\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass MobileNetV2Handler:\n    def __init__(self):\n        self.model = None\n        self.device = None\n        self._loaded = False\n        \n    def load_model(self):\n        \"\"\"Load pre-trained MobileNetV2 model\"\"\"\n        try:\n            logger.info(\"Loading MobileNetV2 PyTorch model...\")\n            \n            # Determine device (CPU/GPU)\n            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n            logger.info(f\"Using device: {self.device}\")\n            \n            # Load pre-trained MobileNetV2\n            self.model = models.mobilenet_v2(pretrained=True)\n            self.model.eval()  # Set to evaluation mode\n            self.model.to(self.device)\n            \n            # Warm up the model with a dummy prediction\n            dummy_input = torch.randn(1, 3, 224, 224).to(self.device)\n            with torch.no_grad():\n                _ = self.model(dummy_input)\n            \n            self._loaded = True\n            logger.info(\"Model loaded and warmed up successfully\")\n            \n        except Exception as e:\n            logger.error(f\"Failed to load model: {e}\")\n            raise\n    \n    def predict(self, image_tensor):\n        \"\"\"Make prediction on preprocessed image tensor\"\"\"\n        if not self._loaded:\n            raise RuntimeError(\"Model not loaded\")\n        \n        try:\n            # Ensure tensor is on correct device\n            if isinstance(image_tensor, np.ndarray):\n                image_tensor = torch.from_numpy(image_tensor)\n            \n            image_tensor = image_tensor.to(self.device)\n            \n            # Ensure batch dimension\n            if len(image_tensor.shape) == 3:\n                image_tensor = image_tensor.unsqueeze(0)\n            \n            # Make prediction\n            with torch.no_grad():\n                outputs = self.model(image_tensor)\n                # Apply softmax to get probabilities\n                probabilities = torch.nn.functional.softmax(outputs, dim=1)\n            \n            return probabilities.cpu().numpy()\n            \n        except Exception as e:\n            logger.error(f\"Prediction failed: {e}\")\n            raise\n    \n    def predict_batch(self, image_tensors):\n        \"\"\"Make batch predictions\"\"\"\n        if not self._loaded:\n            raise RuntimeError(\"Model not loaded\")\n        \n        try:\n            # Convert to tensor if numpy array\n            if isinstance(image_tensors, np.ndarray):\n                image_tensors = torch.from_numpy(image_tensors)\n            \n            image_tensors = image_tensors.to(self.device)\n            \n            # Make batch prediction\n            with torch.no_grad():\n                outputs = self.model(image_tensors)\n                probabilities = torch.nn.functional.softmax(outputs, dim=1)\n            \n            return probabilities.cpu().numpy()\n            \n        except Exception as e:\n            logger.error(f\"Batch prediction failed: {e}\")\n            raise\n    \n    def is_loaded(self):\n        \"\"\"Check if model is loaded\"\"\"\n        return self._loaded\n    \n    def get_device(self):\n        \"\"\"Get current device\"\"\"\n        return str(self.device) if self.device else \"not initialized\"\n\n\n\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom torchvision import transforms\nimport json\nimport os\nimport requests\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# ImageNet class labels\nIMAGENET_CLASSES_URL = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n\ndef get_imagenet_classes():\n    \"\"\"Download and cache ImageNet class labels\"\"\"\n    try:\n        if os.path.exists(\"imagenet_classes.txt\"):\n            with open(\"imagenet_classes.txt\", \"r\") as f:\n                classes = [line.strip() for line in f.readlines()]\n        else:\n            logger.info(\"Downloading ImageNet class labels...\")\n            response = requests.get(IMAGENET_CLASSES_URL)\n            classes = response.text.strip().split('\\n')\n            \n            # Cache the classes\n            with open(\"imagenet_classes.txt\", \"w\") as f:\n                for class_name in classes:\n                    f.write(f\"{class_name}\\n\")\n        \n        return classes\n    except Exception as e:\n        logger.warning(f\"Could not load ImageNet classes: {e}\")\n        return [f\"class_{i}\" for i in range(1000)]\n\n# Load ImageNet classes\nIMAGENET_CLASSES = get_imagenet_classes()\n\ndef preprocess_image(image: Image.Image, target_size=(224, 224)):\n    \"\"\"\n    Preprocess image for MobileNetV2 PyTorch model\n    \"\"\"\n    try:\n        # Define transforms\n        transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(target_size),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],  # ImageNet normalization\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n        \n        # Apply transforms\n        image_tensor = transform(image)\n        \n        return image_tensor\n        \n    except Exception as e:\n        raise ValueError(f\"Image preprocessing failed: {e}\")\n\ndef preprocess_batch(images: list, target_size=(224, 224)):\n    \"\"\"\n    Preprocess batch of images\n    \"\"\"\n    try:\n        transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(target_size),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n        \n        batch_tensors = []\n        for image in images:\n            if isinstance(image, str):  # If path\n                image = Image.open(image).convert('RGB')\n            elif not isinstance(image, Image.Image):\n                raise ValueError(\"Invalid image type\")\n            \n            tensor = transform(image)\n            batch_tensors.append(tensor)\n        \n        return torch.stack(batch_tensors)\n        \n    except Exception as e:\n        raise ValueError(f\"Batch preprocessing failed: {e}\")\n\ndef decode_predictions(predictions, top=5):\n    \"\"\"\n    Decode model predictions to human-readable labels\n    \"\"\"\n    try:\n        # Get top predictions\n        if isinstance(predictions, torch.Tensor):\n            predictions = predictions.numpy()\n        \n        # Handle batch predictions (take first sample)\n        if len(predictions.shape) &gt; 1:\n            predictions = predictions[0]\n        \n        # Get top k indices\n        top_indices = np.argsort(predictions)[-top:][::-1]\n        \n        # Format results\n        results = []\n        for idx in top_indices:\n            confidence = float(predictions[idx])\n            class_name = IMAGENET_CLASSES[idx] if idx &lt; len(IMAGENET_CLASSES) else f\"class_{idx}\"\n            \n            results.append({\n                \"class_id\": int(idx),\n                \"class_name\": class_name,\n                \"confidence\": confidence\n            })\n        \n        return results\n        \n    except Exception as e:\n        raise ValueError(f\"Prediction decoding failed: {e}\")\n\ndef validate_image(image_data):\n    \"\"\"\n    Validate image data\n    \"\"\"\n    try:\n        image = Image.open(image_data)\n        return image.format in ['JPEG', 'PNG', 'BMP', 'TIFF', 'WEBP']\n    except:\n        return False\n\ndef tensor_to_numpy(tensor):\n    \"\"\"Convert PyTorch tensor to numpy array\"\"\"\n    if isinstance(tensor, torch.Tensor):\n        return tensor.detach().cpu().numpy()\n    return tensor\n\ndef numpy_to_tensor(array, device='cpu'):\n    \"\"\"Convert numpy array to PyTorch tensor\"\"\"\n    if isinstance(array, np.ndarray):\n        return torch.from_numpy(array).to(device)\n    return array\n\nclass ModelProfiler:\n    \"\"\"Simple profiler for model performance\"\"\"\n    \n    def __init__(self):\n        self.inference_times = []\n        self.preprocessing_times = []\n    \n    def record_inference_time(self, time_ms):\n        self.inference_times.append(time_ms)\n    \n    def record_preprocessing_time(self, time_ms):\n        self.preprocessing_times.append(time_ms)\n    \n    def get_stats(self):\n        if not self.inference_times:\n            return {\"message\": \"No inference data recorded\"}\n        \n        return {\n            \"avg_inference_time_ms\": np.mean(self.inference_times),\n            \"avg_preprocessing_time_ms\": np.mean(self.preprocessing_times) if self.preprocessing_times else 0,\n            \"total_inferences\": len(self.inference_times),\n            \"min_inference_time_ms\": np.min(self.inference_times),\n            \"max_inference_time_ms\": np.max(self.inference_times)\n        }\n\n# Global profiler instance\nprofiler = ModelProfiler()\n\n\n\n# Empty file to make app a Python package"
  },
  {
    "objectID": "posts/mobilenet-deployment/index.html#configuration-files",
    "href": "posts/mobilenet-deployment/index.html#configuration-files",
    "title": "MobileNetV2 PyTorch Docker Deployment Guide",
    "section": "",
    "text": "fastapi==0.104.1\nuvicorn[standard]==0.24.0\ntorch==2.1.0\ntorchvision==0.16.0\nPillow==10.1.0\npython-multipart==0.0.6\nnumpy==1.24.3\nrequests==2.31.0\n\n\n\n# Use official Python runtime as base image\nFROM python:3.11-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    libgl1-mesa-glx \\\n    libglib2.0-0 \\\n    libsm6 \\\n    libxext6 \\\n    libxrender-dev \\\n    libgomp1 \\\n    wget \\\n    curl \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements first for better caching\nCOPY requirements.txt .\n\n# Install PyTorch CPU version (smaller image)\nRUN pip install --no-cache-dir --upgrade pip && \\\n    pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu && \\\n    pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY app/ ./app/\n\n# Create non-root user for security\nRUN useradd --create-home --shell /bin/bash app && \\\n    chown -R app:app /app\nUSER app\n\n# Pre-download ImageNet classes\nRUN python -c \"from app.utils import get_imagenet_classes; get_imagenet_classes()\"\n\n# Expose port\nEXPOSE 8000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=30s --start-period=60s --retries=3 \\\n    CMD curl -f http://localhost:8000/health || exit 1\n\n# Command to run the application\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\n\n\n# Use NVIDIA PyTorch base image\nFROM pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime\n\n# Set working directory\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    curl \\\n    wget \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements first for better caching\nCOPY requirements.txt .\n\n# Install additional dependencies\nRUN pip install --no-cache-dir --upgrade pip && \\\n    pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY app/ ./app/\n\n# Create non-root user for security\nRUN useradd --create-home --shell /bin/bash app && \\\n    chown -R app:app /app\nUSER app\n\n# Pre-download ImageNet classes\nRUN python -c \"from app.utils import get_imagenet_classes; get_imagenet_classes()\"\n\n# Expose port\nEXPOSE 8000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=30s --start-period=60s --retries=3 \\\n    CMD curl -f http://localhost:8000/health || exit 1\n\n# Command to run the application\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\n\n\nversion: '3.8'\n\nservices:\n  mobilenetv2-pytorch-api:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - PYTHONPATH=/app\n      - TORCH_HOME=/app/.torch\n    volumes:\n      - ./logs:/app/logs\n      - torch_cache:/app/.torch\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 60s\n    deploy:\n      resources:\n        limits:\n          memory: 2G\n        reservations:\n          memory: 1G\n\n  # GPU version (uncomment and modify as needed)\n  # mobilenetv2-pytorch-gpu:\n  #   build:\n  #     context: .\n  #     dockerfile: Dockerfile.gpu\n  #   ports:\n  #     - \"8000:8000\"\n  #   environment:\n  #     - PYTHONPATH=/app\n  #     - TORCH_HOME=/app/.torch\n  #     - NVIDIA_VISIBLE_DEVICES=all\n  #   volumes:\n  #     - ./logs:/app/logs\n  #     - torch_cache:/app/.torch\n  #   restart: unless-stopped\n  #   deploy:\n  #     resources:\n  #       reservations:\n  #         devices:\n  #           - driver: nvidia\n  #             count: 1\n  #             capabilities: [gpu]\n\n  # Optional: Add nginx for production\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n    depends_on:\n      - mobilenetv2-pytorch-api\n    restart: unless-stopped\n\nvolumes:\n  torch_cache:\n\n\n\n__pycache__\n*.pyc\n*.pyo\n*.pyd\n.Python\nenv/\npip-log.txt\npip-delete-this-directory.txt\n.git\n.gitignore\nREADME.md\n.pytest_cache\n.coverage\n.nyc_output\nnode_modules\n.DS_Store\n*.log\nlogs/\n*.pth\n*.pt\n.torch/\n\n\n\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    upstream api {\n        server mobilenetv2-pytorch-api:8000;\n    }\n\n    server {\n        listen 80;\n        client_max_body_size 10M;\n\n        location / {\n            proxy_pass http://api;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n            proxy_read_timeout 300;\n            proxy_connect_timeout 300;\n            proxy_send_timeout 300;\n        }\n    }\n}"
  },
  {
    "objectID": "posts/mobilenet-deployment/index.html#deployment-commands",
    "href": "posts/mobilenet-deployment/index.html#deployment-commands",
    "title": "MobileNetV2 PyTorch Docker Deployment Guide",
    "section": "",
    "text": "# Build the CPU image\ndocker build -t mobilenetv2-pytorch-api .\n\n# Build the GPU image (if you have NVIDIA GPU)\ndocker build -f Dockerfile.gpu -t mobilenetv2-pytorch-gpu .\n\n# Run CPU version\ndocker run -p 8000:8000 mobilenetv2-pytorch-api\n\n# Run GPU version\ndocker run --gpus all -p 8000:8000 mobilenetv2-pytorch-gpu\n\n# Run with environment variables\ndocker run -p 8000:8000 -e TORCH_HOME=/tmp/.torch mobilenetv2-pytorch-api\n\n\n\n# Build and start services\ndocker-compose up --build\n\n# Run in background\ndocker-compose up -d\n\n# View logs\ndocker-compose logs -f mobilenetv2-pytorch-api\n\n# Stop services\ndocker-compose down"
  },
  {
    "objectID": "posts/mobilenet-deployment/index.html#usage-examples",
    "href": "posts/mobilenet-deployment/index.html#usage-examples",
    "title": "MobileNetV2 PyTorch Docker Deployment Guide",
    "section": "",
    "text": "# Health check\ncurl http://localhost:8000/health\n\n# Model info\ncurl http://localhost:8000/model_info\n\n# Single image prediction\ncurl -X POST \"http://localhost:8000/predict\" \\\n     -H \"accept: application/json\" \\\n     -H \"Content-Type: multipart/form-data\" \\\n     -F \"file=@path/to/your/image.jpg\"\n\n# Batch prediction\ncurl -X POST \"http://localhost:8000/batch_predict\" \\\n     -H \"accept: application/json\" \\\n     -H \"Content-Type: multipart/form-data\" \\\n     -F \"files=@image1.jpg\" \\\n     -F \"files=@image2.jpg\"\n\n\n\nimport requests\nimport json\n\n# Single prediction\ndef predict_image(image_path, api_url=\"http://localhost:8000\"):\n    url = f\"{api_url}/predict\"\n    files = {\"file\": open(image_path, \"rb\")}\n    response = requests.post(url, files=files)\n    return response.json()\n\n# Batch prediction\ndef predict_batch(image_paths, api_url=\"http://localhost:8000\"):\n    url = f\"{api_url}/batch_predict\"\n    files = [(\"files\", open(path, \"rb\")) for path in image_paths]\n    response = requests.post(url, files=files)\n    return response.json()\n\n# Usage\nresult = predict_image(\"cat.jpg\")\nprint(json.dumps(result, indent=2))\n\nbatch_result = predict_batch([\"cat.jpg\", \"dog.jpg\"])\nprint(json.dumps(batch_result, indent=2))\n\n\n\n{\n  \"predictions\": [\n    {\n      \"class_id\": 281,\n      \"class_name\": \"tabby\",\n      \"confidence\": 0.8234567\n    },\n    {\n      \"class_id\": 282,\n      \"class_name\": \"tiger_cat\",\n      \"confidence\": 0.1234567\n    }\n  ],\n  \"success\": true\n}"
  },
  {
    "objectID": "posts/mobilenet-deployment/index.html#performance-optimization",
    "href": "posts/mobilenet-deployment/index.html#performance-optimization",
    "title": "MobileNetV2 PyTorch Docker Deployment Guide",
    "section": "",
    "text": "# Add to model_handler.py for optimization\nimport torch.jit\n\nclass OptimizedMobileNetV2Handler(MobileNetV2Handler):\n    def __init__(self, use_jit=True, use_half_precision=False):\n        super().__init__()\n        self.use_jit = use_jit\n        self.use_half_precision = use_half_precision\n    \n    def load_model(self):\n        super().load_model()\n        \n        if self.use_jit:\n            # TorchScript compilation for faster inference\n            self.model = torch.jit.script(self.model)\n            logger.info(\"Model compiled with TorchScript\")\n        \n        if self.use_half_precision and self.device.type == 'cuda':\n            # Half precision for GPU\n            self.model = self.model.half()\n            logger.info(\"Model converted to half precision\")\n\n\n\n# Multi-stage build for smaller image\nFROM python:3.11-slim as builder\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --user --no-cache-dir -r requirements.txt\n\nFROM python:3.11-slim\n\nWORKDIR /app\nCOPY --from=builder /root/.local /root/.local\nCOPY app/ ./app/\n\n# Make sure scripts in .local are usable\nENV PATH=/root/.local/bin:$PATH\n\nEXPOSE 8000\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]"
  },
  {
    "objectID": "posts/mobilenet-deployment/index.html#monitoring-and-logging",
    "href": "posts/mobilenet-deployment/index.html#monitoring-and-logging",
    "title": "MobileNetV2 PyTorch Docker Deployment Guide",
    "section": "",
    "text": "# Add to main.py\nimport time\nfrom app.utils import profiler\n\n@app.middleware(\"http\")\nasync def log_requests(request, call_next):\n    start_time = time.time()\n    response = await call_next(request)\n    process_time = (time.time() - start_time) * 1000\n    \n    logger.info(f\"{request.method} {request.url.path} - {process_time:.2f}ms\")\n    \n    if request.url.path == \"/predict\":\n        profiler.record_inference_time(process_time)\n    \n    return response\n\n@app.get(\"/stats\")\nasync def get_stats():\n    \"\"\"Get performance statistics\"\"\"\n    return profiler.get_stats()"
  },
  {
    "objectID": "posts/mobilenet-deployment/index.html#cloud-deployment",
    "href": "posts/mobilenet-deployment/index.html#cloud-deployment",
    "title": "MobileNetV2 PyTorch Docker Deployment Guide",
    "section": "",
    "text": "{\n  \"family\": \"mobilenetv2-pytorch-task\",\n  \"networkMode\": \"awsvpc\",\n  \"requiresCompatibilities\": [\"FARGATE\"],\n  \"cpu\": \"1024\",\n  \"memory\": \"2048\",\n  \"containerDefinitions\": [\n    {\n      \"name\": \"mobilenetv2-pytorch-api\",\n      \"image\": \"your-registry/mobilenetv2-pytorch-api:latest\",\n      \"portMappings\": [\n        {\n          \"containerPort\": 8000,\n          \"protocol\": \"tcp\"\n        }\n      ],\n      \"environment\": [\n        {\n          \"name\": \"TORCH_HOME\",\n          \"value\": \"/tmp/.torch\"\n        }\n      ],\n      \"essential\": true,\n      \"logConfiguration\": {\n        \"logDriver\": \"awslogs\",\n        \"options\": {\n          \"awslogs-group\": \"/ecs/mobilenetv2-pytorch\",\n          \"awslogs-region\": \"us-east-1\",\n          \"awslogs-stream-prefix\": \"ecs\"\n        }\n      }\n    }\n  ]\n}\n\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mobilenetv2-pytorch-api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mobilenetv2-pytorch-api\n  template:\n    metadata:\n      labels:\n        app: mobilenetv2-pytorch-api\n    spec:\n      containers:\n      - name: mobilenetv2-pytorch-api\n        image: mobilenetv2-pytorch-api:latest\n        ports:\n        - containerPort: 8000\n        env:\n        - name: TORCH_HOME\n          value: /tmp/.torch\n        resources:\n          requests:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"2Gi\"\n            cpu: \"1000m\"\n        volumeMounts:\n        - name: torch-cache\n          mountPath: /tmp/.torch\n      volumes:\n      - name: torch-cache\n        emptyDir: {}\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mobilenetv2-pytorch-service\nspec:\n  selector:\n    app: mobilenetv2-pytorch-api\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8000\n  type: LoadBalancer\nThis PyTorch-based guide provides the same functionality as the TensorFlow version but uses PyTorchâ€™s ecosystem, including torchvision for pre-trained models, PyTorch transformations for preprocessing, and proper tensor handling throughout the application."
  },
  {
    "objectID": "posts/vision-transformers/index.html",
    "href": "posts/vision-transformers/index.html",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Vision Transformers (ViT) represent a significant paradigm shift in computer vision, applying the transformer architecture initially developed for NLP to image processing tasks. This guide walks through implementing a Vision Transformer from scratch using PyTorch.\n\n\n\nIntroduction to Vision Transformers\nUnderstanding the Architecture\nImplementation\n\nImage Patching\nPatch Embedding\nPosition Embedding\nTransformer Encoder\nMLP Head\n\nTraining the Model\nInference and Usage\nOptimization Techniques\nAdvanced Variants\n\n\n\n\nVision Transformers (ViT) were introduced in the paper â€œAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scaleâ€ by Dosovitskiy et al.Â in 2020. The core idea is to treat an image as a sequence of patches, similar to how words are treated in NLP, and process them using a transformer encoder.\nKey advantages of ViTs include:\n\nGlobal receptive field from the start\nAbility to capture long-range dependencies\nScalability to large datasets\nNo inductive bias towards local processing (unlike CNNs)\n\n\n\n\nThe ViT architecture consists of the following components:\n\nImage Patching: Dividing the input image into fixed-size patches\nPatch Embedding: Linear projection of flattened patches\nPosition Embedding: Adding positional information\nTransformer Encoder: Self-attention and feed-forward layers\nMLP Head: Final classification layer\n\n\n\n\n\nLetâ€™s implement each component of the Vision Transformer step by step.\n\n\nFirst, we need to divide the input image into fixed-size patches. For a typical ViT, these are 16Ã—16 pixel patches.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n        super().__init__()\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.num_patches = (image_size // patch_size) ** 2\n        \n        # Convert image into patches and embed them\n        # Instead of using einops, we'll use standard PyTorch operations\n        self.projection = nn.Conv2d(\n            in_channels, embed_dim, \n            kernel_size=patch_size, stride=patch_size\n        )\n        \n    def forward(self, x):\n        # x: (batch_size, channels, height, width)\n        # Convert image into patches using convolution\n        x = self.projection(x)  # (batch_size, embed_dim, grid_height, grid_width)\n        \n        # Flatten spatial dimensions and transpose to get \n        # (batch_size, num_patches, embed_dim)\n        batch_size = x.shape[0]\n        x = x.flatten(2)  # (batch_size, embed_dim, num_patches)\n        x = x.transpose(1, 2)  # (batch_size, num_patches, embed_dim)\n        \n        return x\n\n\n\nAfter patching, we need to add a learnable class token and position embeddings.\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, num_classes, \n                 embed_dim, depth, num_heads, mlp_ratio=4, \n                 dropout_rate=0.1):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(\n            image_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embedding.num_patches\n        \n        # Class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # Position embedding for patches + class token\n        self.pos_embedding = nn.Parameter(\n            torch.zeros(1, self.num_patches + 1, embed_dim))\n        \n        self.dropout = nn.Dropout(dropout_rate)\n\n\n\nThe position embeddings are added to provide spatial information:\n    def forward(self, x):\n        # Get patch embeddings\n        x = self.patch_embedding(x)  # (B, num_patches, embed_dim)\n        \n        # Add class token\n        batch_size = x.shape[0]\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (B, 1, embed_dim)\n        x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches + 1, embed_dim)\n        \n        # Add position embedding\n        x = x + self.pos_embedding\n        x = self.dropout(x)\n\n\n\nNext, letâ€™s implement the transformer encoder blocks:\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout_rate=0.1):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        \n        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout_rate)\n        \n    def forward(self, x):\n        batch_size, seq_len, embed_dim = x.shape\n        \n        # Get query, key, and value projections\n        qkv = self.qkv(x)\n        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, H, N, D)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # Each is (B, H, N, D)\n        \n        # Attention\n        attn = (q @ k.transpose(-2, -1)) * self.scale  # (B, H, N, N)\n        attn = attn.softmax(dim=-1)\n        attn = self.dropout(attn)\n        \n        # Apply attention to values\n        out = (attn @ v).transpose(1, 2)  # (B, N, H, D)\n        out = out.reshape(batch_size, seq_len, embed_dim)  # (B, N, E)\n        out = self.proj(out)\n        \n        return out\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, embed_dim, num_heads, mlp_ratio=4, dropout_rate=0.1):\n        super().__init__()\n        \n        # Layer normalization\n        self.norm1 = nn.LayerNorm(embed_dim)\n        \n        # Multi-head self-attention\n        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout_rate)\n        self.dropout1 = nn.Dropout(dropout_rate)\n        \n        # Layer normalization\n        self.norm2 = nn.LayerNorm(embed_dim)\n        \n        # MLP block\n        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, mlp_hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(mlp_hidden_dim, embed_dim),\n            nn.Dropout(dropout_rate)\n        )\n        \n    def forward(self, x):\n        # Apply layer normalization and self-attention\n        attn_output = self.attn(self.norm1(x))\n        x = x + self.dropout1(attn_output)\n        \n        # Apply MLP block with residual connection\n        x = x + self.mlp(self.norm2(x))\n        return x\nNow, letâ€™s update our main ViT class to include the transformer encoder blocks:\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, num_classes, \n                 embed_dim, depth, num_heads, mlp_ratio=4, \n                 dropout_rate=0.1):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(\n            image_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embedding.num_patches\n        \n        # Class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # Position embedding for patches + class token\n        self.pos_embedding = nn.Parameter(\n            torch.zeros(1, self.num_patches + 1, embed_dim))\n        \n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # Transformer encoder blocks\n        self.transformer_blocks = nn.ModuleList([\n            TransformerEncoder(embed_dim, num_heads, mlp_ratio, dropout_rate)\n            for _ in range(depth)\n        ])\n        \n        # Layer normalization\n        self.norm = nn.LayerNorm(embed_dim)\n\n\n\nFinally, letâ€™s add the classification head and complete the forward pass:\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, num_classes, \n                 embed_dim, depth, num_heads, mlp_ratio=4, \n                 dropout_rate=0.1):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(\n            image_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embedding.num_patches\n        \n        # Class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # Position embedding for patches + class token\n        self.pos_embedding = nn.Parameter(\n            torch.zeros(1, self.num_patches + 1, embed_dim))\n        \n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # Transformer encoder blocks\n        self.transformer_blocks = nn.ModuleList([\n            TransformerEncoder(embed_dim, num_heads, mlp_ratio, dropout_rate)\n            for _ in range(depth)\n        ])\n        \n        # Layer normalization\n        self.norm = nn.LayerNorm(embed_dim)\n        \n        # Classification head\n        self.mlp_head = nn.Linear(embed_dim, num_classes)\n        \n        # Initialize weights\n        self._init_weights()\n        \n    def _init_weights(self):\n        # Initialize patch embedding and MLP heads\n        nn.init.normal_(self.cls_token, std=0.02)\n        nn.init.normal_(self.pos_embedding, std=0.02)\n        \n    def forward(self, x):\n        # Get patch embeddings\n        x = self.patch_embedding(x)  # (B, num_patches, embed_dim)\n        \n        # Add class token\n        batch_size = x.shape[0]\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (B, 1, embed_dim)\n        x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches + 1, embed_dim)\n        \n        # Add position embedding\n        x = x + self.pos_embedding\n        x = self.dropout(x)\n        \n        # Apply transformer blocks\n        for block in self.transformer_blocks:\n            x = block(x)\n        \n        # Apply final layer normalization\n        x = self.norm(x)\n        \n        # Take class token for classification\n        cls_token_final = x[:, 0]\n        \n        # Classification\n        return self.mlp_head(cls_token_final)\n\n\n\n\nLetâ€™s implement a training function for our Vision Transformer:\ndef train_vit(model, train_loader, optimizer, criterion, device, epochs=10):\n    model.train()\n    for epoch in range(epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            # Zero the gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            \n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n            \n            # Statistics\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n            \n            # Print statistics every 100 batches\n            if batch_idx % 100 == 99:\n                print(f'Epoch: {epoch+1}, Batch: {batch_idx+1}, '\n                      f'Loss: {running_loss/100:.3f}, '\n                      f'Accuracy: {100.*correct/total:.2f}%')\n                running_loss = 0.0\n                \n        # Epoch statistics\n        print(f'Epoch {epoch+1} completed. '\n              f'Accuracy: {100.*correct/total:.2f}%')\nExample usage:\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Create ViT model\nmodel = VisionTransformer(\n    image_size=224,\n    patch_size=16,\n    in_channels=3,\n    num_classes=10,\n    embed_dim=768,\n    depth=12,\n    num_heads=12,\n    mlp_ratio=4,\n    dropout_rate=0.1\n).to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# Load data\ntransform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = datasets.FakeData(\n    transform=transforms.ToTensor()\n)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n\n# Train model\ntrain_vit(model, train_loader, optimizer, criterion, device, epochs=10)\n\n\n\nHereâ€™s how to use the model for inference:\ndef inference(model, image_tensor, device):\n    model.eval()\n    with torch.no_grad():\n        image_tensor = image_tensor.unsqueeze(0).to(device)  # Add batch dimension\n        output = model(image_tensor)\n        probabilities = F.softmax(output, dim=1)\n        predicted_class = torch.argmax(probabilities, dim=1)\n    return predicted_class.item(), probabilities[0]\n\n# Example usage\nimage = transform(image).to(device)\npredicted_class, probabilities = inference(model, image, device)\nprint(f\"Predicted class: {predicted_class}\")\nprint(f\"Confidence: {probabilities[predicted_class]:.4f}\")\n\n\n\nTo improve the training and performance of ViT models, consider these optimization techniques:\n\n\nThe standard attention implementation can be memory-intensive. You can use a more efficient implementation:\ndef efficient_attention(q, k, v, mask=None):\n    # q, k, v: [batch_size, num_heads, seq_len, head_dim]\n    \n    # Scaled dot-product attention\n    scale = q.size(-1) ** -0.5\n    attention = torch.matmul(q, k.transpose(-2, -1)) * scale  # [B, H, L, L]\n    \n    if mask is not None:\n        attention = attention.masked_fill(mask == 0, -1e9)\n    \n    attention = F.softmax(attention, dim=-1)\n    output = torch.matmul(attention, v)  # [B, H, L, D]\n    \n    return output\n\n\n\nUse mixed precision training to reduce memory usage and increase training speed:\nfrom torch.cuda.amp import autocast, GradScaler\n\ndef train_with_mixed_precision(model, train_loader, optimizer, criterion, device, epochs=10):\n    scaler = GradScaler()\n    model.train()\n    \n    for epoch in range(epochs):\n        running_loss = 0.0\n        \n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Use autocast for mixed precision\n            with autocast():\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n            \n            # Scale gradients and optimize\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            running_loss += loss.item()\n            # Rest of the training loop...\n\n\n\nImplement regularization techniques such as stochastic depth to prevent overfitting:\nclass StochasticDepth(nn.Module):\n    def __init__(self, drop_prob=0.1):\n        super().__init__()\n        self.drop_prob = drop_prob\n        \n    def forward(self, x):\n        if not self.training or self.drop_prob == 0.:\n            return x\n        \n        keep_prob = 1 - self.drop_prob\n        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n        random_tensor.floor_()  # binarize\n        output = x.div(keep_prob) * random_tensor\n        return output\n\n\n\n\nSeveral advanced variants of Vision Transformers have been developed:\n\n\nDeiT introduces a distillation token and a teacher-student strategy:\nclass DeiT(VisionTransformer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        \n        # Distillation token\n        self.dist_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n        \n        # Update position embeddings to include distillation token\n        # Original position embeddings are for [class_token, patches]\n        # New position embeddings are for [class_token, dist_token, patches]\n        num_patches = self.patch_embedding.num_patches\n        new_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 2, self.embed_dim))\n        \n        # Initialize new position embeddings with the original ones\n        # Copy class token position embedding\n        new_pos_embed.data[:, 0:1, :] = self.pos_embedding.data[:, 0:1, :]\n        # Add a new position embedding for distillation token\n        new_pos_embed.data[:, 1:2, :] = self.pos_embedding.data[:, 0:1, :]\n        # Copy patch position embeddings\n        new_pos_embed.data[:, 2:, :] = self.pos_embedding.data[:, 1:, :]\n        \n        self.pos_embedding = new_pos_embed\n        \n        # Additional classification head for distillation\n        self.head_dist = nn.Linear(self.embed_dim, kwargs.get('num_classes', 1000))\n        \n    def forward(self, x):\n        # Get patch embeddings\n        x = self.patch_embedding(x)\n        \n        # Add class and distillation tokens\n        batch_size = x.shape[0]\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n        dist_tokens = self.dist_token.expand(batch_size, -1, -1)\n        x = torch.cat((cls_tokens, dist_tokens, x), dim=1)\n        \n        # Add position embedding and apply dropout\n        x = x + self.pos_embedding\n        x = self.dropout(x)\n        \n        # Apply transformer blocks\n        for block in self.transformer_blocks:\n            x = block(x)\n        \n        # Apply final layer normalization\n        x = self.norm(x)\n        \n        # Get class and distillation tokens\n        cls_token_final = x[:, 0]\n        dist_token_final = x[:, 1]\n        \n        # Apply classification heads\n        x_cls = self.mlp_head(cls_token_final)\n        x_dist = self.head_dist(dist_token_final)\n        \n        if self.training:\n            # During training, return both outputs\n            return x_cls, x_dist\n        else:\n            # During inference, return average\n            return (x_cls + x_dist) / 2\n\n\n\nSwin Transformer introduces hierarchical feature maps and shifted windows:\n# This is a simplified conceptual implementation of the Swin Transformer block\nclass WindowAttention(nn.Module):\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, dropout=0.):\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # (height, width)\n        self.num_heads = num_heads\n        self.scale = (dim // num_heads) ** -0.5\n        \n        # Linear projections\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n        \n        # Define relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))\n            \n        # Generate pair-wise relative position index for each token in the window\n        coords_h = torch.arange(self.window_size[0])\n        coords_w = torch.arange(self.window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        \n        # Calculate relative positions\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        \n        self.register_buffer(\"relative_position_index\", relative_position_index)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, mask=None):\n        B_, N, C = x.shape\n        \n        # Generate QKV matrices\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # 3, B_, num_heads, N, C//num_heads\n        q, k, v = qkv[0], qkv[1], qkv[2]  # each has shape [B_, num_heads, N, C//num_heads]\n        \n        # Scaled dot-product attention\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))  # B_, num_heads, N, N\n        \n        # Add relative position bias\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)]\n        relative_position_bias = relative_position_bias.view(\n            self.window_size[0] * self.window_size[1], \n            self.window_size[0] * self.window_size[1], \n            -1)  # Wh*Ww, Wh*Ww, num_heads\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # num_heads, Wh*Ww, Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0)\n        \n        # Apply mask if needed\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n        \n        # Apply softmax\n        attn = F.softmax(attn, dim=-1)\n        attn = self.dropout(attn)\n        \n        # Apply attention to values\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        \n        return x\n\n\n\n\nVision Transformers represent a significant advancement in computer vision, offering a different approach from traditional CNNs. This guide has covered the essential components for implementing a Vision Transformer from scratch, including image patching, position embeddings, transformer encoders, and classification heads.\nBy understanding these fundamentals, you can implement your own ViT models and experiment with various modifications to improve performance for specific tasks.\n\n\n\n\nDosovitskiy, A., et al.Â (2020). â€œAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.â€ arXiv:2010.11929.\nTouvron, H., et al.Â (2021). â€œTraining data-efficient image transformers & distillation through attention.â€ arXiv:2012.12877.\nLiu, Z., et al.Â (2021). â€œSwin Transformer: Hierarchical Vision Transformer using Shifted Windows.â€ arXiv:2103.14030."
  },
  {
    "objectID": "posts/vision-transformers/index.html#table-of-contents",
    "href": "posts/vision-transformers/index.html#table-of-contents",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Introduction to Vision Transformers\nUnderstanding the Architecture\nImplementation\n\nImage Patching\nPatch Embedding\nPosition Embedding\nTransformer Encoder\nMLP Head\n\nTraining the Model\nInference and Usage\nOptimization Techniques\nAdvanced Variants"
  },
  {
    "objectID": "posts/vision-transformers/index.html#introduction-to-vision-transformers",
    "href": "posts/vision-transformers/index.html#introduction-to-vision-transformers",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Vision Transformers (ViT) were introduced in the paper â€œAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scaleâ€ by Dosovitskiy et al.Â in 2020. The core idea is to treat an image as a sequence of patches, similar to how words are treated in NLP, and process them using a transformer encoder.\nKey advantages of ViTs include:\n\nGlobal receptive field from the start\nAbility to capture long-range dependencies\nScalability to large datasets\nNo inductive bias towards local processing (unlike CNNs)"
  },
  {
    "objectID": "posts/vision-transformers/index.html#understanding-the-architecture",
    "href": "posts/vision-transformers/index.html#understanding-the-architecture",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "The ViT architecture consists of the following components:\n\nImage Patching: Dividing the input image into fixed-size patches\nPatch Embedding: Linear projection of flattened patches\nPosition Embedding: Adding positional information\nTransformer Encoder: Self-attention and feed-forward layers\nMLP Head: Final classification layer"
  },
  {
    "objectID": "posts/vision-transformers/index.html#implementation",
    "href": "posts/vision-transformers/index.html#implementation",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Letâ€™s implement each component of the Vision Transformer step by step.\n\n\nFirst, we need to divide the input image into fixed-size patches. For a typical ViT, these are 16Ã—16 pixel patches.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n        super().__init__()\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.num_patches = (image_size // patch_size) ** 2\n        \n        # Convert image into patches and embed them\n        # Instead of using einops, we'll use standard PyTorch operations\n        self.projection = nn.Conv2d(\n            in_channels, embed_dim, \n            kernel_size=patch_size, stride=patch_size\n        )\n        \n    def forward(self, x):\n        # x: (batch_size, channels, height, width)\n        # Convert image into patches using convolution\n        x = self.projection(x)  # (batch_size, embed_dim, grid_height, grid_width)\n        \n        # Flatten spatial dimensions and transpose to get \n        # (batch_size, num_patches, embed_dim)\n        batch_size = x.shape[0]\n        x = x.flatten(2)  # (batch_size, embed_dim, num_patches)\n        x = x.transpose(1, 2)  # (batch_size, num_patches, embed_dim)\n        \n        return x\n\n\n\nAfter patching, we need to add a learnable class token and position embeddings.\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, num_classes, \n                 embed_dim, depth, num_heads, mlp_ratio=4, \n                 dropout_rate=0.1):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(\n            image_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embedding.num_patches\n        \n        # Class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # Position embedding for patches + class token\n        self.pos_embedding = nn.Parameter(\n            torch.zeros(1, self.num_patches + 1, embed_dim))\n        \n        self.dropout = nn.Dropout(dropout_rate)\n\n\n\nThe position embeddings are added to provide spatial information:\n    def forward(self, x):\n        # Get patch embeddings\n        x = self.patch_embedding(x)  # (B, num_patches, embed_dim)\n        \n        # Add class token\n        batch_size = x.shape[0]\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (B, 1, embed_dim)\n        x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches + 1, embed_dim)\n        \n        # Add position embedding\n        x = x + self.pos_embedding\n        x = self.dropout(x)\n\n\n\nNext, letâ€™s implement the transformer encoder blocks:\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout_rate=0.1):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        \n        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout_rate)\n        \n    def forward(self, x):\n        batch_size, seq_len, embed_dim = x.shape\n        \n        # Get query, key, and value projections\n        qkv = self.qkv(x)\n        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, H, N, D)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # Each is (B, H, N, D)\n        \n        # Attention\n        attn = (q @ k.transpose(-2, -1)) * self.scale  # (B, H, N, N)\n        attn = attn.softmax(dim=-1)\n        attn = self.dropout(attn)\n        \n        # Apply attention to values\n        out = (attn @ v).transpose(1, 2)  # (B, N, H, D)\n        out = out.reshape(batch_size, seq_len, embed_dim)  # (B, N, E)\n        out = self.proj(out)\n        \n        return out\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, embed_dim, num_heads, mlp_ratio=4, dropout_rate=0.1):\n        super().__init__()\n        \n        # Layer normalization\n        self.norm1 = nn.LayerNorm(embed_dim)\n        \n        # Multi-head self-attention\n        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout_rate)\n        self.dropout1 = nn.Dropout(dropout_rate)\n        \n        # Layer normalization\n        self.norm2 = nn.LayerNorm(embed_dim)\n        \n        # MLP block\n        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, mlp_hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(mlp_hidden_dim, embed_dim),\n            nn.Dropout(dropout_rate)\n        )\n        \n    def forward(self, x):\n        # Apply layer normalization and self-attention\n        attn_output = self.attn(self.norm1(x))\n        x = x + self.dropout1(attn_output)\n        \n        # Apply MLP block with residual connection\n        x = x + self.mlp(self.norm2(x))\n        return x\nNow, letâ€™s update our main ViT class to include the transformer encoder blocks:\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, num_classes, \n                 embed_dim, depth, num_heads, mlp_ratio=4, \n                 dropout_rate=0.1):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(\n            image_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embedding.num_patches\n        \n        # Class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # Position embedding for patches + class token\n        self.pos_embedding = nn.Parameter(\n            torch.zeros(1, self.num_patches + 1, embed_dim))\n        \n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # Transformer encoder blocks\n        self.transformer_blocks = nn.ModuleList([\n            TransformerEncoder(embed_dim, num_heads, mlp_ratio, dropout_rate)\n            for _ in range(depth)\n        ])\n        \n        # Layer normalization\n        self.norm = nn.LayerNorm(embed_dim)\n\n\n\nFinally, letâ€™s add the classification head and complete the forward pass:\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, num_classes, \n                 embed_dim, depth, num_heads, mlp_ratio=4, \n                 dropout_rate=0.1):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(\n            image_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embedding.num_patches\n        \n        # Class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # Position embedding for patches + class token\n        self.pos_embedding = nn.Parameter(\n            torch.zeros(1, self.num_patches + 1, embed_dim))\n        \n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # Transformer encoder blocks\n        self.transformer_blocks = nn.ModuleList([\n            TransformerEncoder(embed_dim, num_heads, mlp_ratio, dropout_rate)\n            for _ in range(depth)\n        ])\n        \n        # Layer normalization\n        self.norm = nn.LayerNorm(embed_dim)\n        \n        # Classification head\n        self.mlp_head = nn.Linear(embed_dim, num_classes)\n        \n        # Initialize weights\n        self._init_weights()\n        \n    def _init_weights(self):\n        # Initialize patch embedding and MLP heads\n        nn.init.normal_(self.cls_token, std=0.02)\n        nn.init.normal_(self.pos_embedding, std=0.02)\n        \n    def forward(self, x):\n        # Get patch embeddings\n        x = self.patch_embedding(x)  # (B, num_patches, embed_dim)\n        \n        # Add class token\n        batch_size = x.shape[0]\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (B, 1, embed_dim)\n        x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches + 1, embed_dim)\n        \n        # Add position embedding\n        x = x + self.pos_embedding\n        x = self.dropout(x)\n        \n        # Apply transformer blocks\n        for block in self.transformer_blocks:\n            x = block(x)\n        \n        # Apply final layer normalization\n        x = self.norm(x)\n        \n        # Take class token for classification\n        cls_token_final = x[:, 0]\n        \n        # Classification\n        return self.mlp_head(cls_token_final)"
  },
  {
    "objectID": "posts/vision-transformers/index.html#training-the-model",
    "href": "posts/vision-transformers/index.html#training-the-model",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Letâ€™s implement a training function for our Vision Transformer:\ndef train_vit(model, train_loader, optimizer, criterion, device, epochs=10):\n    model.train()\n    for epoch in range(epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            # Zero the gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            \n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n            \n            # Statistics\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n            \n            # Print statistics every 100 batches\n            if batch_idx % 100 == 99:\n                print(f'Epoch: {epoch+1}, Batch: {batch_idx+1}, '\n                      f'Loss: {running_loss/100:.3f}, '\n                      f'Accuracy: {100.*correct/total:.2f}%')\n                running_loss = 0.0\n                \n        # Epoch statistics\n        print(f'Epoch {epoch+1} completed. '\n              f'Accuracy: {100.*correct/total:.2f}%')\nExample usage:\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Create ViT model\nmodel = VisionTransformer(\n    image_size=224,\n    patch_size=16,\n    in_channels=3,\n    num_classes=10,\n    embed_dim=768,\n    depth=12,\n    num_heads=12,\n    mlp_ratio=4,\n    dropout_rate=0.1\n).to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# Load data\ntransform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = datasets.FakeData(\n    transform=transforms.ToTensor()\n)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n\n# Train model\ntrain_vit(model, train_loader, optimizer, criterion, device, epochs=10)"
  },
  {
    "objectID": "posts/vision-transformers/index.html#inference-and-usage",
    "href": "posts/vision-transformers/index.html#inference-and-usage",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Hereâ€™s how to use the model for inference:\ndef inference(model, image_tensor, device):\n    model.eval()\n    with torch.no_grad():\n        image_tensor = image_tensor.unsqueeze(0).to(device)  # Add batch dimension\n        output = model(image_tensor)\n        probabilities = F.softmax(output, dim=1)\n        predicted_class = torch.argmax(probabilities, dim=1)\n    return predicted_class.item(), probabilities[0]\n\n# Example usage\nimage = transform(image).to(device)\npredicted_class, probabilities = inference(model, image, device)\nprint(f\"Predicted class: {predicted_class}\")\nprint(f\"Confidence: {probabilities[predicted_class]:.4f}\")"
  },
  {
    "objectID": "posts/vision-transformers/index.html#optimization-techniques",
    "href": "posts/vision-transformers/index.html#optimization-techniques",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "To improve the training and performance of ViT models, consider these optimization techniques:\n\n\nThe standard attention implementation can be memory-intensive. You can use a more efficient implementation:\ndef efficient_attention(q, k, v, mask=None):\n    # q, k, v: [batch_size, num_heads, seq_len, head_dim]\n    \n    # Scaled dot-product attention\n    scale = q.size(-1) ** -0.5\n    attention = torch.matmul(q, k.transpose(-2, -1)) * scale  # [B, H, L, L]\n    \n    if mask is not None:\n        attention = attention.masked_fill(mask == 0, -1e9)\n    \n    attention = F.softmax(attention, dim=-1)\n    output = torch.matmul(attention, v)  # [B, H, L, D]\n    \n    return output\n\n\n\nUse mixed precision training to reduce memory usage and increase training speed:\nfrom torch.cuda.amp import autocast, GradScaler\n\ndef train_with_mixed_precision(model, train_loader, optimizer, criterion, device, epochs=10):\n    scaler = GradScaler()\n    model.train()\n    \n    for epoch in range(epochs):\n        running_loss = 0.0\n        \n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Use autocast for mixed precision\n            with autocast():\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n            \n            # Scale gradients and optimize\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            running_loss += loss.item()\n            # Rest of the training loop...\n\n\n\nImplement regularization techniques such as stochastic depth to prevent overfitting:\nclass StochasticDepth(nn.Module):\n    def __init__(self, drop_prob=0.1):\n        super().__init__()\n        self.drop_prob = drop_prob\n        \n    def forward(self, x):\n        if not self.training or self.drop_prob == 0.:\n            return x\n        \n        keep_prob = 1 - self.drop_prob\n        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n        random_tensor.floor_()  # binarize\n        output = x.div(keep_prob) * random_tensor\n        return output"
  },
  {
    "objectID": "posts/vision-transformers/index.html#advanced-variants",
    "href": "posts/vision-transformers/index.html#advanced-variants",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Several advanced variants of Vision Transformers have been developed:\n\n\nDeiT introduces a distillation token and a teacher-student strategy:\nclass DeiT(VisionTransformer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        \n        # Distillation token\n        self.dist_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n        \n        # Update position embeddings to include distillation token\n        # Original position embeddings are for [class_token, patches]\n        # New position embeddings are for [class_token, dist_token, patches]\n        num_patches = self.patch_embedding.num_patches\n        new_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 2, self.embed_dim))\n        \n        # Initialize new position embeddings with the original ones\n        # Copy class token position embedding\n        new_pos_embed.data[:, 0:1, :] = self.pos_embedding.data[:, 0:1, :]\n        # Add a new position embedding for distillation token\n        new_pos_embed.data[:, 1:2, :] = self.pos_embedding.data[:, 0:1, :]\n        # Copy patch position embeddings\n        new_pos_embed.data[:, 2:, :] = self.pos_embedding.data[:, 1:, :]\n        \n        self.pos_embedding = new_pos_embed\n        \n        # Additional classification head for distillation\n        self.head_dist = nn.Linear(self.embed_dim, kwargs.get('num_classes', 1000))\n        \n    def forward(self, x):\n        # Get patch embeddings\n        x = self.patch_embedding(x)\n        \n        # Add class and distillation tokens\n        batch_size = x.shape[0]\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n        dist_tokens = self.dist_token.expand(batch_size, -1, -1)\n        x = torch.cat((cls_tokens, dist_tokens, x), dim=1)\n        \n        # Add position embedding and apply dropout\n        x = x + self.pos_embedding\n        x = self.dropout(x)\n        \n        # Apply transformer blocks\n        for block in self.transformer_blocks:\n            x = block(x)\n        \n        # Apply final layer normalization\n        x = self.norm(x)\n        \n        # Get class and distillation tokens\n        cls_token_final = x[:, 0]\n        dist_token_final = x[:, 1]\n        \n        # Apply classification heads\n        x_cls = self.mlp_head(cls_token_final)\n        x_dist = self.head_dist(dist_token_final)\n        \n        if self.training:\n            # During training, return both outputs\n            return x_cls, x_dist\n        else:\n            # During inference, return average\n            return (x_cls + x_dist) / 2\n\n\n\nSwin Transformer introduces hierarchical feature maps and shifted windows:\n# This is a simplified conceptual implementation of the Swin Transformer block\nclass WindowAttention(nn.Module):\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, dropout=0.):\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # (height, width)\n        self.num_heads = num_heads\n        self.scale = (dim // num_heads) ** -0.5\n        \n        # Linear projections\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n        \n        # Define relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))\n            \n        # Generate pair-wise relative position index for each token in the window\n        coords_h = torch.arange(self.window_size[0])\n        coords_w = torch.arange(self.window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        \n        # Calculate relative positions\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        \n        self.register_buffer(\"relative_position_index\", relative_position_index)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, mask=None):\n        B_, N, C = x.shape\n        \n        # Generate QKV matrices\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # 3, B_, num_heads, N, C//num_heads\n        q, k, v = qkv[0], qkv[1], qkv[2]  # each has shape [B_, num_heads, N, C//num_heads]\n        \n        # Scaled dot-product attention\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))  # B_, num_heads, N, N\n        \n        # Add relative position bias\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)]\n        relative_position_bias = relative_position_bias.view(\n            self.window_size[0] * self.window_size[1], \n            self.window_size[0] * self.window_size[1], \n            -1)  # Wh*Ww, Wh*Ww, num_heads\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # num_heads, Wh*Ww, Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0)\n        \n        # Apply mask if needed\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n        \n        # Apply softmax\n        attn = F.softmax(attn, dim=-1)\n        attn = self.dropout(attn)\n        \n        # Apply attention to values\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        \n        return x"
  },
  {
    "objectID": "posts/vision-transformers/index.html#conclusion",
    "href": "posts/vision-transformers/index.html#conclusion",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Vision Transformers represent a significant advancement in computer vision, offering a different approach from traditional CNNs. This guide has covered the essential components for implementing a Vision Transformer from scratch, including image patching, position embeddings, transformer encoders, and classification heads.\nBy understanding these fundamentals, you can implement your own ViT models and experiment with various modifications to improve performance for specific tasks."
  },
  {
    "objectID": "posts/vision-transformers/index.html#references",
    "href": "posts/vision-transformers/index.html#references",
    "title": "Vision Transformer (ViT) Implementation Guide",
    "section": "",
    "text": "Dosovitskiy, A., et al.Â (2020). â€œAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.â€ arXiv:2010.11929.\nTouvron, H., et al.Â (2021). â€œTraining data-efficient image transformers & distillation through attention.â€ arXiv:2012.12877.\nLiu, Z., et al.Â (2021). â€œSwin Transformer: Hierarchical Vision Transformer using Shifted Windows.â€ arXiv:2103.14030."
  },
  {
    "objectID": "posts/python-pi/index.html",
    "href": "posts/python-pi/index.html",
    "title": "Python 3.14: The Next Evolution in Python Development",
    "section": "",
    "text": "Python continues its steady march forward with the anticipated release of Python 3.14, marking another significant milestone in the languageâ€™s evolution. As the Python Software Foundation maintains its annual release cycle, Python 3.14 represents the ongoing commitment to improving performance, developer experience, and language capabilities.\n\n\nFollowing Pythonâ€™s established release schedule, Python 3.14 continues the pattern of annual major releases that began with Python 3.9. The development process follows the standard Python Enhancement Proposal (PEP) system, where community members propose, discuss, and refine new features before implementation.\nThe release represents months of collaborative work from core developers, contributors, and the broader Python community, focusing on backward compatibility while introducing meaningful improvements to the language.\n\n\n\nPython 3.14 builds upon the performance improvements introduced in recent versions. The development team has continued optimizing the interpreter, with particular attention to:\n\nMemory Management: Further refinements to Pythonâ€™s garbage collection system, reducing memory overhead and improving allocation efficiency\nBytecode Optimization: Enhanced compilation processes that generate more efficient bytecode\nStandard Library Performance: Optimizations to frequently-used modules and functions\n\nThese improvements contribute to faster execution times and reduced resource consumption, particularly beneficial for long-running applications and data-intensive workloads.\n\n\n\nWhile maintaining Pythonâ€™s philosophy of readability and simplicity, Python 3.14 introduces carefully considered language enhancements:\n\n\nThe static typing ecosystem continues to mature, with enhancements to type hints and better integration between runtime and static analysis tools. These improvements make it easier for developers to write type-safe code while maintaining Pythonâ€™s dynamic nature.\n\n\n\nSeveral quality-of-life improvements have been introduced to make Python development more efficient and enjoyable. Error messages have been further refined to provide clearer guidance, and debugging capabilities have been enhanced.\n\n\n\n\nPythonâ€™s â€œbatteries includedâ€ philosophy remains strong in 3.14, with updates across the standard library:\n\nNew Modules: Introduction of modules addressing modern development needs\nDeprecated Module Updates: Continued modernization of older modules while maintaining backward compatibility\nSecurity Enhancements: Strengthened cryptographic modules and security-related functionality\n\n\n\n\nPython 3.14 maintains the projectâ€™s commitment to stability. Any breaking changes are minimal and well-documented, with clear migration paths provided. The development team continues to balance innovation with the needs of existing codebases.\nMost Python 3.13 code should run without modification on Python 3.14, though developers are encouraged to review the official migration guide for any project-specific considerations.\n\n\n\nThe release reflects the vibrant Python ecosystem, with contributions from developers worldwide. The Python Software Foundationâ€™s governance model ensures that changes serve the broad community while maintaining the languageâ€™s core principles.\n\n\n\nPython 3.14 sets the foundation for future developments while addressing current needs. The development team continues to explore areas such as:\n\nPerformance optimization strategies\nImproved tooling integration\nEnhanced support for modern development practices\nContinued evolution of the type system\n\n\n\n\nDevelopers interested in trying Python 3.14 can download it from the official Python website. The comprehensive documentation includes migration notes, feature explanations, and examples to help developers transition smoothly.\nFor production environments, thorough testing is recommended before upgrading, though the Python teamâ€™s commitment to stability makes the transition process straightforward for most applications.\n\n\n\nPython 3.14 represents another solid step forward for the language, balancing innovation with stability. The release demonstrates the Python communityâ€™s continued dedication to creating a language that remains accessible to beginners while powerful enough for the most demanding applications.\nAs Python approaches its fourth decade, releases like 3.14 show that the language continues to evolve thoughtfully, maintaining its position as one of the worldâ€™s most popular and versatile programming languages.\n\nFor the most current information about Python 3.14, including detailed release notes and migration guides, visit the official Python documentation at python.org."
  },
  {
    "objectID": "posts/python-pi/index.html#release-timeline-and-development",
    "href": "posts/python-pi/index.html#release-timeline-and-development",
    "title": "Python 3.14: The Next Evolution in Python Development",
    "section": "",
    "text": "Following Pythonâ€™s established release schedule, Python 3.14 continues the pattern of annual major releases that began with Python 3.9. The development process follows the standard Python Enhancement Proposal (PEP) system, where community members propose, discuss, and refine new features before implementation.\nThe release represents months of collaborative work from core developers, contributors, and the broader Python community, focusing on backward compatibility while introducing meaningful improvements to the language."
  },
  {
    "objectID": "posts/python-pi/index.html#performance-enhancements",
    "href": "posts/python-pi/index.html#performance-enhancements",
    "title": "Python 3.14: The Next Evolution in Python Development",
    "section": "",
    "text": "Python 3.14 builds upon the performance improvements introduced in recent versions. The development team has continued optimizing the interpreter, with particular attention to:\n\nMemory Management: Further refinements to Pythonâ€™s garbage collection system, reducing memory overhead and improving allocation efficiency\nBytecode Optimization: Enhanced compilation processes that generate more efficient bytecode\nStandard Library Performance: Optimizations to frequently-used modules and functions\n\nThese improvements contribute to faster execution times and reduced resource consumption, particularly beneficial for long-running applications and data-intensive workloads."
  },
  {
    "objectID": "posts/python-pi/index.html#language-features-and-syntax",
    "href": "posts/python-pi/index.html#language-features-and-syntax",
    "title": "Python 3.14: The Next Evolution in Python Development",
    "section": "",
    "text": "While maintaining Pythonâ€™s philosophy of readability and simplicity, Python 3.14 introduces carefully considered language enhancements:\n\n\nThe static typing ecosystem continues to mature, with enhancements to type hints and better integration between runtime and static analysis tools. These improvements make it easier for developers to write type-safe code while maintaining Pythonâ€™s dynamic nature.\n\n\n\nSeveral quality-of-life improvements have been introduced to make Python development more efficient and enjoyable. Error messages have been further refined to provide clearer guidance, and debugging capabilities have been enhanced."
  },
  {
    "objectID": "posts/python-pi/index.html#standard-library-updates",
    "href": "posts/python-pi/index.html#standard-library-updates",
    "title": "Python 3.14: The Next Evolution in Python Development",
    "section": "",
    "text": "Pythonâ€™s â€œbatteries includedâ€ philosophy remains strong in 3.14, with updates across the standard library:\n\nNew Modules: Introduction of modules addressing modern development needs\nDeprecated Module Updates: Continued modernization of older modules while maintaining backward compatibility\nSecurity Enhancements: Strengthened cryptographic modules and security-related functionality"
  },
  {
    "objectID": "posts/python-pi/index.html#breaking-changes-and-migration",
    "href": "posts/python-pi/index.html#breaking-changes-and-migration",
    "title": "Python 3.14: The Next Evolution in Python Development",
    "section": "",
    "text": "Python 3.14 maintains the projectâ€™s commitment to stability. Any breaking changes are minimal and well-documented, with clear migration paths provided. The development team continues to balance innovation with the needs of existing codebases.\nMost Python 3.13 code should run without modification on Python 3.14, though developers are encouraged to review the official migration guide for any project-specific considerations."
  },
  {
    "objectID": "posts/python-pi/index.html#community-impact",
    "href": "posts/python-pi/index.html#community-impact",
    "title": "Python 3.14: The Next Evolution in Python Development",
    "section": "",
    "text": "The release reflects the vibrant Python ecosystem, with contributions from developers worldwide. The Python Software Foundationâ€™s governance model ensures that changes serve the broad community while maintaining the languageâ€™s core principles."
  },
  {
    "objectID": "posts/python-pi/index.html#looking-forward",
    "href": "posts/python-pi/index.html#looking-forward",
    "title": "Python 3.14: The Next Evolution in Python Development",
    "section": "",
    "text": "Python 3.14 sets the foundation for future developments while addressing current needs. The development team continues to explore areas such as:\n\nPerformance optimization strategies\nImproved tooling integration\nEnhanced support for modern development practices\nContinued evolution of the type system"
  },
  {
    "objectID": "posts/python-pi/index.html#getting-started-with-python-3.14",
    "href": "posts/python-pi/index.html#getting-started-with-python-3.14",
    "title": "Python 3.14: The Next Evolution in Python Development",
    "section": "",
    "text": "Developers interested in trying Python 3.14 can download it from the official Python website. The comprehensive documentation includes migration notes, feature explanations, and examples to help developers transition smoothly.\nFor production environments, thorough testing is recommended before upgrading, though the Python teamâ€™s commitment to stability makes the transition process straightforward for most applications."
  },
  {
    "objectID": "posts/python-pi/index.html#conclusion",
    "href": "posts/python-pi/index.html#conclusion",
    "title": "Python 3.14: The Next Evolution in Python Development",
    "section": "",
    "text": "Python 3.14 represents another solid step forward for the language, balancing innovation with stability. The release demonstrates the Python communityâ€™s continued dedication to creating a language that remains accessible to beginners while powerful enough for the most demanding applications.\nAs Python approaches its fourth decade, releases like 3.14 show that the language continues to evolve thoughtfully, maintaining its position as one of the worldâ€™s most popular and versatile programming languages.\n\nFor the most current information about Python 3.14, including detailed release notes and migration guides, visit the official Python documentation at python.org."
  },
  {
    "objectID": "posts/dino-explained/index.html",
    "href": "posts/dino-explained/index.html",
    "title": "DINO: Emerging Properties in Self-Supervised Vision Transformers",
    "section": "",
    "text": "In 2021, Facebook AI Research (now Meta AI) introduced DINO (Self-Distillation with No Labels), a groundbreaking approach to self-supervised learning in computer vision. Published in the paper â€œEmerging Properties in Self-Supervised Vision Transformersâ€ by Mathilde Caron and colleagues, DINO represented a significant leap forward in learning visual representations without relying on labeled data. This article explores the key aspects of the original DINO research, its methodology, and its implications for computer vision.\n\n\n\nTraditionally, computer vision models have relied heavily on supervised learning using massive labeled datasets like ImageNet. However, creating such datasets requires enormous human effort for annotation. Self-supervised learning aims to overcome this limitation by teaching models to learn meaningful representations from unlabeled images, which are abundantly available.\nSeveral approaches to self-supervised learning had been proposed before DINO, including:\n\nContrastive learning (SimCLR, MoCo)\nClustering-based methods (SwAV, DeepCluster)\nPredictive methods (predicting rotations, solving jigsaw puzzles)\n\nDINO introduced a novel approach that combined elements of knowledge distillation and self-supervision to produce surprisingly effective visual representations.\n\n\n\nDINOâ€™s key innovation was adapting the concept of knowledge distillation to a self-supervised setting. Traditional knowledge distillation involves a teacher model transferring knowledge to a student model, but DINO cleverly applies this concept without requiring separate pre-trained teacher models.\n\n\nIn DINO:\n\nTeacher and Student Networks: Both networks share the same architecture but have different parameters.\nParameter Updates:\n\nThe student network is updated through standard backpropagation\nThe teacher is updated as an exponential moving average (EMA) of the studentâ€™s parameters\n\n\nThis creates a bootstrapping effect where the teacher continually provides slightly better targets for the student to learn from.\n\n\n\nDINO employs a sophisticated data augmentation approach:\n\nGlobal Views: Two larger crops of an image (covering significant portions)\nLocal Views: Several smaller crops that focus on details\n\nThe student network processes all views (global and local), while the teacher only processes the global views. The student network is trained to predict the teacherâ€™s output for the global views from the local views, forcing it to understand both global context and local details.\n\n\n\nThe training objective minimizes the cross-entropy between the teacherâ€™s output distribution for global views and the studentâ€™s output distribution for all views (both global and local). This encourages consistency across different scales and regions of the image.\n\n\n\nA major challenge in self-supervised learning is representation collapseâ€”where the model outputs the same representation regardless of input. DINO prevents this through:\n\nCentering: Subtracting a running average of the networkâ€™s output from the current output\nSharpening: Using a temperature parameter in the softmax that gradually decreases throughout training\n\nThese techniques ensure the model learns diverse and meaningful features.\n\n\n\n\nWhile DINO can be applied to various neural network architectures, the paper demonstrated particularly impressive results using Vision Transformers (ViT). The combination of DINO with ViT offered several advantages:\n\nPatch-based processing: ViT divides images into patches, which aligns well with DINOâ€™s local-global view approach\nSelf-attention mechanism: Enables capturing long-range dependencies in images\nScalability: The architecture scales effectively with more data and parameters\n\nDINO was implemented with various sizes of ViT models: - ViT-S: Small (22M parameters) - ViT-B: Base (86M parameters)\n\n\n\nThe most surprising aspect of DINO was the emergence of properties that werenâ€™t explicitly trained for:\n\n\nRemarkably, the self-attention maps from DINO-trained Vision Transformers naturally highlighted object boundaries in images. Without any segmentation supervision, the model learned to focus attention on semantically meaningful regions. This surprised the research community and suggested that the model had developed a deeper understanding of visual structures than previous self-supervised approaches.\n\n\n\nDINO produced local features (from patch tokens) that proved extremely effective for tasks requiring spatial understanding, like semantic segmentation. The features exhibited strong semantic coherence across spatial regions.\n\n\n\nUsing DINO features with simple k-nearest neighbor classifiers achieved impressive accuracy on ImageNet classification, demonstrating the quality of the learned representations.\n\n\n\n\nThe original DINO paper described several important implementation details:\n\n\nThe augmentation pipeline included: - Random resized cropping - Horizontal flipping - Color jittering - Gaussian blur - Solarization (for some views)\n\n\n\n\nOptimizer: AdamW with weight decay\nLearning rate: Cosine schedule with linear warmup\nBatch size: 1024 images\n\n\n\n\n\nProjection head: 3-layer MLP with bottleneck structure\nCLS token: Used as global image representation\nPositional embeddings: Standard learnable embeddings\n\n\n\n\n\nDINO achieved remarkable results on several benchmarks:\n\n\n\n80.1% top-1 accuracy with k-NN classification using ViT-B\nCompetitive with supervised methods and superior to previous self-supervised approaches\n\n\n\n\nDINO features transferred successfully to: - Object detection - Semantic segmentation - Video instance segmentation\n\n\n\nThe features showed strong robustness to distribution shifts and generalized well to out-of-distribution data.\n\n\n\n\nDINO differed from earlier self-supervised approaches in several key ways:\n\n\n\nNo need for large negative sample sets\nNo dependence on intricate data augmentation strategies\nMore stable training dynamics\n\n\n\n\n\nNo explicit clustering objective\nMore straightforward implementation\nBetter scaling properties with model size\n\n\n\n\n\nThe original DINO research represented a significant step forward in self-supervised visual representation learning. By combining knowledge distillation techniques with self-supervision and leveraging the Vision Transformer architecture, DINO produced features with remarkable properties for a wide range of computer vision tasks.\nThe emergence of semantic features and unsupervised segmentation abilities demonstrated that well-designed self-supervised methods could lead to models that understand visual concepts in ways previously thought to require explicit supervision. DINO laid the groundwork for subsequent advances in this field, including its successor DINOv2, and helped establish self-supervised learning as a powerful paradigm for computer vision.\nThe success of DINO highlighted the potential for self-supervised learning to reduce reliance on large labeled datasets and pointed toward a future where visual foundation models could be developed primarily through self-supervision â€“ mirroring similar developments in natural language processing with large language models."
  },
  {
    "objectID": "posts/dino-explained/index.html#introduction",
    "href": "posts/dino-explained/index.html#introduction",
    "title": "DINO: Emerging Properties in Self-Supervised Vision Transformers",
    "section": "",
    "text": "In 2021, Facebook AI Research (now Meta AI) introduced DINO (Self-Distillation with No Labels), a groundbreaking approach to self-supervised learning in computer vision. Published in the paper â€œEmerging Properties in Self-Supervised Vision Transformersâ€ by Mathilde Caron and colleagues, DINO represented a significant leap forward in learning visual representations without relying on labeled data. This article explores the key aspects of the original DINO research, its methodology, and its implications for computer vision."
  },
  {
    "objectID": "posts/dino-explained/index.html#the-challenge-of-self-supervised-learning",
    "href": "posts/dino-explained/index.html#the-challenge-of-self-supervised-learning",
    "title": "DINO: Emerging Properties in Self-Supervised Vision Transformers",
    "section": "",
    "text": "Traditionally, computer vision models have relied heavily on supervised learning using massive labeled datasets like ImageNet. However, creating such datasets requires enormous human effort for annotation. Self-supervised learning aims to overcome this limitation by teaching models to learn meaningful representations from unlabeled images, which are abundantly available.\nSeveral approaches to self-supervised learning had been proposed before DINO, including:\n\nContrastive learning (SimCLR, MoCo)\nClustering-based methods (SwAV, DeepCluster)\nPredictive methods (predicting rotations, solving jigsaw puzzles)\n\nDINO introduced a novel approach that combined elements of knowledge distillation and self-supervision to produce surprisingly effective visual representations."
  },
  {
    "objectID": "posts/dino-explained/index.html#dinos-core-methodology",
    "href": "posts/dino-explained/index.html#dinos-core-methodology",
    "title": "DINO: Emerging Properties in Self-Supervised Vision Transformers",
    "section": "",
    "text": "DINOâ€™s key innovation was adapting the concept of knowledge distillation to a self-supervised setting. Traditional knowledge distillation involves a teacher model transferring knowledge to a student model, but DINO cleverly applies this concept without requiring separate pre-trained teacher models.\n\n\nIn DINO:\n\nTeacher and Student Networks: Both networks share the same architecture but have different parameters.\nParameter Updates:\n\nThe student network is updated through standard backpropagation\nThe teacher is updated as an exponential moving average (EMA) of the studentâ€™s parameters\n\n\nThis creates a bootstrapping effect where the teacher continually provides slightly better targets for the student to learn from.\n\n\n\nDINO employs a sophisticated data augmentation approach:\n\nGlobal Views: Two larger crops of an image (covering significant portions)\nLocal Views: Several smaller crops that focus on details\n\nThe student network processes all views (global and local), while the teacher only processes the global views. The student network is trained to predict the teacherâ€™s output for the global views from the local views, forcing it to understand both global context and local details.\n\n\n\nThe training objective minimizes the cross-entropy between the teacherâ€™s output distribution for global views and the studentâ€™s output distribution for all views (both global and local). This encourages consistency across different scales and regions of the image.\n\n\n\nA major challenge in self-supervised learning is representation collapseâ€”where the model outputs the same representation regardless of input. DINO prevents this through:\n\nCentering: Subtracting a running average of the networkâ€™s output from the current output\nSharpening: Using a temperature parameter in the softmax that gradually decreases throughout training\n\nThese techniques ensure the model learns diverse and meaningful features."
  },
  {
    "objectID": "posts/dino-explained/index.html#vision-transformer-architecture",
    "href": "posts/dino-explained/index.html#vision-transformer-architecture",
    "title": "DINO: Emerging Properties in Self-Supervised Vision Transformers",
    "section": "",
    "text": "While DINO can be applied to various neural network architectures, the paper demonstrated particularly impressive results using Vision Transformers (ViT). The combination of DINO with ViT offered several advantages:\n\nPatch-based processing: ViT divides images into patches, which aligns well with DINOâ€™s local-global view approach\nSelf-attention mechanism: Enables capturing long-range dependencies in images\nScalability: The architecture scales effectively with more data and parameters\n\nDINO was implemented with various sizes of ViT models: - ViT-S: Small (22M parameters) - ViT-B: Base (86M parameters)"
  },
  {
    "objectID": "posts/dino-explained/index.html#emergent-properties",
    "href": "posts/dino-explained/index.html#emergent-properties",
    "title": "DINO: Emerging Properties in Self-Supervised Vision Transformers",
    "section": "",
    "text": "The most surprising aspect of DINO was the emergence of properties that werenâ€™t explicitly trained for:\n\n\nRemarkably, the self-attention maps from DINO-trained Vision Transformers naturally highlighted object boundaries in images. Without any segmentation supervision, the model learned to focus attention on semantically meaningful regions. This surprised the research community and suggested that the model had developed a deeper understanding of visual structures than previous self-supervised approaches.\n\n\n\nDINO produced local features (from patch tokens) that proved extremely effective for tasks requiring spatial understanding, like semantic segmentation. The features exhibited strong semantic coherence across spatial regions.\n\n\n\nUsing DINO features with simple k-nearest neighbor classifiers achieved impressive accuracy on ImageNet classification, demonstrating the quality of the learned representations."
  },
  {
    "objectID": "posts/dino-explained/index.html#training-details",
    "href": "posts/dino-explained/index.html#training-details",
    "title": "DINO: Emerging Properties in Self-Supervised Vision Transformers",
    "section": "",
    "text": "The original DINO paper described several important implementation details:\n\n\nThe augmentation pipeline included: - Random resized cropping - Horizontal flipping - Color jittering - Gaussian blur - Solarization (for some views)\n\n\n\n\nOptimizer: AdamW with weight decay\nLearning rate: Cosine schedule with linear warmup\nBatch size: 1024 images\n\n\n\n\n\nProjection head: 3-layer MLP with bottleneck structure\nCLS token: Used as global image representation\nPositional embeddings: Standard learnable embeddings"
  },
  {
    "objectID": "posts/dino-explained/index.html#results-and-impact",
    "href": "posts/dino-explained/index.html#results-and-impact",
    "title": "DINO: Emerging Properties in Self-Supervised Vision Transformers",
    "section": "",
    "text": "DINO achieved remarkable results on several benchmarks:\n\n\n\n80.1% top-1 accuracy with k-NN classification using ViT-B\nCompetitive with supervised methods and superior to previous self-supervised approaches\n\n\n\n\nDINO features transferred successfully to: - Object detection - Semantic segmentation - Video instance segmentation\n\n\n\nThe features showed strong robustness to distribution shifts and generalized well to out-of-distribution data."
  },
  {
    "objectID": "posts/dino-explained/index.html#comparison-with-previous-methods",
    "href": "posts/dino-explained/index.html#comparison-with-previous-methods",
    "title": "DINO: Emerging Properties in Self-Supervised Vision Transformers",
    "section": "",
    "text": "DINO differed from earlier self-supervised approaches in several key ways:\n\n\n\nNo need for large negative sample sets\nNo dependence on intricate data augmentation strategies\nMore stable training dynamics\n\n\n\n\n\nNo explicit clustering objective\nMore straightforward implementation\nBetter scaling properties with model size"
  },
  {
    "objectID": "posts/dino-explained/index.html#conclusion",
    "href": "posts/dino-explained/index.html#conclusion",
    "title": "DINO: Emerging Properties in Self-Supervised Vision Transformers",
    "section": "",
    "text": "The original DINO research represented a significant step forward in self-supervised visual representation learning. By combining knowledge distillation techniques with self-supervision and leveraging the Vision Transformer architecture, DINO produced features with remarkable properties for a wide range of computer vision tasks.\nThe emergence of semantic features and unsupervised segmentation abilities demonstrated that well-designed self-supervised methods could lead to models that understand visual concepts in ways previously thought to require explicit supervision. DINO laid the groundwork for subsequent advances in this field, including its successor DINOv2, and helped establish self-supervised learning as a powerful paradigm for computer vision.\nThe success of DINO highlighted the potential for self-supervised learning to reduce reliance on large labeled datasets and pointed toward a future where visual foundation models could be developed primarily through self-supervision â€“ mirroring similar developments in natural language processing with large language models."
  },
  {
    "objectID": "posts/litserve-basics/index.html",
    "href": "posts/litserve-basics/index.html",
    "title": "LitServe Code Guide",
    "section": "",
    "text": "LitServe is a high-performance, flexible AI model serving framework designed to deploy machine learning models with minimal code. It provides automatic batching, GPU acceleration, and easy scaling capabilities.\n\n\n\nInstallation\nBasic Usage\nCore Concepts\nAdvanced Features\nConfiguration Options\nExamples\nBest Practices\nTroubleshooting\n\n\n\n\n# Install LitServe\npip install litserve\n\n# For GPU support\npip install litserve[gpu]\n\n# For development dependencies\npip install litserve[dev]\n\n\n\n\n\nimport litserve as ls\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nclass SimpleTextGenerator(ls.LitAPI):\n    def setup(self, device):\n        # Load model and tokenizer during server startup\n        self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n        self.model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n        self.model.to(device)\n        self.model.eval()\n    \n    def decode_request(self, request):\n        # Process incoming request\n        return request[\"prompt\"]\n    \n    def predict(self, x):\n        # Run inference\n        inputs = self.tokenizer.encode(x, return_tensors=\"pt\")\n        with torch.no_grad():\n            outputs = self.model.generate(\n                inputs, \n                max_length=100, \n                num_return_sequences=1,\n                temperature=0.7\n            )\n        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    def encode_response(self, output):\n        # Format response\n        return {\"generated_text\": output}\n\n# Create and start server\nif __name__ == \"__main__\":\n    api = SimpleTextGenerator()\n    server = ls.LitServer(api, accelerator=\"auto\", max_batch_size=4)\n    server.run(port=8000)\n\n\n\nimport requests\n\n# Test the API\nresponse = requests.post(\n    \"http://localhost:8000/predict\",\n    json={\"prompt\": \"The future of AI is\"}\n)\nprint(response.json())\n\n\n\n\n\n\nEvery LitServe API must inherit from ls.LitAPI and implement these core methods:\nclass MyAPI(ls.LitAPI):\n    def setup(self, device):\n        \"\"\"Initialize models, load weights, set up preprocessing\"\"\"\n        pass\n    \n    def decode_request(self, request):\n        \"\"\"Parse and validate incoming requests\"\"\"\n        pass\n    \n    def predict(self, x):\n        \"\"\"Run model inference\"\"\"\n        pass\n    \n    def encode_response(self, output):\n        \"\"\"Format model output for HTTP response\"\"\"\n        pass\n\n\n\nclass AdvancedAPI(ls.LitAPI):\n    def batch(self, inputs):\n        \"\"\"Custom batching logic (optional)\"\"\"\n        return inputs\n    \n    def unbatch(self, output):\n        \"\"\"Custom unbatching logic (optional)\"\"\"\n        return output\n    \n    def preprocess(self, input_data):\n        \"\"\"Additional preprocessing (optional)\"\"\"\n        return input_data\n    \n    def postprocess(self, output):\n        \"\"\"Additional postprocessing (optional)\"\"\"\n        return output\n\n\n\n\n\n\nclass BatchedImageClassifier(ls.LitAPI):\n    def setup(self, device):\n        from torchvision import models, transforms\n        self.model = models.resnet50(pretrained=True)\n        self.model.to(device)\n        self.model.eval()\n        \n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                               std=[0.229, 0.224, 0.225])\n        ])\n    \n    def decode_request(self, request):\n        from PIL import Image\n        import base64\n        import io\n        \n        # Decode base64 image\n        image_data = base64.b64decode(request[\"image\"])\n        image = Image.open(io.BytesIO(image_data))\n        return self.transform(image).unsqueeze(0)\n    \n    def batch(self, inputs):\n        # Custom batching for images\n        return torch.cat(inputs, dim=0)\n    \n    def predict(self, batch):\n        with torch.no_grad():\n            outputs = self.model(batch)\n            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n        return probabilities\n    \n    def unbatch(self, output):\n        # Split batch back to individual predictions\n        return [pred.unsqueeze(0) for pred in output]\n    \n    def encode_response(self, output):\n        # Get top prediction\n        confidence, predicted = torch.max(output, 1)\n        return {\n            \"class_id\": predicted.item(),\n            \"confidence\": confidence.item()\n        }\n\n\n\nclass StreamingChatAPI(ls.LitAPI):\n    def setup(self, device):\n        from transformers import AutoTokenizer, AutoModelForCausalLM\n        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n        self.model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n        self.model.to(device)\n    \n    def decode_request(self, request):\n        return request[\"message\"]\n    \n    def predict(self, x):\n        # Generator for streaming\n        inputs = self.tokenizer.encode(x, return_tensors=\"pt\")\n        \n        for i in range(50):  # Generate up to 50 tokens\n            with torch.no_grad():\n                outputs = self.model(inputs)\n                next_token_logits = outputs.logits[:, -1, :]\n                next_token = torch.multinomial(\n                    torch.softmax(next_token_logits, dim=-1), \n                    num_samples=1\n                )\n                inputs = torch.cat([inputs, next_token], dim=-1)\n                \n                # Yield each token\n                token_text = self.tokenizer.decode(next_token[0])\n                yield {\"token\": token_text}\n                \n                # Stop if end token\n                if next_token.item() == self.tokenizer.eos_token_id:\n                    break\n    \n    def encode_response(self, output):\n        return output\n\n# Enable streaming\nserver = ls.LitServer(api, accelerator=\"auto\", stream=True)\n\n\n\n# Automatic multi-GPU scaling\nserver = ls.LitServer(\n    api, \n    accelerator=\"auto\",\n    devices=\"auto\",  # Use all available GPUs\n    max_batch_size=8\n)\n\n# Specify specific GPUs\nserver = ls.LitServer(\n    api,\n    accelerator=\"gpu\",\n    devices=[0, 1, 2],  # Use GPUs 0, 1, and 2\n    max_batch_size=16\n)\n\n\n\nclass AuthenticatedAPI(ls.LitAPI):\n    def setup(self, device):\n        # Your model setup\n        pass\n    \n    def authenticate(self, request):\n        \"\"\"Custom authentication logic\"\"\"\n        api_key = request.headers.get(\"Authorization\")\n        if not api_key or not self.validate_api_key(api_key):\n            raise ls.AuthenticationError(\"Invalid API key\")\n        return True\n    \n    def validate_api_key(self, api_key):\n        # Your API key validation logic\n        valid_keys = [\"your-secret-key-1\", \"your-secret-key-2\"]\n        return api_key.replace(\"Bearer \", \"\") in valid_keys\n    \n    def decode_request(self, request):\n        return request[\"data\"]\n    \n    def predict(self, x):\n        # Your prediction logic\n        return f\"Processed: {x}\"\n    \n    def encode_response(self, output):\n        return {\"result\": output}\n\n\n\n\n\n\nserver = ls.LitServer(\n    api=api,\n    accelerator=\"auto\",           # \"auto\", \"cpu\", \"gpu\", \"mps\"\n    devices=\"auto\",               # Device selection\n    max_batch_size=4,            # Maximum batch size\n    batch_timeout=0.1,           # Batch timeout in seconds\n    workers_per_device=1,        # Workers per device\n    timeout=30,                  # Request timeout\n    stream=False,                # Enable streaming\n    spec=None,                   # Custom OpenAPI spec\n)\n\n\n\n# Set device preferences\nexport CUDA_VISIBLE_DEVICES=0,1,2\n\n# Set batch configuration\nexport LITSERVE_MAX_BATCH_SIZE=8\nexport LITSERVE_BATCH_TIMEOUT=0.05\n\n# Set worker configuration\nexport LITSERVE_WORKERS_PER_DEVICE=2\n\n\n\n# config.py\nclass Config:\n    MAX_BATCH_SIZE = 8\n    BATCH_TIMEOUT = 0.1\n    WORKERS_PER_DEVICE = 2\n    ACCELERATOR = \"auto\"\n    TIMEOUT = 60\n\n# Use in your API\nfrom config import Config\n\nserver = ls.LitServer(\n    api, \n    max_batch_size=Config.MAX_BATCH_SIZE,\n    batch_timeout=Config.BATCH_TIMEOUT,\n    workers_per_device=Config.WORKERS_PER_DEVICE\n)\n\n\n\n\n\n\nimport litserve as ls\nimport torch\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport base64\nimport io\n\nclass ImageClassificationAPI(ls.LitAPI):\n    def setup(self, device):\n        # Load pre-trained ResNet model\n        self.model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n        self.model.to(device)\n        self.model.eval()\n        \n        # Image preprocessing\n        self.preprocess = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                               std=[0.229, 0.224, 0.225]),\n        ])\n        \n        # Load ImageNet class labels\n        with open('imagenet_classes.txt') as f:\n            self.classes = [line.strip() for line in f.readlines()]\n    \n    def decode_request(self, request):\n        # Decode base64 image\n        encoded_image = request[\"image\"]\n        image_bytes = base64.b64decode(encoded_image)\n        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n        return self.preprocess(image).unsqueeze(0)\n    \n    def predict(self, x):\n        with torch.no_grad():\n            output = self.model(x)\n            probabilities = torch.nn.functional.softmax(output[0], dim=0)\n        return probabilities\n    \n    def encode_response(self, output):\n        # Get top 5 predictions\n        top5_prob, top5_catid = torch.topk(output, 5)\n        results = []\n        for i in range(top5_prob.size(0)):\n            results.append({\n                \"class\": self.classes[top5_catid[i]],\n                \"probability\": float(top5_prob[i])\n            })\n        return {\"predictions\": results}\n\nif __name__ == \"__main__\":\n    api = ImageClassificationAPI()\n    server = ls.LitServer(api, accelerator=\"auto\", max_batch_size=4)\n    server.run(port=8000)\n\n\n\nimport litserve as ls\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\n\nclass TextEmbeddingAPI(ls.LitAPI):\n    def setup(self, device):\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        self.model.to(device)\n    \n    def decode_request(self, request):\n        texts = request.get(\"texts\", [])\n        if isinstance(texts, str):\n            texts = [texts]\n        return texts\n    \n    def predict(self, texts):\n        embeddings = self.model.encode(texts)\n        return embeddings\n    \n    def encode_response(self, embeddings):\n        return {\n            \"embeddings\": embeddings.tolist(),\n            \"dimension\": embeddings.shape[1] if len(embeddings.shape) &gt; 1 else len(embeddings)\n        }\n\nif __name__ == \"__main__\":\n    api = TextEmbeddingAPI()\n    server = ls.LitServer(api, accelerator=\"auto\", max_batch_size=32)\n    server.run(port=8000)\n\n\n\nimport litserve as ls\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\nimport base64\nimport io\n\nclass ImageCaptioningAPI(ls.LitAPI):\n    def setup(self, device):\n        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n        self.model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n        self.model.to(device)\n    \n    def decode_request(self, request):\n        # Handle both image and optional text input\n        encoded_image = request[\"image\"]\n        text_prompt = request.get(\"text\", \"\")\n        \n        image_bytes = base64.b64decode(encoded_image)\n        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n        \n        return {\"image\": image, \"text\": text_prompt}\n    \n    def predict(self, inputs):\n        processed = self.processor(\n            images=inputs[\"image\"], \n            text=inputs[\"text\"], \n            return_tensors=\"pt\"\n        )\n        \n        with torch.no_grad():\n            outputs = self.model.generate(**processed, max_length=50)\n        \n        caption = self.processor.decode(outputs[0], skip_special_tokens=True)\n        return caption\n    \n    def encode_response(self, caption):\n        return {\"caption\": caption}\n\n\n\n\n\n\nclass OptimizedAPI(ls.LitAPI):\n    def setup(self, device):\n        # Use torch.jit.script for optimization\n        self.model = torch.jit.script(your_model)\n        \n        # Enable mixed precision if using GPU\n        if device.type == 'cuda':\n            self.scaler = torch.cuda.amp.GradScaler()\n        \n        # Pre-allocate tensors for common shapes\n        self.common_shapes = {}\n    \n    def predict(self, x):\n        # Use autocast for mixed precision\n        if hasattr(self, 'scaler'):\n            with torch.cuda.amp.autocast():\n                return self.model(x)\n        return self.model(x)\n\n\n\nclass RobustAPI(ls.LitAPI):\n    def decode_request(self, request):\n        try:\n            # Validate required fields\n            if \"input\" not in request:\n                raise ls.ValidationError(\"Missing required field: input\")\n            \n            data = request[\"input\"]\n            \n            # Type validation\n            if not isinstance(data, (str, list)):\n                raise ls.ValidationError(\"Input must be string or list\")\n            \n            return data\n        except Exception as e:\n            raise ls.ValidationError(f\"Request parsing failed: {str(e)}\")\n    \n    def predict(self, x):\n        try:\n            result = self.model(x)\n            return result\n        except torch.cuda.OutOfMemoryError:\n            raise ls.ServerError(\"GPU memory exhausted\")\n        except Exception as e:\n            raise ls.ServerError(f\"Prediction failed: {str(e)}\")\n\n\n\nimport logging\nimport time\n\nclass MonitoredAPI(ls.LitAPI):\n    def setup(self, device):\n        self.logger = logging.getLogger(__name__)\n        self.request_count = 0\n        # Your model setup\n    \n    def decode_request(self, request):\n        self.request_count += 1\n        self.logger.info(f\"Processing request #{self.request_count}\")\n        return request[\"data\"]\n    \n    def predict(self, x):\n        start_time = time.time()\n        result = self.model(x)\n        inference_time = time.time() - start_time\n        \n        self.logger.info(f\"Inference completed in {inference_time:.3f}s\")\n        return result\n\n\n\nclass VersionedAPI(ls.LitAPI):\n    def setup(self, device):\n        self.version = \"1.0.0\"\n        self.model_path = f\"models/model_v{self.version}\"\n        # Load versioned model\n    \n    def encode_response(self, output):\n        return {\n            \"result\": output,\n            \"model_version\": self.version,\n            \"timestamp\": time.time()\n        }\n\n\n\n\n\n\n\n\n# Solution: Reduce batch size or implement gradient checkpointing\nserver = ls.LitServer(api, max_batch_size=2)  # Reduce batch size\n\n# Or clear cache in your predict method\ndef predict(self, x):\n    torch.cuda.empty_cache()  # Clear unused memory\n    result = self.model(x)\n    return result\n\n\n\n# Enable model optimization\ndef setup(self, device):\n    self.model.eval()  # Set to evaluation mode\n    self.model = torch.jit.script(self.model)  # JIT compilation\n    \n    # Use half precision if supported\n    if device.type == 'cuda':\n        self.model.half()\n\n\n\n# Increase timeout settings\nserver = ls.LitServer(\n    api, \n    timeout=60,  # Increase request timeout\n    batch_timeout=1.0  # Increase batch timeout\n)\n\n\n\n# Check and kill existing processes\nimport subprocess\nsubprocess.run([\"lsof\", \"-ti:8000\", \"|\", \"xargs\", \"kill\", \"-9\"], shell=True)\n\n# Or use a different port\nserver.run(port=8001)\n\n\n\n\n\nUse appropriate batch sizes: Start with small batches and gradually increase\nEnable GPU acceleration: Use accelerator=\"auto\" for automatic GPU detection\nOptimize model loading: Load models once in setup(), not in predict()\nUse mixed precision: Enable autocast for GPU inference\nProfile your code: Use tools like torch.profiler to identify bottlenecks\nCache preprocessed data: Store frequently used transformations\n\n\n\n\n\nTest API locally with various input types\nValidate error handling for malformed requests\nCheck memory usage under load\nVerify GPU utilization (if using GPUs)\nTest with maximum expected batch size\nImplement proper logging and monitoring\nSet up health check endpoints\nConfigure appropriate timeouts\nTest authentication (if implemented)\nVerify response format consistency\n\nThis guide covers the essential aspects of using LitServe for deploying AI models. For the most up-to-date information, always refer to the official LitServe documentation."
  },
  {
    "objectID": "posts/litserve-basics/index.html#table-of-contents",
    "href": "posts/litserve-basics/index.html#table-of-contents",
    "title": "LitServe Code Guide",
    "section": "",
    "text": "Installation\nBasic Usage\nCore Concepts\nAdvanced Features\nConfiguration Options\nExamples\nBest Practices\nTroubleshooting"
  },
  {
    "objectID": "posts/litserve-basics/index.html#installation",
    "href": "posts/litserve-basics/index.html#installation",
    "title": "LitServe Code Guide",
    "section": "",
    "text": "# Install LitServe\npip install litserve\n\n# For GPU support\npip install litserve[gpu]\n\n# For development dependencies\npip install litserve[dev]"
  },
  {
    "objectID": "posts/litserve-basics/index.html#basic-usage",
    "href": "posts/litserve-basics/index.html#basic-usage",
    "title": "LitServe Code Guide",
    "section": "",
    "text": "import litserve as ls\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nclass SimpleTextGenerator(ls.LitAPI):\n    def setup(self, device):\n        # Load model and tokenizer during server startup\n        self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n        self.model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n        self.model.to(device)\n        self.model.eval()\n    \n    def decode_request(self, request):\n        # Process incoming request\n        return request[\"prompt\"]\n    \n    def predict(self, x):\n        # Run inference\n        inputs = self.tokenizer.encode(x, return_tensors=\"pt\")\n        with torch.no_grad():\n            outputs = self.model.generate(\n                inputs, \n                max_length=100, \n                num_return_sequences=1,\n                temperature=0.7\n            )\n        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    def encode_response(self, output):\n        # Format response\n        return {\"generated_text\": output}\n\n# Create and start server\nif __name__ == \"__main__\":\n    api = SimpleTextGenerator()\n    server = ls.LitServer(api, accelerator=\"auto\", max_batch_size=4)\n    server.run(port=8000)\n\n\n\nimport requests\n\n# Test the API\nresponse = requests.post(\n    \"http://localhost:8000/predict\",\n    json={\"prompt\": \"The future of AI is\"}\n)\nprint(response.json())"
  },
  {
    "objectID": "posts/litserve-basics/index.html#core-concepts",
    "href": "posts/litserve-basics/index.html#core-concepts",
    "title": "LitServe Code Guide",
    "section": "",
    "text": "Every LitServe API must inherit from ls.LitAPI and implement these core methods:\nclass MyAPI(ls.LitAPI):\n    def setup(self, device):\n        \"\"\"Initialize models, load weights, set up preprocessing\"\"\"\n        pass\n    \n    def decode_request(self, request):\n        \"\"\"Parse and validate incoming requests\"\"\"\n        pass\n    \n    def predict(self, x):\n        \"\"\"Run model inference\"\"\"\n        pass\n    \n    def encode_response(self, output):\n        \"\"\"Format model output for HTTP response\"\"\"\n        pass\n\n\n\nclass AdvancedAPI(ls.LitAPI):\n    def batch(self, inputs):\n        \"\"\"Custom batching logic (optional)\"\"\"\n        return inputs\n    \n    def unbatch(self, output):\n        \"\"\"Custom unbatching logic (optional)\"\"\"\n        return output\n    \n    def preprocess(self, input_data):\n        \"\"\"Additional preprocessing (optional)\"\"\"\n        return input_data\n    \n    def postprocess(self, output):\n        \"\"\"Additional postprocessing (optional)\"\"\"\n        return output"
  },
  {
    "objectID": "posts/litserve-basics/index.html#advanced-features",
    "href": "posts/litserve-basics/index.html#advanced-features",
    "title": "LitServe Code Guide",
    "section": "",
    "text": "class BatchedImageClassifier(ls.LitAPI):\n    def setup(self, device):\n        from torchvision import models, transforms\n        self.model = models.resnet50(pretrained=True)\n        self.model.to(device)\n        self.model.eval()\n        \n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                               std=[0.229, 0.224, 0.225])\n        ])\n    \n    def decode_request(self, request):\n        from PIL import Image\n        import base64\n        import io\n        \n        # Decode base64 image\n        image_data = base64.b64decode(request[\"image\"])\n        image = Image.open(io.BytesIO(image_data))\n        return self.transform(image).unsqueeze(0)\n    \n    def batch(self, inputs):\n        # Custom batching for images\n        return torch.cat(inputs, dim=0)\n    \n    def predict(self, batch):\n        with torch.no_grad():\n            outputs = self.model(batch)\n            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n        return probabilities\n    \n    def unbatch(self, output):\n        # Split batch back to individual predictions\n        return [pred.unsqueeze(0) for pred in output]\n    \n    def encode_response(self, output):\n        # Get top prediction\n        confidence, predicted = torch.max(output, 1)\n        return {\n            \"class_id\": predicted.item(),\n            \"confidence\": confidence.item()\n        }\n\n\n\nclass StreamingChatAPI(ls.LitAPI):\n    def setup(self, device):\n        from transformers import AutoTokenizer, AutoModelForCausalLM\n        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n        self.model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n        self.model.to(device)\n    \n    def decode_request(self, request):\n        return request[\"message\"]\n    \n    def predict(self, x):\n        # Generator for streaming\n        inputs = self.tokenizer.encode(x, return_tensors=\"pt\")\n        \n        for i in range(50):  # Generate up to 50 tokens\n            with torch.no_grad():\n                outputs = self.model(inputs)\n                next_token_logits = outputs.logits[:, -1, :]\n                next_token = torch.multinomial(\n                    torch.softmax(next_token_logits, dim=-1), \n                    num_samples=1\n                )\n                inputs = torch.cat([inputs, next_token], dim=-1)\n                \n                # Yield each token\n                token_text = self.tokenizer.decode(next_token[0])\n                yield {\"token\": token_text}\n                \n                # Stop if end token\n                if next_token.item() == self.tokenizer.eos_token_id:\n                    break\n    \n    def encode_response(self, output):\n        return output\n\n# Enable streaming\nserver = ls.LitServer(api, accelerator=\"auto\", stream=True)\n\n\n\n# Automatic multi-GPU scaling\nserver = ls.LitServer(\n    api, \n    accelerator=\"auto\",\n    devices=\"auto\",  # Use all available GPUs\n    max_batch_size=8\n)\n\n# Specify specific GPUs\nserver = ls.LitServer(\n    api,\n    accelerator=\"gpu\",\n    devices=[0, 1, 2],  # Use GPUs 0, 1, and 2\n    max_batch_size=16\n)\n\n\n\nclass AuthenticatedAPI(ls.LitAPI):\n    def setup(self, device):\n        # Your model setup\n        pass\n    \n    def authenticate(self, request):\n        \"\"\"Custom authentication logic\"\"\"\n        api_key = request.headers.get(\"Authorization\")\n        if not api_key or not self.validate_api_key(api_key):\n            raise ls.AuthenticationError(\"Invalid API key\")\n        return True\n    \n    def validate_api_key(self, api_key):\n        # Your API key validation logic\n        valid_keys = [\"your-secret-key-1\", \"your-secret-key-2\"]\n        return api_key.replace(\"Bearer \", \"\") in valid_keys\n    \n    def decode_request(self, request):\n        return request[\"data\"]\n    \n    def predict(self, x):\n        # Your prediction logic\n        return f\"Processed: {x}\"\n    \n    def encode_response(self, output):\n        return {\"result\": output}"
  },
  {
    "objectID": "posts/litserve-basics/index.html#configuration-options",
    "href": "posts/litserve-basics/index.html#configuration-options",
    "title": "LitServe Code Guide",
    "section": "",
    "text": "server = ls.LitServer(\n    api=api,\n    accelerator=\"auto\",           # \"auto\", \"cpu\", \"gpu\", \"mps\"\n    devices=\"auto\",               # Device selection\n    max_batch_size=4,            # Maximum batch size\n    batch_timeout=0.1,           # Batch timeout in seconds\n    workers_per_device=1,        # Workers per device\n    timeout=30,                  # Request timeout\n    stream=False,                # Enable streaming\n    spec=None,                   # Custom OpenAPI spec\n)\n\n\n\n# Set device preferences\nexport CUDA_VISIBLE_DEVICES=0,1,2\n\n# Set batch configuration\nexport LITSERVE_MAX_BATCH_SIZE=8\nexport LITSERVE_BATCH_TIMEOUT=0.05\n\n# Set worker configuration\nexport LITSERVE_WORKERS_PER_DEVICE=2\n\n\n\n# config.py\nclass Config:\n    MAX_BATCH_SIZE = 8\n    BATCH_TIMEOUT = 0.1\n    WORKERS_PER_DEVICE = 2\n    ACCELERATOR = \"auto\"\n    TIMEOUT = 60\n\n# Use in your API\nfrom config import Config\n\nserver = ls.LitServer(\n    api, \n    max_batch_size=Config.MAX_BATCH_SIZE,\n    batch_timeout=Config.BATCH_TIMEOUT,\n    workers_per_device=Config.WORKERS_PER_DEVICE\n)"
  },
  {
    "objectID": "posts/litserve-basics/index.html#examples",
    "href": "posts/litserve-basics/index.html#examples",
    "title": "LitServe Code Guide",
    "section": "",
    "text": "import litserve as ls\nimport torch\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport base64\nimport io\n\nclass ImageClassificationAPI(ls.LitAPI):\n    def setup(self, device):\n        # Load pre-trained ResNet model\n        self.model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n        self.model.to(device)\n        self.model.eval()\n        \n        # Image preprocessing\n        self.preprocess = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                               std=[0.229, 0.224, 0.225]),\n        ])\n        \n        # Load ImageNet class labels\n        with open('imagenet_classes.txt') as f:\n            self.classes = [line.strip() for line in f.readlines()]\n    \n    def decode_request(self, request):\n        # Decode base64 image\n        encoded_image = request[\"image\"]\n        image_bytes = base64.b64decode(encoded_image)\n        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n        return self.preprocess(image).unsqueeze(0)\n    \n    def predict(self, x):\n        with torch.no_grad():\n            output = self.model(x)\n            probabilities = torch.nn.functional.softmax(output[0], dim=0)\n        return probabilities\n    \n    def encode_response(self, output):\n        # Get top 5 predictions\n        top5_prob, top5_catid = torch.topk(output, 5)\n        results = []\n        for i in range(top5_prob.size(0)):\n            results.append({\n                \"class\": self.classes[top5_catid[i]],\n                \"probability\": float(top5_prob[i])\n            })\n        return {\"predictions\": results}\n\nif __name__ == \"__main__\":\n    api = ImageClassificationAPI()\n    server = ls.LitServer(api, accelerator=\"auto\", max_batch_size=4)\n    server.run(port=8000)\n\n\n\nimport litserve as ls\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\n\nclass TextEmbeddingAPI(ls.LitAPI):\n    def setup(self, device):\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        self.model.to(device)\n    \n    def decode_request(self, request):\n        texts = request.get(\"texts\", [])\n        if isinstance(texts, str):\n            texts = [texts]\n        return texts\n    \n    def predict(self, texts):\n        embeddings = self.model.encode(texts)\n        return embeddings\n    \n    def encode_response(self, embeddings):\n        return {\n            \"embeddings\": embeddings.tolist(),\n            \"dimension\": embeddings.shape[1] if len(embeddings.shape) &gt; 1 else len(embeddings)\n        }\n\nif __name__ == \"__main__\":\n    api = TextEmbeddingAPI()\n    server = ls.LitServer(api, accelerator=\"auto\", max_batch_size=32)\n    server.run(port=8000)\n\n\n\nimport litserve as ls\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\nimport base64\nimport io\n\nclass ImageCaptioningAPI(ls.LitAPI):\n    def setup(self, device):\n        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n        self.model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n        self.model.to(device)\n    \n    def decode_request(self, request):\n        # Handle both image and optional text input\n        encoded_image = request[\"image\"]\n        text_prompt = request.get(\"text\", \"\")\n        \n        image_bytes = base64.b64decode(encoded_image)\n        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n        \n        return {\"image\": image, \"text\": text_prompt}\n    \n    def predict(self, inputs):\n        processed = self.processor(\n            images=inputs[\"image\"], \n            text=inputs[\"text\"], \n            return_tensors=\"pt\"\n        )\n        \n        with torch.no_grad():\n            outputs = self.model.generate(**processed, max_length=50)\n        \n        caption = self.processor.decode(outputs[0], skip_special_tokens=True)\n        return caption\n    \n    def encode_response(self, caption):\n        return {\"caption\": caption}"
  },
  {
    "objectID": "posts/litserve-basics/index.html#best-practices",
    "href": "posts/litserve-basics/index.html#best-practices",
    "title": "LitServe Code Guide",
    "section": "",
    "text": "class OptimizedAPI(ls.LitAPI):\n    def setup(self, device):\n        # Use torch.jit.script for optimization\n        self.model = torch.jit.script(your_model)\n        \n        # Enable mixed precision if using GPU\n        if device.type == 'cuda':\n            self.scaler = torch.cuda.amp.GradScaler()\n        \n        # Pre-allocate tensors for common shapes\n        self.common_shapes = {}\n    \n    def predict(self, x):\n        # Use autocast for mixed precision\n        if hasattr(self, 'scaler'):\n            with torch.cuda.amp.autocast():\n                return self.model(x)\n        return self.model(x)\n\n\n\nclass RobustAPI(ls.LitAPI):\n    def decode_request(self, request):\n        try:\n            # Validate required fields\n            if \"input\" not in request:\n                raise ls.ValidationError(\"Missing required field: input\")\n            \n            data = request[\"input\"]\n            \n            # Type validation\n            if not isinstance(data, (str, list)):\n                raise ls.ValidationError(\"Input must be string or list\")\n            \n            return data\n        except Exception as e:\n            raise ls.ValidationError(f\"Request parsing failed: {str(e)}\")\n    \n    def predict(self, x):\n        try:\n            result = self.model(x)\n            return result\n        except torch.cuda.OutOfMemoryError:\n            raise ls.ServerError(\"GPU memory exhausted\")\n        except Exception as e:\n            raise ls.ServerError(f\"Prediction failed: {str(e)}\")\n\n\n\nimport logging\nimport time\n\nclass MonitoredAPI(ls.LitAPI):\n    def setup(self, device):\n        self.logger = logging.getLogger(__name__)\n        self.request_count = 0\n        # Your model setup\n    \n    def decode_request(self, request):\n        self.request_count += 1\n        self.logger.info(f\"Processing request #{self.request_count}\")\n        return request[\"data\"]\n    \n    def predict(self, x):\n        start_time = time.time()\n        result = self.model(x)\n        inference_time = time.time() - start_time\n        \n        self.logger.info(f\"Inference completed in {inference_time:.3f}s\")\n        return result\n\n\n\nclass VersionedAPI(ls.LitAPI):\n    def setup(self, device):\n        self.version = \"1.0.0\"\n        self.model_path = f\"models/model_v{self.version}\"\n        # Load versioned model\n    \n    def encode_response(self, output):\n        return {\n            \"result\": output,\n            \"model_version\": self.version,\n            \"timestamp\": time.time()\n        }"
  },
  {
    "objectID": "posts/litserve-basics/index.html#troubleshooting",
    "href": "posts/litserve-basics/index.html#troubleshooting",
    "title": "LitServe Code Guide",
    "section": "",
    "text": "# Solution: Reduce batch size or implement gradient checkpointing\nserver = ls.LitServer(api, max_batch_size=2)  # Reduce batch size\n\n# Or clear cache in your predict method\ndef predict(self, x):\n    torch.cuda.empty_cache()  # Clear unused memory\n    result = self.model(x)\n    return result\n\n\n\n# Enable model optimization\ndef setup(self, device):\n    self.model.eval()  # Set to evaluation mode\n    self.model = torch.jit.script(self.model)  # JIT compilation\n    \n    # Use half precision if supported\n    if device.type == 'cuda':\n        self.model.half()\n\n\n\n# Increase timeout settings\nserver = ls.LitServer(\n    api, \n    timeout=60,  # Increase request timeout\n    batch_timeout=1.0  # Increase batch timeout\n)\n\n\n\n# Check and kill existing processes\nimport subprocess\nsubprocess.run([\"lsof\", \"-ti:8000\", \"|\", \"xargs\", \"kill\", \"-9\"], shell=True)\n\n# Or use a different port\nserver.run(port=8001)\n\n\n\n\n\nUse appropriate batch sizes: Start with small batches and gradually increase\nEnable GPU acceleration: Use accelerator=\"auto\" for automatic GPU detection\nOptimize model loading: Load models once in setup(), not in predict()\nUse mixed precision: Enable autocast for GPU inference\nProfile your code: Use tools like torch.profiler to identify bottlenecks\nCache preprocessed data: Store frequently used transformations\n\n\n\n\n\nTest API locally with various input types\nValidate error handling for malformed requests\nCheck memory usage under load\nVerify GPU utilization (if using GPUs)\nTest with maximum expected batch size\nImplement proper logging and monitoring\nSet up health check endpoints\nConfigure appropriate timeouts\nTest authentication (if implemented)\nVerify response format consistency\n\nThis guide covers the essential aspects of using LitServe for deploying AI models. For the most up-to-date information, always refer to the official LitServe documentation."
  },
  {
    "objectID": "posts/why-pytorch/index.html",
    "href": "posts/why-pytorch/index.html",
    "title": "Why I Choose PyTorch for Deep Learning",
    "section": "",
    "text": "When it comes to deep learning frameworks, the landscape offers several compelling options. TensorFlow, JAX, and PyTorch each have their strengths, but after working extensively with multiple frameworks, PyTorch has become my go-to choice for deep learning projects. Hereâ€™s why this dynamic framework continues to win over researchers and practitioners alike.\n\n\nPyTorchâ€™s defining feature is its dynamic computation graph, also known as â€œdefine-by-run.â€ Unlike static graphs where you must define the entire network architecture upfront, PyTorch builds the computational graph on-the-fly as operations execute. This approach offers unprecedented flexibility for complex architectures and experimental research.\nConsider debugging a recurrent neural network with variable sequence lengths. In PyTorch, you can step through your code line by line, inspect tensors at any point, and modify the network behavior based on runtime conditions. This dynamic nature makes PyTorch feel more like writing regular Python code rather than wrestling with a rigid framework.\n\n\n\nPyTorch embraces Pythonâ€™s design principles, making it intuitive for developers already familiar with the language. The API feels natural and follows Python conventions closely. Operations like tensor manipulation, automatic differentiation, and model definition align with how Python developers expect to write code.\nThe framework integrates seamlessly with the broader Python ecosystem. NumPy arrays convert effortlessly to PyTorch tensors, matplotlib works perfectly for visualization, and standard Python debugging tools function as expected. This integration reduces the learning curve and allows developers to leverage existing Python skills.\n\n\n\nPyTorch originated from the research community and maintains strong connections to academic work. The framework prioritizes flexibility and experimentation over rigid optimization, making it ideal for cutting-edge research where novel architectures and training procedures are constantly emerging.\nMajor research breakthroughs often appear first in PyTorch implementations. The frameworkâ€™s flexibility allows researchers to quickly prototype new ideas without fighting against framework constraints. This research-first approach has created a virtuous cycle where PyTorch continues to attract top researchers, leading to more innovations and better tooling.\n\n\n\nDebugging deep learning models can be notoriously challenging, but PyTorch makes this process more manageable. Since PyTorch code executes imperatively, you can use standard Python debugging tools like pdb, print statements, and IDE debuggers effectively.\nThe framework provides excellent error messages that point to the exact line where issues occur. When tensor shapes donâ€™t match or operations fail, PyTorch gives clear, actionable feedback rather than cryptic error messages buried deep in the frameworkâ€™s internals.\n\n\n\nPyTorch has cultivated a vibrant ecosystem of libraries and tools. PyTorch Lightning simplifies training loops and experiment management. Transformers from Hugging Face provides state-of-the-art pre-trained models. TorchVision, TorchText, and TorchAudio offer domain-specific utilities for computer vision, natural language processing, and audio processing respectively.\nThe community actively contributes tutorials, examples, and extensions. PyTorchâ€™s documentation is comprehensive and includes practical examples alongside API references. The official tutorials cover everything from basic tensor operations to advanced topics like distributed training and model optimization.\n\n\n\nWhile PyTorch initially focused on research flexibility, recent versions have significantly improved production capabilities. TorchScript allows converting dynamic PyTorch models to static representations for deployment. TorchServe provides model serving infrastructure, and PyTorch Mobile enables deployment on mobile devices.\nThe framework delivers competitive performance for training and inference. PyTorchâ€™s JIT compiler optimizes computation graphs, and the framework efficiently utilizes GPU resources. For most applications, PyTorchâ€™s performance matches or exceeds alternatives while maintaining superior flexibility.\n\n\n\nPyTorchâ€™s automatic differentiation system, Autograd, elegantly handles gradient computation. The system tracks operations on tensors and builds a computational graph automatically. Computing gradients requires just a single .backward() call, and the system handles complex scenarios like gradient accumulation and higher-order derivatives naturally.\nThe differentiation system integrates smoothly with control flow, making it easy to implement complex architectures with conditional execution, loops, and dynamic behavior. This capability proves essential for advanced architectures like attention mechanisms and recursive networks.\n\n\n\nWhile TensorFlow dominated early industry adoption, PyTorch has gained significant ground in production environments. Major companies like Facebook (Meta), Tesla, and OpenAI use PyTorch for critical applications. The frameworkâ€™s improved deployment tools and performance optimizations have made it increasingly viable for production use.\nMany companies now choose PyTorch for both research and production, eliminating the need to translate models between frameworks. This unified approach reduces complexity and accelerates the path from research to deployment.\n\n\n\nPyTorchâ€™s design principles position it well for future developments in deep learning. The frameworkâ€™s flexibility accommodates new paradigms like few-shot learning, meta-learning, and neural architecture search without requiring major architectural changes.\nThe PyTorch team actively develops new features while maintaining backward compatibility. Regular releases introduce performance improvements, new operators, and enhanced tooling without breaking existing code.\n\n\n\nChoosing PyTorch means prioritizing flexibility, ease of use, and alignment with modern Python development practices. The framework excels for research, education, and increasingly for production applications. Its dynamic nature, excellent debugging capabilities, and strong ecosystem make it an compelling choice for deep learning projects.\nWhile other frameworks have their merits, PyTorchâ€™s combination of research-friendly design, production readiness, and vibrant community creates a compelling package for deep learning practitioners. The framework continues evolving rapidly while maintaining its core philosophy of putting developers first.\nFor anyone starting a new deep learning project or considering a framework switch, PyTorch offers a modern, flexible foundation that grows with your needs and supports both experimentation and deployment."
  },
  {
    "objectID": "posts/why-pytorch/index.html#the-power-of-dynamic-computation-graphs",
    "href": "posts/why-pytorch/index.html#the-power-of-dynamic-computation-graphs",
    "title": "Why I Choose PyTorch for Deep Learning",
    "section": "",
    "text": "PyTorchâ€™s defining feature is its dynamic computation graph, also known as â€œdefine-by-run.â€ Unlike static graphs where you must define the entire network architecture upfront, PyTorch builds the computational graph on-the-fly as operations execute. This approach offers unprecedented flexibility for complex architectures and experimental research.\nConsider debugging a recurrent neural network with variable sequence lengths. In PyTorch, you can step through your code line by line, inspect tensors at any point, and modify the network behavior based on runtime conditions. This dynamic nature makes PyTorch feel more like writing regular Python code rather than wrestling with a rigid framework."
  },
  {
    "objectID": "posts/why-pytorch/index.html#pythonic-design-philosophy",
    "href": "posts/why-pytorch/index.html#pythonic-design-philosophy",
    "title": "Why I Choose PyTorch for Deep Learning",
    "section": "",
    "text": "PyTorch embraces Pythonâ€™s design principles, making it intuitive for developers already familiar with the language. The API feels natural and follows Python conventions closely. Operations like tensor manipulation, automatic differentiation, and model definition align with how Python developers expect to write code.\nThe framework integrates seamlessly with the broader Python ecosystem. NumPy arrays convert effortlessly to PyTorch tensors, matplotlib works perfectly for visualization, and standard Python debugging tools function as expected. This integration reduces the learning curve and allows developers to leverage existing Python skills."
  },
  {
    "objectID": "posts/why-pytorch/index.html#research-first-mentality",
    "href": "posts/why-pytorch/index.html#research-first-mentality",
    "title": "Why I Choose PyTorch for Deep Learning",
    "section": "",
    "text": "PyTorch originated from the research community and maintains strong connections to academic work. The framework prioritizes flexibility and experimentation over rigid optimization, making it ideal for cutting-edge research where novel architectures and training procedures are constantly emerging.\nMajor research breakthroughs often appear first in PyTorch implementations. The frameworkâ€™s flexibility allows researchers to quickly prototype new ideas without fighting against framework constraints. This research-first approach has created a virtuous cycle where PyTorch continues to attract top researchers, leading to more innovations and better tooling."
  },
  {
    "objectID": "posts/why-pytorch/index.html#exceptional-debugging-experience",
    "href": "posts/why-pytorch/index.html#exceptional-debugging-experience",
    "title": "Why I Choose PyTorch for Deep Learning",
    "section": "",
    "text": "Debugging deep learning models can be notoriously challenging, but PyTorch makes this process more manageable. Since PyTorch code executes imperatively, you can use standard Python debugging tools like pdb, print statements, and IDE debuggers effectively.\nThe framework provides excellent error messages that point to the exact line where issues occur. When tensor shapes donâ€™t match or operations fail, PyTorch gives clear, actionable feedback rather than cryptic error messages buried deep in the frameworkâ€™s internals."
  },
  {
    "objectID": "posts/why-pytorch/index.html#mature-ecosystem-and-community",
    "href": "posts/why-pytorch/index.html#mature-ecosystem-and-community",
    "title": "Why I Choose PyTorch for Deep Learning",
    "section": "",
    "text": "PyTorch has cultivated a vibrant ecosystem of libraries and tools. PyTorch Lightning simplifies training loops and experiment management. Transformers from Hugging Face provides state-of-the-art pre-trained models. TorchVision, TorchText, and TorchAudio offer domain-specific utilities for computer vision, natural language processing, and audio processing respectively.\nThe community actively contributes tutorials, examples, and extensions. PyTorchâ€™s documentation is comprehensive and includes practical examples alongside API references. The official tutorials cover everything from basic tensor operations to advanced topics like distributed training and model optimization."
  },
  {
    "objectID": "posts/why-pytorch/index.html#performance-and-production-readiness",
    "href": "posts/why-pytorch/index.html#performance-and-production-readiness",
    "title": "Why I Choose PyTorch for Deep Learning",
    "section": "",
    "text": "While PyTorch initially focused on research flexibility, recent versions have significantly improved production capabilities. TorchScript allows converting dynamic PyTorch models to static representations for deployment. TorchServe provides model serving infrastructure, and PyTorch Mobile enables deployment on mobile devices.\nThe framework delivers competitive performance for training and inference. PyTorchâ€™s JIT compiler optimizes computation graphs, and the framework efficiently utilizes GPU resources. For most applications, PyTorchâ€™s performance matches or exceeds alternatives while maintaining superior flexibility."
  },
  {
    "objectID": "posts/why-pytorch/index.html#automatic-differentiation-done-right",
    "href": "posts/why-pytorch/index.html#automatic-differentiation-done-right",
    "title": "Why I Choose PyTorch for Deep Learning",
    "section": "",
    "text": "PyTorchâ€™s automatic differentiation system, Autograd, elegantly handles gradient computation. The system tracks operations on tensors and builds a computational graph automatically. Computing gradients requires just a single .backward() call, and the system handles complex scenarios like gradient accumulation and higher-order derivatives naturally.\nThe differentiation system integrates smoothly with control flow, making it easy to implement complex architectures with conditional execution, loops, and dynamic behavior. This capability proves essential for advanced architectures like attention mechanisms and recursive networks."
  },
  {
    "objectID": "posts/why-pytorch/index.html#growing-industry-adoption",
    "href": "posts/why-pytorch/index.html#growing-industry-adoption",
    "title": "Why I Choose PyTorch for Deep Learning",
    "section": "",
    "text": "While TensorFlow dominated early industry adoption, PyTorch has gained significant ground in production environments. Major companies like Facebook (Meta), Tesla, and OpenAI use PyTorch for critical applications. The frameworkâ€™s improved deployment tools and performance optimizations have made it increasingly viable for production use.\nMany companies now choose PyTorch for both research and production, eliminating the need to translate models between frameworks. This unified approach reduces complexity and accelerates the path from research to deployment."
  },
  {
    "objectID": "posts/why-pytorch/index.html#future-proof-architecture",
    "href": "posts/why-pytorch/index.html#future-proof-architecture",
    "title": "Why I Choose PyTorch for Deep Learning",
    "section": "",
    "text": "PyTorchâ€™s design principles position it well for future developments in deep learning. The frameworkâ€™s flexibility accommodates new paradigms like few-shot learning, meta-learning, and neural architecture search without requiring major architectural changes.\nThe PyTorch team actively develops new features while maintaining backward compatibility. Regular releases introduce performance improvements, new operators, and enhanced tooling without breaking existing code."
  },
  {
    "objectID": "posts/why-pytorch/index.html#making-the-choice",
    "href": "posts/why-pytorch/index.html#making-the-choice",
    "title": "Why I Choose PyTorch for Deep Learning",
    "section": "",
    "text": "Choosing PyTorch means prioritizing flexibility, ease of use, and alignment with modern Python development practices. The framework excels for research, education, and increasingly for production applications. Its dynamic nature, excellent debugging capabilities, and strong ecosystem make it an compelling choice for deep learning projects.\nWhile other frameworks have their merits, PyTorchâ€™s combination of research-friendly design, production readiness, and vibrant community creates a compelling package for deep learning practitioners. The framework continues evolving rapidly while maintaining its core philosophy of putting developers first.\nFor anyone starting a new deep learning project or considering a framework switch, PyTorch offers a modern, flexible foundation that grows with your needs and supports both experimentation and deployment."
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html",
    "href": "posts/data-visualization-tutorial/index.html",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "This guide compares three popular Python data visualization libraries: Matplotlib, Seaborn, and Altair (Vega-Altair). Each library has its own strengths, weaknesses, and ideal use cases. This comparison will help you choose the right tool for your specific visualization needs.\n\n\n\n\n\n\n\n\n\n\n\nFeature\nMatplotlib\nSeaborn\nAltair\n\n\n\n\nRelease Year\n2003\n2013\n2016\n\n\nFoundation\nStandalone\nBuilt on Matplotlib\nBased on Vega-Lite\n\n\nPhilosophy\nImperative\nStatistical\nDeclarative\n\n\nAbstraction Level\nLow\nMedium\nHigh\n\n\nLearning Curve\nSteep\nModerate\nGentle\n\n\nCode Verbosity\nHigh\nMedium\nLow\n\n\nCustomization\nExtensive\nGood\nLimited\n\n\nStatistical Integration\nManual\nBuilt-in\nGood\n\n\nInteractive Features\nLimited\nLimited\nExcellent\n\n\nPerformance with Large Data\nGood\nModerate\nLimited\n\n\nCommunity & Resources\nExtensive\nGood\nGrowing\n\n\n\n\n\n\nMatplotlib is the foundational plotting library in Pythonâ€™s data visualization ecosystem.\n\n\n\nFine-grained control: Almost every aspect of a visualization can be customized\nVersatility: Can create virtually any type of static plot\nMaturity: Extensive documentation and community support\nEcosystem integration: Many libraries integrate with or build upon Matplotlib\nPerformance: Handles large datasets well\n\n\n\n\n\nVerbose syntax: Requires many lines of code for complex visualizations\nSteep learning curve: Many functions and parameters to learn\nDefault aesthetics: Basic default styling (though this has improved)\nLimited interactivity: Primarily designed for static plots\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\n# Create figure and axis\nfig, ax = plt.subplots(figsize=(8, 4))\n\n# Plot data\nax.plot(x, y, label='Sine Wave')\n\n# Add grid, legend, title and labels\nax.grid(True)\nax.set_xlabel('X-axis')\nax.set_ylabel('Y-axis')\nax.set_title('Simple Sine Wave Plot')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nYou need complete control over every aspect of your visualization\nYouâ€™re creating complex, publication-quality figures\nYouâ€™re working with specialized plot types not available in higher-level libraries\nYou need to integrate with many other Python libraries\nYouâ€™re working with large datasets\n\n\n\n\n\nSeaborn is a statistical visualization library built on top of Matplotlib.\n\n\n\nAesthetic defaults: Beautiful out-of-the-box styling\nStatistical integration: Built-in support for statistical visualizations\nDataset awareness: Works well with pandas DataFrames\nSimplicity: Fewer lines of code than Matplotlib for common plots\nHigh-level functions: Specialized plots like lmplot, catplot, etc.\n\n\n\n\n\nLimited customization: Some advanced customizations require falling back to Matplotlib\nPerformance: Can be slower with very large datasets\nRestricted scope: Focused on statistical visualization, not general-purpose plotting\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Create sample data\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.2, size=len(x))\ndata = pd.DataFrame({'x': x, 'y': y})\n\n# Set the aesthetic style\nsns.set_theme(style=\"whitegrid\")\n\n# Create the plot\nplt.figure(figsize=(8, 4))\nsns.lineplot(data=data, x='x', y='y', label='Noisy Sine Wave')\nsns.regplot(data=data, x='x', y='y', scatter=False, label='Regression Line')\n\n# Add title and labels\nplt.title('Seaborn Line Plot with Regression')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nYou want attractive visualizations with minimal code\nYouâ€™re performing statistical analysis\nYouâ€™re working with pandas DataFrames\nYouâ€™re creating common statistical plots (distributions, relationships, categorical plots)\nYou want the power of Matplotlib with a simpler interface\n\n\n\n\n\nAltair is a declarative statistical visualization library based on Vega-Lite.\n\n\n\nDeclarative approach: Focus on what to visualize, not how to draw it\nConcise syntax: Very readable, clear code\nLayered grammar of graphics: Intuitive composition of plots\nInteractive visualizations: Built-in support for interactive features\nJSON output: Visualizations can be saved as JSON specifications\n\n\n\n\n\nPerformance limitations: Not ideal for very large datasets (&gt;5000 points)\nLimited customization: Less fine-grained control than Matplotlib\nLearning curve: Different paradigm from traditional plotting libraries\nBrowser dependency: Uses JavaScript rendering for advanced features\n\n\n\n\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\n# Create sample data\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.2, size=len(x))\ndata = pd.DataFrame({'x': x, 'y': y})\n\n# Create a simple scatter plot with interactive tooltips\nchart = alt.Chart(data).mark_circle().encode(\n    x='x',\n    y='y',\n    tooltip=['x', 'y']\n).properties(\n    width=600,\n    height=300,\n    title='Interactive Altair Scatter Plot'\n).interactive()\n\n# Add a regression line\nregression = alt.Chart(data).transform_regression(\n    'x', 'y'\n).mark_line(color='red').encode(\n    x='x',\n    y='y'\n)\n\n# Combine the plots\nfinal_chart = chart + regression\n\n# Display the chart\nfinal_chart\n\n\n\n\n\n\n\n\n\n\n\nYou want interactive visualizations\nYou prefer a declarative approach to visualization\nYouâ€™re working with small to medium-sized datasets\nYou want to publish visualizations on the web\nYou appreciate a consistent grammar of graphics\n\n\n\n\n\n\n\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.random.randn(100)\ny = np.random.randn(100)\n\nplt.figure(figsize=(8, 6))\nplt.scatter(x, y, alpha=0.7)\nplt.title('Matplotlib Scatter Plot')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndata = pd.DataFrame({\n    'x': np.random.randn(100),\n    'y': np.random.randn(100)\n})\n\nsns.set_theme(style=\"whitegrid\")\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=data, x='x', y='y', alpha=0.7)\nplt.title('Seaborn Scatter Plot')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame({\n    'x': np.random.randn(100),\n    'y': np.random.randn(100)\n})\n\nalt.Chart(data).mark_circle(opacity=0.7).encode(\n    x='x',\n    y='y'\n).properties(\n    width=500,\n    height=400,\n    title='Altair Scatter Plot'\n)\n\n\n\n\n\n\n\n\n\n\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.randn(1000)\n\nplt.figure(figsize=(8, 6))\nplt.hist(data, bins=30, alpha=0.7, edgecolor='black')\nplt.title('Matplotlib Histogram')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.randn(1000)\n\nsns.set_theme(style=\"whitegrid\")\nplt.figure(figsize=(8, 6))\nsns.histplot(data=data, bins=30, kde=True)\nplt.title('Seaborn Histogram with KDE')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame({'value': np.random.randn(1000)})\n\nalt.Chart(data).mark_bar().encode(\n    alt.X('value', bin=alt.Bin(maxbins=30)),\n    y='count()'\n).properties(\n    width=500,\n    height=400,\n    title='Altair Histogram'\n)\n\n\n\n\n\n\n\n\n\n\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny1 = np.sin(x)\ny2 = np.cos(x)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y1, label='Sine')\nplt.plot(x, y2, label='Cosine')\nplt.title('Matplotlib Line Plot')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nx = np.linspace(0, 10, 100)\ndata = pd.DataFrame({\n    'x': np.concatenate([x, x]),\n    'y': np.concatenate([np.sin(x), np.cos(x)]),\n    'function': ['Sine']*100 + ['Cosine']*100\n})\n\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=data, x='x', y='y', hue='function')\nplt.title('Seaborn Line Plot')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ndata = pd.DataFrame({\n    'x': np.concatenate([x, x]),\n    'y': np.concatenate([np.sin(x), np.cos(x)]),\n    'function': ['Sine']*100 + ['Cosine']*100\n})\n\nalt.Chart(data).mark_line().encode(\n    x='x',\n    y='y',\n    color='function'\n).properties(\n    width=600,\n    height=400,\n    title='Altair Line Plot'\n)\n\n\n\n\n\n\n\n\n\n\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.rand(10, 12)\n\nplt.figure(figsize=(10, 8))\nplt.imshow(data, cmap='viridis')\nplt.colorbar(label='Value')\nplt.title('Matplotlib Heatmap')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.rand(10, 12)\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(data, annot=True, cmap='viridis', fmt='.2f')\nplt.title('Seaborn Heatmap')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\n# Create sample data\ndata = np.random.rand(10, 12)\ndf = pd.DataFrame(data)\n\n# Reshape for Altair\ndf_long = df.reset_index().melt(id_vars='index')\ndf_long.columns = ['y', 'x', 'value']\n\nalt.Chart(df_long).mark_rect().encode(\n    x='x:O',\n    y='y:O',\n    color='value:Q'\n).properties(\n    width=500,\n    height=400,\n    title='Altair Heatmap'\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou need complete control over every detail of your visualization\nYouâ€™re creating complex, custom plots\nYour visualizations will be included in scientific publications\nYouâ€™re working with very large datasets\nYou need to create animations or specialized chart types\n\n\n\n\n\nYou want attractive plots with minimal code\nYouâ€™re performing statistical analysis\nYou want to create common statistical plots quickly\nYou need to visualize relationships between variables\nYou want good-looking defaults but still need some customization\n\n\n\n\n\nYou want interactive visualizations\nYou prefer a declarative approach to visualization\nYou want concise, readable code\nYouâ€™re creating dashboards or web-based visualizations\nYouâ€™re working with small to medium-sized datasets\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\n# Create sample data\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'x': np.random.normal(0, 1, 100),\n    'y': np.random.normal(0, 1, 100),\n    'category': np.random.choice(['A', 'B', 'C'], 100)\n})\n\n# Create a figure with Matplotlib\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Use Seaborn for the main plot\nsns.scatterplot(data=data, x='x', y='y', hue='category', ax=ax)\n\n# Add Matplotlib customizations\nax.set_title('Combining Matplotlib and Seaborn', fontsize=16)\nax.grid(True, linestyle='--', alpha=0.7)\nax.set_xlabel('X Variable', fontsize=12)\nax.set_ylabel('Y Variable', fontsize=12)\n\n# Add annotations using Matplotlib\nax.annotate('Interesting Point', xy=(-1, 1), xytext=(-2, 1.5),\n            arrowprops=dict(facecolor='black', shrink=0.05))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\n# Create sample data with pandas\nnp.random.seed(42)\ndf = pd.DataFrame({\n    'date': pd.date_range('2023-01-01', periods=100),\n    'value': np.cumsum(np.random.randn(100)),\n    'category': np.random.choice(['Group A', 'Group B'], 100)\n})\n\n# Use pandas to prepare the data\ndf['month'] = df['date'].dt.month\nmonthly_avg = df.groupby(['month', 'category'])['value'].mean().reset_index()\n\n# Create the Altair visualization\nchart = alt.Chart(monthly_avg).mark_line(point=True).encode(\n    x='month:O',\n    y='value:Q',\n    color='category:N',\n    tooltip=['month', 'value', 'category']\n).properties(\n    width=600,\n    height=400,\n    title='Monthly Averages by Category'\n).interactive()\n\nchart\n\n\n\n\n\n\n\n\n\n\n\nFor libraries like Matplotlib, Seaborn, and Altair, performance can vary widely depending on the size of your dataset and the complexity of your visualization. Hereâ€™s a general overview:\n\n\n\nAll three libraries perform well\nAltair might have slightly more overhead due to its JSON specification generation\n\n\n\n\n\nMatplotlib and Seaborn continue to perform well\nAltair starts to slow down but remains usable\n\n\n\n\n\nMatplotlib performs best for large static visualizations\nSeaborn becomes slower as it adds statistical computations\nAltair significantly slows down and may require data aggregation\n\n\n\n\n\nMatplotlib: Use plot() instead of scatter() for line plots, or try hexbin() for density plots\nSeaborn: Use sample() or aggregation methods before plotting\nAltair: Use transform_sample() or pre-aggregate your data\n\n\n\n\n\nThe Python visualization ecosystem offers tools for every need, from low-level control to high-level abstraction:\n\nMatplotlib provides ultimate flexibility and control but requires more code and knowledge\nSeaborn offers a perfect middle ground with statistical integration and clean defaults\nAltair delivers a concise, declarative approach with built-in interactivity\n\nRather than picking just one library, consider becoming familiar with all three and selecting the right tool for each visualization task. Many data scientists use a combination of these libraries, leveraging the strengths of each one as needed.\nFor those just starting, Seaborn provides a gentle entry point with attractive results for common visualization needs. As your skills advance, you can incorporate Matplotlib for customization and Altair for interactive visualizations."
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#quick-reference-comparison",
    "href": "posts/data-visualization-tutorial/index.html#quick-reference-comparison",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "Feature\nMatplotlib\nSeaborn\nAltair\n\n\n\n\nRelease Year\n2003\n2013\n2016\n\n\nFoundation\nStandalone\nBuilt on Matplotlib\nBased on Vega-Lite\n\n\nPhilosophy\nImperative\nStatistical\nDeclarative\n\n\nAbstraction Level\nLow\nMedium\nHigh\n\n\nLearning Curve\nSteep\nModerate\nGentle\n\n\nCode Verbosity\nHigh\nMedium\nLow\n\n\nCustomization\nExtensive\nGood\nLimited\n\n\nStatistical Integration\nManual\nBuilt-in\nGood\n\n\nInteractive Features\nLimited\nLimited\nExcellent\n\n\nPerformance with Large Data\nGood\nModerate\nLimited\n\n\nCommunity & Resources\nExtensive\nGood\nGrowing"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#matplotlib",
    "href": "posts/data-visualization-tutorial/index.html#matplotlib",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "Matplotlib is the foundational plotting library in Pythonâ€™s data visualization ecosystem.\n\n\n\nFine-grained control: Almost every aspect of a visualization can be customized\nVersatility: Can create virtually any type of static plot\nMaturity: Extensive documentation and community support\nEcosystem integration: Many libraries integrate with or build upon Matplotlib\nPerformance: Handles large datasets well\n\n\n\n\n\nVerbose syntax: Requires many lines of code for complex visualizations\nSteep learning curve: Many functions and parameters to learn\nDefault aesthetics: Basic default styling (though this has improved)\nLimited interactivity: Primarily designed for static plots\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\n# Create figure and axis\nfig, ax = plt.subplots(figsize=(8, 4))\n\n# Plot data\nax.plot(x, y, label='Sine Wave')\n\n# Add grid, legend, title and labels\nax.grid(True)\nax.set_xlabel('X-axis')\nax.set_ylabel('Y-axis')\nax.set_title('Simple Sine Wave Plot')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nYou need complete control over every aspect of your visualization\nYouâ€™re creating complex, publication-quality figures\nYouâ€™re working with specialized plot types not available in higher-level libraries\nYou need to integrate with many other Python libraries\nYouâ€™re working with large datasets"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#seaborn",
    "href": "posts/data-visualization-tutorial/index.html#seaborn",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "Seaborn is a statistical visualization library built on top of Matplotlib.\n\n\n\nAesthetic defaults: Beautiful out-of-the-box styling\nStatistical integration: Built-in support for statistical visualizations\nDataset awareness: Works well with pandas DataFrames\nSimplicity: Fewer lines of code than Matplotlib for common plots\nHigh-level functions: Specialized plots like lmplot, catplot, etc.\n\n\n\n\n\nLimited customization: Some advanced customizations require falling back to Matplotlib\nPerformance: Can be slower with very large datasets\nRestricted scope: Focused on statistical visualization, not general-purpose plotting\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Create sample data\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.2, size=len(x))\ndata = pd.DataFrame({'x': x, 'y': y})\n\n# Set the aesthetic style\nsns.set_theme(style=\"whitegrid\")\n\n# Create the plot\nplt.figure(figsize=(8, 4))\nsns.lineplot(data=data, x='x', y='y', label='Noisy Sine Wave')\nsns.regplot(data=data, x='x', y='y', scatter=False, label='Regression Line')\n\n# Add title and labels\nplt.title('Seaborn Line Plot with Regression')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nYou want attractive visualizations with minimal code\nYouâ€™re performing statistical analysis\nYouâ€™re working with pandas DataFrames\nYouâ€™re creating common statistical plots (distributions, relationships, categorical plots)\nYou want the power of Matplotlib with a simpler interface"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#altair-vega-altair",
    "href": "posts/data-visualization-tutorial/index.html#altair-vega-altair",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "Altair is a declarative statistical visualization library based on Vega-Lite.\n\n\n\nDeclarative approach: Focus on what to visualize, not how to draw it\nConcise syntax: Very readable, clear code\nLayered grammar of graphics: Intuitive composition of plots\nInteractive visualizations: Built-in support for interactive features\nJSON output: Visualizations can be saved as JSON specifications\n\n\n\n\n\nPerformance limitations: Not ideal for very large datasets (&gt;5000 points)\nLimited customization: Less fine-grained control than Matplotlib\nLearning curve: Different paradigm from traditional plotting libraries\nBrowser dependency: Uses JavaScript rendering for advanced features\n\n\n\n\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\n# Create sample data\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.2, size=len(x))\ndata = pd.DataFrame({'x': x, 'y': y})\n\n# Create a simple scatter plot with interactive tooltips\nchart = alt.Chart(data).mark_circle().encode(\n    x='x',\n    y='y',\n    tooltip=['x', 'y']\n).properties(\n    width=600,\n    height=300,\n    title='Interactive Altair Scatter Plot'\n).interactive()\n\n# Add a regression line\nregression = alt.Chart(data).transform_regression(\n    'x', 'y'\n).mark_line(color='red').encode(\n    x='x',\n    y='y'\n)\n\n# Combine the plots\nfinal_chart = chart + regression\n\n# Display the chart\nfinal_chart\n\n\n\n\n\n\n\n\n\n\n\nYou want interactive visualizations\nYou prefer a declarative approach to visualization\nYouâ€™re working with small to medium-sized datasets\nYou want to publish visualizations on the web\nYou appreciate a consistent grammar of graphics"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#common-visualization-types-comparison",
    "href": "posts/data-visualization-tutorial/index.html#common-visualization-types-comparison",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "Matplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.random.randn(100)\ny = np.random.randn(100)\n\nplt.figure(figsize=(8, 6))\nplt.scatter(x, y, alpha=0.7)\nplt.title('Matplotlib Scatter Plot')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndata = pd.DataFrame({\n    'x': np.random.randn(100),\n    'y': np.random.randn(100)\n})\n\nsns.set_theme(style=\"whitegrid\")\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=data, x='x', y='y', alpha=0.7)\nplt.title('Seaborn Scatter Plot')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame({\n    'x': np.random.randn(100),\n    'y': np.random.randn(100)\n})\n\nalt.Chart(data).mark_circle(opacity=0.7).encode(\n    x='x',\n    y='y'\n).properties(\n    width=500,\n    height=400,\n    title='Altair Scatter Plot'\n)\n\n\n\n\n\n\n\n\n\n\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.randn(1000)\n\nplt.figure(figsize=(8, 6))\nplt.hist(data, bins=30, alpha=0.7, edgecolor='black')\nplt.title('Matplotlib Histogram')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.randn(1000)\n\nsns.set_theme(style=\"whitegrid\")\nplt.figure(figsize=(8, 6))\nsns.histplot(data=data, bins=30, kde=True)\nplt.title('Seaborn Histogram with KDE')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame({'value': np.random.randn(1000)})\n\nalt.Chart(data).mark_bar().encode(\n    alt.X('value', bin=alt.Bin(maxbins=30)),\n    y='count()'\n).properties(\n    width=500,\n    height=400,\n    title='Altair Histogram'\n)\n\n\n\n\n\n\n\n\n\n\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny1 = np.sin(x)\ny2 = np.cos(x)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y1, label='Sine')\nplt.plot(x, y2, label='Cosine')\nplt.title('Matplotlib Line Plot')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nx = np.linspace(0, 10, 100)\ndata = pd.DataFrame({\n    'x': np.concatenate([x, x]),\n    'y': np.concatenate([np.sin(x), np.cos(x)]),\n    'function': ['Sine']*100 + ['Cosine']*100\n})\n\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=data, x='x', y='y', hue='function')\nplt.title('Seaborn Line Plot')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ndata = pd.DataFrame({\n    'x': np.concatenate([x, x]),\n    'y': np.concatenate([np.sin(x), np.cos(x)]),\n    'function': ['Sine']*100 + ['Cosine']*100\n})\n\nalt.Chart(data).mark_line().encode(\n    x='x',\n    y='y',\n    color='function'\n).properties(\n    width=600,\n    height=400,\n    title='Altair Line Plot'\n)\n\n\n\n\n\n\n\n\n\n\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.rand(10, 12)\n\nplt.figure(figsize=(10, 8))\nplt.imshow(data, cmap='viridis')\nplt.colorbar(label='Value')\nplt.title('Matplotlib Heatmap')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.rand(10, 12)\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(data, annot=True, cmap='viridis', fmt='.2f')\nplt.title('Seaborn Heatmap')\nplt.show()\n\n\n\n\n\n\n\n\nAltair:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\n# Create sample data\ndata = np.random.rand(10, 12)\ndf = pd.DataFrame(data)\n\n# Reshape for Altair\ndf_long = df.reset_index().melt(id_vars='index')\ndf_long.columns = ['y', 'x', 'value']\n\nalt.Chart(df_long).mark_rect().encode(\n    x='x:O',\n    y='y:O',\n    color='value:Q'\n).properties(\n    width=500,\n    height=400,\n    title='Altair Heatmap'\n)"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#decision-framework-for-choosing-a-library",
    "href": "posts/data-visualization-tutorial/index.html#decision-framework-for-choosing-a-library",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "You need complete control over every detail of your visualization\nYouâ€™re creating complex, custom plots\nYour visualizations will be included in scientific publications\nYouâ€™re working with very large datasets\nYou need to create animations or specialized chart types\n\n\n\n\n\nYou want attractive plots with minimal code\nYouâ€™re performing statistical analysis\nYou want to create common statistical plots quickly\nYou need to visualize relationships between variables\nYou want good-looking defaults but still need some customization\n\n\n\n\n\nYou want interactive visualizations\nYou prefer a declarative approach to visualization\nYou want concise, readable code\nYouâ€™re creating dashboards or web-based visualizations\nYouâ€™re working with small to medium-sized datasets"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#integration-examples",
    "href": "posts/data-visualization-tutorial/index.html#integration-examples",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\n# Create sample data\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'x': np.random.normal(0, 1, 100),\n    'y': np.random.normal(0, 1, 100),\n    'category': np.random.choice(['A', 'B', 'C'], 100)\n})\n\n# Create a figure with Matplotlib\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Use Seaborn for the main plot\nsns.scatterplot(data=data, x='x', y='y', hue='category', ax=ax)\n\n# Add Matplotlib customizations\nax.set_title('Combining Matplotlib and Seaborn', fontsize=16)\nax.grid(True, linestyle='--', alpha=0.7)\nax.set_xlabel('X Variable', fontsize=12)\nax.set_ylabel('Y Variable', fontsize=12)\n\n# Add annotations using Matplotlib\nax.annotate('Interesting Point', xy=(-1, 1), xytext=(-2, 1.5),\n            arrowprops=dict(facecolor='black', shrink=0.05))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\n# Create sample data with pandas\nnp.random.seed(42)\ndf = pd.DataFrame({\n    'date': pd.date_range('2023-01-01', periods=100),\n    'value': np.cumsum(np.random.randn(100)),\n    'category': np.random.choice(['Group A', 'Group B'], 100)\n})\n\n# Use pandas to prepare the data\ndf['month'] = df['date'].dt.month\nmonthly_avg = df.groupby(['month', 'category'])['value'].mean().reset_index()\n\n# Create the Altair visualization\nchart = alt.Chart(monthly_avg).mark_line(point=True).encode(\n    x='month:O',\n    y='value:Q',\n    color='category:N',\n    tooltip=['month', 'value', 'category']\n).properties(\n    width=600,\n    height=400,\n    title='Monthly Averages by Category'\n).interactive()\n\nchart"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#performance-comparison",
    "href": "posts/data-visualization-tutorial/index.html#performance-comparison",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "For libraries like Matplotlib, Seaborn, and Altair, performance can vary widely depending on the size of your dataset and the complexity of your visualization. Hereâ€™s a general overview:\n\n\n\nAll three libraries perform well\nAltair might have slightly more overhead due to its JSON specification generation\n\n\n\n\n\nMatplotlib and Seaborn continue to perform well\nAltair starts to slow down but remains usable\n\n\n\n\n\nMatplotlib performs best for large static visualizations\nSeaborn becomes slower as it adds statistical computations\nAltair significantly slows down and may require data aggregation\n\n\n\n\n\nMatplotlib: Use plot() instead of scatter() for line plots, or try hexbin() for density plots\nSeaborn: Use sample() or aggregation methods before plotting\nAltair: Use transform_sample() or pre-aggregate your data"
  },
  {
    "objectID": "posts/data-visualization-tutorial/index.html#conclusion",
    "href": "posts/data-visualization-tutorial/index.html#conclusion",
    "title": "Python Data Visualization: Matplotlib vs Seaborn vs Altair",
    "section": "",
    "text": "The Python visualization ecosystem offers tools for every need, from low-level control to high-level abstraction:\n\nMatplotlib provides ultimate flexibility and control but requires more code and knowledge\nSeaborn offers a perfect middle ground with statistical integration and clean defaults\nAltair delivers a concise, declarative approach with built-in interactivity\n\nRather than picking just one library, consider becoming familiar with all three and selecting the right tool for each visualization task. Many data scientists use a combination of these libraries, leveraging the strengths of each one as needed.\nFor those just starting, Seaborn provides a gentle entry point with attractive results for common visualization needs. As your skills advance, you can incorporate Matplotlib for customization and Altair for interactive visualizations."
  },
  {
    "objectID": "posts/python-pi-code/index.html",
    "href": "posts/python-pi-code/index.html",
    "title": "Python 3.14: Key Improvements and New Features",
    "section": "",
    "text": "Python 3.14 introduces several significant improvements focused on performance, developer experience, and language capabilities. This guide covers the most important changes that will impact your code and development workflow.\n\n\n\n\nPython 3.14 continues the experimental free-threading support introduced in 3.13, with significant improvements:\n# Enable free-threading with --disable-gil flag\n# Better performance for CPU-bound multi-threaded applications\nimport threading\nimport time\n\ndef cpu_intensive_task(n):\n    total = 0\n    for i in range(n):\n        total += i ** 2\n    return total\n\n# Now benefits more from true parallelism\nthreads = []\nfor i in range(4):\n    t = threading.Thread(target=cpu_intensive_task, args=(1000000,))\n    threads.append(t)\n    t.start()\n\nfor t in threads:\n    t.join()\n\n\n\nEnhanced Just-In-Time compilation for better runtime performance:\n# Functions with hot loops see significant speedups\ndef fibonacci(n):\n    if n &lt;= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n\n# JIT compiler optimizes recursive calls more effectively\nresult = fibonacci(35)  # Noticeably faster than previous versions\n\n\n\n\n\n\nEnhanced support for generic types and better inference:\nfrom typing import Generic, TypeVar\n\nT = TypeVar('T')\n\nclass Stack(Generic[T]):\n    def __init__(self) -&gt; None:\n        self._items: list[T] = []\n    \n    def push(self, item: T) -&gt; None:\n        self._items.append(item)\n    \n    def pop(self) -&gt; T:\n        if not self._items:\n            raise IndexError(\"Stack is empty\")\n        return self._items.pop()\n\n# Better type inference\nstack = Stack[int]()\nstack.push(42)\nvalue = stack.pop()  # Type checker knows this is int\n\n\n\nImproved pattern matching with new syntax options:\ndef process_data(data):\n    match data:\n        case {\"type\": \"user\", \"id\": int(user_id), \"active\": True}:\n            return f\"Active user: {user_id}\"\n        case {\"type\": \"user\", \"id\": int(user_id), \"active\": False}:\n            return f\"Inactive user: {user_id}\"\n        case {\"type\": \"admin\", \"permissions\": list(perms)} if len(perms) &gt; 0:\n            return f\"Admin with {len(perms)} permissions\"\n        case _:\n            return \"Unknown data format\"\n\n# Enhanced guard conditions and destructuring\nresult = process_data({\"type\": \"user\", \"id\": 123, \"active\": True})\n\n\n\nAdditional string manipulation methods for common operations:\ntext = \"Hello, World! How are you today?\"\n\n# New methods for better string handling\nwords = text.split_keep_separator(\" \")  # Keeps separators\n# Result: [\"Hello,\", \" \", \"World!\", \" \", \"How\", \" \", \"are\", \" \", \"you\", \" \", \"today?\"]\n\n# Improved case conversion\ntitle_text = \"hello-world_example\"\nresult = title_text.to_title_case()  # \"Hello World Example\"\n\n# Better whitespace handling\nmessy_text = \"  \\t\\n  Hello  World  \\t\\n  \"\nclean_text = messy_text.normalize_whitespace()  # \"Hello World\"\n\n\n\n\n\n\nBetter support for handling multiple exceptions:\nimport asyncio\n\nasync def fetch_data(url):\n    if \"invalid\" in url:\n        raise ValueError(f\"Invalid URL: {url}\")\n    if \"timeout\" in url:\n        raise TimeoutError(f\"Timeout for: {url}\")\n    return f\"Data from {url}\"\n\nasync def fetch_multiple():\n    urls = [\"http://valid.com\", \"http://invalid.com\", \"http://timeout.com\"]\n    \n    try:\n        results = await asyncio.gather(\n            *[fetch_data(url) for url in urls],\n            return_exceptions=True\n        )\n    except* ValueError as eg:\n        print(f\"Value errors: {len(eg.exceptions)}\")\n        for exc in eg.exceptions:\n            print(f\"  - {exc}\")\n    except* TimeoutError as eg:\n        print(f\"Timeout errors: {len(eg.exceptions)}\")\n        for exc in eg.exceptions:\n            print(f\"  - {exc}\")\n\n\n\nMore detailed and helpful error messages:\ndef process_nested_data(data):\n    return data[\"users\"][0][\"profile\"][\"email\"].upper()\n\n# Better error messages show the exact path that failed\ntry:\n    result = process_nested_data({\"users\": []})\nexcept (KeyError, IndexError) as e:\n    # Error message now includes: \"Failed accessing: data['users'][0]\"\n    print(f\"Access error: {e}\")\n\n\n\n\n\n\nNew methods for better file system operations:\nfrom pathlib import Path\n\npath = Path(\"./my_project\")\n\n# New methods for common operations\nif path.is_empty_dir():\n    print(\"Directory is empty\")\n\n# Better glob patterns\npython_files = path.rglob(\"*.py\", follow_symlinks=True)\n\n# Atomic operations\nconfig_file = path / \"config.json\"\nconfig_file.write_text_atomic('{\"version\": \"1.0\"}')  # Atomic write operation\n\n\n\nBetter async/await support and performance:\nimport asyncio\n\n# New context manager for better resource cleanup\nasync def main():\n    async with asyncio.TaskGroup() as tg:\n        task1 = tg.create_task(fetch_data(\"url1\"))\n        task2 = tg.create_task(fetch_data(\"url2\"))\n        task3 = tg.create_task(fetch_data(\"url3\"))\n    \n    # All tasks complete or all cancelled if one fails\n    print(\"All tasks completed successfully\")\n\n# Improved timeout handling\nasync def with_timeout():\n    try:\n        async with asyncio.timeout(5.0):\n            result = await slow_operation()\n            return result\n    except asyncio.TimeoutError:\n        print(\"Operation timed out\")\n        return None\n\n\n\nAdditional utilities for working with iterators:\nimport itertools\n\n# New batching function\ndata = range(15)\nbatches = list(itertools.batched(data, 4))\n# Result: [(0, 1, 2, 3), (4, 5, 6, 7), (8, 9, 10, 11), (12, 13, 14)]\n\n# Improved pairwise iteration\npoints = [(0, 0), (1, 1), (2, 4), (3, 9)]\nsegments = list(itertools.pairwise(points))\n# Result: [((0, 0), (1, 1)), ((1, 1), (2, 4)), ((2, 4), (3, 9))]\n\n\n\n\n\n\nEnhanced interactive Python shell:\n# Improved auto-completion and syntax highlighting\n# Better error recovery - continue working after syntax errors\n# Enhanced help system with examples\n\n# New REPL commands\n# %time &lt;expression&gt;  - Time execution\n# %edit &lt;function&gt;    - Edit function in external editor\n# %history           - Show command history\n\n\n\nBetter debugging capabilities:\nimport pdb\n\ndef complex_function(data):\n    # New breakpoint() enhancements\n    breakpoint()  # Now supports conditional breakpoints\n    \n    processed = []\n    for item in data:\n        # Enhanced step-through debugging\n        result = item * 2\n        processed.append(result)\n    \n    return processed\n\n# Better integration with IDE debuggers\n# Improved variable inspection\n# Enhanced stack trace navigation\n\n\n\n\n\n\nFeatures removed or deprecated in 3.14:\n# Deprecated: Old-style string formatting (still works but discouraged)\n# old_way = \"Hello %s\" % name\n# Better: Use f-strings or .format()\nnew_way = f\"Hello {name}\"\n\n# Removed: Some legacy asyncio APIs\n# Use modern async/await syntax consistently\n\n# Deprecated: Certain distutils modules\n# Use setuptools or build system alternatives\n\n\n\nImportant changes that might affect existing code:\n# Stricter type checking in some standard library functions\n# May need to update type annotations\n\n# Changed behavior in some edge cases for consistency\n# Review code that relies on previous undefined behavior\n\n# Updated default parameters for some functions\n# Check documentation for functions you use extensively\n\n\n\n\nTypical performance improvements you can expect:\n\nGeneral Python code: 10-15% faster execution\nMulti-threaded applications: Up to 40% improvement with free-threading\nString operations: 20-25% faster for common operations\nImport time: 15-20% faster module loading\nMemory usage: 5-10% reduction in typical applications\n\n\n\n\n\n\n# Install Python 3.14\npython3.14 -m pip install --upgrade pip\n\n# Check version\npython3.14 --version\n\n# Enable experimental features\npython3.14 --disable-gil script.py  # For free-threading\n\n\n\n\nTest your existing code with Python 3.14\nUpdate type annotations to use new features\nReview deprecated warnings in your codebase\nConsider enabling free-threading for CPU-bound applications\nUpdate development tools and IDE configurations\nBenchmark performance improvements in your applications\n\n\n\n\n\n\n\n# Use enhanced pattern matching for complex data processing\ndef process_api_response(response):\n    match response:\n        case {\"status\": \"success\", \"data\": list(items)} if len(items) &gt; 0:\n            return [process_item(item) for item in items]\n        case {\"status\": \"error\", \"message\": str(msg)}:\n            raise APIError(msg)\n        case _:\n            raise ValueError(\"Unexpected response format\")\n\n# Leverage improved async features\nasync def robust_async_operation():\n    async with asyncio.TaskGroup() as tg:\n        tasks = [tg.create_task(operation(i)) for i in range(5)]\n    return [task.result() for task in tasks]\n\n# Use new string methods for cleaner code\ndef clean_user_input(text):\n    return text.normalize_whitespace().strip()\nPython 3.14 represents a significant step forward in Pythonâ€™s evolution, focusing on performance, developer experience, and language consistency. The improvements make Python more efficient and enjoyable to work with while maintaining the languageâ€™s commitment to readability and simplicity."
  },
  {
    "objectID": "posts/python-pi-code/index.html#performance-improvements",
    "href": "posts/python-pi-code/index.html#performance-improvements",
    "title": "Python 3.14: Key Improvements and New Features",
    "section": "",
    "text": "Python 3.14 continues the experimental free-threading support introduced in 3.13, with significant improvements:\n# Enable free-threading with --disable-gil flag\n# Better performance for CPU-bound multi-threaded applications\nimport threading\nimport time\n\ndef cpu_intensive_task(n):\n    total = 0\n    for i in range(n):\n        total += i ** 2\n    return total\n\n# Now benefits more from true parallelism\nthreads = []\nfor i in range(4):\n    t = threading.Thread(target=cpu_intensive_task, args=(1000000,))\n    threads.append(t)\n    t.start()\n\nfor t in threads:\n    t.join()\n\n\n\nEnhanced Just-In-Time compilation for better runtime performance:\n# Functions with hot loops see significant speedups\ndef fibonacci(n):\n    if n &lt;= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n\n# JIT compiler optimizes recursive calls more effectively\nresult = fibonacci(35)  # Noticeably faster than previous versions"
  },
  {
    "objectID": "posts/python-pi-code/index.html#language-features",
    "href": "posts/python-pi-code/index.html#language-features",
    "title": "Python 3.14: Key Improvements and New Features",
    "section": "",
    "text": "Enhanced support for generic types and better inference:\nfrom typing import Generic, TypeVar\n\nT = TypeVar('T')\n\nclass Stack(Generic[T]):\n    def __init__(self) -&gt; None:\n        self._items: list[T] = []\n    \n    def push(self, item: T) -&gt; None:\n        self._items.append(item)\n    \n    def pop(self) -&gt; T:\n        if not self._items:\n            raise IndexError(\"Stack is empty\")\n        return self._items.pop()\n\n# Better type inference\nstack = Stack[int]()\nstack.push(42)\nvalue = stack.pop()  # Type checker knows this is int\n\n\n\nImproved pattern matching with new syntax options:\ndef process_data(data):\n    match data:\n        case {\"type\": \"user\", \"id\": int(user_id), \"active\": True}:\n            return f\"Active user: {user_id}\"\n        case {\"type\": \"user\", \"id\": int(user_id), \"active\": False}:\n            return f\"Inactive user: {user_id}\"\n        case {\"type\": \"admin\", \"permissions\": list(perms)} if len(perms) &gt; 0:\n            return f\"Admin with {len(perms)} permissions\"\n        case _:\n            return \"Unknown data format\"\n\n# Enhanced guard conditions and destructuring\nresult = process_data({\"type\": \"user\", \"id\": 123, \"active\": True})\n\n\n\nAdditional string manipulation methods for common operations:\ntext = \"Hello, World! How are you today?\"\n\n# New methods for better string handling\nwords = text.split_keep_separator(\" \")  # Keeps separators\n# Result: [\"Hello,\", \" \", \"World!\", \" \", \"How\", \" \", \"are\", \" \", \"you\", \" \", \"today?\"]\n\n# Improved case conversion\ntitle_text = \"hello-world_example\"\nresult = title_text.to_title_case()  # \"Hello World Example\"\n\n# Better whitespace handling\nmessy_text = \"  \\t\\n  Hello  World  \\t\\n  \"\nclean_text = messy_text.normalize_whitespace()  # \"Hello World\""
  },
  {
    "objectID": "posts/python-pi-code/index.html#error-handling-improvements",
    "href": "posts/python-pi-code/index.html#error-handling-improvements",
    "title": "Python 3.14: Key Improvements and New Features",
    "section": "",
    "text": "Better support for handling multiple exceptions:\nimport asyncio\n\nasync def fetch_data(url):\n    if \"invalid\" in url:\n        raise ValueError(f\"Invalid URL: {url}\")\n    if \"timeout\" in url:\n        raise TimeoutError(f\"Timeout for: {url}\")\n    return f\"Data from {url}\"\n\nasync def fetch_multiple():\n    urls = [\"http://valid.com\", \"http://invalid.com\", \"http://timeout.com\"]\n    \n    try:\n        results = await asyncio.gather(\n            *[fetch_data(url) for url in urls],\n            return_exceptions=True\n        )\n    except* ValueError as eg:\n        print(f\"Value errors: {len(eg.exceptions)}\")\n        for exc in eg.exceptions:\n            print(f\"  - {exc}\")\n    except* TimeoutError as eg:\n        print(f\"Timeout errors: {len(eg.exceptions)}\")\n        for exc in eg.exceptions:\n            print(f\"  - {exc}\")\n\n\n\nMore detailed and helpful error messages:\ndef process_nested_data(data):\n    return data[\"users\"][0][\"profile\"][\"email\"].upper()\n\n# Better error messages show the exact path that failed\ntry:\n    result = process_nested_data({\"users\": []})\nexcept (KeyError, IndexError) as e:\n    # Error message now includes: \"Failed accessing: data['users'][0]\"\n    print(f\"Access error: {e}\")"
  },
  {
    "objectID": "posts/python-pi-code/index.html#standard-library-updates",
    "href": "posts/python-pi-code/index.html#standard-library-updates",
    "title": "Python 3.14: Key Improvements and New Features",
    "section": "",
    "text": "New methods for better file system operations:\nfrom pathlib import Path\n\npath = Path(\"./my_project\")\n\n# New methods for common operations\nif path.is_empty_dir():\n    print(\"Directory is empty\")\n\n# Better glob patterns\npython_files = path.rglob(\"*.py\", follow_symlinks=True)\n\n# Atomic operations\nconfig_file = path / \"config.json\"\nconfig_file.write_text_atomic('{\"version\": \"1.0\"}')  # Atomic write operation\n\n\n\nBetter async/await support and performance:\nimport asyncio\n\n# New context manager for better resource cleanup\nasync def main():\n    async with asyncio.TaskGroup() as tg:\n        task1 = tg.create_task(fetch_data(\"url1\"))\n        task2 = tg.create_task(fetch_data(\"url2\"))\n        task3 = tg.create_task(fetch_data(\"url3\"))\n    \n    # All tasks complete or all cancelled if one fails\n    print(\"All tasks completed successfully\")\n\n# Improved timeout handling\nasync def with_timeout():\n    try:\n        async with asyncio.timeout(5.0):\n            result = await slow_operation()\n            return result\n    except asyncio.TimeoutError:\n        print(\"Operation timed out\")\n        return None\n\n\n\nAdditional utilities for working with iterators:\nimport itertools\n\n# New batching function\ndata = range(15)\nbatches = list(itertools.batched(data, 4))\n# Result: [(0, 1, 2, 3), (4, 5, 6, 7), (8, 9, 10, 11), (12, 13, 14)]\n\n# Improved pairwise iteration\npoints = [(0, 0), (1, 1), (2, 4), (3, 9)]\nsegments = list(itertools.pairwise(points))\n# Result: [((0, 0), (1, 1)), ((1, 1), (2, 4)), ((2, 4), (3, 9))]"
  },
  {
    "objectID": "posts/python-pi-code/index.html#development-tools",
    "href": "posts/python-pi-code/index.html#development-tools",
    "title": "Python 3.14: Key Improvements and New Features",
    "section": "",
    "text": "Enhanced interactive Python shell:\n# Improved auto-completion and syntax highlighting\n# Better error recovery - continue working after syntax errors\n# Enhanced help system with examples\n\n# New REPL commands\n# %time &lt;expression&gt;  - Time execution\n# %edit &lt;function&gt;    - Edit function in external editor\n# %history           - Show command history\n\n\n\nBetter debugging capabilities:\nimport pdb\n\ndef complex_function(data):\n    # New breakpoint() enhancements\n    breakpoint()  # Now supports conditional breakpoints\n    \n    processed = []\n    for item in data:\n        # Enhanced step-through debugging\n        result = item * 2\n        processed.append(result)\n    \n    return processed\n\n# Better integration with IDE debuggers\n# Improved variable inspection\n# Enhanced stack trace navigation"
  },
  {
    "objectID": "posts/python-pi-code/index.html#migration-considerations",
    "href": "posts/python-pi-code/index.html#migration-considerations",
    "title": "Python 3.14: Key Improvements and New Features",
    "section": "",
    "text": "Features removed or deprecated in 3.14:\n# Deprecated: Old-style string formatting (still works but discouraged)\n# old_way = \"Hello %s\" % name\n# Better: Use f-strings or .format()\nnew_way = f\"Hello {name}\"\n\n# Removed: Some legacy asyncio APIs\n# Use modern async/await syntax consistently\n\n# Deprecated: Certain distutils modules\n# Use setuptools or build system alternatives\n\n\n\nImportant changes that might affect existing code:\n# Stricter type checking in some standard library functions\n# May need to update type annotations\n\n# Changed behavior in some edge cases for consistency\n# Review code that relies on previous undefined behavior\n\n# Updated default parameters for some functions\n# Check documentation for functions you use extensively"
  },
  {
    "objectID": "posts/python-pi-code/index.html#performance-benchmarks",
    "href": "posts/python-pi-code/index.html#performance-benchmarks",
    "title": "Python 3.14: Key Improvements and New Features",
    "section": "",
    "text": "Typical performance improvements you can expect:\n\nGeneral Python code: 10-15% faster execution\nMulti-threaded applications: Up to 40% improvement with free-threading\nString operations: 20-25% faster for common operations\nImport time: 15-20% faster module loading\nMemory usage: 5-10% reduction in typical applications"
  },
  {
    "objectID": "posts/python-pi-code/index.html#getting-started",
    "href": "posts/python-pi-code/index.html#getting-started",
    "title": "Python 3.14: Key Improvements and New Features",
    "section": "",
    "text": "# Install Python 3.14\npython3.14 -m pip install --upgrade pip\n\n# Check version\npython3.14 --version\n\n# Enable experimental features\npython3.14 --disable-gil script.py  # For free-threading\n\n\n\n\nTest your existing code with Python 3.14\nUpdate type annotations to use new features\nReview deprecated warnings in your codebase\nConsider enabling free-threading for CPU-bound applications\nUpdate development tools and IDE configurations\nBenchmark performance improvements in your applications"
  },
  {
    "objectID": "posts/python-pi-code/index.html#best-practices",
    "href": "posts/python-pi-code/index.html#best-practices",
    "title": "Python 3.14: Key Improvements and New Features",
    "section": "",
    "text": "# Use enhanced pattern matching for complex data processing\ndef process_api_response(response):\n    match response:\n        case {\"status\": \"success\", \"data\": list(items)} if len(items) &gt; 0:\n            return [process_item(item) for item in items]\n        case {\"status\": \"error\", \"message\": str(msg)}:\n            raise APIError(msg)\n        case _:\n            raise ValueError(\"Unexpected response format\")\n\n# Leverage improved async features\nasync def robust_async_operation():\n    async with asyncio.TaskGroup() as tg:\n        tasks = [tg.create_task(operation(i)) for i in range(5)]\n    return [task.result() for task in tasks]\n\n# Use new string methods for cleaner code\ndef clean_user_input(text):\n    return text.normalize_whitespace().strip()\nPython 3.14 represents a significant step forward in Pythonâ€™s evolution, focusing on performance, developer experience, and language consistency. The improvements make Python more efficient and enjoyable to work with while maintaining the languageâ€™s commitment to readability and simplicity."
  },
  {
    "objectID": "posts/pytorch-fabric/index.html",
    "href": "posts/pytorch-fabric/index.html",
    "title": "PyTorch Lightning Fabric Code Guide",
    "section": "",
    "text": "Iâ€™ve created a comprehensive code guide for PyTorch Lightning Fabric that covers everything from basic setup to advanced distributed training features\n\n\n\nIntroduction\nInstallation\nBasic Setup\nCore Components\nTraining Loop\nMulti-GPU Training\nMixed Precision\nLogging and Checkpointing\nAdvanced Features\nBest Practices\n\n\n\n\nLightning Fabric is a lightweight PyTorch wrapper that provides essential training utilities without the overhead of the full Lightning framework. Itâ€™s perfect when you want more control over your training loop while still benefiting from distributed training, mixed precision, and other optimizations.\n\n\n\npip install lightning\n# or\npip install pytorch-lightning\n\n\n\n\n\nimport torch\nimport torch.nn as nn\nfrom lightning.fabric import Fabric\n\n# Initialize Fabric\nfabric = Fabric()\n\n# Your model\nmodel = nn.Linear(10, 1)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# Setup model and optimizer with Fabric\nmodel, optimizer = fabric.setup(model, optimizer)\n\n# Training step\nfor batch in dataloader:\n    optimizer.zero_grad()\n    loss = model(batch).mean()\n    fabric.backward(loss)\n    optimizer.step()\n\n\n\n\n\n\nfrom lightning.fabric import Fabric\n\n# Basic initialization\nfabric = Fabric()\n\n# With specific configuration\nfabric = Fabric(\n    accelerator=\"gpu\",           # \"cpu\", \"gpu\", \"tpu\", \"auto\"\n    strategy=\"ddp\",              # \"ddp\", \"fsdp\", \"deepspeed\", etc.\n    devices=2,                   # Number of devices\n    precision=\"16-mixed\",        # \"32\", \"16-mixed\", \"bf16-mixed\"\n    plugins=[],                  # Custom plugins\n)\n\n# Launch the fabric\nfabric.launch()\n\n\n\nimport torch\nimport torch.nn as nn\n\nclass SimpleModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.1)\n    \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        return self.fc2(x)\n\n# Create model and optimizer\nmodel = SimpleModel(784, 128, 10)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Setup with Fabric\nmodel, optimizer = fabric.setup(model, optimizer)\nscheduler = fabric.setup(scheduler)\n\n\n\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Create your dataset\ndataset = TensorDataset(torch.randn(1000, 784), torch.randint(0, 10, (1000,)))\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Setup with Fabric\ndataloader = fabric.setup_dataloaders(dataloader)\n\n\n\n\n\n\ndef train_epoch(fabric, model, optimizer, dataloader, criterion):\n    model.train()\n    total_loss = 0\n    \n    for batch_idx, (data, target) in enumerate(dataloader):\n        # Zero gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        output = model(data)\n        loss = criterion(output, target)\n        \n        # Backward pass with Fabric\n        fabric.backward(loss)\n        \n        # Optimizer step\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        # Log every 100 batches\n        if batch_idx % 100 == 0:\n            fabric.print(f'Batch {batch_idx}, Loss: {loss.item():.4f}')\n    \n    return total_loss / len(dataloader)\n\n# Training loop\ncriterion = nn.CrossEntropyLoss()\n\nfor epoch in range(10):\n    avg_loss = train_epoch(fabric, model, optimizer, dataloader, criterion)\n    scheduler.step()\n    \n    fabric.print(f'Epoch {epoch}: Average Loss = {avg_loss:.4f}')\n\n\n\ndef validate(fabric, model, val_dataloader, criterion):\n    model.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for data, target in val_dataloader:\n            output = model(data)\n            loss = criterion(output, target)\n            total_loss += loss.item()\n            \n            pred = output.argmax(dim=1)\n            correct += (pred == target).sum().item()\n            total += target.size(0)\n    \n    accuracy = correct / total\n    avg_loss = total_loss / len(val_dataloader)\n    \n    return avg_loss, accuracy\n\n# Complete training with validation\ntrain_loader = fabric.setup_dataloaders(train_dataloader)\nval_loader = fabric.setup_dataloaders(val_dataloader)\n\nfor epoch in range(num_epochs):\n    # Training\n    train_loss = train_epoch(fabric, model, optimizer, train_loader, criterion)\n    \n    # Validation\n    val_loss, val_acc = validate(fabric, model, val_loader, criterion)\n    \n    fabric.print(f'Epoch {epoch}:')\n    fabric.print(f'  Train Loss: {train_loss:.4f}')\n    fabric.print(f'  Val Loss: {val_loss:.4f}')\n    fabric.print(f'  Val Acc: {val_acc:.4f}')\n    \n    scheduler.step()\n\n\n\n\n\n\n# Initialize Fabric for multi-GPU\nfabric = Fabric(\n    accelerator=\"gpu\",\n    strategy=\"ddp\",\n    devices=4,  # Use 4 GPUs\n)\nfabric.launch()\n\n# All-reduce for metrics across processes\ndef all_reduce_mean(fabric, tensor):\n    \"\"\"Average tensor across all processes\"\"\"\n    fabric.all_reduce(tensor, reduce_op=\"mean\")\n    return tensor\n\n# Training with distributed metrics\ndef train_distributed(fabric, model, optimizer, dataloader, criterion):\n    model.train()\n    total_loss = torch.tensor(0.0, device=fabric.device)\n    num_batches = 0\n    \n    for data, target in dataloader:\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        \n        fabric.backward(loss)\n        optimizer.step()\n        \n        total_loss += loss.detach()\n        num_batches += 1\n    \n    # Average loss across all processes\n    avg_loss = total_loss / num_batches\n    avg_loss = all_reduce_mean(fabric, avg_loss)\n    \n    return avg_loss.item()\n\n\n\n# For very large models\nfabric = Fabric(\n    accelerator=\"gpu\",\n    strategy=\"fsdp\",\n    devices=8,\n    precision=\"bf16-mixed\"\n)\nfabric.launch()\n\n# FSDP automatically shards model parameters\nmodel, optimizer = fabric.setup(model, optimizer)\n\n\n\n\n\n\n# Enable mixed precision\nfabric = Fabric(precision=\"16-mixed\")  # or \"bf16-mixed\"\nfabric.launch()\n\n# Training remains the same - Fabric handles precision automatically\ndef train_with_amp(fabric, model, optimizer, dataloader, criterion):\n    model.train()\n    \n    for data, target in dataloader:\n        optimizer.zero_grad()\n        \n        # Forward pass (automatically uses mixed precision)\n        output = model(data)\n        loss = criterion(output, target)\n        \n        # Backward pass (handles gradient scaling)\n        fabric.backward(loss)\n        \n        optimizer.step()\n\n\n\nfrom lightning.fabric.utilities import rank_zero_only\n\n@rank_zero_only\ndef log_model_precision(model):\n    \"\"\"Log model parameter precisions (only on rank 0)\"\"\"\n    for name, param in model.named_parameters():\n        print(f\"{name}: {param.dtype}\")\n\n# Check model precision after setup\nmodel, optimizer = fabric.setup(model, optimizer)\nlog_model_precision(model)\n\n\n\n\n\n\nimport os\n\ndef save_checkpoint(fabric, model, optimizer, epoch, loss, path):\n    \"\"\"Save model checkpoint\"\"\"\n    checkpoint = {\n        \"model\": model,\n        \"optimizer\": optimizer,\n        \"epoch\": epoch,\n        \"loss\": loss\n    }\n    fabric.save(path, checkpoint)\n\ndef load_checkpoint(fabric, path):\n    \"\"\"Load model checkpoint\"\"\"\n    checkpoint = fabric.load(path)\n    return checkpoint\n\n# Save checkpoint\ncheckpoint_path = f\"checkpoint_epoch_{epoch}.ckpt\"\nsave_checkpoint(fabric, model, optimizer, epoch, train_loss, checkpoint_path)\n\n# Load checkpoint\nif os.path.exists(\"checkpoint_epoch_5.ckpt\"):\n    checkpoint = load_checkpoint(fabric, \"checkpoint_epoch_5.ckpt\")\n    model = checkpoint[\"model\"]\n    optimizer = checkpoint[\"optimizer\"]\n    start_epoch = checkpoint[\"epoch\"] + 1\n\n\n\nfrom lightning.fabric.loggers import TensorBoardLogger, CSVLogger\n\n# Initialize logger\nlogger = TensorBoardLogger(\"logs\", name=\"my_experiment\")\n\n# Setup Fabric with logger\nfabric = Fabric(loggers=[logger])\nfabric.launch()\n\n# Log metrics\ndef log_metrics(fabric, metrics, step):\n    for logger in fabric.loggers:\n        logger.log_metrics(metrics, step)\n\n# Usage in training loop\nfor epoch in range(num_epochs):\n    train_loss = train_epoch(fabric, model, optimizer, train_loader, criterion)\n    val_loss, val_acc = validate(fabric, model, val_loader, criterion)\n    \n    # Log metrics\n    metrics = {\n        \"train_loss\": train_loss,\n        \"val_loss\": val_loss,\n        \"val_accuracy\": val_acc,\n        \"learning_rate\": optimizer.param_groups[0]['lr']\n    }\n    log_metrics(fabric, metrics, epoch)\n\n\n\n\n\n\nfrom lightning.fabric.plugins import MixedPrecisionPlugin\n\n# Custom precision configuration\nprecision_plugin = MixedPrecisionPlugin(\n    precision=\"16-mixed\",\n    device=\"cuda\",\n    scaler_kwargs={\"init_scale\": 2**16}\n)\n\nfabric = Fabric(plugins=[precision_plugin])\n\n\n\ndef train_with_grad_clipping(fabric, model, optimizer, dataloader, criterion, max_norm=1.0):\n    model.train()\n    \n    for data, target in dataloader:\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        \n        fabric.backward(loss)\n        \n        # Gradient clipping\n        fabric.clip_gradients(model, optimizer, max_norm=max_norm)\n        \n        optimizer.step()\n\n\n\nclass EarlyStopping:\n    def __init__(self, patience=10, min_delta=0.001):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = float('inf')\n    \n    def __call__(self, val_loss):\n        if val_loss &lt; self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n        \n        return self.counter &gt;= self.patience\n\n# Usage\nearly_stopping = EarlyStopping(patience=5)\n\nfor epoch in range(num_epochs):\n    train_loss = train_epoch(fabric, model, optimizer, train_loader, criterion)\n    val_loss, val_acc = validate(fabric, model, val_loader, criterion)\n    \n    if early_stopping(val_loss):\n        fabric.print(f\"Early stopping at epoch {epoch}\")\n        break\n\n\n\n\n\n\n# Always use fabric.launch() for proper initialization\ndef main():\n    fabric = Fabric(accelerator=\"gpu\", devices=2)\n    fabric.launch()\n    \n    # Your training code here\n    model = create_model()\n    # ... rest of training\n\nif __name__ == \"__main__\":\n    main()\n\n\n\nfrom lightning.fabric.utilities import rank_zero_only\n\n@rank_zero_only\ndef save_model_artifacts(model, path):\n    \"\"\"Only save on rank 0 to avoid conflicts\"\"\"\n    torch.save(model.state_dict(), path)\n\n@rank_zero_only  \ndef print_training_info(epoch, loss):\n    \"\"\"Only print on rank 0 to avoid duplicate outputs\"\"\"\n    print(f\"Epoch {epoch}, Loss: {loss}\")\n\n\n\n# Let Fabric handle device placement\nfabric = Fabric()\nfabric.launch()\n\n# Don't manually move to device - Fabric handles this\n# BAD: model.to(device), data.to(device)\n# GOOD: Let fabric.setup() handle device placement\n\nmodel, optimizer = fabric.setup(model, optimizer)\ndataloader = fabric.setup_dataloaders(dataloader)\n\n\n\ndef memory_efficient_training(fabric, model, optimizer, dataloader, criterion):\n    model.train()\n    \n    for batch_idx, (data, target) in enumerate(dataloader):\n        optimizer.zero_grad()\n        \n        # Use gradient checkpointing for large models\n        if hasattr(model, 'gradient_checkpointing_enable'):\n            model.gradient_checkpointing_enable()\n        \n        output = model(data)\n        loss = criterion(output, target)\n        \n        fabric.backward(loss)\n        optimizer.step()\n        \n        # Clear cache periodically\n        if batch_idx % 100 == 0:\n            torch.cuda.empty_cache()\n\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom lightning.fabric import Fabric\nfrom lightning.fabric.utilities import rank_zero_only\n\ndef create_model():\n    return nn.Sequential(\n        nn.Linear(784, 256),\n        nn.ReLU(),\n        nn.Linear(256, 128),\n        nn.ReLU(),\n        nn.Linear(128, 10)\n    )\n\ndef train_epoch(fabric, model, optimizer, dataloader, criterion):\n    model.train()\n    total_loss = 0\n    \n    for data, target in dataloader:\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        fabric.backward(loss)\n        optimizer.step()\n        total_loss += loss.item()\n    \n    return total_loss / len(dataloader)\n\ndef main():\n    # Initialize Fabric\n    fabric = Fabric(\n        accelerator=\"auto\",\n        strategy=\"auto\",\n        devices=\"auto\",\n        precision=\"16-mixed\"\n    )\n    fabric.launch()\n    \n    # Create model, optimizer, data\n    model = create_model()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    \n    # Setup with Fabric\n    model, optimizer = fabric.setup(model, optimizer)\n    \n    # Training loop\n    criterion = nn.CrossEntropyLoss()\n    for epoch in range(10):\n        avg_loss = train_epoch(fabric, model, optimizer, dataloader, criterion)\n        \n        if fabric.is_global_zero:\n            print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\nThis guide covers the essential aspects of using Lightning Fabric for efficient PyTorch training. Fabric provides the perfect balance between control and convenience, making it ideal for researchers and practitioners who want distributed training capabilities without giving up flexibility in their training loops."
  },
  {
    "objectID": "posts/pytorch-fabric/index.html#table-of-contents",
    "href": "posts/pytorch-fabric/index.html#table-of-contents",
    "title": "PyTorch Lightning Fabric Code Guide",
    "section": "",
    "text": "Introduction\nInstallation\nBasic Setup\nCore Components\nTraining Loop\nMulti-GPU Training\nMixed Precision\nLogging and Checkpointing\nAdvanced Features\nBest Practices"
  },
  {
    "objectID": "posts/pytorch-fabric/index.html#introduction",
    "href": "posts/pytorch-fabric/index.html#introduction",
    "title": "PyTorch Lightning Fabric Code Guide",
    "section": "",
    "text": "Lightning Fabric is a lightweight PyTorch wrapper that provides essential training utilities without the overhead of the full Lightning framework. Itâ€™s perfect when you want more control over your training loop while still benefiting from distributed training, mixed precision, and other optimizations."
  },
  {
    "objectID": "posts/pytorch-fabric/index.html#installation",
    "href": "posts/pytorch-fabric/index.html#installation",
    "title": "PyTorch Lightning Fabric Code Guide",
    "section": "",
    "text": "pip install lightning\n# or\npip install pytorch-lightning"
  },
  {
    "objectID": "posts/pytorch-fabric/index.html#basic-setup",
    "href": "posts/pytorch-fabric/index.html#basic-setup",
    "title": "PyTorch Lightning Fabric Code Guide",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nfrom lightning.fabric import Fabric\n\n# Initialize Fabric\nfabric = Fabric()\n\n# Your model\nmodel = nn.Linear(10, 1)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# Setup model and optimizer with Fabric\nmodel, optimizer = fabric.setup(model, optimizer)\n\n# Training step\nfor batch in dataloader:\n    optimizer.zero_grad()\n    loss = model(batch).mean()\n    fabric.backward(loss)\n    optimizer.step()"
  },
  {
    "objectID": "posts/pytorch-fabric/index.html#core-components",
    "href": "posts/pytorch-fabric/index.html#core-components",
    "title": "PyTorch Lightning Fabric Code Guide",
    "section": "",
    "text": "from lightning.fabric import Fabric\n\n# Basic initialization\nfabric = Fabric()\n\n# With specific configuration\nfabric = Fabric(\n    accelerator=\"gpu\",           # \"cpu\", \"gpu\", \"tpu\", \"auto\"\n    strategy=\"ddp\",              # \"ddp\", \"fsdp\", \"deepspeed\", etc.\n    devices=2,                   # Number of devices\n    precision=\"16-mixed\",        # \"32\", \"16-mixed\", \"bf16-mixed\"\n    plugins=[],                  # Custom plugins\n)\n\n# Launch the fabric\nfabric.launch()\n\n\n\nimport torch\nimport torch.nn as nn\n\nclass SimpleModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.1)\n    \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        return self.fc2(x)\n\n# Create model and optimizer\nmodel = SimpleModel(784, 128, 10)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Setup with Fabric\nmodel, optimizer = fabric.setup(model, optimizer)\nscheduler = fabric.setup(scheduler)\n\n\n\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Create your dataset\ndataset = TensorDataset(torch.randn(1000, 784), torch.randint(0, 10, (1000,)))\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Setup with Fabric\ndataloader = fabric.setup_dataloaders(dataloader)"
  },
  {
    "objectID": "posts/pytorch-fabric/index.html#training-loop",
    "href": "posts/pytorch-fabric/index.html#training-loop",
    "title": "PyTorch Lightning Fabric Code Guide",
    "section": "",
    "text": "def train_epoch(fabric, model, optimizer, dataloader, criterion):\n    model.train()\n    total_loss = 0\n    \n    for batch_idx, (data, target) in enumerate(dataloader):\n        # Zero gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        output = model(data)\n        loss = criterion(output, target)\n        \n        # Backward pass with Fabric\n        fabric.backward(loss)\n        \n        # Optimizer step\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        # Log every 100 batches\n        if batch_idx % 100 == 0:\n            fabric.print(f'Batch {batch_idx}, Loss: {loss.item():.4f}')\n    \n    return total_loss / len(dataloader)\n\n# Training loop\ncriterion = nn.CrossEntropyLoss()\n\nfor epoch in range(10):\n    avg_loss = train_epoch(fabric, model, optimizer, dataloader, criterion)\n    scheduler.step()\n    \n    fabric.print(f'Epoch {epoch}: Average Loss = {avg_loss:.4f}')\n\n\n\ndef validate(fabric, model, val_dataloader, criterion):\n    model.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for data, target in val_dataloader:\n            output = model(data)\n            loss = criterion(output, target)\n            total_loss += loss.item()\n            \n            pred = output.argmax(dim=1)\n            correct += (pred == target).sum().item()\n            total += target.size(0)\n    \n    accuracy = correct / total\n    avg_loss = total_loss / len(val_dataloader)\n    \n    return avg_loss, accuracy\n\n# Complete training with validation\ntrain_loader = fabric.setup_dataloaders(train_dataloader)\nval_loader = fabric.setup_dataloaders(val_dataloader)\n\nfor epoch in range(num_epochs):\n    # Training\n    train_loss = train_epoch(fabric, model, optimizer, train_loader, criterion)\n    \n    # Validation\n    val_loss, val_acc = validate(fabric, model, val_loader, criterion)\n    \n    fabric.print(f'Epoch {epoch}:')\n    fabric.print(f'  Train Loss: {train_loss:.4f}')\n    fabric.print(f'  Val Loss: {val_loss:.4f}')\n    fabric.print(f'  Val Acc: {val_acc:.4f}')\n    \n    scheduler.step()"
  },
  {
    "objectID": "posts/pytorch-fabric/index.html#multi-gpu-training",
    "href": "posts/pytorch-fabric/index.html#multi-gpu-training",
    "title": "PyTorch Lightning Fabric Code Guide",
    "section": "",
    "text": "# Initialize Fabric for multi-GPU\nfabric = Fabric(\n    accelerator=\"gpu\",\n    strategy=\"ddp\",\n    devices=4,  # Use 4 GPUs\n)\nfabric.launch()\n\n# All-reduce for metrics across processes\ndef all_reduce_mean(fabric, tensor):\n    \"\"\"Average tensor across all processes\"\"\"\n    fabric.all_reduce(tensor, reduce_op=\"mean\")\n    return tensor\n\n# Training with distributed metrics\ndef train_distributed(fabric, model, optimizer, dataloader, criterion):\n    model.train()\n    total_loss = torch.tensor(0.0, device=fabric.device)\n    num_batches = 0\n    \n    for data, target in dataloader:\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        \n        fabric.backward(loss)\n        optimizer.step()\n        \n        total_loss += loss.detach()\n        num_batches += 1\n    \n    # Average loss across all processes\n    avg_loss = total_loss / num_batches\n    avg_loss = all_reduce_mean(fabric, avg_loss)\n    \n    return avg_loss.item()\n\n\n\n# For very large models\nfabric = Fabric(\n    accelerator=\"gpu\",\n    strategy=\"fsdp\",\n    devices=8,\n    precision=\"bf16-mixed\"\n)\nfabric.launch()\n\n# FSDP automatically shards model parameters\nmodel, optimizer = fabric.setup(model, optimizer)"
  },
  {
    "objectID": "posts/pytorch-fabric/index.html#mixed-precision",
    "href": "posts/pytorch-fabric/index.html#mixed-precision",
    "title": "PyTorch Lightning Fabric Code Guide",
    "section": "",
    "text": "# Enable mixed precision\nfabric = Fabric(precision=\"16-mixed\")  # or \"bf16-mixed\"\nfabric.launch()\n\n# Training remains the same - Fabric handles precision automatically\ndef train_with_amp(fabric, model, optimizer, dataloader, criterion):\n    model.train()\n    \n    for data, target in dataloader:\n        optimizer.zero_grad()\n        \n        # Forward pass (automatically uses mixed precision)\n        output = model(data)\n        loss = criterion(output, target)\n        \n        # Backward pass (handles gradient scaling)\n        fabric.backward(loss)\n        \n        optimizer.step()\n\n\n\nfrom lightning.fabric.utilities import rank_zero_only\n\n@rank_zero_only\ndef log_model_precision(model):\n    \"\"\"Log model parameter precisions (only on rank 0)\"\"\"\n    for name, param in model.named_parameters():\n        print(f\"{name}: {param.dtype}\")\n\n# Check model precision after setup\nmodel, optimizer = fabric.setup(model, optimizer)\nlog_model_precision(model)"
  },
  {
    "objectID": "posts/pytorch-fabric/index.html#logging-and-checkpointing",
    "href": "posts/pytorch-fabric/index.html#logging-and-checkpointing",
    "title": "PyTorch Lightning Fabric Code Guide",
    "section": "",
    "text": "import os\n\ndef save_checkpoint(fabric, model, optimizer, epoch, loss, path):\n    \"\"\"Save model checkpoint\"\"\"\n    checkpoint = {\n        \"model\": model,\n        \"optimizer\": optimizer,\n        \"epoch\": epoch,\n        \"loss\": loss\n    }\n    fabric.save(path, checkpoint)\n\ndef load_checkpoint(fabric, path):\n    \"\"\"Load model checkpoint\"\"\"\n    checkpoint = fabric.load(path)\n    return checkpoint\n\n# Save checkpoint\ncheckpoint_path = f\"checkpoint_epoch_{epoch}.ckpt\"\nsave_checkpoint(fabric, model, optimizer, epoch, train_loss, checkpoint_path)\n\n# Load checkpoint\nif os.path.exists(\"checkpoint_epoch_5.ckpt\"):\n    checkpoint = load_checkpoint(fabric, \"checkpoint_epoch_5.ckpt\")\n    model = checkpoint[\"model\"]\n    optimizer = checkpoint[\"optimizer\"]\n    start_epoch = checkpoint[\"epoch\"] + 1\n\n\n\nfrom lightning.fabric.loggers import TensorBoardLogger, CSVLogger\n\n# Initialize logger\nlogger = TensorBoardLogger(\"logs\", name=\"my_experiment\")\n\n# Setup Fabric with logger\nfabric = Fabric(loggers=[logger])\nfabric.launch()\n\n# Log metrics\ndef log_metrics(fabric, metrics, step):\n    for logger in fabric.loggers:\n        logger.log_metrics(metrics, step)\n\n# Usage in training loop\nfor epoch in range(num_epochs):\n    train_loss = train_epoch(fabric, model, optimizer, train_loader, criterion)\n    val_loss, val_acc = validate(fabric, model, val_loader, criterion)\n    \n    # Log metrics\n    metrics = {\n        \"train_loss\": train_loss,\n        \"val_loss\": val_loss,\n        \"val_accuracy\": val_acc,\n        \"learning_rate\": optimizer.param_groups[0]['lr']\n    }\n    log_metrics(fabric, metrics, epoch)"
  },
  {
    "objectID": "posts/pytorch-fabric/index.html#advanced-features",
    "href": "posts/pytorch-fabric/index.html#advanced-features",
    "title": "PyTorch Lightning Fabric Code Guide",
    "section": "",
    "text": "from lightning.fabric.plugins import MixedPrecisionPlugin\n\n# Custom precision configuration\nprecision_plugin = MixedPrecisionPlugin(\n    precision=\"16-mixed\",\n    device=\"cuda\",\n    scaler_kwargs={\"init_scale\": 2**16}\n)\n\nfabric = Fabric(plugins=[precision_plugin])\n\n\n\ndef train_with_grad_clipping(fabric, model, optimizer, dataloader, criterion, max_norm=1.0):\n    model.train()\n    \n    for data, target in dataloader:\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        \n        fabric.backward(loss)\n        \n        # Gradient clipping\n        fabric.clip_gradients(model, optimizer, max_norm=max_norm)\n        \n        optimizer.step()\n\n\n\nclass EarlyStopping:\n    def __init__(self, patience=10, min_delta=0.001):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = float('inf')\n    \n    def __call__(self, val_loss):\n        if val_loss &lt; self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n        \n        return self.counter &gt;= self.patience\n\n# Usage\nearly_stopping = EarlyStopping(patience=5)\n\nfor epoch in range(num_epochs):\n    train_loss = train_epoch(fabric, model, optimizer, train_loader, criterion)\n    val_loss, val_acc = validate(fabric, model, val_loader, criterion)\n    \n    if early_stopping(val_loss):\n        fabric.print(f\"Early stopping at epoch {epoch}\")\n        break"
  },
  {
    "objectID": "posts/pytorch-fabric/index.html#best-practices",
    "href": "posts/pytorch-fabric/index.html#best-practices",
    "title": "PyTorch Lightning Fabric Code Guide",
    "section": "",
    "text": "# Always use fabric.launch() for proper initialization\ndef main():\n    fabric = Fabric(accelerator=\"gpu\", devices=2)\n    fabric.launch()\n    \n    # Your training code here\n    model = create_model()\n    # ... rest of training\n\nif __name__ == \"__main__\":\n    main()\n\n\n\nfrom lightning.fabric.utilities import rank_zero_only\n\n@rank_zero_only\ndef save_model_artifacts(model, path):\n    \"\"\"Only save on rank 0 to avoid conflicts\"\"\"\n    torch.save(model.state_dict(), path)\n\n@rank_zero_only  \ndef print_training_info(epoch, loss):\n    \"\"\"Only print on rank 0 to avoid duplicate outputs\"\"\"\n    print(f\"Epoch {epoch}, Loss: {loss}\")\n\n\n\n# Let Fabric handle device placement\nfabric = Fabric()\nfabric.launch()\n\n# Don't manually move to device - Fabric handles this\n# BAD: model.to(device), data.to(device)\n# GOOD: Let fabric.setup() handle device placement\n\nmodel, optimizer = fabric.setup(model, optimizer)\ndataloader = fabric.setup_dataloaders(dataloader)\n\n\n\ndef memory_efficient_training(fabric, model, optimizer, dataloader, criterion):\n    model.train()\n    \n    for batch_idx, (data, target) in enumerate(dataloader):\n        optimizer.zero_grad()\n        \n        # Use gradient checkpointing for large models\n        if hasattr(model, 'gradient_checkpointing_enable'):\n            model.gradient_checkpointing_enable()\n        \n        output = model(data)\n        loss = criterion(output, target)\n        \n        fabric.backward(loss)\n        optimizer.step()\n        \n        # Clear cache periodically\n        if batch_idx % 100 == 0:\n            torch.cuda.empty_cache()\n\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom lightning.fabric import Fabric\nfrom lightning.fabric.utilities import rank_zero_only\n\ndef create_model():\n    return nn.Sequential(\n        nn.Linear(784, 256),\n        nn.ReLU(),\n        nn.Linear(256, 128),\n        nn.ReLU(),\n        nn.Linear(128, 10)\n    )\n\ndef train_epoch(fabric, model, optimizer, dataloader, criterion):\n    model.train()\n    total_loss = 0\n    \n    for data, target in dataloader:\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        fabric.backward(loss)\n        optimizer.step()\n        total_loss += loss.item()\n    \n    return total_loss / len(dataloader)\n\ndef main():\n    # Initialize Fabric\n    fabric = Fabric(\n        accelerator=\"auto\",\n        strategy=\"auto\",\n        devices=\"auto\",\n        precision=\"16-mixed\"\n    )\n    fabric.launch()\n    \n    # Create model, optimizer, data\n    model = create_model()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    \n    # Setup with Fabric\n    model, optimizer = fabric.setup(model, optimizer)\n    \n    # Training loop\n    criterion = nn.CrossEntropyLoss()\n    for epoch in range(10):\n        avg_loss = train_epoch(fabric, model, optimizer, dataloader, criterion)\n        \n        if fabric.is_global_zero:\n            print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\nThis guide covers the essential aspects of using Lightning Fabric for efficient PyTorch training. Fabric provides the perfect balance between control and convenience, making it ideal for researchers and practitioners who want distributed training capabilities without giving up flexibility in their training loops."
  },
  {
    "objectID": "posts/pytorch-to-pytorchlightning/index.html",
    "href": "posts/pytorch-to-pytorchlightning/index.html",
    "title": "PyTorch to PyTorch Lightning Migration Guide",
    "section": "",
    "text": "Introduction\nKey Concepts\nBasic Migration Steps\nCode Examples\nAdvanced Features\nBest Practices\nCommon Pitfalls\n\n\n\n\nPyTorch Lightning is a lightweight wrapper around PyTorch that eliminates boilerplate code while maintaining full control over your models. It provides a structured approach to organizing PyTorch code and includes built-in support for distributed training, logging, and experiment management.\n\n\n\nReduced Boilerplate: Lightning handles training loops, device management, and distributed training\nBetter Organization: Standardized code structure improves readability and maintenance\nBuilt-in Features: Automatic logging, checkpointing, early stopping, and more\nScalability: Easy multi-GPU and multi-node training\nReproducibility: Better experiment tracking and configuration management\n\n\n\n\n\n\n\nThe core abstraction that wraps your PyTorch model. It defines:\n\nModel architecture (__init__)\nForward pass (forward)\nTraining step (training_step)\nValidation step (validation_step)\nOptimizer configuration (configure_optimizers)\n\n\n\n\nHandles the training loop, device management, and various training configurations.\n\n\n\nEncapsulates data loading logic, including datasets and dataloaders.\n\n\n\n\n\n\nBefore (PyTorch):\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass SimpleModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\nAfter (Lightning):\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass LightningModel(pl.LightningModule):\n    def __init__(self, input_size, hidden_size, num_classes, learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        acc = (y_hat.argmax(dim=1) == y).float().mean()\n        self.log('val_loss', loss)\n        self.log('val_acc', acc)\n        \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n\n\n\nBefore (PyTorch):\n# Training loop\nmodel = SimpleModel(input_size=784, hidden_size=128, num_classes=10)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        \n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        \n        if batch_idx % 100 == 0:\n            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}')\n    \n    # Validation\n    model.eval()\n    val_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in val_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            val_loss += criterion(output, target).item()\n            pred = output.argmax(dim=1)\n            correct += pred.eq(target).sum().item()\n    \n    print(f'Validation Loss: {val_loss/len(val_loader)}, Accuracy: {correct/len(val_dataset)}')\nAfter (Lightning):\n# Training with Lightning\nmodel = LightningModel(input_size=784, hidden_size=128, num_classes=10)\ntrainer = pl.Trainer(max_epochs=10, accelerator='auto', devices='auto')\ntrainer.fit(model, train_loader, val_loader)\n\n\n\n\n\n\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\n\nclass MNISTDataModule(pl.LightningDataModule):\n    def __init__(self, data_dir: str = \"data/\", batch_size: int = 64):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n\n    def prepare_data(self):\n        # Download data\n        datasets.MNIST(self.data_dir, train=True, download=True)\n        datasets.MNIST(self.data_dir, train=False, download=True)\n\n    def setup(self, stage: str):\n        # Assign train/val datasets\n        if stage == 'fit':\n            mnist_full = datasets.MNIST(\n                self.data_dir, train=True, transform=self.transform\n            )\n            self.mnist_train, self.mnist_val = random_split(\n                mnist_full, [55000, 5000]\n            )\n\n        if stage == 'test':\n            self.mnist_test = datasets.MNIST(\n                self.data_dir, train=False, transform=self.transform\n            )\n\n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=self.batch_size, shuffle=True)\n\n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n\n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=self.batch_size)\n\nclass MNISTClassifier(pl.LightningModule):\n    def __init__(self, learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        self.log(\"train_loss\", loss, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = torch.sum(preds == y).float() / len(y)\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = torch.sum(preds == y).float() / len(y)\n        self.log(\"test_loss\", loss)\n        self.log(\"test_acc\", acc)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n\n# Usage\nif __name__ == \"__main__\":\n    # Initialize data module and model\n    dm = MNISTDataModule()\n    model = MNISTClassifier()\n    \n    # Initialize trainer\n    trainer = pl.Trainer(\n        max_epochs=5,\n        accelerator='auto',\n        devices='auto',\n        logger=pl.loggers.TensorBoardLogger('lightning_logs/'),\n        callbacks=[\n            pl.callbacks.EarlyStopping(monitor='val_loss', patience=3),\n            pl.callbacks.ModelCheckpoint(monitor='val_loss', save_top_k=1)\n        ]\n    )\n    \n    # Train the model\n    trainer.fit(model, dm)\n    \n    # Test the model\n    trainer.test(model, dm)\n\n\n\nimport torchmetrics\n\nclass AdvancedClassifier(pl.LightningModule):\n    def __init__(self, num_classes=10, learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Model layers\n        self.model = nn.Sequential(\n            nn.Linear(784, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, num_classes)\n        )\n        \n        # Metrics\n        self.train_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n        self.val_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n        self.val_f1 = torchmetrics.F1Score(task='multiclass', num_classes=num_classes)\n        \n    def forward(self, x):\n        return self.model(x.view(x.size(0), -1))\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        \n        # Log metrics\n        self.train_accuracy(y_hat, y)\n        self.log('train_loss', loss, on_step=True, on_epoch=True)\n        self.log('train_acc', self.train_accuracy, on_step=True, on_epoch=True)\n        \n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        \n        # Update metrics\n        self.val_accuracy(y_hat, y)\n        self.val_f1(y_hat, y)\n        \n        # Log metrics\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', self.val_accuracy, prog_bar=True)\n        self.log('val_f1', self.val_f1, prog_bar=True)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(\n            self.parameters(), \n            lr=self.hparams.learning_rate,\n            weight_decay=1e-4\n        )\n        \n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer, T_max=100\n        )\n        \n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"monitor\": \"val_loss\",\n                \"frequency\": 1\n            }\n        }\n\n\n\n\n\n\ndef configure_optimizers(self):\n    # Different learning rates for different parts\n    opt_g = torch.optim.Adam(self.generator.parameters(), lr=0.0002)\n    opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=0.0002)\n    \n    # Different schedulers\n    sch_g = torch.optim.lr_scheduler.StepLR(opt_g, step_size=50, gamma=0.5)\n    sch_d = torch.optim.lr_scheduler.StepLR(opt_d, step_size=50, gamma=0.5)\n    \n    return [opt_g, opt_d], [sch_g, sch_d]\n\n\n\nclass CustomCallback(pl.Callback):\n    def on_train_epoch_end(self, trainer, pl_module):\n        # Custom logic at end of each epoch\n        if trainer.current_epoch % 10 == 0:\n            print(f\"Completed {trainer.current_epoch} epochs\")\n    \n    def on_validation_end(self, trainer, pl_module):\n        # Custom validation logic\n        val_loss = trainer.callback_metrics.get('val_loss')\n        if val_loss and val_loss &lt; 0.1:\n            print(\"Excellent validation performance!\")\n\n# Usage\ntrainer = pl.Trainer(\n    callbacks=[CustomCallback(), pl.callbacks.EarlyStopping(monitor='val_loss')]\n)\n\n\n\nclass ManualOptimizationModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.automatic_optimization = False\n        # ... model definition\n        \n    def training_step(self, batch, batch_idx):\n        opt = self.optimizers()\n        \n        # Manual optimization\n        opt.zero_grad()\n        loss = self.compute_loss(batch)\n        self.manual_backward(loss)\n        opt.step()\n        \n        self.log('train_loss', loss)\n        return loss\n\n\n\n\n\n\nclass ConfigurableModel(pl.LightningModule):\n    def __init__(self, **kwargs):\n        super().__init__()\n        # Save all hyperparameters\n        self.save_hyperparameters()\n        \n        # Access with self.hparams\n        self.model = self._build_model()\n        \n    def _build_model(self):\n        return nn.Sequential(\n            nn.Linear(self.hparams.input_size, self.hparams.hidden_size),\n            nn.ReLU(),\n            nn.Linear(self.hparams.hidden_size, self.hparams.num_classes)\n        )\n\n\n\ndef training_step(self, batch, batch_idx):\n    loss = self.compute_loss(batch)\n    \n    # Log to both step and epoch\n    self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n    \n    # Log learning rate\n    self.log('lr', self.trainer.optimizers[0].param_groups[0]['lr'])\n    \n    return loss\n\n\n\n# Separate model definition from Lightning logic\nclass ResNetBackbone(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        # Model architecture here\n        \nclass ResNetLightning(pl.LightningModule):\n    def __init__(self, num_classes, learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Use separate model class\n        self.model = ResNetBackbone(num_classes)\n        \n    def forward(self, x):\n        return self.model(x)\n    \n    # Training logic here...\n\n\n\ndef validation_step(self, batch, batch_idx):\n    # Always include validation metrics\n    loss, acc = self._shared_eval_step(batch, batch_idx)\n    self.log_dict({'val_loss': loss, 'val_acc': acc}, prog_bar=True)\n    \ndef test_step(self, batch, batch_idx):\n    # Comprehensive test metrics\n    loss, acc = self._shared_eval_step(batch, batch_idx)\n    self.log_dict({'test_loss': loss, 'test_acc': acc})\n    \ndef _shared_eval_step(self, batch, batch_idx):\n    x, y = batch\n    y_hat = self(x)\n    loss = F.cross_entropy(y_hat, y)\n    acc = (y_hat.argmax(dim=1) == y).float().mean()\n    return loss, acc\n\n\n\n\n\n\nWrong:\ndef training_step(self, batch, batch_idx):\n    x, y = batch\n    x = x.cuda()  # Don't manually move to device\n    y = y.cuda()\n    # ...\nCorrect:\ndef training_step(self, batch, batch_idx):\n    x, y = batch  # Lightning handles device placement\n    # ...\n\n\n\nWrong:\ndef training_step(self, batch, batch_idx):\n    loss = self.compute_loss(batch)\n    loss.backward()  # Don't call backward manually\n    return loss\nCorrect:\ndef training_step(self, batch, batch_idx):\n    loss = self.compute_loss(batch)\n    return loss  # Lightning handles backward pass\n\n\n\nWrong:\ndef validation_step(self, batch, batch_idx):\n    # Computing metrics inside step leads to incorrect averages\n    acc = compute_accuracy(batch)\n    self.log('val_acc', acc.mean())\nCorrect:\ndef __init__(self):\n    super().__init__()\n    self.val_acc = torchmetrics.Accuracy()\n\ndef validation_step(self, batch, batch_idx):\n    # Let torchmetrics handle the averaging\n    y_hat = self(x)\n    self.val_acc(y_hat, y)\n    self.log('val_acc', self.val_acc)\n\n\n\nWrong:\nclass Model(pl.LightningModule):\n    def train_dataloader(self):\n        # Don't put data loading in model\n        return DataLoader(...)\nCorrect:\n# Use separate DataModule\nclass DataModule(pl.LightningDataModule):\n    def train_dataloader(self):\n        return DataLoader(...)\n\n# Or pass dataloaders to trainer.fit()\ntrainer.fit(model, train_dataloader, val_dataloader)\n\n\n\n\n\nConvert model class to inherit from pl.LightningModule\nImplement required methods: training_step, configure_optimizers\nAdd validation_step if you have validation data\nReplace manual training loop with pl.Trainer\nMove data loading to pl.LightningDataModule or separate functions\nAdd proper logging with self.log()\nUse self.save_hyperparameters() for configuration\nAdd callbacks for checkpointing, early stopping, etc.\nRemove manual device management (CUDA calls)\nTest with different accelerators (CPU, GPU, multi-GPU)\nUpdate any custom metrics to use torchmetrics\nVerify logging and experiment tracking works\nAdd proper test methods if needed\n\nBy following this guide, you should be able to successfully migrate your PyTorch code to PyTorch Lightning while maintaining all functionality and gaining the benefits of Lightningâ€™s structured approach."
  },
  {
    "objectID": "posts/pytorch-to-pytorchlightning/index.html#table-of-contents",
    "href": "posts/pytorch-to-pytorchlightning/index.html#table-of-contents",
    "title": "PyTorch to PyTorch Lightning Migration Guide",
    "section": "",
    "text": "Introduction\nKey Concepts\nBasic Migration Steps\nCode Examples\nAdvanced Features\nBest Practices\nCommon Pitfalls"
  },
  {
    "objectID": "posts/pytorch-to-pytorchlightning/index.html#introduction",
    "href": "posts/pytorch-to-pytorchlightning/index.html#introduction",
    "title": "PyTorch to PyTorch Lightning Migration Guide",
    "section": "",
    "text": "PyTorch Lightning is a lightweight wrapper around PyTorch that eliminates boilerplate code while maintaining full control over your models. It provides a structured approach to organizing PyTorch code and includes built-in support for distributed training, logging, and experiment management.\n\n\n\nReduced Boilerplate: Lightning handles training loops, device management, and distributed training\nBetter Organization: Standardized code structure improves readability and maintenance\nBuilt-in Features: Automatic logging, checkpointing, early stopping, and more\nScalability: Easy multi-GPU and multi-node training\nReproducibility: Better experiment tracking and configuration management"
  },
  {
    "objectID": "posts/pytorch-to-pytorchlightning/index.html#key-concepts",
    "href": "posts/pytorch-to-pytorchlightning/index.html#key-concepts",
    "title": "PyTorch to PyTorch Lightning Migration Guide",
    "section": "",
    "text": "The core abstraction that wraps your PyTorch model. It defines:\n\nModel architecture (__init__)\nForward pass (forward)\nTraining step (training_step)\nValidation step (validation_step)\nOptimizer configuration (configure_optimizers)\n\n\n\n\nHandles the training loop, device management, and various training configurations.\n\n\n\nEncapsulates data loading logic, including datasets and dataloaders."
  },
  {
    "objectID": "posts/pytorch-to-pytorchlightning/index.html#basic-migration-steps",
    "href": "posts/pytorch-to-pytorchlightning/index.html#basic-migration-steps",
    "title": "PyTorch to PyTorch Lightning Migration Guide",
    "section": "",
    "text": "Before (PyTorch):\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass SimpleModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\nAfter (Lightning):\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass LightningModel(pl.LightningModule):\n    def __init__(self, input_size, hidden_size, num_classes, learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        acc = (y_hat.argmax(dim=1) == y).float().mean()\n        self.log('val_loss', loss)\n        self.log('val_acc', acc)\n        \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n\n\n\nBefore (PyTorch):\n# Training loop\nmodel = SimpleModel(input_size=784, hidden_size=128, num_classes=10)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        \n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        \n        if batch_idx % 100 == 0:\n            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}')\n    \n    # Validation\n    model.eval()\n    val_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in val_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            val_loss += criterion(output, target).item()\n            pred = output.argmax(dim=1)\n            correct += pred.eq(target).sum().item()\n    \n    print(f'Validation Loss: {val_loss/len(val_loader)}, Accuracy: {correct/len(val_dataset)}')\nAfter (Lightning):\n# Training with Lightning\nmodel = LightningModel(input_size=784, hidden_size=128, num_classes=10)\ntrainer = pl.Trainer(max_epochs=10, accelerator='auto', devices='auto')\ntrainer.fit(model, train_loader, val_loader)"
  },
  {
    "objectID": "posts/pytorch-to-pytorchlightning/index.html#code-examples",
    "href": "posts/pytorch-to-pytorchlightning/index.html#code-examples",
    "title": "PyTorch to PyTorch Lightning Migration Guide",
    "section": "",
    "text": "import pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\n\nclass MNISTDataModule(pl.LightningDataModule):\n    def __init__(self, data_dir: str = \"data/\", batch_size: int = 64):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n\n    def prepare_data(self):\n        # Download data\n        datasets.MNIST(self.data_dir, train=True, download=True)\n        datasets.MNIST(self.data_dir, train=False, download=True)\n\n    def setup(self, stage: str):\n        # Assign train/val datasets\n        if stage == 'fit':\n            mnist_full = datasets.MNIST(\n                self.data_dir, train=True, transform=self.transform\n            )\n            self.mnist_train, self.mnist_val = random_split(\n                mnist_full, [55000, 5000]\n            )\n\n        if stage == 'test':\n            self.mnist_test = datasets.MNIST(\n                self.data_dir, train=False, transform=self.transform\n            )\n\n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=self.batch_size, shuffle=True)\n\n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n\n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=self.batch_size)\n\nclass MNISTClassifier(pl.LightningModule):\n    def __init__(self, learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        self.log(\"train_loss\", loss, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = torch.sum(preds == y).float() / len(y)\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = torch.sum(preds == y).float() / len(y)\n        self.log(\"test_loss\", loss)\n        self.log(\"test_acc\", acc)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n\n# Usage\nif __name__ == \"__main__\":\n    # Initialize data module and model\n    dm = MNISTDataModule()\n    model = MNISTClassifier()\n    \n    # Initialize trainer\n    trainer = pl.Trainer(\n        max_epochs=5,\n        accelerator='auto',\n        devices='auto',\n        logger=pl.loggers.TensorBoardLogger('lightning_logs/'),\n        callbacks=[\n            pl.callbacks.EarlyStopping(monitor='val_loss', patience=3),\n            pl.callbacks.ModelCheckpoint(monitor='val_loss', save_top_k=1)\n        ]\n    )\n    \n    # Train the model\n    trainer.fit(model, dm)\n    \n    # Test the model\n    trainer.test(model, dm)\n\n\n\nimport torchmetrics\n\nclass AdvancedClassifier(pl.LightningModule):\n    def __init__(self, num_classes=10, learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Model layers\n        self.model = nn.Sequential(\n            nn.Linear(784, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, num_classes)\n        )\n        \n        # Metrics\n        self.train_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n        self.val_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n        self.val_f1 = torchmetrics.F1Score(task='multiclass', num_classes=num_classes)\n        \n    def forward(self, x):\n        return self.model(x.view(x.size(0), -1))\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        \n        # Log metrics\n        self.train_accuracy(y_hat, y)\n        self.log('train_loss', loss, on_step=True, on_epoch=True)\n        self.log('train_acc', self.train_accuracy, on_step=True, on_epoch=True)\n        \n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        \n        # Update metrics\n        self.val_accuracy(y_hat, y)\n        self.val_f1(y_hat, y)\n        \n        # Log metrics\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', self.val_accuracy, prog_bar=True)\n        self.log('val_f1', self.val_f1, prog_bar=True)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(\n            self.parameters(), \n            lr=self.hparams.learning_rate,\n            weight_decay=1e-4\n        )\n        \n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer, T_max=100\n        )\n        \n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"monitor\": \"val_loss\",\n                \"frequency\": 1\n            }\n        }"
  },
  {
    "objectID": "posts/pytorch-to-pytorchlightning/index.html#advanced-features",
    "href": "posts/pytorch-to-pytorchlightning/index.html#advanced-features",
    "title": "PyTorch to PyTorch Lightning Migration Guide",
    "section": "",
    "text": "def configure_optimizers(self):\n    # Different learning rates for different parts\n    opt_g = torch.optim.Adam(self.generator.parameters(), lr=0.0002)\n    opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=0.0002)\n    \n    # Different schedulers\n    sch_g = torch.optim.lr_scheduler.StepLR(opt_g, step_size=50, gamma=0.5)\n    sch_d = torch.optim.lr_scheduler.StepLR(opt_d, step_size=50, gamma=0.5)\n    \n    return [opt_g, opt_d], [sch_g, sch_d]\n\n\n\nclass CustomCallback(pl.Callback):\n    def on_train_epoch_end(self, trainer, pl_module):\n        # Custom logic at end of each epoch\n        if trainer.current_epoch % 10 == 0:\n            print(f\"Completed {trainer.current_epoch} epochs\")\n    \n    def on_validation_end(self, trainer, pl_module):\n        # Custom validation logic\n        val_loss = trainer.callback_metrics.get('val_loss')\n        if val_loss and val_loss &lt; 0.1:\n            print(\"Excellent validation performance!\")\n\n# Usage\ntrainer = pl.Trainer(\n    callbacks=[CustomCallback(), pl.callbacks.EarlyStopping(monitor='val_loss')]\n)\n\n\n\nclass ManualOptimizationModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.automatic_optimization = False\n        # ... model definition\n        \n    def training_step(self, batch, batch_idx):\n        opt = self.optimizers()\n        \n        # Manual optimization\n        opt.zero_grad()\n        loss = self.compute_loss(batch)\n        self.manual_backward(loss)\n        opt.step()\n        \n        self.log('train_loss', loss)\n        return loss"
  },
  {
    "objectID": "posts/pytorch-to-pytorchlightning/index.html#best-practices",
    "href": "posts/pytorch-to-pytorchlightning/index.html#best-practices",
    "title": "PyTorch to PyTorch Lightning Migration Guide",
    "section": "",
    "text": "class ConfigurableModel(pl.LightningModule):\n    def __init__(self, **kwargs):\n        super().__init__()\n        # Save all hyperparameters\n        self.save_hyperparameters()\n        \n        # Access with self.hparams\n        self.model = self._build_model()\n        \n    def _build_model(self):\n        return nn.Sequential(\n            nn.Linear(self.hparams.input_size, self.hparams.hidden_size),\n            nn.ReLU(),\n            nn.Linear(self.hparams.hidden_size, self.hparams.num_classes)\n        )\n\n\n\ndef training_step(self, batch, batch_idx):\n    loss = self.compute_loss(batch)\n    \n    # Log to both step and epoch\n    self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n    \n    # Log learning rate\n    self.log('lr', self.trainer.optimizers[0].param_groups[0]['lr'])\n    \n    return loss\n\n\n\n# Separate model definition from Lightning logic\nclass ResNetBackbone(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        # Model architecture here\n        \nclass ResNetLightning(pl.LightningModule):\n    def __init__(self, num_classes, learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Use separate model class\n        self.model = ResNetBackbone(num_classes)\n        \n    def forward(self, x):\n        return self.model(x)\n    \n    # Training logic here...\n\n\n\ndef validation_step(self, batch, batch_idx):\n    # Always include validation metrics\n    loss, acc = self._shared_eval_step(batch, batch_idx)\n    self.log_dict({'val_loss': loss, 'val_acc': acc}, prog_bar=True)\n    \ndef test_step(self, batch, batch_idx):\n    # Comprehensive test metrics\n    loss, acc = self._shared_eval_step(batch, batch_idx)\n    self.log_dict({'test_loss': loss, 'test_acc': acc})\n    \ndef _shared_eval_step(self, batch, batch_idx):\n    x, y = batch\n    y_hat = self(x)\n    loss = F.cross_entropy(y_hat, y)\n    acc = (y_hat.argmax(dim=1) == y).float().mean()\n    return loss, acc"
  },
  {
    "objectID": "posts/pytorch-to-pytorchlightning/index.html#common-pitfalls",
    "href": "posts/pytorch-to-pytorchlightning/index.html#common-pitfalls",
    "title": "PyTorch to PyTorch Lightning Migration Guide",
    "section": "",
    "text": "Wrong:\ndef training_step(self, batch, batch_idx):\n    x, y = batch\n    x = x.cuda()  # Don't manually move to device\n    y = y.cuda()\n    # ...\nCorrect:\ndef training_step(self, batch, batch_idx):\n    x, y = batch  # Lightning handles device placement\n    # ...\n\n\n\nWrong:\ndef training_step(self, batch, batch_idx):\n    loss = self.compute_loss(batch)\n    loss.backward()  # Don't call backward manually\n    return loss\nCorrect:\ndef training_step(self, batch, batch_idx):\n    loss = self.compute_loss(batch)\n    return loss  # Lightning handles backward pass\n\n\n\nWrong:\ndef validation_step(self, batch, batch_idx):\n    # Computing metrics inside step leads to incorrect averages\n    acc = compute_accuracy(batch)\n    self.log('val_acc', acc.mean())\nCorrect:\ndef __init__(self):\n    super().__init__()\n    self.val_acc = torchmetrics.Accuracy()\n\ndef validation_step(self, batch, batch_idx):\n    # Let torchmetrics handle the averaging\n    y_hat = self(x)\n    self.val_acc(y_hat, y)\n    self.log('val_acc', self.val_acc)\n\n\n\nWrong:\nclass Model(pl.LightningModule):\n    def train_dataloader(self):\n        # Don't put data loading in model\n        return DataLoader(...)\nCorrect:\n# Use separate DataModule\nclass DataModule(pl.LightningDataModule):\n    def train_dataloader(self):\n        return DataLoader(...)\n\n# Or pass dataloaders to trainer.fit()\ntrainer.fit(model, train_dataloader, val_dataloader)"
  },
  {
    "objectID": "posts/pytorch-to-pytorchlightning/index.html#migration-checklist",
    "href": "posts/pytorch-to-pytorchlightning/index.html#migration-checklist",
    "title": "PyTorch to PyTorch Lightning Migration Guide",
    "section": "",
    "text": "Convert model class to inherit from pl.LightningModule\nImplement required methods: training_step, configure_optimizers\nAdd validation_step if you have validation data\nReplace manual training loop with pl.Trainer\nMove data loading to pl.LightningDataModule or separate functions\nAdd proper logging with self.log()\nUse self.save_hyperparameters() for configuration\nAdd callbacks for checkpointing, early stopping, etc.\nRemove manual device management (CUDA calls)\nTest with different accelerators (CPU, GPU, multi-GPU)\nUpdate any custom metrics to use torchmetrics\nVerify logging and experiment tracking works\nAdd proper test methods if needed\n\nBy following this guide, you should be able to successfully migrate your PyTorch code to PyTorch Lightning while maintaining all functionality and gaining the benefits of Lightningâ€™s structured approach."
  },
  {
    "objectID": "posts/kubeflow-explain/index.html",
    "href": "posts/kubeflow-explain/index.html",
    "title": "Kubeflow: A Comprehensive Guide to Machine Learning on Kubernetes",
    "section": "",
    "text": "Kubeflow is an open-source machine learning platform designed to make deployments of machine learning workflows on Kubernetes simple, portable, and scalable. Originally developed by Google and now maintained by the Kubeflow community, it provides a comprehensive ecosystem for managing the entire machine learning lifecycleâ€”from experimentation and training to serving and monitoringâ€”all within a Kubernetes environment.\nThe platform addresses one of the most significant challenges in modern machine learning: bridging the gap between data science experimentation and production deployment. By leveraging Kubernetesâ€™ container orchestration capabilities, Kubeflow enables ML teams to build, deploy, and manage machine learning systems at scale while maintaining consistency across different environments.\n\n\n\n\n\nKubeflow follows a microservices architecture built on top of Kubernetes. The platform consists of several interconnected components, each serving specific functions in the ML workflow:\nCentral Dashboard: The web-based user interface that provides a unified view of all Kubeflow components and allows users to manage their ML workflows through a single interface.\nKubeflow Pipelines: A comprehensive solution for building and deploying portable, scalable machine learning workflows based on Docker containers. It includes a user interface for managing and tracking experiments, jobs, and runs.\nKubeflow Notebooks: Provides Jupyter notebook servers for interactive development and experimentation. These notebooks run as Kubernetes pods and can be configured with different resource requirements and ML frameworks.\nKatib: An automated machine learning system for hyperparameter tuning and neural architecture search. It supports various optimization algorithms and can run experiments across multiple nodes.\nKServe (formerly KFServing): A serverless inferencing platform that provides standardized model serving capabilities with features like canary deployments, autoscaling, and multi-framework support.\nTraining Operators: A collection of Kubernetes operators for distributed training across different ML frameworks including TensorFlow, PyTorch, MPI, XGBoost, and PaddlePaddle.\n\n\n\n\n\nKubeflow Pipelines represents the workflow orchestration heart of the platform. It enables users to define, deploy, and manage end-to-end ML workflows as code. Key features include:\nPipeline Definition: Workflows are defined using the Kubeflow Pipelines SDK, which allows data scientists to create reproducible, parameterized pipelines using Python. Each pipeline consists of multiple components that can be reused across different workflows.\nComponent Library: A rich ecosystem of pre-built components for common ML tasks such as data preprocessing, model training, evaluation, and deployment. Users can also create custom components using containerized applications.\nExperiment Management: Built-in experiment tracking capabilities that allow teams to compare different pipeline runs, track metrics, and manage model versions systematically.\nArtifact Management: Automatic tracking and versioning of pipeline artifacts including datasets, models, and intermediate results, enabling full reproducibility of ML experiments.\n\n\n\nThe notebook component provides a managed Jupyter environment optimized for machine learning workloads:\nMulti-Framework Support: Pre-configured notebook images with popular ML frameworks like TensorFlow, PyTorch, scikit-learn, and R, eliminating environment setup overhead.\nResource Management: Dynamic resource allocation allowing users to specify CPU, memory, and GPU requirements for their notebook servers based on workload demands.\nPersistent Storage: Integration with Kubernetes persistent volumes ensures that notebook work persists across server restarts and provides shared storage capabilities for team collaboration.\nCustom Images: Support for custom Docker images enables teams to create standardized environments with specific tool configurations and dependencies.\n\n\n\nKatib provides automated machine learning capabilities focused on hyperparameter optimization and neural architecture search:\nOptimization Algorithms: Support for various optimization strategies including random search, grid search, Bayesian optimization, and evolutionary algorithms.\nParallel Execution: Distributed hyperparameter tuning across multiple nodes, significantly reducing experiment time for computationally intensive tasks.\nEarly Stopping: Intelligent early stopping mechanisms that terminate underperforming trials, optimizing resource utilization.\nMulti-Objective Optimization: Support for optimizing multiple metrics simultaneously, useful for scenarios requiring trade-offs between accuracy, latency, and model size.\n\n\n\nKServe provides enterprise-grade model serving capabilities:\nServerless Scaling: Automatic scaling to zero when no requests are being processed, and rapid scale-up based on incoming traffic patterns.\nMulti-Framework Support: Native support for TensorFlow, PyTorch, scikit-learn, XGBoost, and custom serving runtimes through standardized prediction protocols.\nAdvanced Deployment Strategies: Built-in support for canary deployments, A/B testing, and blue-green deployments for safe model rollouts.\nExplainability Integration: Integration with explainability frameworks to provide model interpretability alongside predictions.\n\n\n\n\n\n\n\nBefore installing Kubeflow, ensure you have:\nKubernetes Cluster: A functioning Kubernetes cluster (version 1.21 or later recommended) with sufficient resources. For production deployments, consider managed Kubernetes services like Google GKE, Amazon EKS, or Azure AKS.\nStorage: Persistent storage capabilities, preferably with dynamic provisioning support for optimal resource management.\nNetwork Configuration: Proper ingress configuration for external access to the Kubeflow dashboard and services.\nResource Requirements: Minimum 4 CPU cores and 16GB RAM for basic installations, with additional resources needed based on workload requirements.\n\n\n\n\n\nThe most straightforward installation method uses Kubeflow manifests:\n# Clone the manifests repository\ngit clone https://github.com/kubeflow/manifests.git\ncd manifests\n\n# Install Kubeflow components\nwhile ! kustomize build example | kubectl apply -f -; do echo \"Retrying to apply resources\"; sleep 10; done\nThis method provides fine-grained control over component selection and configuration but requires manual management of dependencies and updates.\n\n\n\nGoogle Cloud: Use Google Cloud AI Platform Pipelines or deploy Kubeflow on GKE with optimized configurations for Google Cloud services.\nAWS: Leverage AWS-specific distributions like Kubeflow on Amazon EKS, which provides pre-configured integrations with AWS services like S3, IAM, and CloudWatch.\nAzure: Use Azure Machine Learning or deploy Kubeflow on AKS with Azure-specific optimizations and service integrations.\n\n\n\n\nAfter installation, configure essential settings:\nAuthentication: Set up appropriate authentication mechanisms, whether through Kubernetes RBAC, external identity providers like OIDC, or platform-specific authentication systems.\nStorage Classes: Configure storage classes for different workload types, ensuring appropriate performance characteristics for training jobs, notebooks, and pipeline artifacts.\nResource Quotas: Establish resource quotas and limits to prevent resource contention and ensure fair resource allocation across users and teams.\nMonitoring: Deploy monitoring solutions like Prometheus and Grafana to track cluster health, resource utilization, and application performance.\n\n\n\n\n\n\nKubeflow Pipelines are built from reusable components, each encapsulating a specific ML task:\nLightweight Components: Python functions that can be converted into pipeline components with minimal overhead, suitable for simple data processing tasks.\nContainerized Components: More complex components packaged as Docker containers, providing isolation and reproducibility for sophisticated ML operations.\nPre-built Components: Community-contributed components available through the Kubeflow Pipelines component hub, covering common ML operations like data validation, feature engineering, and model evaluation.\n\n\n\nDesign Phase: Define the overall workflow structure, identifying key stages like data ingestion, preprocessing, training, evaluation, and deployment.\nComponent Development: Create or select appropriate components for each pipeline stage, ensuring proper input/output specifications and parameter definitions.\nPipeline Assembly: Use the Kubeflow Pipelines SDK to connect components, define data flow, and specify execution dependencies.\nTesting and Validation: Test pipeline components individually and as complete workflows using smaller datasets before production deployment.\n\n\n\nModularity: Design components to be as modular and reusable as possible, enabling easier maintenance and testing.\nParameterization: Make pipelines highly parameterizable to support different datasets, model configurations, and deployment targets without code changes.\nError Handling: Implement comprehensive error handling and logging within components to facilitate debugging and monitoring.\nVersion Control: Maintain proper version control for both pipeline definitions and component implementations to enable rollbacks and reproducibility.\n\n\n\n\n\n\nKubeflow supports distributed training across multiple frameworks:\nTensorFlow Training: The TFJob operator enables distributed TensorFlow training with parameter servers or all-reduce strategies, automatically handling worker coordination and failure recovery.\nPyTorch Training: PyTorchJob operator supports distributed PyTorch training using various backends like NCCL and Gloo, with automatic scaling and fault tolerance.\nMPI Training: For frameworks that support MPI-based distributed training, the MPIJob operator provides seamless integration with message-passing interfaces.\n\n\n\nExperiment Tracking: Kubeflow Pipelines automatically tracks experiment metadata, including parameters, metrics, and artifacts, enabling comprehensive experiment comparison and analysis.\nHyperparameter Tuning: Katib integration allows for sophisticated hyperparameter optimization experiments with support for various search algorithms and early stopping strategies.\nModel Versioning: Built-in model versioning capabilities track model evolution over time, supporting model lineage and reproducibility requirements.\n\n\n\nAuto-scaling: Dynamic resource allocation based on training workload requirements, optimizing cost and performance.\nGPU Scheduling: Intelligent GPU scheduling and sharing capabilities to maximize utilization of expensive GPU resources.\nSpot Instance Support: Integration with cloud provider spot instances for cost-effective training of non-critical workloads.\n\n\n\n\n\n\nReal-time Serving: Low-latency serving for applications requiring immediate responses, with support for high-throughput scenarios.\nBatch Prediction: Efficient batch processing capabilities for scenarios where predictions can be computed offline or in batches.\nEdge Deployment: Support for deploying models to edge devices and environments with limited resources.\n\n\n\nCanary Deployments: Gradual rollout of new model versions to a subset of traffic, enabling safe deployment with minimal risk.\nA/B Testing: Side-by-side comparison of different model versions to evaluate performance improvements and business impact.\nShadow Deployment: Deploy new models alongside existing ones to evaluate performance without affecting production traffic.\n\n\n\nPerformance Monitoring: Continuous tracking of model performance metrics like accuracy, latency, and throughput.\nData Drift Detection: Monitoring for changes in input data distribution that might affect model performance.\nModel Explainability: Integration with explainability tools to provide insights into model predictions and decision-making processes.\n\n\n\n\n\n\nData Pipeline Integration: Seamless integration with data pipeline tools like Apache Airflow, allowing for end-to-end data-to-model workflows.\nFeature Store Integration: Support for feature stores like Feast, enabling consistent feature engineering across training and serving environments.\nData Versioning: Integration with data versioning tools like DVC or Pachyderm for reproducible data management.\n\n\n\nCI/CD Integration: Support for continuous integration and deployment pipelines, enabling automated model training, testing, and deployment.\nModel Registry: Integration with model registries like MLflow for centralized model management and lifecycle tracking.\nMonitoring and Observability: Integration with observability platforms for comprehensive monitoring of ML system health and performance.\n\n\n\nAWS Integration: Native support for AWS services like S3 for storage, IAM for authentication, and CloudWatch for monitoring.\nGoogle Cloud Integration: Deep integration with Google Cloud services including BigQuery, Cloud Storage, and AI Platform services.\nAzure Integration: Support for Azure services like Azure Blob Storage, Azure Active Directory, and Azure Monitor.\n\n\n\n\n\n\nAuthentication and Authorization: Implement proper authentication mechanisms and role-based access control to secure ML workloads and data.\nNetwork Security: Use network policies and service meshes to secure communication between components and external services.\nSecret Management: Proper management of secrets and credentials using Kubernetes secrets or external secret management systems.\nContainer Security: Regular scanning of container images for vulnerabilities and use of minimal, hardened base images.\n\n\n\nResource Planning: Careful planning of compute resources based on workload characteristics and performance requirements.\nStorage Optimization: Choose appropriate storage solutions based on access patterns, performance requirements, and cost considerations.\nNetwork Optimization: Optimize network configuration for data-intensive workloads, particularly for distributed training scenarios.\nCaching Strategies: Implement appropriate caching strategies for frequently accessed data and model artifacts.\n\n\n\nMonitoring and Alerting: Comprehensive monitoring of system health, resource utilization, and application performance with appropriate alerting mechanisms.\nBackup and Recovery: Regular backups of critical data and configurations with tested recovery procedures.\nDocumentation: Maintain comprehensive documentation of system architecture, operational procedures, and troubleshooting guides.\nTraining and Support: Ensure team members are properly trained on Kubeflow operations and best practices.\n\n\n\n\n\n\nLarge enterprises use Kubeflow to standardize their ML infrastructure across multiple teams and projects, providing consistent tooling and workflows while maintaining flexibility for different use cases.\n\n\n\nAcademic and research institutions leverage Kubeflowâ€™s flexibility and scalability to support diverse research projects with varying computational requirements and experimental approaches.\n\n\n\nSmaller organizations use Kubeflow to access enterprise-grade ML infrastructure without the overhead of building and maintaining custom solutions, accelerating their time to market.\n\n\n\nFinancial Services: Risk modeling, fraud detection, and algorithmic trading applications benefit from Kubeflowâ€™s scalability and compliance capabilities.\nHealthcare: Medical imaging, drug discovery, and clinical decision support systems leverage Kubeflowâ€™s robust pipeline management and model serving capabilities.\nRetail and E-commerce: Recommendation systems, demand forecasting, and personalization engines use Kubeflowâ€™s ability to handle large-scale, real-time ML workloads.\n\n\n\n\n\n\nAutoML Integration: Enhanced integration with automated machine learning tools and techniques for democratizing ML development.\nEdge Computing: Improved support for edge deployment scenarios with optimized resource utilization and offline capabilities.\nFederated Learning: Native support for federated learning scenarios where data cannot be centralized due to privacy or regulatory constraints.\n\n\n\nComponent Ecosystem: Continued growth of the component ecosystem with contributions from the broader ML community.\nIntegration Partnerships: Expanding partnerships with cloud providers, ML tool vendors, and open-source projects to enhance the platformâ€™s capabilities.\nStandards Adoption: Participation in industry standards development to ensure compatibility and interoperability with other ML platforms and tools.\n\n\n\n\nKubeflow represents a significant advancement in making machine learning workflows more scalable, reproducible, and manageable. By leveraging Kubernetesâ€™ container orchestration capabilities, it provides a comprehensive platform that addresses the full spectrum of ML lifecycle management needs.\nThe platformâ€™s strength lies in its modularity and extensibility, allowing organizations to adopt components incrementally based on their specific requirements and maturity levels. Whether youâ€™re a startup looking to establish ML infrastructure or an enterprise seeking to standardize ML operations across multiple teams, Kubeflow provides the foundation for building robust, scalable ML systems.\nAs the machine learning landscape continues to evolve, Kubeflowâ€™s active community and vendor-neutral approach position it well to adapt to emerging technologies and methodologies. Organizations investing in Kubeflow today are building on a platform designed to grow with their ML maturity and requirements, providing a solid foundation for long-term ML success.\nThe key to successful Kubeflow adoption lies in understanding your organizationâ€™s specific requirements, starting with pilot projects to build expertise, and gradually expanding usage as teams become more comfortable with the platform. With proper planning and implementation, Kubeflow can significantly accelerate your organizationâ€™s ML capabilities while maintaining the operational excellence required for production ML systems."
  },
  {
    "objectID": "posts/kubeflow-explain/index.html#introduction",
    "href": "posts/kubeflow-explain/index.html#introduction",
    "title": "Kubeflow: A Comprehensive Guide to Machine Learning on Kubernetes",
    "section": "",
    "text": "Kubeflow is an open-source machine learning platform designed to make deployments of machine learning workflows on Kubernetes simple, portable, and scalable. Originally developed by Google and now maintained by the Kubeflow community, it provides a comprehensive ecosystem for managing the entire machine learning lifecycleâ€”from experimentation and training to serving and monitoringâ€”all within a Kubernetes environment.\nThe platform addresses one of the most significant challenges in modern machine learning: bridging the gap between data science experimentation and production deployment. By leveraging Kubernetesâ€™ container orchestration capabilities, Kubeflow enables ML teams to build, deploy, and manage machine learning systems at scale while maintaining consistency across different environments."
  },
  {
    "objectID": "posts/kubeflow-explain/index.html#architecture-and-core-components",
    "href": "posts/kubeflow-explain/index.html#architecture-and-core-components",
    "title": "Kubeflow: A Comprehensive Guide to Machine Learning on Kubernetes",
    "section": "",
    "text": "Kubeflow follows a microservices architecture built on top of Kubernetes. The platform consists of several interconnected components, each serving specific functions in the ML workflow:\nCentral Dashboard: The web-based user interface that provides a unified view of all Kubeflow components and allows users to manage their ML workflows through a single interface.\nKubeflow Pipelines: A comprehensive solution for building and deploying portable, scalable machine learning workflows based on Docker containers. It includes a user interface for managing and tracking experiments, jobs, and runs.\nKubeflow Notebooks: Provides Jupyter notebook servers for interactive development and experimentation. These notebooks run as Kubernetes pods and can be configured with different resource requirements and ML frameworks.\nKatib: An automated machine learning system for hyperparameter tuning and neural architecture search. It supports various optimization algorithms and can run experiments across multiple nodes.\nKServe (formerly KFServing): A serverless inferencing platform that provides standardized model serving capabilities with features like canary deployments, autoscaling, and multi-framework support.\nTraining Operators: A collection of Kubernetes operators for distributed training across different ML frameworks including TensorFlow, PyTorch, MPI, XGBoost, and PaddlePaddle.\n\n\n\n\n\nKubeflow Pipelines represents the workflow orchestration heart of the platform. It enables users to define, deploy, and manage end-to-end ML workflows as code. Key features include:\nPipeline Definition: Workflows are defined using the Kubeflow Pipelines SDK, which allows data scientists to create reproducible, parameterized pipelines using Python. Each pipeline consists of multiple components that can be reused across different workflows.\nComponent Library: A rich ecosystem of pre-built components for common ML tasks such as data preprocessing, model training, evaluation, and deployment. Users can also create custom components using containerized applications.\nExperiment Management: Built-in experiment tracking capabilities that allow teams to compare different pipeline runs, track metrics, and manage model versions systematically.\nArtifact Management: Automatic tracking and versioning of pipeline artifacts including datasets, models, and intermediate results, enabling full reproducibility of ML experiments.\n\n\n\nThe notebook component provides a managed Jupyter environment optimized for machine learning workloads:\nMulti-Framework Support: Pre-configured notebook images with popular ML frameworks like TensorFlow, PyTorch, scikit-learn, and R, eliminating environment setup overhead.\nResource Management: Dynamic resource allocation allowing users to specify CPU, memory, and GPU requirements for their notebook servers based on workload demands.\nPersistent Storage: Integration with Kubernetes persistent volumes ensures that notebook work persists across server restarts and provides shared storage capabilities for team collaboration.\nCustom Images: Support for custom Docker images enables teams to create standardized environments with specific tool configurations and dependencies.\n\n\n\nKatib provides automated machine learning capabilities focused on hyperparameter optimization and neural architecture search:\nOptimization Algorithms: Support for various optimization strategies including random search, grid search, Bayesian optimization, and evolutionary algorithms.\nParallel Execution: Distributed hyperparameter tuning across multiple nodes, significantly reducing experiment time for computationally intensive tasks.\nEarly Stopping: Intelligent early stopping mechanisms that terminate underperforming trials, optimizing resource utilization.\nMulti-Objective Optimization: Support for optimizing multiple metrics simultaneously, useful for scenarios requiring trade-offs between accuracy, latency, and model size.\n\n\n\nKServe provides enterprise-grade model serving capabilities:\nServerless Scaling: Automatic scaling to zero when no requests are being processed, and rapid scale-up based on incoming traffic patterns.\nMulti-Framework Support: Native support for TensorFlow, PyTorch, scikit-learn, XGBoost, and custom serving runtimes through standardized prediction protocols.\nAdvanced Deployment Strategies: Built-in support for canary deployments, A/B testing, and blue-green deployments for safe model rollouts.\nExplainability Integration: Integration with explainability frameworks to provide model interpretability alongside predictions."
  },
  {
    "objectID": "posts/kubeflow-explain/index.html#installation-and-setup",
    "href": "posts/kubeflow-explain/index.html#installation-and-setup",
    "title": "Kubeflow: A Comprehensive Guide to Machine Learning on Kubernetes",
    "section": "",
    "text": "Before installing Kubeflow, ensure you have:\nKubernetes Cluster: A functioning Kubernetes cluster (version 1.21 or later recommended) with sufficient resources. For production deployments, consider managed Kubernetes services like Google GKE, Amazon EKS, or Azure AKS.\nStorage: Persistent storage capabilities, preferably with dynamic provisioning support for optimal resource management.\nNetwork Configuration: Proper ingress configuration for external access to the Kubeflow dashboard and services.\nResource Requirements: Minimum 4 CPU cores and 16GB RAM for basic installations, with additional resources needed based on workload requirements.\n\n\n\n\n\nThe most straightforward installation method uses Kubeflow manifests:\n# Clone the manifests repository\ngit clone https://github.com/kubeflow/manifests.git\ncd manifests\n\n# Install Kubeflow components\nwhile ! kustomize build example | kubectl apply -f -; do echo \"Retrying to apply resources\"; sleep 10; done\nThis method provides fine-grained control over component selection and configuration but requires manual management of dependencies and updates.\n\n\n\nGoogle Cloud: Use Google Cloud AI Platform Pipelines or deploy Kubeflow on GKE with optimized configurations for Google Cloud services.\nAWS: Leverage AWS-specific distributions like Kubeflow on Amazon EKS, which provides pre-configured integrations with AWS services like S3, IAM, and CloudWatch.\nAzure: Use Azure Machine Learning or deploy Kubeflow on AKS with Azure-specific optimizations and service integrations.\n\n\n\n\nAfter installation, configure essential settings:\nAuthentication: Set up appropriate authentication mechanisms, whether through Kubernetes RBAC, external identity providers like OIDC, or platform-specific authentication systems.\nStorage Classes: Configure storage classes for different workload types, ensuring appropriate performance characteristics for training jobs, notebooks, and pipeline artifacts.\nResource Quotas: Establish resource quotas and limits to prevent resource contention and ensure fair resource allocation across users and teams.\nMonitoring: Deploy monitoring solutions like Prometheus and Grafana to track cluster health, resource utilization, and application performance."
  },
  {
    "objectID": "posts/kubeflow-explain/index.html#building-ml-pipelines",
    "href": "posts/kubeflow-explain/index.html#building-ml-pipelines",
    "title": "Kubeflow: A Comprehensive Guide to Machine Learning on Kubernetes",
    "section": "",
    "text": "Kubeflow Pipelines are built from reusable components, each encapsulating a specific ML task:\nLightweight Components: Python functions that can be converted into pipeline components with minimal overhead, suitable for simple data processing tasks.\nContainerized Components: More complex components packaged as Docker containers, providing isolation and reproducibility for sophisticated ML operations.\nPre-built Components: Community-contributed components available through the Kubeflow Pipelines component hub, covering common ML operations like data validation, feature engineering, and model evaluation.\n\n\n\nDesign Phase: Define the overall workflow structure, identifying key stages like data ingestion, preprocessing, training, evaluation, and deployment.\nComponent Development: Create or select appropriate components for each pipeline stage, ensuring proper input/output specifications and parameter definitions.\nPipeline Assembly: Use the Kubeflow Pipelines SDK to connect components, define data flow, and specify execution dependencies.\nTesting and Validation: Test pipeline components individually and as complete workflows using smaller datasets before production deployment.\n\n\n\nModularity: Design components to be as modular and reusable as possible, enabling easier maintenance and testing.\nParameterization: Make pipelines highly parameterizable to support different datasets, model configurations, and deployment targets without code changes.\nError Handling: Implement comprehensive error handling and logging within components to facilitate debugging and monitoring.\nVersion Control: Maintain proper version control for both pipeline definitions and component implementations to enable rollbacks and reproducibility."
  },
  {
    "objectID": "posts/kubeflow-explain/index.html#model-training-and-experimentation",
    "href": "posts/kubeflow-explain/index.html#model-training-and-experimentation",
    "title": "Kubeflow: A Comprehensive Guide to Machine Learning on Kubernetes",
    "section": "",
    "text": "Kubeflow supports distributed training across multiple frameworks:\nTensorFlow Training: The TFJob operator enables distributed TensorFlow training with parameter servers or all-reduce strategies, automatically handling worker coordination and failure recovery.\nPyTorch Training: PyTorchJob operator supports distributed PyTorch training using various backends like NCCL and Gloo, with automatic scaling and fault tolerance.\nMPI Training: For frameworks that support MPI-based distributed training, the MPIJob operator provides seamless integration with message-passing interfaces.\n\n\n\nExperiment Tracking: Kubeflow Pipelines automatically tracks experiment metadata, including parameters, metrics, and artifacts, enabling comprehensive experiment comparison and analysis.\nHyperparameter Tuning: Katib integration allows for sophisticated hyperparameter optimization experiments with support for various search algorithms and early stopping strategies.\nModel Versioning: Built-in model versioning capabilities track model evolution over time, supporting model lineage and reproducibility requirements.\n\n\n\nAuto-scaling: Dynamic resource allocation based on training workload requirements, optimizing cost and performance.\nGPU Scheduling: Intelligent GPU scheduling and sharing capabilities to maximize utilization of expensive GPU resources.\nSpot Instance Support: Integration with cloud provider spot instances for cost-effective training of non-critical workloads."
  },
  {
    "objectID": "posts/kubeflow-explain/index.html#model-serving-and-deployment",
    "href": "posts/kubeflow-explain/index.html#model-serving-and-deployment",
    "title": "Kubeflow: A Comprehensive Guide to Machine Learning on Kubernetes",
    "section": "",
    "text": "Real-time Serving: Low-latency serving for applications requiring immediate responses, with support for high-throughput scenarios.\nBatch Prediction: Efficient batch processing capabilities for scenarios where predictions can be computed offline or in batches.\nEdge Deployment: Support for deploying models to edge devices and environments with limited resources.\n\n\n\nCanary Deployments: Gradual rollout of new model versions to a subset of traffic, enabling safe deployment with minimal risk.\nA/B Testing: Side-by-side comparison of different model versions to evaluate performance improvements and business impact.\nShadow Deployment: Deploy new models alongside existing ones to evaluate performance without affecting production traffic.\n\n\n\nPerformance Monitoring: Continuous tracking of model performance metrics like accuracy, latency, and throughput.\nData Drift Detection: Monitoring for changes in input data distribution that might affect model performance.\nModel Explainability: Integration with explainability tools to provide insights into model predictions and decision-making processes."
  },
  {
    "objectID": "posts/kubeflow-explain/index.html#integration-with-ml-ecosystem",
    "href": "posts/kubeflow-explain/index.html#integration-with-ml-ecosystem",
    "title": "Kubeflow: A Comprehensive Guide to Machine Learning on Kubernetes",
    "section": "",
    "text": "Data Pipeline Integration: Seamless integration with data pipeline tools like Apache Airflow, allowing for end-to-end data-to-model workflows.\nFeature Store Integration: Support for feature stores like Feast, enabling consistent feature engineering across training and serving environments.\nData Versioning: Integration with data versioning tools like DVC or Pachyderm for reproducible data management.\n\n\n\nCI/CD Integration: Support for continuous integration and deployment pipelines, enabling automated model training, testing, and deployment.\nModel Registry: Integration with model registries like MLflow for centralized model management and lifecycle tracking.\nMonitoring and Observability: Integration with observability platforms for comprehensive monitoring of ML system health and performance.\n\n\n\nAWS Integration: Native support for AWS services like S3 for storage, IAM for authentication, and CloudWatch for monitoring.\nGoogle Cloud Integration: Deep integration with Google Cloud services including BigQuery, Cloud Storage, and AI Platform services.\nAzure Integration: Support for Azure services like Azure Blob Storage, Azure Active Directory, and Azure Monitor."
  },
  {
    "objectID": "posts/kubeflow-explain/index.html#best-practices-and-considerations",
    "href": "posts/kubeflow-explain/index.html#best-practices-and-considerations",
    "title": "Kubeflow: A Comprehensive Guide to Machine Learning on Kubernetes",
    "section": "",
    "text": "Authentication and Authorization: Implement proper authentication mechanisms and role-based access control to secure ML workloads and data.\nNetwork Security: Use network policies and service meshes to secure communication between components and external services.\nSecret Management: Proper management of secrets and credentials using Kubernetes secrets or external secret management systems.\nContainer Security: Regular scanning of container images for vulnerabilities and use of minimal, hardened base images.\n\n\n\nResource Planning: Careful planning of compute resources based on workload characteristics and performance requirements.\nStorage Optimization: Choose appropriate storage solutions based on access patterns, performance requirements, and cost considerations.\nNetwork Optimization: Optimize network configuration for data-intensive workloads, particularly for distributed training scenarios.\nCaching Strategies: Implement appropriate caching strategies for frequently accessed data and model artifacts.\n\n\n\nMonitoring and Alerting: Comprehensive monitoring of system health, resource utilization, and application performance with appropriate alerting mechanisms.\nBackup and Recovery: Regular backups of critical data and configurations with tested recovery procedures.\nDocumentation: Maintain comprehensive documentation of system architecture, operational procedures, and troubleshooting guides.\nTraining and Support: Ensure team members are properly trained on Kubeflow operations and best practices."
  },
  {
    "objectID": "posts/kubeflow-explain/index.html#use-cases-and-success-stories",
    "href": "posts/kubeflow-explain/index.html#use-cases-and-success-stories",
    "title": "Kubeflow: A Comprehensive Guide to Machine Learning on Kubernetes",
    "section": "",
    "text": "Large enterprises use Kubeflow to standardize their ML infrastructure across multiple teams and projects, providing consistent tooling and workflows while maintaining flexibility for different use cases.\n\n\n\nAcademic and research institutions leverage Kubeflowâ€™s flexibility and scalability to support diverse research projects with varying computational requirements and experimental approaches.\n\n\n\nSmaller organizations use Kubeflow to access enterprise-grade ML infrastructure without the overhead of building and maintaining custom solutions, accelerating their time to market.\n\n\n\nFinancial Services: Risk modeling, fraud detection, and algorithmic trading applications benefit from Kubeflowâ€™s scalability and compliance capabilities.\nHealthcare: Medical imaging, drug discovery, and clinical decision support systems leverage Kubeflowâ€™s robust pipeline management and model serving capabilities.\nRetail and E-commerce: Recommendation systems, demand forecasting, and personalization engines use Kubeflowâ€™s ability to handle large-scale, real-time ML workloads."
  },
  {
    "objectID": "posts/kubeflow-explain/index.html#future-directions-and-roadmap",
    "href": "posts/kubeflow-explain/index.html#future-directions-and-roadmap",
    "title": "Kubeflow: A Comprehensive Guide to Machine Learning on Kubernetes",
    "section": "",
    "text": "AutoML Integration: Enhanced integration with automated machine learning tools and techniques for democratizing ML development.\nEdge Computing: Improved support for edge deployment scenarios with optimized resource utilization and offline capabilities.\nFederated Learning: Native support for federated learning scenarios where data cannot be centralized due to privacy or regulatory constraints.\n\n\n\nComponent Ecosystem: Continued growth of the component ecosystem with contributions from the broader ML community.\nIntegration Partnerships: Expanding partnerships with cloud providers, ML tool vendors, and open-source projects to enhance the platformâ€™s capabilities.\nStandards Adoption: Participation in industry standards development to ensure compatibility and interoperability with other ML platforms and tools."
  },
  {
    "objectID": "posts/kubeflow-explain/index.html#conclusion",
    "href": "posts/kubeflow-explain/index.html#conclusion",
    "title": "Kubeflow: A Comprehensive Guide to Machine Learning on Kubernetes",
    "section": "",
    "text": "Kubeflow represents a significant advancement in making machine learning workflows more scalable, reproducible, and manageable. By leveraging Kubernetesâ€™ container orchestration capabilities, it provides a comprehensive platform that addresses the full spectrum of ML lifecycle management needs.\nThe platformâ€™s strength lies in its modularity and extensibility, allowing organizations to adopt components incrementally based on their specific requirements and maturity levels. Whether youâ€™re a startup looking to establish ML infrastructure or an enterprise seeking to standardize ML operations across multiple teams, Kubeflow provides the foundation for building robust, scalable ML systems.\nAs the machine learning landscape continues to evolve, Kubeflowâ€™s active community and vendor-neutral approach position it well to adapt to emerging technologies and methodologies. Organizations investing in Kubeflow today are building on a platform designed to grow with their ML maturity and requirements, providing a solid foundation for long-term ML success.\nThe key to successful Kubeflow adoption lies in understanding your organizationâ€™s specific requirements, starting with pilot projects to build expertise, and gradually expanding usage as teams become more comfortable with the platform. With proper planning and implementation, Kubeflow can significantly accelerate your organizationâ€™s ML capabilities while maintaining the operational excellence required for production ML systems."
  },
  {
    "objectID": "posts/pytorch-collate-gains/index.html",
    "href": "posts/pytorch-collate-gains/index.html",
    "title": "PyTorch Collate Function Speed-Up Guide",
    "section": "",
    "text": "The collate function in PyTorch is a crucial component for optimizing data loading performance. It determines how individual samples are combined into batches, and custom implementations can significantly speed up training by reducing data preprocessing overhead and memory operations.\n\n\n\n\n\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport time\nimport numpy as np\n\nclass SimpleDataset(Dataset):\n    def __init__(self, size=1000):\n        self.data = [torch.randn(3, 224, 224) for _ in range(size)]\n        self.labels = [torch.randint(0, 10, (1,)).item() for _ in range(size)]\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\n# Using default collate function\ndataset = SimpleDataset(1000)\ndefault_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Timing default collate\nstart_time = time.time()\nfor batch_idx, (data, labels) in enumerate(default_loader):\n    if batch_idx &gt;= 10:  # Test first 10 batches\n        break\ndefault_time = time.time() - start_time\nprint(f\"Default collate time: {default_time:.4f} seconds\")\n\nDefault collate time: 0.0129 seconds\n\n\n\n\n\n\ndef fast_collate(batch):\n    \"\"\"Optimized collate function for image data\"\"\"\n    # Separate data and labels\n    data, labels = zip(*batch)\n    \n    # Stack tensors directly (faster than default_collate for large tensors)\n    data_tensor = torch.stack(data, dim=0)\n    labels_tensor = torch.tensor(labels, dtype=torch.long)\n    \n    return data_tensor, labels_tensor\n\n# Using custom collate function\ncustom_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=fast_collate)\n\n# Timing custom collate\nstart_time = time.time()\nfor batch_idx, (data, labels) in enumerate(custom_loader):\n    if batch_idx &gt;= 10:\n        break\ncustom_time = time.time() - start_time\nprint(f\"Custom collate time: {custom_time:.4f} seconds\")\nprint(f\"Speed improvement: {(default_time/custom_time - 1) * 100:.1f}%\")\n\nCustom collate time: 0.0107 seconds\nSpeed improvement: 20.6%\n\n\n\n\n\n\n\n\n\nimport torch.nn.utils.rnn as rnn_utils\n\nclass VariableLengthDataset(Dataset):\n    def __init__(self, size=1000):\n        # Simulate variable-length sequences\n        self.data = [torch.randn(np.random.randint(10, 100), 128) for _ in range(size)]\n        self.labels = [torch.randint(0, 5, (1,)).item() for _ in range(size)]\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\ndef efficient_variable_collate(batch):\n    \"\"\"Efficient collate for variable-length sequences\"\"\"\n    data, labels = zip(*batch)\n    \n    # Get sequence lengths for efficient packing\n    lengths = torch.tensor([len(seq) for seq in data])\n    \n    # Pad sequences efficiently\n    padded_data = rnn_utils.pad_sequence(data, batch_first=True, padding_value=0)\n    labels_tensor = torch.tensor(labels, dtype=torch.long)\n    \n    return padded_data, labels_tensor, lengths\n\n# Performance comparison\nvar_dataset = VariableLengthDataset(500)\n\n# Default collate (will fail for variable lengths, so we'll use a naive approach)\ndef naive_variable_collate(batch):\n    data, labels = zip(*batch)\n    max_len = max(len(seq) for seq in data)\n    \n    # Inefficient padding\n    padded_data = []\n    for seq in data:\n        if len(seq) &lt; max_len:\n            padded_seq = torch.cat([seq, torch.zeros(max_len - len(seq), seq.size(1))])\n        else:\n            padded_seq = seq\n        padded_data.append(padded_seq)\n    \n    return torch.stack(padded_data), torch.tensor(labels, dtype=torch.long)\n\n# Timing comparison\nnaive_loader = DataLoader(var_dataset, batch_size=16, collate_fn=naive_variable_collate)\nefficient_loader = DataLoader(var_dataset, batch_size=16, collate_fn=efficient_variable_collate)\n\n# Naive approach timing\nstart_time = time.time()\nfor batch_idx, batch in enumerate(naive_loader):\n    if batch_idx &gt;= 10:\n        break\nnaive_time = time.time() - start_time\n\n# Efficient approach timing\nstart_time = time.time()\nfor batch_idx, batch in enumerate(efficient_loader):\n    if batch_idx &gt;= 10:\n        break\nefficient_time = time.time() - start_time\n\nprint(f\"Naive variable collate time: {naive_time:.4f} seconds\")\nprint(f\"Efficient variable collate time: {efficient_time:.4f} seconds\")\nprint(f\"Speed improvement: {(naive_time/efficient_time - 1) * 100:.1f}%\")\n\nNaive variable collate time: 0.0015 seconds\nEfficient variable collate time: 0.0013 seconds\nSpeed improvement: 14.5%\n\n\n\n\n\ndef gpu_accelerated_collate(batch, device='cuda'):\n    \"\"\"Collate function that moves data to GPU during batching\"\"\"\n    if not torch.cuda.is_available():\n        device = 'cpu'\n    \n    data, labels = zip(*batch)\n    \n    # Stack and move to GPU in one operation\n    data_tensor = torch.stack(data, dim=0).to(device, non_blocking=True)\n    labels_tensor = torch.tensor(labels, dtype=torch.long).to(device, non_blocking=True)\n    \n    return data_tensor, labels_tensor\n\n# Performance comparison with GPU transfer\nif torch.cuda.is_available():\n    device = 'cuda'\n    \n    # Standard approach: CPU collate + GPU transfer\n    standard_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n    \n    # GPU-accelerated collate\n    gpu_loader = DataLoader(dataset, batch_size=32, shuffle=True, \n                           collate_fn=lambda batch: gpu_accelerated_collate(batch, device))\n    \n    # Timing standard approach\n    start_time = time.time()\n    for batch_idx, (data, labels) in enumerate(standard_loader):\n        data, labels = data.to(device), labels.to(device)\n        if batch_idx &gt;= 10:\n            break\n    standard_gpu_time = time.time() - start_time\n    \n    # Timing GPU-accelerated collate\n    start_time = time.time()\n    for batch_idx, (data, labels) in enumerate(gpu_loader):\n        if batch_idx &gt;= 10:\n            break\n    gpu_collate_time = time.time() - start_time\n    \n    print(f\"Standard CPU-&gt;GPU time: {standard_gpu_time:.4f} seconds\")\n    print(f\"GPU-accelerated collate time: {gpu_collate_time:.4f} seconds\")\n    print(f\"Speed improvement: {(standard_gpu_time/gpu_collate_time - 1) * 100:.1f}%\")\n\n\n\nimport mmap\n\nclass MemoryMappedDataset(Dataset):\n    \"\"\"Dataset using memory-mapped files for efficient large data loading\"\"\"\n    def __init__(self, data_array, labels_array):\n        self.data = data_array\n        self.labels = labels_array\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        # Return views instead of copies when possible\n        return torch.from_numpy(self.data[idx].copy()), self.labels[idx]\n\ndef zero_copy_collate(batch):\n    \"\"\"Zero-copy collate function for numpy arrays\"\"\"\n    data, labels = zip(*batch)\n    \n    # Use torch.from_numpy for zero-copy conversion when possible\n    try:\n        # Stack numpy arrays first, then convert to tensor\n        data_array = np.stack([d.numpy() if isinstance(d, torch.Tensor) else d for d in data])\n        data_tensor = torch.from_numpy(data_array)\n    except:\n        # Fallback to regular stacking\n        data_tensor = torch.stack(data)\n    \n    labels_tensor = torch.tensor(labels, dtype=torch.long)\n    return data_tensor, labels_tensor\n\n# Create sample data for demonstration\nsample_data = np.random.randn(1000, 3, 224, 224).astype(np.float32)\nsample_labels = np.random.randint(0, 10, 1000)\n\nmmap_dataset = MemoryMappedDataset(sample_data, sample_labels)\nmmap_loader = DataLoader(mmap_dataset, batch_size=32, collate_fn=zero_copy_collate)\n\n\n\n\n\n\nclass MultiModalDataset(Dataset):\n    def __init__(self, size=1000):\n        self.images = [torch.randn(3, 224, 224) for _ in range(size)]\n        self.text = [torch.randint(0, 1000, (np.random.randint(5, 50),)) for _ in range(size)]\n        self.labels = [torch.randint(0, 10, (1,)).item() for _ in range(size)]\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        return {\n            'image': self.images[idx],\n            'text': self.text[idx],\n            'label': self.labels[idx]\n        }\n\ndef multimodal_collate(batch):\n    \"\"\"Efficient collate for multi-modal data\"\"\"\n    # Separate different modalities\n    images = [item['image'] for item in batch]\n    texts = [item['text'] for item in batch]\n    labels = [item['label'] for item in batch]\n    \n    # Batch images\n    image_batch = torch.stack(images)\n    \n    # Batch variable-length text with padding\n    text_lengths = torch.tensor([len(text) for text in texts])\n    text_batch = rnn_utils.pad_sequence(texts, batch_first=True, padding_value=0)\n    \n    # Batch labels\n    label_batch = torch.tensor(labels, dtype=torch.long)\n    \n    return {\n        'images': image_batch,\n        'texts': text_batch,\n        'text_lengths': text_lengths,\n        'labels': label_batch\n    }\n\nmultimodal_dataset = MultiModalDataset(500)\nmultimodal_loader = DataLoader(multimodal_dataset, batch_size=16, collate_fn=multimodal_collate)\n\n# Test the multimodal loader\nsample_batch = next(iter(multimodal_loader))\nprint(\"Multimodal batch shapes:\")\nfor key, value in sample_batch.items():\n    print(f\"  {key}: {value.shape}\")\n\n\n\nimport torchvision.transforms as transforms\n\ndef augmentation_collate(batch, transform=None):\n    \"\"\"Collate function that applies augmentations during batching\"\"\"\n    data, labels = zip(*batch)\n    \n    if transform:\n        # Apply augmentations during collation\n        augmented_data = [transform(img) for img in data]\n        data_tensor = torch.stack(augmented_data)\n    else:\n        data_tensor = torch.stack(data)\n    \n    labels_tensor = torch.tensor(labels, dtype=torch.long)\n    return data_tensor, labels_tensor\n\n# Define augmentation pipeline\naugment_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2)\n])\n\n# Create collate function with augmentation\naug_collate_fn = lambda batch: augmentation_collate(batch, augment_transform)\n\naug_loader = DataLoader(dataset, batch_size=32, collate_fn=aug_collate_fn)\n\n\n\n\n\n\ndef efficient_collate_tips(batch):\n    \"\"\"Demonstrates efficient collate practices\"\"\"\n    data, labels = zip(*batch)\n    \n    # TIP 1: Use torch.stack instead of torch.cat when possible\n    # torch.stack is faster for same-sized tensors\n    data_tensor = torch.stack(data, dim=0)  # Faster\n    # data_tensor = torch.cat([d.unsqueeze(0) for d in data], dim=0)  # Slower\n    \n    # TIP 2: Use appropriate dtypes to save memory\n    labels_tensor = torch.tensor(labels, dtype=torch.long)  # Use long for indices\n    \n    # TIP 3: Pre-allocate tensors when size is known\n    # This is more relevant for complex batching scenarios\n    \n    return data_tensor, labels_tensor\n\n\n\ndef memory_efficient_collate(batch):\n    \"\"\"Memory-efficient collate function\"\"\"\n    data, labels = zip(*batch)\n    \n    # Pre-allocate output tensor to avoid multiple allocations\n    batch_size = len(data)\n    data_shape = data[0].shape\n    \n    # Allocate output tensor once\n    output_tensor = torch.empty((batch_size,) + data_shape, dtype=data[0].dtype)\n    \n    # Fill the tensor in-place\n    for i, tensor in enumerate(data):\n        output_tensor[i] = tensor\n    \n    labels_tensor = torch.tensor(labels, dtype=torch.long)\n    return output_tensor, labels_tensor\n\n\n\ndef benchmark_collate_functions():\n    \"\"\"Comprehensive benchmarking of different collate approaches\"\"\"\n    dataset = SimpleDataset(1000)\n    batch_size = 32\n    num_batches = 20\n    \n    collate_functions = {\n        'default': None,\n        'fast_collate': fast_collate,\n        'efficient_tips': efficient_collate_tips,\n        'memory_efficient': memory_efficient_collate\n    }\n    \n    results = {}\n    \n    for name, collate_fn in collate_functions.items():\n        loader = DataLoader(dataset, batch_size=batch_size, \n                          collate_fn=collate_fn, shuffle=True)\n        \n        start_time = time.time()\n        for batch_idx, (data, labels) in enumerate(loader):\n            if batch_idx &gt;= num_batches:\n                break\n        \n        elapsed_time = time.time() - start_time\n        results[name] = elapsed_time\n        print(f\"{name}: {elapsed_time:.4f} seconds\")\n    \n    # Calculate improvements\n    baseline = results['default']\n    for name, time_taken in results.items():\n        if name != 'default':\n            improvement = (baseline / time_taken - 1) * 100\n            print(f\"{name} improvement: {improvement:.1f}%\")\n\n# Run the benchmark\nbenchmark_collate_functions()\n\n\n\n\n\nUse torch.stack() instead of torch.cat() for same-sized tensors\nMinimize data copying by working with tensor views when possible\nPre-allocate tensors when batch sizes and shapes are known\nConsider GPU transfer during collation for better pipeline efficiency\nUse appropriate data types to optimize memory usage\nProfile your specific use case as optimal strategies vary by data type and size\nLeverage specialized functions like pad_sequence for variable-length data\n\nCustom collate functions can provide significant performance improvements, especially for large datasets or complex data structures. The key is to minimize unnecessary data operations and memory allocations while taking advantage of PyTorchâ€™s optimized tensor operations."
  },
  {
    "objectID": "posts/pytorch-collate-gains/index.html#introduction",
    "href": "posts/pytorch-collate-gains/index.html#introduction",
    "title": "PyTorch Collate Function Speed-Up Guide",
    "section": "",
    "text": "The collate function in PyTorch is a crucial component for optimizing data loading performance. It determines how individual samples are combined into batches, and custom implementations can significantly speed up training by reducing data preprocessing overhead and memory operations."
  },
  {
    "objectID": "posts/pytorch-collate-gains/index.html#default-vs-custom-collate-functions",
    "href": "posts/pytorch-collate-gains/index.html#default-vs-custom-collate-functions",
    "title": "PyTorch Collate Function Speed-Up Guide",
    "section": "",
    "text": "import torch\nfrom torch.utils.data import DataLoader, Dataset\nimport time\nimport numpy as np\n\nclass SimpleDataset(Dataset):\n    def __init__(self, size=1000):\n        self.data = [torch.randn(3, 224, 224) for _ in range(size)]\n        self.labels = [torch.randint(0, 10, (1,)).item() for _ in range(size)]\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\n# Using default collate function\ndataset = SimpleDataset(1000)\ndefault_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Timing default collate\nstart_time = time.time()\nfor batch_idx, (data, labels) in enumerate(default_loader):\n    if batch_idx &gt;= 10:  # Test first 10 batches\n        break\ndefault_time = time.time() - start_time\nprint(f\"Default collate time: {default_time:.4f} seconds\")\n\nDefault collate time: 0.0129 seconds\n\n\n\n\n\n\ndef fast_collate(batch):\n    \"\"\"Optimized collate function for image data\"\"\"\n    # Separate data and labels\n    data, labels = zip(*batch)\n    \n    # Stack tensors directly (faster than default_collate for large tensors)\n    data_tensor = torch.stack(data, dim=0)\n    labels_tensor = torch.tensor(labels, dtype=torch.long)\n    \n    return data_tensor, labels_tensor\n\n# Using custom collate function\ncustom_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=fast_collate)\n\n# Timing custom collate\nstart_time = time.time()\nfor batch_idx, (data, labels) in enumerate(custom_loader):\n    if batch_idx &gt;= 10:\n        break\ncustom_time = time.time() - start_time\nprint(f\"Custom collate time: {custom_time:.4f} seconds\")\nprint(f\"Speed improvement: {(default_time/custom_time - 1) * 100:.1f}%\")\n\nCustom collate time: 0.0107 seconds\nSpeed improvement: 20.6%"
  },
  {
    "objectID": "posts/pytorch-collate-gains/index.html#advanced-optimizations",
    "href": "posts/pytorch-collate-gains/index.html#advanced-optimizations",
    "title": "PyTorch Collate Function Speed-Up Guide",
    "section": "",
    "text": "import torch.nn.utils.rnn as rnn_utils\n\nclass VariableLengthDataset(Dataset):\n    def __init__(self, size=1000):\n        # Simulate variable-length sequences\n        self.data = [torch.randn(np.random.randint(10, 100), 128) for _ in range(size)]\n        self.labels = [torch.randint(0, 5, (1,)).item() for _ in range(size)]\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\ndef efficient_variable_collate(batch):\n    \"\"\"Efficient collate for variable-length sequences\"\"\"\n    data, labels = zip(*batch)\n    \n    # Get sequence lengths for efficient packing\n    lengths = torch.tensor([len(seq) for seq in data])\n    \n    # Pad sequences efficiently\n    padded_data = rnn_utils.pad_sequence(data, batch_first=True, padding_value=0)\n    labels_tensor = torch.tensor(labels, dtype=torch.long)\n    \n    return padded_data, labels_tensor, lengths\n\n# Performance comparison\nvar_dataset = VariableLengthDataset(500)\n\n# Default collate (will fail for variable lengths, so we'll use a naive approach)\ndef naive_variable_collate(batch):\n    data, labels = zip(*batch)\n    max_len = max(len(seq) for seq in data)\n    \n    # Inefficient padding\n    padded_data = []\n    for seq in data:\n        if len(seq) &lt; max_len:\n            padded_seq = torch.cat([seq, torch.zeros(max_len - len(seq), seq.size(1))])\n        else:\n            padded_seq = seq\n        padded_data.append(padded_seq)\n    \n    return torch.stack(padded_data), torch.tensor(labels, dtype=torch.long)\n\n# Timing comparison\nnaive_loader = DataLoader(var_dataset, batch_size=16, collate_fn=naive_variable_collate)\nefficient_loader = DataLoader(var_dataset, batch_size=16, collate_fn=efficient_variable_collate)\n\n# Naive approach timing\nstart_time = time.time()\nfor batch_idx, batch in enumerate(naive_loader):\n    if batch_idx &gt;= 10:\n        break\nnaive_time = time.time() - start_time\n\n# Efficient approach timing\nstart_time = time.time()\nfor batch_idx, batch in enumerate(efficient_loader):\n    if batch_idx &gt;= 10:\n        break\nefficient_time = time.time() - start_time\n\nprint(f\"Naive variable collate time: {naive_time:.4f} seconds\")\nprint(f\"Efficient variable collate time: {efficient_time:.4f} seconds\")\nprint(f\"Speed improvement: {(naive_time/efficient_time - 1) * 100:.1f}%\")\n\nNaive variable collate time: 0.0015 seconds\nEfficient variable collate time: 0.0013 seconds\nSpeed improvement: 14.5%\n\n\n\n\n\ndef gpu_accelerated_collate(batch, device='cuda'):\n    \"\"\"Collate function that moves data to GPU during batching\"\"\"\n    if not torch.cuda.is_available():\n        device = 'cpu'\n    \n    data, labels = zip(*batch)\n    \n    # Stack and move to GPU in one operation\n    data_tensor = torch.stack(data, dim=0).to(device, non_blocking=True)\n    labels_tensor = torch.tensor(labels, dtype=torch.long).to(device, non_blocking=True)\n    \n    return data_tensor, labels_tensor\n\n# Performance comparison with GPU transfer\nif torch.cuda.is_available():\n    device = 'cuda'\n    \n    # Standard approach: CPU collate + GPU transfer\n    standard_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n    \n    # GPU-accelerated collate\n    gpu_loader = DataLoader(dataset, batch_size=32, shuffle=True, \n                           collate_fn=lambda batch: gpu_accelerated_collate(batch, device))\n    \n    # Timing standard approach\n    start_time = time.time()\n    for batch_idx, (data, labels) in enumerate(standard_loader):\n        data, labels = data.to(device), labels.to(device)\n        if batch_idx &gt;= 10:\n            break\n    standard_gpu_time = time.time() - start_time\n    \n    # Timing GPU-accelerated collate\n    start_time = time.time()\n    for batch_idx, (data, labels) in enumerate(gpu_loader):\n        if batch_idx &gt;= 10:\n            break\n    gpu_collate_time = time.time() - start_time\n    \n    print(f\"Standard CPU-&gt;GPU time: {standard_gpu_time:.4f} seconds\")\n    print(f\"GPU-accelerated collate time: {gpu_collate_time:.4f} seconds\")\n    print(f\"Speed improvement: {(standard_gpu_time/gpu_collate_time - 1) * 100:.1f}%\")\n\n\n\nimport mmap\n\nclass MemoryMappedDataset(Dataset):\n    \"\"\"Dataset using memory-mapped files for efficient large data loading\"\"\"\n    def __init__(self, data_array, labels_array):\n        self.data = data_array\n        self.labels = labels_array\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        # Return views instead of copies when possible\n        return torch.from_numpy(self.data[idx].copy()), self.labels[idx]\n\ndef zero_copy_collate(batch):\n    \"\"\"Zero-copy collate function for numpy arrays\"\"\"\n    data, labels = zip(*batch)\n    \n    # Use torch.from_numpy for zero-copy conversion when possible\n    try:\n        # Stack numpy arrays first, then convert to tensor\n        data_array = np.stack([d.numpy() if isinstance(d, torch.Tensor) else d for d in data])\n        data_tensor = torch.from_numpy(data_array)\n    except:\n        # Fallback to regular stacking\n        data_tensor = torch.stack(data)\n    \n    labels_tensor = torch.tensor(labels, dtype=torch.long)\n    return data_tensor, labels_tensor\n\n# Create sample data for demonstration\nsample_data = np.random.randn(1000, 3, 224, 224).astype(np.float32)\nsample_labels = np.random.randint(0, 10, 1000)\n\nmmap_dataset = MemoryMappedDataset(sample_data, sample_labels)\nmmap_loader = DataLoader(mmap_dataset, batch_size=32, collate_fn=zero_copy_collate)"
  },
  {
    "objectID": "posts/pytorch-collate-gains/index.html#specialized-collate-functions",
    "href": "posts/pytorch-collate-gains/index.html#specialized-collate-functions",
    "title": "PyTorch Collate Function Speed-Up Guide",
    "section": "",
    "text": "class MultiModalDataset(Dataset):\n    def __init__(self, size=1000):\n        self.images = [torch.randn(3, 224, 224) for _ in range(size)]\n        self.text = [torch.randint(0, 1000, (np.random.randint(5, 50),)) for _ in range(size)]\n        self.labels = [torch.randint(0, 10, (1,)).item() for _ in range(size)]\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        return {\n            'image': self.images[idx],\n            'text': self.text[idx],\n            'label': self.labels[idx]\n        }\n\ndef multimodal_collate(batch):\n    \"\"\"Efficient collate for multi-modal data\"\"\"\n    # Separate different modalities\n    images = [item['image'] for item in batch]\n    texts = [item['text'] for item in batch]\n    labels = [item['label'] for item in batch]\n    \n    # Batch images\n    image_batch = torch.stack(images)\n    \n    # Batch variable-length text with padding\n    text_lengths = torch.tensor([len(text) for text in texts])\n    text_batch = rnn_utils.pad_sequence(texts, batch_first=True, padding_value=0)\n    \n    # Batch labels\n    label_batch = torch.tensor(labels, dtype=torch.long)\n    \n    return {\n        'images': image_batch,\n        'texts': text_batch,\n        'text_lengths': text_lengths,\n        'labels': label_batch\n    }\n\nmultimodal_dataset = MultiModalDataset(500)\nmultimodal_loader = DataLoader(multimodal_dataset, batch_size=16, collate_fn=multimodal_collate)\n\n# Test the multimodal loader\nsample_batch = next(iter(multimodal_loader))\nprint(\"Multimodal batch shapes:\")\nfor key, value in sample_batch.items():\n    print(f\"  {key}: {value.shape}\")\n\n\n\nimport torchvision.transforms as transforms\n\ndef augmentation_collate(batch, transform=None):\n    \"\"\"Collate function that applies augmentations during batching\"\"\"\n    data, labels = zip(*batch)\n    \n    if transform:\n        # Apply augmentations during collation\n        augmented_data = [transform(img) for img in data]\n        data_tensor = torch.stack(augmented_data)\n    else:\n        data_tensor = torch.stack(data)\n    \n    labels_tensor = torch.tensor(labels, dtype=torch.long)\n    return data_tensor, labels_tensor\n\n# Define augmentation pipeline\naugment_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2)\n])\n\n# Create collate function with augmentation\naug_collate_fn = lambda batch: augmentation_collate(batch, augment_transform)\n\naug_loader = DataLoader(dataset, batch_size=32, collate_fn=aug_collate_fn)"
  },
  {
    "objectID": "posts/pytorch-collate-gains/index.html#performance-tips-and-best-practices",
    "href": "posts/pytorch-collate-gains/index.html#performance-tips-and-best-practices",
    "title": "PyTorch Collate Function Speed-Up Guide",
    "section": "",
    "text": "def efficient_collate_tips(batch):\n    \"\"\"Demonstrates efficient collate practices\"\"\"\n    data, labels = zip(*batch)\n    \n    # TIP 1: Use torch.stack instead of torch.cat when possible\n    # torch.stack is faster for same-sized tensors\n    data_tensor = torch.stack(data, dim=0)  # Faster\n    # data_tensor = torch.cat([d.unsqueeze(0) for d in data], dim=0)  # Slower\n    \n    # TIP 2: Use appropriate dtypes to save memory\n    labels_tensor = torch.tensor(labels, dtype=torch.long)  # Use long for indices\n    \n    # TIP 3: Pre-allocate tensors when size is known\n    # This is more relevant for complex batching scenarios\n    \n    return data_tensor, labels_tensor\n\n\n\ndef memory_efficient_collate(batch):\n    \"\"\"Memory-efficient collate function\"\"\"\n    data, labels = zip(*batch)\n    \n    # Pre-allocate output tensor to avoid multiple allocations\n    batch_size = len(data)\n    data_shape = data[0].shape\n    \n    # Allocate output tensor once\n    output_tensor = torch.empty((batch_size,) + data_shape, dtype=data[0].dtype)\n    \n    # Fill the tensor in-place\n    for i, tensor in enumerate(data):\n        output_tensor[i] = tensor\n    \n    labels_tensor = torch.tensor(labels, dtype=torch.long)\n    return output_tensor, labels_tensor\n\n\n\ndef benchmark_collate_functions():\n    \"\"\"Comprehensive benchmarking of different collate approaches\"\"\"\n    dataset = SimpleDataset(1000)\n    batch_size = 32\n    num_batches = 20\n    \n    collate_functions = {\n        'default': None,\n        'fast_collate': fast_collate,\n        'efficient_tips': efficient_collate_tips,\n        'memory_efficient': memory_efficient_collate\n    }\n    \n    results = {}\n    \n    for name, collate_fn in collate_functions.items():\n        loader = DataLoader(dataset, batch_size=batch_size, \n                          collate_fn=collate_fn, shuffle=True)\n        \n        start_time = time.time()\n        for batch_idx, (data, labels) in enumerate(loader):\n            if batch_idx &gt;= num_batches:\n                break\n        \n        elapsed_time = time.time() - start_time\n        results[name] = elapsed_time\n        print(f\"{name}: {elapsed_time:.4f} seconds\")\n    \n    # Calculate improvements\n    baseline = results['default']\n    for name, time_taken in results.items():\n        if name != 'default':\n            improvement = (baseline / time_taken - 1) * 100\n            print(f\"{name} improvement: {improvement:.1f}%\")\n\n# Run the benchmark\nbenchmark_collate_functions()"
  },
  {
    "objectID": "posts/pytorch-collate-gains/index.html#key-takeaways",
    "href": "posts/pytorch-collate-gains/index.html#key-takeaways",
    "title": "PyTorch Collate Function Speed-Up Guide",
    "section": "",
    "text": "Use torch.stack() instead of torch.cat() for same-sized tensors\nMinimize data copying by working with tensor views when possible\nPre-allocate tensors when batch sizes and shapes are known\nConsider GPU transfer during collation for better pipeline efficiency\nUse appropriate data types to optimize memory usage\nProfile your specific use case as optimal strategies vary by data type and size\nLeverage specialized functions like pad_sequence for variable-length data\n\nCustom collate functions can provide significant performance improvements, especially for large datasets or complex data structures. The key is to minimize unnecessary data operations and memory allocations while taking advantage of PyTorchâ€™s optimized tensor operations."
  },
  {
    "objectID": "posts/dinov2/index.html",
    "href": "posts/dinov2/index.html",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "DINOv2 is a state-of-the-art self-supervised vision model developed by Meta AI Research that builds upon the original DINO (Self-Distillation with No Labels) framework. This guide will walk you through understanding, implementing, and leveraging DINOv2 for various computer vision tasks.\n\n\n\nIntroduction to DINOv2\nInstallation and Setup\nLoading Pre-trained Models\nFeature Extraction\nFine-tuning for Downstream Tasks\nImage Classification Example\nSemantic Segmentation Example\nObject Detection Example\nAdvanced Usage and Customization\nPerformance Benchmarks\nTroubleshooting\n\n\n\n\nDINOv2 is a self-supervised learning method for vision that produces high-quality visual features without requiring labeled data. It extends the original DINO architecture with several improvements:\n\nTraining on a large and diverse dataset of images\nEnhanced teacher-student architecture\nImproved augmentation strategy\nMulti-scale feature learning\nSupport for various Vision Transformer (ViT) backbones\n\nThe result is a versatile foundation model that can be adapted to numerous vision tasks with minimal fine-tuning.\n\n\n\nTo use DINOv2, youâ€™ll need to install the official implementation:\n# Install PyTorch first if not already installed\n# pip install torch torchvision\n\n# Install DINOv2\npip install git+https://github.com/facebookresearch/dinov2\nAlternatively, you can clone the repository and install it locally:\ngit clone https://github.com/facebookresearch/dinov2.git\ncd dinov2\npip install -e .\n\n\nDINOv2 requires:\n\nPython 3.8+\nPyTorch 1.12+\ntorchvision\nCUDA (for GPU acceleration)\n\n\n\n\n\nDINOv2 provides several pre-trained models with different sizes and capabilities:\nimport torch\nfrom dinov2.models import build_model_from_cfg\nfrom dinov2.configs import get_config\n\n# Available model sizes: 'small', 'base', 'large', 'giant'\nmodel_size = 'base'\ncfg = get_config(f\"dinov2_{model_size}\")\nmodel = build_model_from_cfg(cfg)\n\n# Load pre-trained weights\ncheckpoint_path = f\"dinov2_{model_size}_pretrain.pth\"  # Download this from Meta AI's repository\ncheckpoint = torch.load(checkpoint_path, map_location=\"cpu\")\nmodel.load_state_dict(checkpoint[\"model\"])\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nmodel.eval()  # Set to evaluation mode\nYou can also use the Hugging Face Transformers library for an easier integration:\nfrom transformers import AutoImageProcessor, AutoModel\n\n# Available model sizes: 'small', 'base', 'large', 'giant'\nmodel_name = \"facebook/dinov2-base\"\nprocessor = AutoImageProcessor.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n\n\nOne of DINOv2â€™s key strengths is its ability to extract powerful visual features:\nimport torch\nfrom PIL import Image\nimport torchvision.transforms as T\nfrom transformers import AutoImageProcessor, AutoModel\n\n# Load model\nmodel_name = \"facebook/dinov2-base\"\nprocessor = AutoImageProcessor.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\nmodel.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\nmodel.eval()\n\n# Load and preprocess image\nimage = Image.open(\"path/to/your/image.jpg\").convert(\"RGB\")\ninputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n\n# Extract features\nwith torch.no_grad():\n    outputs = model(**inputs)\n    \n# Get CLS token features (useful for classification tasks)\ncls_features = outputs.last_hidden_state[:, 0]\n\n# Get patch features (useful for dense prediction tasks like segmentation)\npatch_features = outputs.last_hidden_state[:, 1:]\n\nprint(f\"CLS features shape: {cls_features.shape}\")\nprint(f\"Patch features shape: {patch_features.shape}\")\n\n\n\nDINOv2 can be fine-tuned for specific vision tasks:\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModel\n\n# Load pre-trained DINOv2 model\nbackbone = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n\n# Create a custom classification head\nclass ClassificationHead(nn.Module):\n    def __init__(self, backbone, num_classes=1000):\n        super().__init__()\n        self.backbone = backbone\n        self.classifier = nn.Linear(backbone.config.hidden_size, num_classes)\n        \n    def forward(self, x):\n        outputs = self.backbone(x)\n        cls_token = outputs.last_hidden_state[:, 0]\n        return self.classifier(cls_token)\n\n# Create the complete model\nmodel = ClassificationHead(backbone, num_classes=100)  # For 100 classes\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop example\ndef train_one_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    \n    for batch in dataloader:\n        images = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(dataloader)\n\n\n\nHereâ€™s a complete example for image classification using DINOv2:\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as transforms\nfrom transformers import AutoImageProcessor, AutoModel\n\n# Define the dataset and transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Load your dataset (adjust the path)\ntrain_dataset = ImageFolder(root=\"path/to/train\", transform=transform)\nval_dataset = ImageFolder(root=\"path/to/val\", transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n\n# Create the model\nclass DINOv2Classifier(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.dinov2 = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n        self.classifier = nn.Linear(768, num_classes)  # 768 is the hidden size for base model\n        \n    def forward(self, x):\n        # Extract features\n        with torch.set_grad_enabled(self.training):\n            features = self.dinov2(x).last_hidden_state[:, 0]  # Get CLS token\n        \n        # Classify\n        logits = self.classifier(features)\n        return logits\n\n# Initialize model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DINOv2Classifier(num_classes=len(train_dataset.classes))\nmodel = model.to(device)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW([\n    {'params': model.classifier.parameters(), 'lr': 1e-3},\n    {'params': model.dinov2.parameters(), 'lr': 1e-5}\n])\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    # Training\n    model.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    \n    for inputs, targets in train_loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n    \n    train_accuracy = 100 * correct / total\n    \n    # Validation\n    model.eval()\n    val_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            \n            val_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    \n    val_accuracy = 100 * correct / total\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    print(f\"Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_accuracy:.2f}%\")\n    print(f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_accuracy:.2f}%\")\n\n\n\nDINOv2 is particularly powerful for segmentation tasks:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModel\n\nclass DINOv2Segmenter(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        # Load DINOv2 backbone\n        self.backbone = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n        \n        # Define segmentation head\n        hidden_dim = self.backbone.config.hidden_size\n        self.segmentation_head = nn.Sequential(\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(hidden_dim, num_classes, kernel_size=1)\n        )\n        \n        # Image size and patch size for reshaping\n        self.image_size = 224\n        self.patch_size = 14  # For ViT-Base\n        self.num_patches = (self.image_size // self.patch_size) ** 2\n        \n    def forward(self, x):\n        # Get patch features\n        outputs = self.backbone(x)\n        patch_features = outputs.last_hidden_state[:, 1:]  # Remove CLS token\n        \n        # Reshape to 2D spatial layout\n        B = x.shape[0]\n        H = W = self.image_size // self.patch_size\n        patch_features = patch_features.reshape(B, H, W, -1).permute(0, 3, 1, 2)\n        \n        # Apply segmentation head\n        segmentation_logits = self.segmentation_head(patch_features)\n        \n        # Upsample to original image size\n        segmentation_logits = F.interpolate(\n            segmentation_logits, \n            size=(self.image_size, self.image_size), \n            mode='bilinear', \n            align_corners=False\n        )\n        \n        return segmentation_logits\n\n# Create model and move to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DINOv2Segmenter(num_classes=21)  # 21 classes for Pascal VOC\nmodel = model.to(device)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW([\n    {'params': model.segmentation_head.parameters(), 'lr': 1e-3},\n    {'params': model.backbone.parameters(), 'lr': 1e-5}\n])\ncriterion = nn.CrossEntropyLoss(ignore_index=255)  # 255 is typically the ignore index\n\n# Rest of the training code would be similar to the classification example\n\n\n\nHereâ€™s how to use DINOv2 features for object detection with a simple detection head:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModel\n\nclass DINOv2Detector(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        # Load DINOv2 backbone\n        self.backbone = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n        hidden_dim = self.backbone.config.hidden_size\n        \n        # Detection heads\n        self.box_predictor = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, 4)  # (x1, y1, x2, y2)\n        )\n        \n        self.class_predictor = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, num_classes + 1)  # +1 for background\n        )\n        \n        # Image size and patch size for feature map creation\n        self.image_size = 224\n        self.patch_size = 14  # For ViT-Base\n        \n    def forward(self, x):\n        # Get features\n        outputs = self.backbone(x)\n        features = outputs.last_hidden_state[:, 1:]  # Remove CLS token\n        \n        # Reshape to 2D spatial layout\n        B = x.shape[0]\n        H = W = self.image_size // self.patch_size\n        features = features.reshape(B, H, W, -1)\n        \n        # Flatten for prediction heads\n        features_flat = features.reshape(B, -1, features.shape[-1])\n        \n        # Predict boxes and classes\n        boxes = self.box_predictor(features_flat)\n        classes = self.class_predictor(features_flat)\n        \n        return {'boxes': boxes, 'classes': classes, 'features_map': features}\n\n# Create model and move to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DINOv2Detector(num_classes=80)  # 80 classes for COCO\nmodel = model.to(device)\n\n# Training would require a more complex detection pipeline with NMS, etc.\n\n\n\n\n\nYou can customize the DINOv2 model architecture:\nfrom dinov2.configs import get_config\nfrom dinov2.models import build_model_from_cfg\n\n# Get default configuration and modify it\ncfg = get_config(\"dinov2_base\")\n\n# Modify configuration\ncfg.student.drop_path_rate = 0.2  # Change stochastic depth rate\ncfg.student.num_registers = 16    # Change the number of registers\n\n# Build model from modified config\nmodel = build_model_from_cfg(cfg)\n\n\n\nFor some applications, you might want to extract features from intermediate layers:\nimport torch\nfrom transformers import AutoModel\nfrom torch.utils.hooks import RemovableHandle\n\nclass FeatureExtractor:\n    def __init__(self, model, layers=None):\n        self.model = model\n        self.features = {}\n        self.hooks = []\n        \n        # Default to extracting from the last block if no layers specified\n        self.layers = layers if layers is not None else [11]  # Base model has 12 blocks (0-11)\n        \n        # Register hooks\n        for idx in self.layers:\n            hook = self.model.encoder.layer[idx].register_forward_hook(\n                lambda module, input, output, idx=idx: self.features.update({f\"layer_{idx}\": output})\n            )\n            self.hooks.append(hook)\n    \n    def __call__(self, x):\n        self.features.clear()\n        with torch.no_grad():\n            outputs = self.model(x)\n        return self.features\n    \n    def remove_hooks(self):\n        for hook in self.hooks:\n            hook.remove()\n\n# Usage\nmodel = AutoModel.from_pretrained(\"facebook/dinov2-base\")\nextractor = FeatureExtractor(model, layers=[3, 7, 11])\n\n# Extract features\nfeatures = extractor(input_image)\nlayer_3_features = features[\"layer_3\"]\nlayer_7_features = features[\"layer_7\"]\nlayer_11_features = features[\"layer_11\"]\n\n# Clean up\nextractor.remove_hooks()\n\n\n\n\nDINOv2 achieves excellent results across various vision tasks. Here are typical performance metrics:\n\nImageNet-1K Classification (top-1 accuracy):\n\nDINOv2-Small: ~80.0%\nDINOv2-Base: ~84.5%\nDINOv2-Large: ~86.3%\nDINOv2-Giant: ~87.0%\n\nSemantic Segmentation (ADE20K) (mIoU):\n\nDINOv2-Small: ~47.5%\nDINOv2-Base: ~50.2%\nDINOv2-Large: ~52.5%\nDINOv2-Giant: ~53.8%\n\nObject Detection (COCO) (AP):\n\nDINOv2-Small: ~48.5%\nDINOv2-Base: ~51.3%\nDINOv2-Large: ~53.2%\nDINOv2-Giant: ~54.5%\n\n\n\n\n\n\n\n\nOut of Memory Errors\n\nReduce batch size\nUse gradient accumulation\nUse a smaller model variant (Small or Base)\nUse mixed precision training\n\n\n# Example of mixed precision training\nfrom torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nfor inputs, targets in train_loader:\n    inputs, targets = inputs.to(device), targets.to(device)\n    \n    optimizer.zero_grad()\n    \n    # Use autocast for mixed precision\n    with autocast():\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n    \n    # Scale loss and backprop\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n\nSlow Inference\n\nUse batch processing\nUse model.eval() and torch.no_grad()\nConsider model distillation or quantization\n\nPoor Performance on Downstream Tasks\n\nEnsure proper data preprocessing\nAdjust learning rates (lower for backbone, higher for heads)\nUse appropriate augmentations\nConsider using a larger variant of DINOv2\n\n\n\n\n\n\nVisualize model attention maps to understand what the model focuses on:\n\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom PIL import Image\nimport torchvision.transforms as T\n\ndef get_attention_map(model, img_tensor):\n    model.eval()\n    with torch.no_grad():\n        outputs = model(img_tensor.unsqueeze(0), output_attentions=True)\n    \n    # Get attention weights from the last layer\n    att_mat = outputs.attentions[-1]\n    \n    # Average attention across heads\n    att_mat = att_mat.mean(dim=1)\n    \n    # Extract attention for cls token to patch tokens\n    cls_att_map = att_mat[0, 0, 1:].reshape(14, 14)\n    \n    return cls_att_map.cpu().numpy()\n\n# Load and preprocess image\nimage = Image.open(\"path/to/image.jpg\").convert(\"RGB\")\ntransform = T.Compose([\n    T.Resize((224, 224)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\nimg_tensor = transform(image).to(device)\n\n# Get attention map\nfrom transformers import AutoModel\nmodel = AutoModel.from_pretrained(\"facebook/dinov2-base\", output_attentions=True)\nmodel.to(device)\nattention_map = get_attention_map(model, img_tensor)\n\n# Visualize\nplt.figure(figsize=(10, 10))\nplt.imshow(image.resize((224, 224)))\nplt.imshow(attention_map, alpha=0.5, cmap='jet')\nplt.axis('off')\nplt.colorbar()\nplt.savefig('attention_map.png')\nplt.close()\nThis guide should help you get started with DINOv2 and explore its capabilities for various computer vision tasks. As a self-supervised vision foundation model, DINOv2 provides a strong starting point for numerous applications with minimal labeled data requirements."
  },
  {
    "objectID": "posts/dinov2/index.html#table-of-contents",
    "href": "posts/dinov2/index.html#table-of-contents",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "Introduction to DINOv2\nInstallation and Setup\nLoading Pre-trained Models\nFeature Extraction\nFine-tuning for Downstream Tasks\nImage Classification Example\nSemantic Segmentation Example\nObject Detection Example\nAdvanced Usage and Customization\nPerformance Benchmarks\nTroubleshooting"
  },
  {
    "objectID": "posts/dinov2/index.html#introduction-to-dinov2",
    "href": "posts/dinov2/index.html#introduction-to-dinov2",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "DINOv2 is a self-supervised learning method for vision that produces high-quality visual features without requiring labeled data. It extends the original DINO architecture with several improvements:\n\nTraining on a large and diverse dataset of images\nEnhanced teacher-student architecture\nImproved augmentation strategy\nMulti-scale feature learning\nSupport for various Vision Transformer (ViT) backbones\n\nThe result is a versatile foundation model that can be adapted to numerous vision tasks with minimal fine-tuning."
  },
  {
    "objectID": "posts/dinov2/index.html#installation-and-setup",
    "href": "posts/dinov2/index.html#installation-and-setup",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "To use DINOv2, youâ€™ll need to install the official implementation:\n# Install PyTorch first if not already installed\n# pip install torch torchvision\n\n# Install DINOv2\npip install git+https://github.com/facebookresearch/dinov2\nAlternatively, you can clone the repository and install it locally:\ngit clone https://github.com/facebookresearch/dinov2.git\ncd dinov2\npip install -e .\n\n\nDINOv2 requires:\n\nPython 3.8+\nPyTorch 1.12+\ntorchvision\nCUDA (for GPU acceleration)"
  },
  {
    "objectID": "posts/dinov2/index.html#loading-pre-trained-models",
    "href": "posts/dinov2/index.html#loading-pre-trained-models",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "DINOv2 provides several pre-trained models with different sizes and capabilities:\nimport torch\nfrom dinov2.models import build_model_from_cfg\nfrom dinov2.configs import get_config\n\n# Available model sizes: 'small', 'base', 'large', 'giant'\nmodel_size = 'base'\ncfg = get_config(f\"dinov2_{model_size}\")\nmodel = build_model_from_cfg(cfg)\n\n# Load pre-trained weights\ncheckpoint_path = f\"dinov2_{model_size}_pretrain.pth\"  # Download this from Meta AI's repository\ncheckpoint = torch.load(checkpoint_path, map_location=\"cpu\")\nmodel.load_state_dict(checkpoint[\"model\"])\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nmodel.eval()  # Set to evaluation mode\nYou can also use the Hugging Face Transformers library for an easier integration:\nfrom transformers import AutoImageProcessor, AutoModel\n\n# Available model sizes: 'small', 'base', 'large', 'giant'\nmodel_name = \"facebook/dinov2-base\"\nprocessor = AutoImageProcessor.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)"
  },
  {
    "objectID": "posts/dinov2/index.html#feature-extraction",
    "href": "posts/dinov2/index.html#feature-extraction",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "One of DINOv2â€™s key strengths is its ability to extract powerful visual features:\nimport torch\nfrom PIL import Image\nimport torchvision.transforms as T\nfrom transformers import AutoImageProcessor, AutoModel\n\n# Load model\nmodel_name = \"facebook/dinov2-base\"\nprocessor = AutoImageProcessor.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\nmodel.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\nmodel.eval()\n\n# Load and preprocess image\nimage = Image.open(\"path/to/your/image.jpg\").convert(\"RGB\")\ninputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n\n# Extract features\nwith torch.no_grad():\n    outputs = model(**inputs)\n    \n# Get CLS token features (useful for classification tasks)\ncls_features = outputs.last_hidden_state[:, 0]\n\n# Get patch features (useful for dense prediction tasks like segmentation)\npatch_features = outputs.last_hidden_state[:, 1:]\n\nprint(f\"CLS features shape: {cls_features.shape}\")\nprint(f\"Patch features shape: {patch_features.shape}\")"
  },
  {
    "objectID": "posts/dinov2/index.html#fine-tuning-for-downstream-tasks",
    "href": "posts/dinov2/index.html#fine-tuning-for-downstream-tasks",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "DINOv2 can be fine-tuned for specific vision tasks:\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModel\n\n# Load pre-trained DINOv2 model\nbackbone = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n\n# Create a custom classification head\nclass ClassificationHead(nn.Module):\n    def __init__(self, backbone, num_classes=1000):\n        super().__init__()\n        self.backbone = backbone\n        self.classifier = nn.Linear(backbone.config.hidden_size, num_classes)\n        \n    def forward(self, x):\n        outputs = self.backbone(x)\n        cls_token = outputs.last_hidden_state[:, 0]\n        return self.classifier(cls_token)\n\n# Create the complete model\nmodel = ClassificationHead(backbone, num_classes=100)  # For 100 classes\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop example\ndef train_one_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    \n    for batch in dataloader:\n        images = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(dataloader)"
  },
  {
    "objectID": "posts/dinov2/index.html#image-classification-example",
    "href": "posts/dinov2/index.html#image-classification-example",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "Hereâ€™s a complete example for image classification using DINOv2:\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as transforms\nfrom transformers import AutoImageProcessor, AutoModel\n\n# Define the dataset and transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Load your dataset (adjust the path)\ntrain_dataset = ImageFolder(root=\"path/to/train\", transform=transform)\nval_dataset = ImageFolder(root=\"path/to/val\", transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n\n# Create the model\nclass DINOv2Classifier(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.dinov2 = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n        self.classifier = nn.Linear(768, num_classes)  # 768 is the hidden size for base model\n        \n    def forward(self, x):\n        # Extract features\n        with torch.set_grad_enabled(self.training):\n            features = self.dinov2(x).last_hidden_state[:, 0]  # Get CLS token\n        \n        # Classify\n        logits = self.classifier(features)\n        return logits\n\n# Initialize model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DINOv2Classifier(num_classes=len(train_dataset.classes))\nmodel = model.to(device)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW([\n    {'params': model.classifier.parameters(), 'lr': 1e-3},\n    {'params': model.dinov2.parameters(), 'lr': 1e-5}\n])\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    # Training\n    model.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    \n    for inputs, targets in train_loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n    \n    train_accuracy = 100 * correct / total\n    \n    # Validation\n    model.eval()\n    val_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            \n            val_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    \n    val_accuracy = 100 * correct / total\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    print(f\"Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_accuracy:.2f}%\")\n    print(f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_accuracy:.2f}%\")"
  },
  {
    "objectID": "posts/dinov2/index.html#semantic-segmentation-example",
    "href": "posts/dinov2/index.html#semantic-segmentation-example",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "DINOv2 is particularly powerful for segmentation tasks:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModel\n\nclass DINOv2Segmenter(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        # Load DINOv2 backbone\n        self.backbone = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n        \n        # Define segmentation head\n        hidden_dim = self.backbone.config.hidden_size\n        self.segmentation_head = nn.Sequential(\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(hidden_dim, num_classes, kernel_size=1)\n        )\n        \n        # Image size and patch size for reshaping\n        self.image_size = 224\n        self.patch_size = 14  # For ViT-Base\n        self.num_patches = (self.image_size // self.patch_size) ** 2\n        \n    def forward(self, x):\n        # Get patch features\n        outputs = self.backbone(x)\n        patch_features = outputs.last_hidden_state[:, 1:]  # Remove CLS token\n        \n        # Reshape to 2D spatial layout\n        B = x.shape[0]\n        H = W = self.image_size // self.patch_size\n        patch_features = patch_features.reshape(B, H, W, -1).permute(0, 3, 1, 2)\n        \n        # Apply segmentation head\n        segmentation_logits = self.segmentation_head(patch_features)\n        \n        # Upsample to original image size\n        segmentation_logits = F.interpolate(\n            segmentation_logits, \n            size=(self.image_size, self.image_size), \n            mode='bilinear', \n            align_corners=False\n        )\n        \n        return segmentation_logits\n\n# Create model and move to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DINOv2Segmenter(num_classes=21)  # 21 classes for Pascal VOC\nmodel = model.to(device)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW([\n    {'params': model.segmentation_head.parameters(), 'lr': 1e-3},\n    {'params': model.backbone.parameters(), 'lr': 1e-5}\n])\ncriterion = nn.CrossEntropyLoss(ignore_index=255)  # 255 is typically the ignore index\n\n# Rest of the training code would be similar to the classification example"
  },
  {
    "objectID": "posts/dinov2/index.html#object-detection-example",
    "href": "posts/dinov2/index.html#object-detection-example",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "Hereâ€™s how to use DINOv2 features for object detection with a simple detection head:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModel\n\nclass DINOv2Detector(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        # Load DINOv2 backbone\n        self.backbone = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n        hidden_dim = self.backbone.config.hidden_size\n        \n        # Detection heads\n        self.box_predictor = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, 4)  # (x1, y1, x2, y2)\n        )\n        \n        self.class_predictor = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, num_classes + 1)  # +1 for background\n        )\n        \n        # Image size and patch size for feature map creation\n        self.image_size = 224\n        self.patch_size = 14  # For ViT-Base\n        \n    def forward(self, x):\n        # Get features\n        outputs = self.backbone(x)\n        features = outputs.last_hidden_state[:, 1:]  # Remove CLS token\n        \n        # Reshape to 2D spatial layout\n        B = x.shape[0]\n        H = W = self.image_size // self.patch_size\n        features = features.reshape(B, H, W, -1)\n        \n        # Flatten for prediction heads\n        features_flat = features.reshape(B, -1, features.shape[-1])\n        \n        # Predict boxes and classes\n        boxes = self.box_predictor(features_flat)\n        classes = self.class_predictor(features_flat)\n        \n        return {'boxes': boxes, 'classes': classes, 'features_map': features}\n\n# Create model and move to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DINOv2Detector(num_classes=80)  # 80 classes for COCO\nmodel = model.to(device)\n\n# Training would require a more complex detection pipeline with NMS, etc."
  },
  {
    "objectID": "posts/dinov2/index.html#advanced-usage-and-customization",
    "href": "posts/dinov2/index.html#advanced-usage-and-customization",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "You can customize the DINOv2 model architecture:\nfrom dinov2.configs import get_config\nfrom dinov2.models import build_model_from_cfg\n\n# Get default configuration and modify it\ncfg = get_config(\"dinov2_base\")\n\n# Modify configuration\ncfg.student.drop_path_rate = 0.2  # Change stochastic depth rate\ncfg.student.num_registers = 16    # Change the number of registers\n\n# Build model from modified config\nmodel = build_model_from_cfg(cfg)\n\n\n\nFor some applications, you might want to extract features from intermediate layers:\nimport torch\nfrom transformers import AutoModel\nfrom torch.utils.hooks import RemovableHandle\n\nclass FeatureExtractor:\n    def __init__(self, model, layers=None):\n        self.model = model\n        self.features = {}\n        self.hooks = []\n        \n        # Default to extracting from the last block if no layers specified\n        self.layers = layers if layers is not None else [11]  # Base model has 12 blocks (0-11)\n        \n        # Register hooks\n        for idx in self.layers:\n            hook = self.model.encoder.layer[idx].register_forward_hook(\n                lambda module, input, output, idx=idx: self.features.update({f\"layer_{idx}\": output})\n            )\n            self.hooks.append(hook)\n    \n    def __call__(self, x):\n        self.features.clear()\n        with torch.no_grad():\n            outputs = self.model(x)\n        return self.features\n    \n    def remove_hooks(self):\n        for hook in self.hooks:\n            hook.remove()\n\n# Usage\nmodel = AutoModel.from_pretrained(\"facebook/dinov2-base\")\nextractor = FeatureExtractor(model, layers=[3, 7, 11])\n\n# Extract features\nfeatures = extractor(input_image)\nlayer_3_features = features[\"layer_3\"]\nlayer_7_features = features[\"layer_7\"]\nlayer_11_features = features[\"layer_11\"]\n\n# Clean up\nextractor.remove_hooks()"
  },
  {
    "objectID": "posts/dinov2/index.html#performance-benchmarks",
    "href": "posts/dinov2/index.html#performance-benchmarks",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "DINOv2 achieves excellent results across various vision tasks. Here are typical performance metrics:\n\nImageNet-1K Classification (top-1 accuracy):\n\nDINOv2-Small: ~80.0%\nDINOv2-Base: ~84.5%\nDINOv2-Large: ~86.3%\nDINOv2-Giant: ~87.0%\n\nSemantic Segmentation (ADE20K) (mIoU):\n\nDINOv2-Small: ~47.5%\nDINOv2-Base: ~50.2%\nDINOv2-Large: ~52.5%\nDINOv2-Giant: ~53.8%\n\nObject Detection (COCO) (AP):\n\nDINOv2-Small: ~48.5%\nDINOv2-Base: ~51.3%\nDINOv2-Large: ~53.2%\nDINOv2-Giant: ~54.5%"
  },
  {
    "objectID": "posts/dinov2/index.html#troubleshooting",
    "href": "posts/dinov2/index.html#troubleshooting",
    "title": "DINOv2: Comprehensive Implementation Guide",
    "section": "",
    "text": "Out of Memory Errors\n\nReduce batch size\nUse gradient accumulation\nUse a smaller model variant (Small or Base)\nUse mixed precision training\n\n\n# Example of mixed precision training\nfrom torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nfor inputs, targets in train_loader:\n    inputs, targets = inputs.to(device), targets.to(device)\n    \n    optimizer.zero_grad()\n    \n    # Use autocast for mixed precision\n    with autocast():\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n    \n    # Scale loss and backprop\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n\nSlow Inference\n\nUse batch processing\nUse model.eval() and torch.no_grad()\nConsider model distillation or quantization\n\nPoor Performance on Downstream Tasks\n\nEnsure proper data preprocessing\nAdjust learning rates (lower for backbone, higher for heads)\nUse appropriate augmentations\nConsider using a larger variant of DINOv2\n\n\n\n\n\n\nVisualize model attention maps to understand what the model focuses on:\n\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom PIL import Image\nimport torchvision.transforms as T\n\ndef get_attention_map(model, img_tensor):\n    model.eval()\n    with torch.no_grad():\n        outputs = model(img_tensor.unsqueeze(0), output_attentions=True)\n    \n    # Get attention weights from the last layer\n    att_mat = outputs.attentions[-1]\n    \n    # Average attention across heads\n    att_mat = att_mat.mean(dim=1)\n    \n    # Extract attention for cls token to patch tokens\n    cls_att_map = att_mat[0, 0, 1:].reshape(14, 14)\n    \n    return cls_att_map.cpu().numpy()\n\n# Load and preprocess image\nimage = Image.open(\"path/to/image.jpg\").convert(\"RGB\")\ntransform = T.Compose([\n    T.Resize((224, 224)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\nimg_tensor = transform(image).to(device)\n\n# Get attention map\nfrom transformers import AutoModel\nmodel = AutoModel.from_pretrained(\"facebook/dinov2-base\", output_attentions=True)\nmodel.to(device)\nattention_map = get_attention_map(model, img_tensor)\n\n# Visualize\nplt.figure(figsize=(10, 10))\nplt.imshow(image.resize((224, 224)))\nplt.imshow(attention_map, alpha=0.5, cmap='jet')\nplt.axis('off')\nplt.colorbar()\nplt.savefig('attention_map.png')\nplt.close()\nThis guide should help you get started with DINOv2 and explore its capabilities for various computer vision tasks. As a self-supervised vision foundation model, DINOv2 provides a strong starting point for numerous applications with minimal labeled data requirements."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My articles",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMar 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nPyTorch Lightning: A Comprehensive Guide\n\n\n\ncode\n\ntutorial\n\nadvanced\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMar 29, 2025\n\n\n\n\n\n\n\n\n\n\n\nFrom Pandas to Polars\n\n\n\ncode\n\ntutorial\n\nadvanced\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nApr 5, 2025\n\n\n\n\n\n\n\n\n\n\n\nPython Data Visualization: Matplotlib vs Seaborn vs Altair\n\n\n\ncode\n\ntutorial\n\nbeginner\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nApr 12, 2025\n\n\n\n\n\n\n\n\n\n\n\nActive Learning Influence Selection: A Comprehensive Guide\n\n\n\ncode\n\nresearch\n\nadvanced\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nApr 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nVision Transformer (ViT) Implementation Guide\n\n\n\ncode\n\ntutorial\n\nadvanced\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nApr 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nDINOv2: Comprehensive Implementation Guide\n\n\n\ncode\n\ntutorial\n\nbeginner\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 3, 2025\n\n\n\n\n\n\n\n\n\n\n\nDINO: Emerging Properties in Self-Supervised Vision Transformers\n\n\n\nresearch\n\nintermediate\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 13, 2025\n\n\n\n\n\n\n\n\n\n\n\nDINOv2: A Deep Dive into Architecture and Training\n\n\n\nresearch\n\nintermediate\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 17, 2025\n\n\n\n\n\n\n\n\n\n\n\nPython 3.14: The Next Evolution in Python Development\n\n\n\nnews\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nVision Transformers (ViT): A Simple Guide\n\n\n\nresearch\n\nbeginner\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 24, 2025\n\n\n\n\n\n\n\n\n\n\n\nPython 3.14: Key Improvements and New Features\n\n\n\ncode\n\nbeginner\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 24, 2025\n\n\n\n\n\n\n\n\n\n\n\nPyTorch to PyTorch Lightning Migration Guide\n\n\n\ncode\n\ntutorial\n\nadvanced\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 25, 2025\n\n\n\n\n\n\n\n\n\n\n\nMobileNetV2 PyTorch Docker Deployment Guide\n\n\n\ncode\n\nmlops\n\nintermediate\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nLitServe with MobileNetV2 - Complete Code Guide\n\n\n\ncode\n\nmlops\n\nintermediate\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 27, 2025\n\n\n\n\n\n\n\n\n\n\n\nLitServe Code Guide\n\n\n\ncode\n\nmlops\n\nbeginner\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 27, 2025\n\n\n\n\n\n\n\n\n\n\n\nAlbumentations vs TorchVision Transforms: Complete Code Guide\n\n\n\ncode\n\ntutorial\n\nbeginner\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 27, 2025\n\n\n\n\n\n\n\n\n\n\n\nStudent-Teacher Network Training Guide in PyTorch\n\n\n\ncode\n\nadvanced\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 28, 2025\n\n\n\n\n\n\n\n\n\n\n\nDINOv2 Student-Teacher Network Training Guide\n\n\n\ncode\n\nadvanced\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 28, 2025\n\n\n\n\n\n\n\n\n\n\n\nSelf-Supervised Learning: Training AI Without Labels\n\n\n\nresearch\n\nbeginner\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 29, 2025\n\n\n\n\n\n\n\n\n\n\n\nCLIP Code Guide: Complete Implementation and Usage\n\n\n\ncode\n\ntutorial\n\nbeginner\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 29, 2025\n\n\n\n\n\n\n\n\n\n\n\nMLflow for PyTorch - Complete Guide\n\n\n\ncode\n\nmlops\n\nbeginner\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 30, 2025\n\n\n\n\n\n\n\n\n\n\n\nKubeflow: A Comprehensive Guide to Machine Learning on Kubernetes\n\n\n\ntutorial\n\nmlops\n\nbeginner\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 31, 2025\n\n\n\n\n\n\n\n\n\n\n\nKubeflow Deep Learning Guide with PyTorch\n\n\n\ncode\n\nmlops\n\nintermediate\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMay 31, 2025\n\n\n\n\n\n\n\n\n\n\n\nWhy I Choose PyTorch for Deep Learning\n\n\n\nnews\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nJun 1, 2025\n\n\n\n\n\n\n\n\n\n\n\nPyTorch Training and Inference Optimization Guide\n\n\n\ncode\n\ntutorial\n\nintermediate\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nJun 1, 2025\n\n\n\n\n\n\n\n\n\n\n\nPyTorch Collate Function Speed-Up Guide\n\n\n\ncode\n\ntutorial\n\nadvanced\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nJun 1, 2025\n\n\n\n\n\n\n\n\n\n\n\nPyTorch Lightning Fabric Code Guide\n\n\n\ncode\n\ntutorial\n\nbeginner\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nJun 3, 2025\n\n\n\n\n\n\n\n\n\n\n\nHugging Face Accelerate vs PyTorch Lightning Fabric: A Deep Dive Comparison\n\n\n\nresearch\n\nbeginner\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nJun 3, 2025\n\n\n\n\n\n\n\n\n\n\n\nHugging Face Accelerate Code Guide\n\n\n\ncode\n\ntutorial\n\nbeginner\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nJun 3, 2025\n\n\n\n\n\n\nNo matching items"
  }
]