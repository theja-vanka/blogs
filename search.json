[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Hi üëã, I‚Äôm Krishnatheja Vanka\n\nMachine Learning Engineer (Applied Computer Vision)"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html",
    "href": "posts/pandas-to-polars/index.html",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "As datasets grow in size and complexity, performance and efficiency become critical in data processing. While Pandas has long been the go-to library for data manipulation in Python, it can struggle with speed and memory usage, especially on large datasets. Polars, a newer DataFrame library written in Rust, offers a faster, more memory-efficient alternative with support for lazy evaluation and multi-threading.\nThis guide explores how to convert Pandas DataFrames to Polars, and highlights key differences in syntax, performance, and functionality. Whether you‚Äôre looking to speed up your data workflows or just exploring modern tools, understanding the transition from Pandas to Polars is a valuable step.\n\n\n\nInstallation and Setup\nCreating DataFrames\nBasic Operations\nFiltering Data\nGrouping and Aggregation\nJoining/Merging DataFrames\nHandling Missing Values\nString Operations\nTime Series Operations\nPerformance Comparison\nAPI Philosophy Differences\nMigration Guide\n\n\n\n\n\n\n\n# Import pandas\nimport pandas as pd\n\n\n\n\n\n# Import polars\nimport polars as pl\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\n\n# Create DataFrame from dictionary\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'age': [25, 30, 35, 40],\n    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n}\ndf_pd = pd.DataFrame(data)\nprint(df_pd)\n\n      name  age         city\n0    Alice   25     New York\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n3    David   40      Houston\n\n\n\n\n\n\nimport polars as pl\n\n# Create DataFrame from dictionary\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'age': [25, 30, 35, 40],\n    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n}\ndf_pl = pl.DataFrame(data)\nprint(df_pl)\n\nshape: (4, 3)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ name    ‚îÜ age ‚îÜ city        ‚îÇ\n‚îÇ ---     ‚îÜ --- ‚îÜ ---         ‚îÇ\n‚îÇ str     ‚îÜ i64 ‚îÜ str         ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ Alice   ‚îÜ 25  ‚îÜ New York    ‚îÇ\n‚îÇ Bob     ‚îÜ 30  ‚îÜ Los Angeles ‚îÇ\n‚îÇ Charlie ‚îÜ 35  ‚îÜ Chicago     ‚îÇ\n‚îÇ David   ‚îÜ 40  ‚îÜ Houston     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\n\n\n\n\n\n\n\n\n# Select a single column (returns Series)\nseries = df_pd['name']\n\n# Select multiple columns\ndf_subset = df_pd[['name', 'age']]\n\n\n\n\n\n# Select a single column (returns Series)\nseries = df_pl['name']\n# Alternative method\nseries = df_pl.select(pl.col('name')).to_series()\n\n# Select multiple columns\ndf_subset = df_pl.select(['name', 'age'])\n# Alternative method\ndf_subset = df_pl.select(pl.col(['name', 'age']))\n\n\n\n\n\n\n\n\n# Add a new column\ndf_pd['is_adult'] = df_pd['age'] &gt;= 18\n\n# Using assign (creates a new DataFrame)\ndf_pd = df_pd.assign(age_squared=df_pd['age'] ** 2)\n\n\n\n\n\n# Add a new column\ndf_pl = df_pl.with_columns(\n    pl.when(pl.col('age') &gt;= 18).then(True).otherwise(False).alias('is_adult')\n)\n\n# Creating derived columns \ndf_pl = df_pl.with_columns(\n    (pl.col('age') ** 2).alias('age_squared')\n)\n\n# Multiple columns at once\ndf_pl = df_pl.with_columns([\n    pl.col('age').is_null().alias('age_is_null'),\n    (pl.col('age') * 2).alias('age_doubled')\n])\n\n\n\n\n\n\n\n\n# Get summary statistics\nsummary = df_pd.describe()\n\n# Individual statistics\nmean_age = df_pd['age'].mean()\nmedian_age = df_pd['age'].median()\nmin_age = df_pd['age'].min()\nmax_age = df_pd['age'].max()\n\n\n\n\n\n# Get summary statistics\nsummary = df_pl.describe()\n\n# Individual statistics\nmean_age = df_pl.select(pl.col('age').mean()).item()\nmedian_age = df_pl.select(pl.col('age').median()).item()\nmin_age = df_pl.select(pl.col('age').min()).item()\nmax_age = df_pl.select(pl.col('age').max()).item()\n\n\n\n\n\n\n\n\n\n\n\n# Filter rows\nadults = df_pd[df_pd['age'] &gt;= 18]\n\n# Multiple conditions\nfiltered = df_pd[(df_pd['age'] &gt; 30) & (df_pd['city'] == 'Chicago')]\n\n\n\n\n\n# Filter rows\nadults = df_pl.filter(pl.col('age') &gt;= 18)\n\n# Multiple conditions\nfiltered = df_pl.filter((pl.col('age') &gt; 30) & (pl.col('city') == 'Chicago'))\n\n\n\n\n\n\n\n\n# Filter with OR conditions\ndf_filtered = df_pd[(df_pd['city'] == 'New York') | (df_pd['city'] == 'Chicago')]\n\n# Using isin\ncities = ['New York', 'Chicago']\ndf_filtered = df_pd[df_pd['city'].isin(cities)]\n\n# String contains\ndf_filtered = df_pd[df_pd['name'].str.contains('li')]\n\n\n\n\n\n# Filter with OR conditions\ndf_filtered = df_pl.filter((pl.col('city') == 'New York') | (pl.col('city') == 'Chicago'))\n\n# Using is_in\ncities = ['New York', 'Chicago']\ndf_filtered = df_pl.filter(pl.col('city').is_in(cities))\n\n# String contains\ndf_filtered = df_pl.filter(pl.col('name').str.contains('li'))\n\n\n\n\n\n\n\n\n\n\n\n# Group by one column and aggregate\ncity_stats = df_pd.groupby('city').agg({\n    'age': ['mean', 'min', 'max', 'count']\n})\n\n# Reset index for flat DataFrame\ncity_stats = city_stats.reset_index()\n\n\n\n\n\n# Group by one column and aggregate\ncity_stats = df_pl.group_by('city').agg([\n    pl.col('age').mean().alias('age_mean'),\n    pl.col('age').min().alias('age_min'),\n    pl.col('age').max().alias('age_max'),\n    pl.col('age').count().alias('age_count')\n])\n\n\n\n\n\n\n\n\n\n\n\n# Create another DataFrame\nemployee_data = {\n    'emp_id': [1, 2, 3, 4],\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'dept': ['HR', 'IT', 'Finance', 'IT']\n}\nemployee_df_pd = pd.DataFrame(employee_data)\n\nsalary_data = {\n    'emp_id': [1, 2, 3, 5],\n    'salary': [50000, 60000, 70000, 80000]\n}\nsalary_df_pd = pd.DataFrame(salary_data)\n\n# Inner join\nmerged_df = employee_df_pd.merge(\n    salary_df_pd,\n    on='emp_id',\n    how='inner'\n)\n\n\n\n\n\n# Create another DataFrame\nemployee_data = {\n    'emp_id': [1, 2, 3, 4],\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'dept': ['HR', 'IT', 'Finance', 'IT']\n}\nemployee_df_pl = pl.DataFrame(employee_data)\n\nsalary_data = {\n    'emp_id': [1, 2, 3, 5],\n    'salary': [50000, 60000, 70000, 80000]\n}\nsalary_df_pl = pl.DataFrame(salary_data)\n\n# Inner join\nmerged_df = employee_df_pl.join(\n    salary_df_pl,\n    on='emp_id',\n    how='inner'\n)\n\n\n\n\n\n\n\n\n# Left join\nleft_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='left')\n\n# Right join\nright_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='right')\n\n# Outer join\nouter_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='outer')\n\n\n\n\n\n# Left join\nleft_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='left')\n\n# Right join\nright_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='right')\n\n# Outer join\nouter_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='full')\n\n\n\n\n\n\n\n\n\n\n\n# Check for missing values\nmissing_count = df_pd.isnull().sum()\n\n# Check if any column has missing values\nhas_missing = df_pd.isnull().any().any()\n\n\n\n\n\n# Check for missing values\nmissing_count = df_pl.null_count()\n\n# Check if specific column has missing values\nhas_missing = df_pl.select(pl.col('age').is_null().any()).item()\n\n\n\n\n\n\n\n\n# Drop rows with any missing values\ndf_pd_clean = df_pd.dropna()\n\n# Fill missing values\ndf_pd_filled = df_pd.fillna({\n    'age': 0,\n    'city': 'Unknown'\n})\n\n# Forward fill\ndf_pd_ffill = df_pd.ffill()\n\n\n\n\n\n# Drop rows with any missing values\ndf_pl_clean = df_pl.drop_nulls()\n\n# Fill missing values\ndf_pl_filled = df_pl.with_columns([\n    pl.col('age').fill_null(0),\n    pl.col('city').fill_null('Unknown')\n])\n\n# Forward fill\ndf_pl_ffill = df_pl.with_columns([\n    pl.col('age').fill_null(strategy='forward'),\n    pl.col('city').fill_null(strategy='forward')\n])\n\n\n\n\n\n\n\n\n\n\n\n# Convert to uppercase\ndf_pd['name_upper'] = df_pd['name'].str.upper()\n\n# Get string length\ndf_pd['name_length'] = df_pd['name'].str.len()\n\n# Extract substring\ndf_pd['name_first_char'] = df_pd['name'].str[0]\n\n# Replace substrings\ndf_pd['city_replaced'] = df_pd['city'].str.replace('New', 'Old')\n\n\n\n\n\n# Convert to uppercase\ndf_pl = df_pl.with_columns(pl.col('name').str.to_uppercase().alias('name_upper'))\n\n# Get string length\ndf_pl = df_pl.with_columns(pl.col('name').str.len_chars().alias('name_length'))\n\n# Extract substring \ndf_pl = df_pl.with_columns(pl.col('name').str.slice(0, 1).alias('name_first_char'))\n\n# Replace substrings\ndf_pl = df_pl.with_columns(pl.col('city').str.replace('New', 'Old').alias('city_replaced'))\n\n\n\n\n\n\n\n\n# Split string\ndf_pd['first_word'] = df_pd['city'].str.split(' ').str[0]\n\n# Pattern matching\nhas_new = df_pd['city'].str.contains('New')\n\n# Extract with regex\ndf_pd['extracted'] = df_pd['city'].str.extract(r'(\\w+)\\s')\n\n\n\n\n\n# Split string\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.split(' ').list.get(0).alias('first_word')\n)\n\n# Pattern matching\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.contains('New').alias('has_new')\n)\n\n# Extract with regex\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.extract(r'(\\w+)\\s').alias('extracted')\n)\n\n\n\n\n\n\n\n\n\n\n\n# Create DataFrame with dates\ndates_pd = pd.DataFrame({\n    'date_str': ['2023-01-01', '2023-02-15', '2023-03-30']\n})\n\n# Parse dates\ndates_pd['date'] = pd.to_datetime(dates_pd['date_str'])\n\n# Extract components\ndates_pd['year'] = dates_pd['date'].dt.year\ndates_pd['month'] = dates_pd['date'].dt.month\ndates_pd['day'] = dates_pd['date'].dt.day\ndates_pd['weekday'] = dates_pd['date'].dt.day_name()\n\n\n\n\n\n# Create DataFrame with dates\ndates_pl = pl.DataFrame({\n    'date_str': ['2023-01-01', '2023-02-15', '2023-03-30']\n})\n\n# Parse dates\ndates_pl = dates_pl.with_columns(\n    pl.col('date_str').str.strptime(pl.Datetime, '%Y-%m-%d').alias('date')\n)\n\n# Extract components\ndates_pl = dates_pl.with_columns([\n    pl.col('date').dt.year().alias('year'),\n    pl.col('date').dt.month().alias('month'),\n    pl.col('date').dt.day().alias('day'),\n    pl.col('date').dt.weekday().replace_strict({\n        0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', \n        3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'\n    }, default=\"unknown\").alias('weekday')\n])\n\n\n\n\n\n\n\n\n# Add days\ndates_pd['next_week'] = dates_pd['date'] + pd.Timedelta(days=7)\n\n# Date difference\ndate_range = pd.date_range(start='2023-01-01', end='2023-01-10')\ndf_dates = pd.DataFrame({'date': date_range})\ndf_dates['days_since_start'] = (df_dates['date'] - df_dates['date'].min()).dt.days\n\n\n\n\n\n# Add days\ndates_pl = dates_pl.with_columns(\n    (pl.col('date') + pl.duration(days=7)).alias('next_week')\n)\n\n# Date difference\ndate_range = pd.date_range(start='2023-01-01', end='2023-01-10')  # Using pandas to generate range\ndf_dates = pl.DataFrame({'date': date_range})\ndf_dates = df_dates.with_columns(\n    (pl.col('date') - pl.col('date').min()).dt.total_days().alias('days_since_start')\n)\n\n\n\n\n\n\nThis section demonstrates performance differences between pandas and polars for a large dataset operation.\n\nimport pandas as pd\nimport polars as pl\nimport time\nimport numpy as np\n\n# Generate a large dataset (10 million rows)\nn = 10_000_000\ndata = {\n    'id': np.arange(n),\n    'value': np.random.randn(n),\n    'group': np.random.choice(['A', 'B', 'C', 'D'], n)\n}\n\n# Convert to pandas DataFrame\ndf_pd = pd.DataFrame(data)\n\n# Convert to polars DataFrame\ndf_pl = pl.DataFrame(data)\n\n# Benchmark: Group by and calculate mean, min, max\nprint(\"Running pandas groupby...\")\nstart = time.time()\nresult_pd = df_pd.groupby('group').agg({\n    'value': ['mean', 'min', 'max', 'count']\n})\npd_time = time.time() - start\nprint(f\"Pandas time: {pd_time:.4f} seconds\")\n\nprint(\"Running polars groupby...\")\nstart = time.time()\nresult_pl = df_pl.group_by('group').agg([\n    pl.col('value').mean().alias('value_mean'),\n    pl.col('value').min().alias('value_min'),\n    pl.col('value').max().alias('value_max'),\n    pl.col('value').count().alias('value_count')\n])\npl_time = time.time() - start\nprint(f\"Polars time: {pl_time:.4f} seconds\")\n\nprint(f\"Polars is {pd_time / pl_time:.2f}x faster\")\n\nRunning pandas groupby...\nPandas time: 0.2343 seconds\nRunning polars groupby...\nPolars time: 0.0697 seconds\nPolars is 3.36x faster\n\n\nTypically, for operations like this, Polars will be 3-10x faster than pandas, especially as data sizes increase. The performance gap widens further with more complex operations that can benefit from query optimization.\n\n\n\nPandas and Polars differ in several fundamental aspects:\n\n\nPandas uses eager execution by default:\nPolars supports both eager and lazy execution:\n\n\n\nPandas often uses assignment operations:\n\n# Many pandas operations use in-place assignment\npd_df['new_col'] = pd_df['new_col'] * 2\npd_df['new_col'] = pd_df['new_col'].fillna(0)\n\n# Some operations return new DataFrames\npd_df = pd_df.sort_values('new_col')\n\nPolars consistently uses method chaining:\n\n# All operations return new DataFrames and can be chained\npl_df = (pl_df\n    .with_columns((pl.col('new_col') * 2).alias('new_col'))\n    .with_columns(pl.col('new_col').fill_null(0))\n    .sort('new_col')\n)\n\n\n\n\nPandas directly references columns:\n\npd_df['result'] = pd_df['age'] + pd_df['new_col']\nfiltered = pd_df[pd_df['age'] &gt; pd_df['age'].mean()]\n\nPolars uses an expression API:\n\npl_df = pl_df.with_columns(\n    (pl.col('age') + pl.col('new_col')).alias('result')\n)\nfiltered = pl_df.filter(pl.col('age') &gt; pl.col('age').mean())\n\n\n\n\n\nIf you‚Äôre transitioning from pandas to polars, here are key mappings between common operations:\n\n\n\n\n\n\n\n\nOperation\nPandas\nPolars\n\n\n\n\nRead CSV\npd.read_csv('file.csv')\npl.read_csv('file.csv')\n\n\nSelect columns\ndf[['col1', 'col2']]\ndf.select(['col1', 'col2'])\n\n\nAdd column\ndf['new'] = df['col1'] * 2\ndf.with_columns((pl.col('col1') * 2).alias('new'))\n\n\nFilter rows\ndf[df['col'] &gt; 5]\ndf.filter(pl.col('col') &gt; 5)\n\n\nSort\ndf.sort_values('col')\ndf.sort('col')\n\n\nGroup by\ndf.groupby('col').agg({'val': 'sum'})\ndf.group_by('col').agg(pl.col('val').sum())\n\n\nJoin\ndf1.merge(df2, on='key')\ndf1.join(df2, on='key')\n\n\nFill NA\ndf.fillna(0)\ndf.fill_null(0)\n\n\nDrop NA\ndf.dropna()\ndf.drop_nulls()\n\n\nRename\ndf.rename(columns={'a': 'b'})\ndf.rename({'a': 'b'})\n\n\nUnique values\ndf['col'].unique()\ndf.select(pl.col('col').unique())\n\n\nValue counts\ndf['col'].value_counts()\ndf.group_by('col').count()\n\n\n\n\n\n\nThink in expressions: Use pl.col() to reference columns in operations\nEmbrace method chaining: String operations together instead of intermediate variables\nTry lazy execution: For complex operations, use pl.scan_csv() and lazy operations\nUse with_columns(): Instead of direct assignment, use with_columns for adding/modifying columns\nLearn the expression functions: Many operations like string manipulation use different syntax\n\n\n\n\nDespite Polars‚Äô advantages, pandas might still be preferred when:\n\nWorking with existing codebases heavily dependent on pandas\nUsing specialized libraries that only support pandas (some visualization tools)\nDealing with very small datasets where performance isn‚Äôt critical\nUsing pandas-specific features without polars equivalents\nWorking with time series data that benefits from pandas‚Äô specialized functionality\n\n\n\n\n\nPolars offers significant performance improvements and a more consistent API compared to pandas, particularly for large datasets and complex operations. While the syntax differences require some adjustment, the benefits in speed and memory efficiency make it a compelling choice for modern data analysis workflows.\nBoth libraries have their place in the Python data ecosystem. Pandas remains the more mature option with broader ecosystem compatibility, while Polars represents the future of high-performance data processing. For new projects dealing with large datasets, Polars is increasingly becoming the recommended choice."
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#table-of-contents",
    "href": "posts/pandas-to-polars/index.html#table-of-contents",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "Installation and Setup\nCreating DataFrames\nBasic Operations\nFiltering Data\nGrouping and Aggregation\nJoining/Merging DataFrames\nHandling Missing Values\nString Operations\nTime Series Operations\nPerformance Comparison\nAPI Philosophy Differences\nMigration Guide"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#installation-and-setup",
    "href": "posts/pandas-to-polars/index.html#installation-and-setup",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Import pandas\nimport pandas as pd\n\n\n\n\n\n# Import polars\nimport polars as pl"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#creating-dataframes",
    "href": "posts/pandas-to-polars/index.html#creating-dataframes",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "import pandas as pd\n\n# Create DataFrame from dictionary\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'age': [25, 30, 35, 40],\n    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n}\ndf_pd = pd.DataFrame(data)\nprint(df_pd)\n\n      name  age         city\n0    Alice   25     New York\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n3    David   40      Houston\n\n\n\n\n\n\nimport polars as pl\n\n# Create DataFrame from dictionary\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'age': [25, 30, 35, 40],\n    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n}\ndf_pl = pl.DataFrame(data)\nprint(df_pl)\n\nshape: (4, 3)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ name    ‚îÜ age ‚îÜ city        ‚îÇ\n‚îÇ ---     ‚îÜ --- ‚îÜ ---         ‚îÇ\n‚îÇ str     ‚îÜ i64 ‚îÜ str         ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ Alice   ‚îÜ 25  ‚îÜ New York    ‚îÇ\n‚îÇ Bob     ‚îÜ 30  ‚îÜ Los Angeles ‚îÇ\n‚îÇ Charlie ‚îÜ 35  ‚îÜ Chicago     ‚îÇ\n‚îÇ David   ‚îÜ 40  ‚îÜ Houston     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#basic-operations",
    "href": "posts/pandas-to-polars/index.html#basic-operations",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Select a single column (returns Series)\nseries = df_pd['name']\n\n# Select multiple columns\ndf_subset = df_pd[['name', 'age']]\n\n\n\n\n\n# Select a single column (returns Series)\nseries = df_pl['name']\n# Alternative method\nseries = df_pl.select(pl.col('name')).to_series()\n\n# Select multiple columns\ndf_subset = df_pl.select(['name', 'age'])\n# Alternative method\ndf_subset = df_pl.select(pl.col(['name', 'age']))\n\n\n\n\n\n\n\n\n# Add a new column\ndf_pd['is_adult'] = df_pd['age'] &gt;= 18\n\n# Using assign (creates a new DataFrame)\ndf_pd = df_pd.assign(age_squared=df_pd['age'] ** 2)\n\n\n\n\n\n# Add a new column\ndf_pl = df_pl.with_columns(\n    pl.when(pl.col('age') &gt;= 18).then(True).otherwise(False).alias('is_adult')\n)\n\n# Creating derived columns \ndf_pl = df_pl.with_columns(\n    (pl.col('age') ** 2).alias('age_squared')\n)\n\n# Multiple columns at once\ndf_pl = df_pl.with_columns([\n    pl.col('age').is_null().alias('age_is_null'),\n    (pl.col('age') * 2).alias('age_doubled')\n])\n\n\n\n\n\n\n\n\n# Get summary statistics\nsummary = df_pd.describe()\n\n# Individual statistics\nmean_age = df_pd['age'].mean()\nmedian_age = df_pd['age'].median()\nmin_age = df_pd['age'].min()\nmax_age = df_pd['age'].max()\n\n\n\n\n\n# Get summary statistics\nsummary = df_pl.describe()\n\n# Individual statistics\nmean_age = df_pl.select(pl.col('age').mean()).item()\nmedian_age = df_pl.select(pl.col('age').median()).item()\nmin_age = df_pl.select(pl.col('age').min()).item()\nmax_age = df_pl.select(pl.col('age').max()).item()"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#filtering-data",
    "href": "posts/pandas-to-polars/index.html#filtering-data",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Filter rows\nadults = df_pd[df_pd['age'] &gt;= 18]\n\n# Multiple conditions\nfiltered = df_pd[(df_pd['age'] &gt; 30) & (df_pd['city'] == 'Chicago')]\n\n\n\n\n\n# Filter rows\nadults = df_pl.filter(pl.col('age') &gt;= 18)\n\n# Multiple conditions\nfiltered = df_pl.filter((pl.col('age') &gt; 30) & (pl.col('city') == 'Chicago'))\n\n\n\n\n\n\n\n\n# Filter with OR conditions\ndf_filtered = df_pd[(df_pd['city'] == 'New York') | (df_pd['city'] == 'Chicago')]\n\n# Using isin\ncities = ['New York', 'Chicago']\ndf_filtered = df_pd[df_pd['city'].isin(cities)]\n\n# String contains\ndf_filtered = df_pd[df_pd['name'].str.contains('li')]\n\n\n\n\n\n# Filter with OR conditions\ndf_filtered = df_pl.filter((pl.col('city') == 'New York') | (pl.col('city') == 'Chicago'))\n\n# Using is_in\ncities = ['New York', 'Chicago']\ndf_filtered = df_pl.filter(pl.col('city').is_in(cities))\n\n# String contains\ndf_filtered = df_pl.filter(pl.col('name').str.contains('li'))"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#grouping-and-aggregation",
    "href": "posts/pandas-to-polars/index.html#grouping-and-aggregation",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Group by one column and aggregate\ncity_stats = df_pd.groupby('city').agg({\n    'age': ['mean', 'min', 'max', 'count']\n})\n\n# Reset index for flat DataFrame\ncity_stats = city_stats.reset_index()\n\n\n\n\n\n# Group by one column and aggregate\ncity_stats = df_pl.group_by('city').agg([\n    pl.col('age').mean().alias('age_mean'),\n    pl.col('age').min().alias('age_min'),\n    pl.col('age').max().alias('age_max'),\n    pl.col('age').count().alias('age_count')\n])"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#joiningmerging-dataframes",
    "href": "posts/pandas-to-polars/index.html#joiningmerging-dataframes",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Create another DataFrame\nemployee_data = {\n    'emp_id': [1, 2, 3, 4],\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'dept': ['HR', 'IT', 'Finance', 'IT']\n}\nemployee_df_pd = pd.DataFrame(employee_data)\n\nsalary_data = {\n    'emp_id': [1, 2, 3, 5],\n    'salary': [50000, 60000, 70000, 80000]\n}\nsalary_df_pd = pd.DataFrame(salary_data)\n\n# Inner join\nmerged_df = employee_df_pd.merge(\n    salary_df_pd,\n    on='emp_id',\n    how='inner'\n)\n\n\n\n\n\n# Create another DataFrame\nemployee_data = {\n    'emp_id': [1, 2, 3, 4],\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'dept': ['HR', 'IT', 'Finance', 'IT']\n}\nemployee_df_pl = pl.DataFrame(employee_data)\n\nsalary_data = {\n    'emp_id': [1, 2, 3, 5],\n    'salary': [50000, 60000, 70000, 80000]\n}\nsalary_df_pl = pl.DataFrame(salary_data)\n\n# Inner join\nmerged_df = employee_df_pl.join(\n    salary_df_pl,\n    on='emp_id',\n    how='inner'\n)\n\n\n\n\n\n\n\n\n# Left join\nleft_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='left')\n\n# Right join\nright_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='right')\n\n# Outer join\nouter_join = employee_df_pd.merge(salary_df_pd, on='emp_id', how='outer')\n\n\n\n\n\n# Left join\nleft_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='left')\n\n# Right join\nright_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='right')\n\n# Outer join\nouter_join = employee_df_pl.join(salary_df_pl, on='emp_id', how='full')"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#handling-missing-values",
    "href": "posts/pandas-to-polars/index.html#handling-missing-values",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Check for missing values\nmissing_count = df_pd.isnull().sum()\n\n# Check if any column has missing values\nhas_missing = df_pd.isnull().any().any()\n\n\n\n\n\n# Check for missing values\nmissing_count = df_pl.null_count()\n\n# Check if specific column has missing values\nhas_missing = df_pl.select(pl.col('age').is_null().any()).item()\n\n\n\n\n\n\n\n\n# Drop rows with any missing values\ndf_pd_clean = df_pd.dropna()\n\n# Fill missing values\ndf_pd_filled = df_pd.fillna({\n    'age': 0,\n    'city': 'Unknown'\n})\n\n# Forward fill\ndf_pd_ffill = df_pd.ffill()\n\n\n\n\n\n# Drop rows with any missing values\ndf_pl_clean = df_pl.drop_nulls()\n\n# Fill missing values\ndf_pl_filled = df_pl.with_columns([\n    pl.col('age').fill_null(0),\n    pl.col('city').fill_null('Unknown')\n])\n\n# Forward fill\ndf_pl_ffill = df_pl.with_columns([\n    pl.col('age').fill_null(strategy='forward'),\n    pl.col('city').fill_null(strategy='forward')\n])"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#string-operations",
    "href": "posts/pandas-to-polars/index.html#string-operations",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Convert to uppercase\ndf_pd['name_upper'] = df_pd['name'].str.upper()\n\n# Get string length\ndf_pd['name_length'] = df_pd['name'].str.len()\n\n# Extract substring\ndf_pd['name_first_char'] = df_pd['name'].str[0]\n\n# Replace substrings\ndf_pd['city_replaced'] = df_pd['city'].str.replace('New', 'Old')\n\n\n\n\n\n# Convert to uppercase\ndf_pl = df_pl.with_columns(pl.col('name').str.to_uppercase().alias('name_upper'))\n\n# Get string length\ndf_pl = df_pl.with_columns(pl.col('name').str.len_chars().alias('name_length'))\n\n# Extract substring \ndf_pl = df_pl.with_columns(pl.col('name').str.slice(0, 1).alias('name_first_char'))\n\n# Replace substrings\ndf_pl = df_pl.with_columns(pl.col('city').str.replace('New', 'Old').alias('city_replaced'))\n\n\n\n\n\n\n\n\n# Split string\ndf_pd['first_word'] = df_pd['city'].str.split(' ').str[0]\n\n# Pattern matching\nhas_new = df_pd['city'].str.contains('New')\n\n# Extract with regex\ndf_pd['extracted'] = df_pd['city'].str.extract(r'(\\w+)\\s')\n\n\n\n\n\n# Split string\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.split(' ').list.get(0).alias('first_word')\n)\n\n# Pattern matching\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.contains('New').alias('has_new')\n)\n\n# Extract with regex\ndf_pl = df_pl.with_columns(\n    pl.col('city').str.extract(r'(\\w+)\\s').alias('extracted')\n)"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#time-series-operations",
    "href": "posts/pandas-to-polars/index.html#time-series-operations",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "# Create DataFrame with dates\ndates_pd = pd.DataFrame({\n    'date_str': ['2023-01-01', '2023-02-15', '2023-03-30']\n})\n\n# Parse dates\ndates_pd['date'] = pd.to_datetime(dates_pd['date_str'])\n\n# Extract components\ndates_pd['year'] = dates_pd['date'].dt.year\ndates_pd['month'] = dates_pd['date'].dt.month\ndates_pd['day'] = dates_pd['date'].dt.day\ndates_pd['weekday'] = dates_pd['date'].dt.day_name()\n\n\n\n\n\n# Create DataFrame with dates\ndates_pl = pl.DataFrame({\n    'date_str': ['2023-01-01', '2023-02-15', '2023-03-30']\n})\n\n# Parse dates\ndates_pl = dates_pl.with_columns(\n    pl.col('date_str').str.strptime(pl.Datetime, '%Y-%m-%d').alias('date')\n)\n\n# Extract components\ndates_pl = dates_pl.with_columns([\n    pl.col('date').dt.year().alias('year'),\n    pl.col('date').dt.month().alias('month'),\n    pl.col('date').dt.day().alias('day'),\n    pl.col('date').dt.weekday().replace_strict({\n        0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', \n        3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'\n    }, default=\"unknown\").alias('weekday')\n])\n\n\n\n\n\n\n\n\n# Add days\ndates_pd['next_week'] = dates_pd['date'] + pd.Timedelta(days=7)\n\n# Date difference\ndate_range = pd.date_range(start='2023-01-01', end='2023-01-10')\ndf_dates = pd.DataFrame({'date': date_range})\ndf_dates['days_since_start'] = (df_dates['date'] - df_dates['date'].min()).dt.days\n\n\n\n\n\n# Add days\ndates_pl = dates_pl.with_columns(\n    (pl.col('date') + pl.duration(days=7)).alias('next_week')\n)\n\n# Date difference\ndate_range = pd.date_range(start='2023-01-01', end='2023-01-10')  # Using pandas to generate range\ndf_dates = pl.DataFrame({'date': date_range})\ndf_dates = df_dates.with_columns(\n    (pl.col('date') - pl.col('date').min()).dt.total_days().alias('days_since_start')\n)"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#performance-comparison",
    "href": "posts/pandas-to-polars/index.html#performance-comparison",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "This section demonstrates performance differences between pandas and polars for a large dataset operation.\n\nimport pandas as pd\nimport polars as pl\nimport time\nimport numpy as np\n\n# Generate a large dataset (10 million rows)\nn = 10_000_000\ndata = {\n    'id': np.arange(n),\n    'value': np.random.randn(n),\n    'group': np.random.choice(['A', 'B', 'C', 'D'], n)\n}\n\n# Convert to pandas DataFrame\ndf_pd = pd.DataFrame(data)\n\n# Convert to polars DataFrame\ndf_pl = pl.DataFrame(data)\n\n# Benchmark: Group by and calculate mean, min, max\nprint(\"Running pandas groupby...\")\nstart = time.time()\nresult_pd = df_pd.groupby('group').agg({\n    'value': ['mean', 'min', 'max', 'count']\n})\npd_time = time.time() - start\nprint(f\"Pandas time: {pd_time:.4f} seconds\")\n\nprint(\"Running polars groupby...\")\nstart = time.time()\nresult_pl = df_pl.group_by('group').agg([\n    pl.col('value').mean().alias('value_mean'),\n    pl.col('value').min().alias('value_min'),\n    pl.col('value').max().alias('value_max'),\n    pl.col('value').count().alias('value_count')\n])\npl_time = time.time() - start\nprint(f\"Polars time: {pl_time:.4f} seconds\")\n\nprint(f\"Polars is {pd_time / pl_time:.2f}x faster\")\n\nRunning pandas groupby...\nPandas time: 0.2343 seconds\nRunning polars groupby...\nPolars time: 0.0697 seconds\nPolars is 3.36x faster\n\n\nTypically, for operations like this, Polars will be 3-10x faster than pandas, especially as data sizes increase. The performance gap widens further with more complex operations that can benefit from query optimization."
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#api-philosophy-differences",
    "href": "posts/pandas-to-polars/index.html#api-philosophy-differences",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "Pandas and Polars differ in several fundamental aspects:\n\n\nPandas uses eager execution by default:\nPolars supports both eager and lazy execution:\n\n\n\nPandas often uses assignment operations:\n\n# Many pandas operations use in-place assignment\npd_df['new_col'] = pd_df['new_col'] * 2\npd_df['new_col'] = pd_df['new_col'].fillna(0)\n\n# Some operations return new DataFrames\npd_df = pd_df.sort_values('new_col')\n\nPolars consistently uses method chaining:\n\n# All operations return new DataFrames and can be chained\npl_df = (pl_df\n    .with_columns((pl.col('new_col') * 2).alias('new_col'))\n    .with_columns(pl.col('new_col').fill_null(0))\n    .sort('new_col')\n)\n\n\n\n\nPandas directly references columns:\n\npd_df['result'] = pd_df['age'] + pd_df['new_col']\nfiltered = pd_df[pd_df['age'] &gt; pd_df['age'].mean()]\n\nPolars uses an expression API:\n\npl_df = pl_df.with_columns(\n    (pl.col('age') + pl.col('new_col')).alias('result')\n)\nfiltered = pl_df.filter(pl.col('age') &gt; pl.col('age').mean())"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#migration-guide",
    "href": "posts/pandas-to-polars/index.html#migration-guide",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "If you‚Äôre transitioning from pandas to polars, here are key mappings between common operations:\n\n\n\n\n\n\n\n\nOperation\nPandas\nPolars\n\n\n\n\nRead CSV\npd.read_csv('file.csv')\npl.read_csv('file.csv')\n\n\nSelect columns\ndf[['col1', 'col2']]\ndf.select(['col1', 'col2'])\n\n\nAdd column\ndf['new'] = df['col1'] * 2\ndf.with_columns((pl.col('col1') * 2).alias('new'))\n\n\nFilter rows\ndf[df['col'] &gt; 5]\ndf.filter(pl.col('col') &gt; 5)\n\n\nSort\ndf.sort_values('col')\ndf.sort('col')\n\n\nGroup by\ndf.groupby('col').agg({'val': 'sum'})\ndf.group_by('col').agg(pl.col('val').sum())\n\n\nJoin\ndf1.merge(df2, on='key')\ndf1.join(df2, on='key')\n\n\nFill NA\ndf.fillna(0)\ndf.fill_null(0)\n\n\nDrop NA\ndf.dropna()\ndf.drop_nulls()\n\n\nRename\ndf.rename(columns={'a': 'b'})\ndf.rename({'a': 'b'})\n\n\nUnique values\ndf['col'].unique()\ndf.select(pl.col('col').unique())\n\n\nValue counts\ndf['col'].value_counts()\ndf.group_by('col').count()\n\n\n\n\n\n\nThink in expressions: Use pl.col() to reference columns in operations\nEmbrace method chaining: String operations together instead of intermediate variables\nTry lazy execution: For complex operations, use pl.scan_csv() and lazy operations\nUse with_columns(): Instead of direct assignment, use with_columns for adding/modifying columns\nLearn the expression functions: Many operations like string manipulation use different syntax\n\n\n\n\nDespite Polars‚Äô advantages, pandas might still be preferred when:\n\nWorking with existing codebases heavily dependent on pandas\nUsing specialized libraries that only support pandas (some visualization tools)\nDealing with very small datasets where performance isn‚Äôt critical\nUsing pandas-specific features without polars equivalents\nWorking with time series data that benefits from pandas‚Äô specialized functionality"
  },
  {
    "objectID": "posts/pandas-to-polars/index.html#conclusion",
    "href": "posts/pandas-to-polars/index.html#conclusion",
    "title": "From Pandas to Polars",
    "section": "",
    "text": "Polars offers significant performance improvements and a more consistent API compared to pandas, particularly for large datasets and complex operations. While the syntax differences require some adjustment, the benefits in speed and memory efficiency make it a compelling choice for modern data analysis workflows.\nBoth libraries have their place in the Python data ecosystem. Pandas remains the more mature option with broader ecosystem compatibility, while Polars represents the future of high-performance data processing. For new projects dealing with large datasets, Polars is increasingly becoming the recommended choice."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "üëÅÔ∏è Welcome to My Computer Vision Blog!\n\nThis is the first post in this blog.\nHello and welcome!\nI‚Äôm thrilled to kick off this blog dedicated to exploring the fascinating world of computer vision ‚Äî a field where machines learn to see, interpret, and understand the visual world around us. Whether you‚Äôre a seasoned AI researcher, an aspiring developer, or simply curious about how technology can ‚Äúsee,‚Äù you‚Äôll find something valuable here.\nFrom image processing techniques to deep learning breakthroughs, from real-world applications to hands-on tutorials ‚Äî this blog will cover it all. My goal is to make computer vision approachable, insightful, and exciting for everyone.\nSo, whether you‚Äôre here to learn, build, or stay on top of the latest innovations, I‚Äôm glad to have you along for the journey. Let‚Äôs dive into the visual future together!\nStay tuned, and let‚Äôs make machines see the world like never before. üöÄ\nCheers,  Krishna"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My posts",
    "section": "",
    "text": "From Pandas to Polars\n\n\n\ncode\n\ntutorial\n\nadvanced\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMar 29, 2025\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\nKrishnatheja Vanka\n\n\nMar 22, 2025\n\n\n\n\n\n\nNo matching items"
  }
]