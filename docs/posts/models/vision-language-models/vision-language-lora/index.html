<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Krishnatheja Vanka">
<meta name="dcterms.date" content="2025-08-02">

<title>LoRA for Vision-Language Models: A Comprehensive Guide – Krishnatheja Vanka’s Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<link href="../../../../favicon.ico" rel="icon">
<script src="../../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-2fef5ea3f8957b3e4ecc936fc74692ca.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-62c28fcd870f55f61984f019219cbd7c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../../site_libs/bootstrap/bootstrap-dark-5a86c4bd0c1f9981a70f893fdae069f2.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../../site_libs/bootstrap/bootstrap-62c28fcd870f55f61984f019219cbd7c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../../../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../../styles/styles.css">
</head>

<body class="floating nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">Krishnatheja Vanka’s Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../about.html"> 
<span class="menu-text">About me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/theja-vanka"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-right">
      <h1 class="title">LoRA for Vision-Language Models: A Comprehensive Guide</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">code</div>
                <div class="quarto-category">tutorial</div>
                <div class="quarto-category">intermediate</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta column-page-right">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Krishnatheja Vanka </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 2, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#lora-for-vision-language-models-a-comprehensive-guide" id="toc-lora-for-vision-language-models-a-comprehensive-guide" class="nav-link active" data-scroll-target="#lora-for-vision-language-models-a-comprehensive-guide">LoRA for Vision-Language Models: A Comprehensive Guide</a>
  <ul class="collapse">
  <li><a href="#abstract" id="toc-abstract" class="nav-link" data-scroll-target="#abstract">Abstract</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#why-lora-for-vlms" id="toc-why-lora-for-vlms" class="nav-link" data-scroll-target="#why-lora-for-vlms">Why LoRA for VLMs?</a></li>
  </ul></li>
  <li><a href="#understanding-lora" id="toc-understanding-lora" class="nav-link" data-scroll-target="#understanding-lora">Understanding LoRA</a>
  <ul class="collapse">
  <li><a href="#core-principles" id="toc-core-principles" class="nav-link" data-scroll-target="#core-principles">Core Principles</a></li>
  <li><a href="#mathematical-foundation" id="toc-mathematical-foundation" class="nav-link" data-scroll-target="#mathematical-foundation">Mathematical Foundation</a></li>
  <li><a href="#key-advantages" id="toc-key-advantages" class="nav-link" data-scroll-target="#key-advantages">Key Advantages</a></li>
  </ul></li>
  <li><a href="#vision-language-models-overview" id="toc-vision-language-models-overview" class="nav-link" data-scroll-target="#vision-language-models-overview">Vision-Language Models Overview</a>
  <ul class="collapse">
  <li><a href="#architecture-components" id="toc-architecture-components" class="nav-link" data-scroll-target="#architecture-components">Architecture Components</a></li>
  <li><a href="#popular-vlm-architectures" id="toc-popular-vlm-architectures" class="nav-link" data-scroll-target="#popular-vlm-architectures">Popular VLM Architectures</a></li>
  </ul></li>
  <li><a href="#lora-architecture-for-vlms" id="toc-lora-architecture-for-vlms" class="nav-link" data-scroll-target="#lora-architecture-for-vlms">LoRA Architecture for VLMs</a>
  <ul class="collapse">
  <li><a href="#component-wise-application" id="toc-component-wise-application" class="nav-link" data-scroll-target="#component-wise-application">Component-wise Application</a></li>
  <li><a href="#layer-selection-strategy" id="toc-layer-selection-strategy" class="nav-link" data-scroll-target="#layer-selection-strategy">Layer Selection Strategy</a></li>
  <li><a href="#rank-selection-guidelines" id="toc-rank-selection-guidelines" class="nav-link" data-scroll-target="#rank-selection-guidelines">Rank Selection Guidelines</a></li>
  </ul></li>
  <li><a href="#configuration-management" id="toc-configuration-management" class="nav-link" data-scroll-target="#configuration-management">Configuration Management</a></li>
  <li><a href="#training-strategies" id="toc-training-strategies" class="nav-link" data-scroll-target="#training-strategies">Training Strategies</a>
  <ul class="collapse">
  <li><a href="#progressive-training" id="toc-progressive-training" class="nav-link" data-scroll-target="#progressive-training">1. Progressive Training</a></li>
  <li><a href="#multi-stage-training" id="toc-multi-stage-training" class="nav-link" data-scroll-target="#multi-stage-training">2. Multi-Stage Training</a></li>
  </ul></li>
  <li><a href="#advanced-techniques" id="toc-advanced-techniques" class="nav-link" data-scroll-target="#advanced-techniques">Advanced Techniques</a>
  <ul class="collapse">
  <li><a href="#adalora-adaptive-lora" id="toc-adalora-adaptive-lora" class="nav-link" data-scroll-target="#adalora-adaptive-lora">1. AdaLoRA (Adaptive LoRA)</a></li>
  <li><a href="#dora-weight-decomposed-lora" id="toc-dora-weight-decomposed-lora" class="nav-link" data-scroll-target="#dora-weight-decomposed-lora">2. DoRA (Weight-Decomposed LoRA)</a></li>
  <li><a href="#mixture-of-loras-molora" id="toc-mixture-of-loras-molora" class="nav-link" data-scroll-target="#mixture-of-loras-molora">3. Mixture of LoRAs (MoLoRA)</a></li>
  </ul></li>
  <li><a href="#performance-optimization" id="toc-performance-optimization" class="nav-link" data-scroll-target="#performance-optimization">Performance Optimization</a>
  <ul class="collapse">
  <li><a href="#memory-optimization" id="toc-memory-optimization" class="nav-link" data-scroll-target="#memory-optimization">Memory Optimization</a></li>
  <li><a href="#training-optimizations" id="toc-training-optimizations" class="nav-link" data-scroll-target="#training-optimizations">Training Optimizations</a></li>
  </ul></li>
  <li><a href="#use-cases-and-applications" id="toc-use-cases-and-applications" class="nav-link" data-scroll-target="#use-cases-and-applications">Use Cases and Applications</a>
  <ul class="collapse">
  <li><a href="#domain-adaptation" id="toc-domain-adaptation" class="nav-link" data-scroll-target="#domain-adaptation">1. Domain Adaptation</a></li>
  <li><a href="#multi-lingual-vision-language" id="toc-multi-lingual-vision-language" class="nav-link" data-scroll-target="#multi-lingual-vision-language">2. Multi-lingual Vision-Language</a></li>
  <li><a href="#few-shot-learning" id="toc-few-shot-learning" class="nav-link" data-scroll-target="#few-shot-learning">3. Few-Shot Learning</a></li>
  </ul></li>
  <li><a href="#best-practices" id="toc-best-practices" class="nav-link" data-scroll-target="#best-practices">Best Practices</a>
  <ul class="collapse">
  <li><a href="#hyperparameter-selection" id="toc-hyperparameter-selection" class="nav-link" data-scroll-target="#hyperparameter-selection">1. Hyperparameter Selection</a></li>
  <li><a href="#module-selection-strategy" id="toc-module-selection-strategy" class="nav-link" data-scroll-target="#module-selection-strategy">2. Module Selection Strategy</a></li>
  <li><a href="#training-best-practices" id="toc-training-best-practices" class="nav-link" data-scroll-target="#training-best-practices">3. Training Best Practices</a></li>
  </ul></li>
  <li><a href="#troubleshooting" id="toc-troubleshooting" class="nav-link" data-scroll-target="#troubleshooting">Troubleshooting</a>
  <ul class="collapse">
  <li><a href="#common-issues-and-solutions" id="toc-common-issues-and-solutions" class="nav-link" data-scroll-target="#common-issues-and-solutions">Common Issues and Solutions</a></li>
  <li><a href="#additional-resources" id="toc-additional-resources" class="nav-link" data-scroll-target="#additional-resources">Additional Resources</a></li>
  <li><a href="#debugging-tools-1" id="toc-debugging-tools-1" class="nav-link" data-scroll-target="#debugging-tools-1">Debugging Tools</a></li>
  </ul></li>
  <li><a href="#production-deployment" id="toc-production-deployment" class="nav-link" data-scroll-target="#production-deployment">Production Deployment</a>
  <ul class="collapse">
  <li><a href="#model-management-system" id="toc-model-management-system" class="nav-link" data-scroll-target="#model-management-system">Model Management System</a></li>
  <li><a href="#api-server-implementation" id="toc-api-server-implementation" class="nav-link" data-scroll-target="#api-server-implementation">API Server Implementation</a></li>
  </ul></li>
  <li><a href="#monitoring-and-observability" id="toc-monitoring-and-observability" class="nav-link" data-scroll-target="#monitoring-and-observability">Monitoring and Observability</a>
  <ul class="collapse">
  <li><a href="#performance-monitoring" id="toc-performance-monitoring" class="nav-link" data-scroll-target="#performance-monitoring">Performance Monitoring</a></li>
  <li><a href="#visualization-and-dashboards" id="toc-visualization-and-dashboards" class="nav-link" data-scroll-target="#visualization-and-dashboards">Visualization and Dashboards</a></li>
  </ul></li>
  <li><a href="#future-directions" id="toc-future-directions" class="nav-link" data-scroll-target="#future-directions">Future Directions</a>
  <ul class="collapse">
  <li><a href="#impact-analysis" id="toc-impact-analysis" class="nav-link" data-scroll-target="#impact-analysis">Impact Analysis</a></li>
  <li><a href="#research-opportunities" id="toc-research-opportunities" class="nav-link" data-scroll-target="#research-opportunities">Research Opportunities</a></li>
  <li><a href="#detailed-proposals" id="toc-detailed-proposals" class="nav-link" data-scroll-target="#detailed-proposals">Detailed Proposals</a></li>
  <li><a href="#impact-assessment" id="toc-impact-assessment" class="nav-link" data-scroll-target="#impact-assessment">Impact Assessment</a></li>
  <li><a href="#impact-scores-summary" id="toc-impact-scores-summary" class="nav-link" data-scroll-target="#impact-scores-summary">Impact Scores Summary</a></li>
  </ul></li>
  <li><a href="#summary-of-key-points" id="toc-summary-of-key-points" class="nav-link" data-scroll-target="#summary-of-key-points">Summary of Key Points</a></li>
  <li><a href="#implementation-checklist" id="toc-implementation-checklist" class="nav-link" data-scroll-target="#implementation-checklist">Implementation Checklist</a></li>
  <li><a href="#future-outlook" id="toc-future-outlook" class="nav-link" data-scroll-target="#future-outlook">Future Outlook</a></li>
  <li><a href="#resources-for-further-learning" id="toc-resources-for-further-learning" class="nav-link" data-scroll-target="#resources-for-further-learning">Resources for Further Learning</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-right" id="quarto-document-content">






<section id="lora-for-vision-language-models-a-comprehensive-guide" class="level1">
<h1>LoRA for Vision-Language Models: A Comprehensive Guide</h1>
<p><img src="lora.png" class="img-fluid"></p>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>Low-Rank Adaptation (LoRA) has emerged as a revolutionary technique for efficient fine-tuning of large language models, and its application to Vision-Language Models (VLMs) represents a significant advancement in multimodal AI. This comprehensive guide provides theoretical foundations, practical implementation strategies, and production deployment techniques for LoRA in VLMs, covering everything from basic concepts to advanced optimization methods.</p>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Vision-Language Models like CLIP, BLIP, LLaVA, and GPT-4V contain billions of parameters, making full fine-tuning computationally expensive and memory-intensive. LoRA addresses these challenges by:</p>
<ul>
<li><strong>Reducing memory requirements</strong> by up to 90%</li>
<li><strong>Accelerating training</strong> by 2-3x</li>
<li><strong>Maintaining model performance</strong> with minimal parameter overhead</li>
<li><strong>Enabling modular adaptation</strong> for different tasks and domains</li>
</ul>
<section id="why-lora-for-vlms" class="level3">
<h3 class="anchored" data-anchor-id="why-lora-for-vlms">Why LoRA for VLMs?</h3>
<div id="cell-fig-lora-benefits" class="cell" data-execution_count="1">
<div class="cell-output cell-output-display">
<div id="fig-lora-benefits" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lora-benefits-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-lora-benefits-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lora-benefits-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: LoRA Benefits Comparison
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="understanding-lora" class="level2">
<h2 class="anchored" data-anchor-id="understanding-lora">Understanding LoRA</h2>
<section id="core-principles" class="level3">
<h3 class="anchored" data-anchor-id="core-principles">Core Principles</h3>
<p>LoRA is based on the hypothesis that weight updates during fine-tuning have a low intrinsic rank. Instead of updating all parameters, LoRA decomposes the weight update matrix into two smaller matrices:</p>
<p><span class="math display">\[\Delta W = BA\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(W\)</span> is the original weight matrix (<span class="math inline">\(d \times d\)</span>)</li>
<li><span class="math inline">\(B\)</span> is a learnable matrix (<span class="math inline">\(d \times r\)</span>)<br>
</li>
<li><span class="math inline">\(A\)</span> is a learnable matrix (<span class="math inline">\(r \times d\)</span>)</li>
<li><span class="math inline">\(r\)</span> is the rank (<span class="math inline">\(r \ll d\)</span>)</li>
</ul>
</section>
<section id="mathematical-foundation" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-foundation">Mathematical Foundation</h3>
<p>For a linear layer with weight matrix <span class="math inline">\(W_0\)</span>, the forward pass becomes:</p>
<p><span class="math display">\[h = W_0x + \Delta Wx = W_0x + BAx\]</span></p>
<p>The adapted weight matrix is: <span class="math display">\[W = W_0 + \alpha BA\]</span></p>
<p>Where <span class="math inline">\(\alpha\)</span> is a scaling factor that controls the magnitude of the adaptation.</p>
<div id="lora-implementation" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LoRALayer(nn.Module):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_features, out_features, rank<span class="op">=</span><span class="dv">16</span>, alpha<span class="op">=</span><span class="dv">16</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rank <span class="op">=</span> rank</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha <span class="op">=</span> alpha</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scaling <span class="op">=</span> alpha <span class="op">/</span> rank</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># LoRA matrices</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lora_A <span class="op">=</span> nn.Linear(in_features, rank, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lora_B <span class="op">=</span> nn.Linear(rank, out_features, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize weights</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        nn.init.kaiming_uniform_(<span class="va">self</span>.lora_A.weight, a<span class="op">=</span>math.sqrt(<span class="dv">5</span>))</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        nn.init.zeros_(<span class="va">self</span>.lora_B.weight)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> <span class="va">self</span>.lora_A(x)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> <span class="va">self</span>.dropout(result)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> <span class="va">self</span>.lora_B(result)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> result <span class="op">*</span> <span class="va">self</span>.scaling</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LoRALinear(nn.Module):</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, original_layer, rank<span class="op">=</span><span class="dv">16</span>, alpha<span class="op">=</span><span class="dv">16</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.original_layer <span class="op">=</span> original_layer</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lora <span class="op">=</span> LoRALayer(</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>            original_layer.in_features,</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>            original_layer.out_features,</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>            rank, alpha, dropout</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Freeze original weights</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> param <span class="kw">in</span> <span class="va">self</span>.original_layer.parameters():</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>            param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.original_layer(x) <span class="op">+</span> <span class="va">self</span>.lora(x)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>original_linear <span class="op">=</span> nn.Linear(<span class="dv">768</span>, <span class="dv">768</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>lora_linear <span class="op">=</span> LoRALinear(original_linear, rank<span class="op">=</span><span class="dv">16</span>, alpha<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Original parameters: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> original_linear.parameters())<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"LoRA parameters: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> lora_linear.lora.parameters())<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Parameter reduction: </span><span class="sc">{</span>(<span class="dv">1</span> <span class="op">-</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> lora_linear.lora.parameters()) <span class="op">/</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> original_linear.parameters())) <span class="op">*</span> <span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Original parameters: 590592
LoRA parameters: 24576
Parameter reduction: 95.8%</code></pre>
</div>
</div>
</section>
<section id="key-advantages" class="level3">
<h3 class="anchored" data-anchor-id="key-advantages">Key Advantages</h3>
<ol type="1">
<li><strong>Parameter Efficiency</strong>: Only trains ~0.1-1% of original parameters</li>
<li><strong>Memory Efficiency</strong>: Reduced GPU memory requirements</li>
<li><strong>Modularity</strong>: Multiple LoRA adapters can be stored and swapped</li>
<li><strong>Preservation</strong>: Original model weights remain unchanged</li>
<li><strong>Composability</strong>: Multiple LoRAs can be combined</li>
</ol>
</section>
</section>
<section id="vision-language-models-overview" class="level2">
<h2 class="anchored" data-anchor-id="vision-language-models-overview">Vision-Language Models Overview</h2>
<section id="architecture-components" class="level3">
<h3 class="anchored" data-anchor-id="architecture-components">Architecture Components</h3>
<p>Modern VLMs typically consist of:</p>
<ol type="1">
<li><strong>Vision Encoder</strong>: Processes visual inputs (e.g., Vision Transformer, ResNet)</li>
<li><strong>Text Encoder</strong>: Processes textual inputs (e.g., BERT, GPT)</li>
<li><strong>Multimodal Fusion</strong>: Combines visual and textual representations</li>
<li><strong>Output Head</strong>: Task-specific prediction layers</li>
</ol>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    A[Image Input] --&gt; B[Vision&lt;br/&gt;Encoder]
    C[Text Input] --&gt; D[Text&lt;br/&gt;Encoder]
    B --&gt; E[Multimodal&lt;br/&gt;Fusion]
    D --&gt; E
    E --&gt; F[Output&lt;br/&gt;Head]
    F --&gt; G[Predictions]
    
    classDef input fill:#add8e6,stroke:#000,stroke-width:2px
    classDef encoder fill:#90ee90,stroke:#000,stroke-width:2px
    classDef fusion fill:#ffffe0,stroke:#000,stroke-width:2px
    classDef output fill:#f08080,stroke:#000,stroke-width:2px
    classDef prediction fill:#d3d3d3,stroke:#000,stroke-width:2px
    
    class A,C input
    class B,D encoder
    class E fusion
    class F output
    class G prediction
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="popular-vlm-architectures" class="level3">
<h3 class="anchored" data-anchor-id="popular-vlm-architectures">Popular VLM Architectures</h3>
<section id="clip-contrastive-language-image-pre-training" class="level4">
<h4 class="anchored" data-anchor-id="clip-contrastive-language-image-pre-training">CLIP (Contrastive Language-Image Pre-training)</h4>
<ul>
<li>Dual-encoder architecture</li>
<li>Contrastive learning objective</li>
<li>Strong zero-shot capabilities</li>
</ul>
</section>
<section id="blip-bootstrapping-language-image-pre-training" class="level4">
<h4 class="anchored" data-anchor-id="blip-bootstrapping-language-image-pre-training">BLIP (Bootstrapping Language-Image Pre-training)</h4>
<ul>
<li>Encoder-decoder architecture</li>
<li>Unified vision-language understanding and generation</li>
<li>Bootstrap learning from noisy web data</li>
</ul>
</section>
<section id="llava-large-language-and-vision-assistant" class="level4">
<h4 class="anchored" data-anchor-id="llava-large-language-and-vision-assistant">LLaVA (Large Language and Vision Assistant)</h4>
<ul>
<li>Combines vision encoder with large language model</li>
<li>Instruction tuning for conversational abilities</li>
<li>Strong multimodal reasoning</li>
</ul>
</section>
</section>
</section>
<section id="lora-architecture-for-vlms" class="level2">
<h2 class="anchored" data-anchor-id="lora-architecture-for-vlms">LoRA Architecture for VLMs</h2>
<section id="component-wise-application" class="level3">
<h3 class="anchored" data-anchor-id="component-wise-application">Component-wise Application</h3>
<p>LoRA can be applied to different components of VLMs:</p>
<div id="vlm-lora-adapter" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VLMLoRAAdapter:</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, config):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.config <span class="op">=</span> config</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lora_layers <span class="op">=</span> {}</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> add_lora_to_attention(<span class="va">self</span>, module_name, attention_layer):</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Add LoRA to attention mechanism"""</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Query, Key, Value projections</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">hasattr</span>(attention_layer, <span class="st">'q_proj'</span>):</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>            attention_layer.q_proj <span class="op">=</span> LoRALinear(</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>                attention_layer.q_proj, </span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>                rank<span class="op">=</span><span class="va">self</span>.config.rank,</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="va">self</span>.config.alpha</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">hasattr</span>(attention_layer, <span class="st">'k_proj'</span>):</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>            attention_layer.k_proj <span class="op">=</span> LoRALinear(</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>                attention_layer.k_proj,</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>                rank<span class="op">=</span><span class="va">self</span>.config.rank,</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="va">self</span>.config.alpha</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">hasattr</span>(attention_layer, <span class="st">'v_proj'</span>):</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>            attention_layer.v_proj <span class="op">=</span> LoRALinear(</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>                attention_layer.v_proj,</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>                rank<span class="op">=</span><span class="va">self</span>.config.rank,</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="va">self</span>.config.alpha</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> add_lora_to_mlp(<span class="va">self</span>, module_name, mlp_layer):</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Add LoRA to feed-forward layers"""</span></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">hasattr</span>(mlp_layer, <span class="st">'fc1'</span>):</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>            mlp_layer.fc1 <span class="op">=</span> LoRALinear(</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>                mlp_layer.fc1,</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>                rank<span class="op">=</span><span class="va">self</span>.config.rank,</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="va">self</span>.config.alpha</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">hasattr</span>(mlp_layer, <span class="st">'fc2'</span>):</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>            mlp_layer.fc2 <span class="op">=</span> LoRALinear(</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>                mlp_layer.fc2,</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>                rank<span class="op">=</span><span class="va">self</span>.config.rank,</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="va">self</span>.config.alpha</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>            )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="layer-selection-strategy" class="level3">
<h3 class="anchored" data-anchor-id="layer-selection-strategy">Layer Selection Strategy</h3>
<p>Not all layers benefit equally from LoRA adaptation:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Priority</th>
<th>Layer Type</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>High</td>
<td>Final attention layers</td>
<td>Most task-specific representations</td>
</tr>
<tr class="even">
<td>High</td>
<td>Cross-modal attention</td>
<td>Critical for multimodal fusion</td>
</tr>
<tr class="odd">
<td>High</td>
<td>Task-specific output heads</td>
<td>Direct impact on outputs</td>
</tr>
<tr class="even">
<td>Medium</td>
<td>Middle transformer layers</td>
<td>Balanced feature extraction</td>
</tr>
<tr class="odd">
<td>Medium</td>
<td>Feed-forward networks</td>
<td>Non-linear transformations</td>
</tr>
<tr class="even">
<td>Low</td>
<td>Early encoder layers</td>
<td>Generic low-level features</td>
</tr>
<tr class="odd">
<td>Low</td>
<td>Embedding layers</td>
<td>Fixed vocabulary representations</td>
</tr>
</tbody>
</table>
</section>
<section id="rank-selection-guidelines" class="level3">
<h3 class="anchored" data-anchor-id="rank-selection-guidelines">Rank Selection Guidelines</h3>
<p>The rank <span class="math inline">\(r\)</span> significantly impacts performance and efficiency:</p>
<div id="cell-fig-rank-comparison" class="cell" data-execution_count="4">
<div class="cell-output cell-output-display">
<div id="fig-rank-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rank-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-rank-comparison-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rank-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: LoRA Rank vs Performance Trade-off
</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Rank Selection Guidelines:</strong></p>
<ul>
<li><strong>r = 1-4</strong>: Minimal parameters, suitable for simple adaptations</li>
<li><strong>r = 8-16</strong>: Balanced efficiency and performance for most tasks</li>
<li><strong>r = 32-64</strong>: Higher capacity for complex domain adaptations</li>
<li><strong>r = 128+</strong>: Approaching full fine-tuning, rarely needed</li>
</ul>
</section>
</section>
<section id="configuration-management" class="level2">
<h2 class="anchored" data-anchor-id="configuration-management">Configuration Management</h2>
<div id="lora-config" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List, Optional</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LoRAConfig:</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Basic LoRA parameters</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    rank: <span class="bu">int</span> <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    alpha: <span class="bu">int</span> <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    dropout: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Target modules</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    target_modules: List[<span class="bu">str</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    vision_target_modules: List[<span class="bu">str</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    text_target_modules: List[<span class="bu">str</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Training parameters</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    learning_rate: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    weight_decay: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    warmup_steps: <span class="bu">int</span> <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Advanced options</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    use_gradient_checkpointing: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    mixed_precision: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    task_type: <span class="bu">str</span> <span class="op">=</span> <span class="st">"multimodal_classification"</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> __post_init__(<span class="va">self</span>):</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.target_modules <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.target_modules <span class="op">=</span> [</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>                <span class="st">"q_proj"</span>, <span class="st">"k_proj"</span>, <span class="st">"v_proj"</span>, <span class="st">"o_proj"</span>,</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>                <span class="st">"gate_proj"</span>, <span class="st">"up_proj"</span>, <span class="st">"down_proj"</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.vision_target_modules <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.vision_target_modules <span class="op">=</span> [</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>                <span class="st">"qkv"</span>, <span class="st">"proj"</span>, <span class="st">"fc1"</span>, <span class="st">"fc2"</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.text_target_modules <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.text_target_modules <span class="op">=</span> [</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>                <span class="st">"q_proj"</span>, <span class="st">"k_proj"</span>, <span class="st">"v_proj"</span>, <span class="st">"dense"</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Example configurations for different tasks</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>task_configs <span class="op">=</span> {</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>    <span class="st">"image_captioning"</span>: LoRAConfig(</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>        rank<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>        alpha<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>        target_modules<span class="op">=</span>[<span class="st">"q_proj"</span>, <span class="st">"v_proj"</span>, <span class="st">"dense"</span>],</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>        task_type<span class="op">=</span><span class="st">"image_captioning"</span></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>    <span class="st">"visual_question_answering"</span>: LoRAConfig(</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>        rank<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>        alpha<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>        target_modules<span class="op">=</span>[<span class="st">"q_proj"</span>, <span class="st">"k_proj"</span>, <span class="st">"v_proj"</span>],</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>        task_type<span class="op">=</span><span class="st">"visual_question_answering"</span></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>    <span class="st">"image_classification"</span>: LoRAConfig(</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>        rank<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>        alpha<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>        target_modules<span class="op">=</span>[<span class="st">"qkv"</span>, <span class="st">"proj"</span>],</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>        task_type<span class="op">=</span><span class="st">"image_classification"</span></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Available task configurations:"</span>)</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> task, config <span class="kw">in</span> task_configs.items():</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"- </span><span class="sc">{</span>task<span class="sc">}</span><span class="ss">: rank=</span><span class="sc">{</span>config<span class="sc">.</span>rank<span class="sc">}</span><span class="ss">, alpha=</span><span class="sc">{</span>config<span class="sc">.</span>alpha<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Available task configurations:
- image_captioning: rank=32, alpha=32
- visual_question_answering: rank=16, alpha=16
- image_classification: rank=8, alpha=16</code></pre>
</div>
</div>
</section>
<section id="training-strategies" class="level2">
<h2 class="anchored" data-anchor-id="training-strategies">Training Strategies</h2>
<section id="progressive-training" class="level3">
<h3 class="anchored" data-anchor-id="progressive-training">1. Progressive Training</h3>
<p>Start with lower ranks and gradually increase:</p>
<div id="progressive-training" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ProgressiveLoRATrainer:</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, initial_rank<span class="op">=</span><span class="dv">4</span>, max_rank<span class="op">=</span><span class="dv">32</span>):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.current_rank <span class="op">=</span> initial_rank</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_rank <span class="op">=</span> max_rank</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> expand_rank(<span class="va">self</span>, new_rank):</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Expand LoRA rank while preserving learned weights"""</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, module <span class="kw">in</span> <span class="va">self</span>.model.named_modules():</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">isinstance</span>(module, LoRALinear):</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>                old_lora <span class="op">=</span> module.lora</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Create new LoRA layer</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>                new_lora <span class="op">=</span> LoRALayer(</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>                    old_lora.lora_A.in_features,</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>                    old_lora.lora_B.out_features,</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>                    rank<span class="op">=</span>new_rank</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Copy existing weights</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>                <span class="cf">with</span> torch.no_grad():</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>                    new_lora.lora_A.weight[:old_lora.rank] <span class="op">=</span> old_lora.lora_A.weight</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>                    new_lora.lora_B.weight[:, :old_lora.rank] <span class="op">=</span> old_lora.lora_B.weight</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>                module.lora <span class="op">=</span> new_lora</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> progressive_training_schedule(<span class="va">self</span>, num_epochs):</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Generate progressive training schedule"""</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>        schedule <span class="op">=</span> []</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>        epochs_per_stage <span class="op">=</span> num_epochs <span class="op">//</span> <span class="dv">3</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Stage 1: Small rank</span></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>        schedule.append({</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>            <span class="st">'epochs'</span>: epochs_per_stage,</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>            <span class="st">'rank'</span>: <span class="dv">4</span>,</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>            <span class="st">'lr'</span>: <span class="fl">1e-3</span>,</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>            <span class="st">'description'</span>: <span class="st">'Initial adaptation with small rank'</span></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>        })</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Stage 2: Medium rank</span></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>        schedule.append({</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>            <span class="st">'epochs'</span>: epochs_per_stage,</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>            <span class="st">'rank'</span>: <span class="dv">16</span>,</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>            <span class="st">'lr'</span>: <span class="fl">5e-4</span>,</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>            <span class="st">'description'</span>: <span class="st">'Expand capacity with medium rank'</span></span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>        })</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Stage 3: Full rank</span></span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>        schedule.append({</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>            <span class="st">'epochs'</span>: num_epochs <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> epochs_per_stage,</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>            <span class="st">'rank'</span>: <span class="dv">32</span>,</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>            <span class="st">'lr'</span>: <span class="fl">1e-4</span>,</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>            <span class="st">'description'</span>: <span class="st">'Fine-tune with full rank'</span></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>        })</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> schedule</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> ProgressiveLoRATrainer(<span class="va">None</span>)  <span class="co"># Would pass actual model</span></span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>schedule <span class="op">=</span> trainer.progressive_training_schedule(<span class="dv">12</span>)</span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Progressive Training Schedule:"</span>)</span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, stage <span class="kw">in</span> <span class="bu">enumerate</span>(schedule, <span class="dv">1</span>):</span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Stage </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>stage[<span class="st">'description'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  - Epochs: </span><span class="sc">{</span>stage[<span class="st">'epochs'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  - Rank: </span><span class="sc">{</span>stage[<span class="st">'rank'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  - Learning Rate: </span><span class="sc">{</span>stage[<span class="st">'lr'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Progressive Training Schedule:
Stage 1: Initial adaptation with small rank
  - Epochs: 4
  - Rank: 4
  - Learning Rate: 0.001

Stage 2: Expand capacity with medium rank
  - Epochs: 4
  - Rank: 16
  - Learning Rate: 0.0005

Stage 3: Fine-tune with full rank
  - Epochs: 4
  - Rank: 32
  - Learning Rate: 0.0001
</code></pre>
</div>
</div>
</section>
<section id="multi-stage-training" class="level3">
<h3 class="anchored" data-anchor-id="multi-stage-training">2. Multi-Stage Training</h3>
<div id="multistage-training" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> multi_stage_training(model, train_loader, config):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Multi-stage training strategy:</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co">    1. Stage 1: Freeze vision encoder, train text components</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">    2. Stage 2: Freeze text encoder, train vision components  </span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co">    3. Stage 3: Joint training with reduced learning rate</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Multi-Stage Training Strategy"</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">40</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Stage 1: Text-only training</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Stage 1: Text-only training"</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Freezing vision encoder"</span>)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Training text LoRA components"</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'vision'</span> <span class="kw">in</span> name:</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>            param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="st">'lora'</span> <span class="kw">in</span> name <span class="kw">and</span> <span class="st">'text'</span> <span class="kw">in</span> name:</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>            param.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    trainable_params_stage1 <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"- Trainable parameters: </span><span class="sc">{</span>trainable_params_stage1<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># train_stage(model, train_loader, epochs=config.stage1_epochs)</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Stage 2: Vision-only training</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Stage 2: Vision-only training"</span>)</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Freezing text encoder"</span>)</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Training vision LoRA components"</span>)</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'text'</span> <span class="kw">in</span> name:</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>            param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="st">'lora'</span> <span class="kw">in</span> name <span class="kw">and</span> <span class="st">'vision'</span> <span class="kw">in</span> name:</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>            param.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>    trainable_params_stage2 <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad)</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"- Trainable parameters: </span><span class="sc">{</span>trainable_params_stage2<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># train_stage(model, train_loader, epochs=config.stage2_epochs)</span></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Stage 3: Joint training</span></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Stage 3: Joint training"</span>)</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Training all LoRA components"</span>)</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Reduced learning rate for stability"</span>)</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'lora'</span> <span class="kw">in</span> name:</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>            param.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>    trainable_params_stage3 <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad)</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"- Trainable parameters: </span><span class="sc">{</span>trainable_params_stage3<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># train_stage(model, train_loader, epochs=config.stage3_epochs, lr=config.lr * 0.1)</span></span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Example configuration</span></span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiStageConfig:</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stage1_epochs <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stage2_epochs <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stage3_epochs <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lr <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> MultiStageConfig()</span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a><span class="co"># multi_stage_training(None, None, config)  # Would pass actual model and data</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="advanced-techniques" class="level2">
<h2 class="anchored" data-anchor-id="advanced-techniques">Advanced Techniques</h2>
<section id="adalora-adaptive-lora" class="level3">
<h3 class="anchored" data-anchor-id="adalora-adaptive-lora">1. AdaLoRA (Adaptive LoRA)</h3>
<p>Dynamically adjusts rank based on importance:</p>
<div id="adalora" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AdaLoRALayer(nn.Module):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_features, out_features, max_rank<span class="op">=</span><span class="dv">64</span>, init_rank<span class="op">=</span><span class="dv">16</span>):</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_rank <span class="op">=</span> max_rank</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.current_rank <span class="op">=</span> init_rank</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Full-rank matrices for potential expansion</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lora_A <span class="op">=</span> nn.Parameter(torch.zeros(max_rank, in_features))</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lora_B <span class="op">=</span> nn.Parameter(torch.zeros(out_features, max_rank))</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Importance scores</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.importance_scores <span class="op">=</span> nn.Parameter(torch.ones(max_rank))</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize only active components</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.reset_parameters()</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset_parameters(<span class="va">self</span>):</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Initialize parameters"""</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>        nn.init.kaiming_uniform_(<span class="va">self</span>.lora_A[:<span class="va">self</span>.current_rank], a<span class="op">=</span>math.sqrt(<span class="dv">5</span>))</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>        nn.init.zeros_(<span class="va">self</span>.lora_B[:, :<span class="va">self</span>.current_rank])</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply importance-weighted LoRA</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>        active_A <span class="op">=</span> <span class="va">self</span>.lora_A[:<span class="va">self</span>.current_rank] <span class="op">*</span> <span class="va">self</span>.importance_scores[:<span class="va">self</span>.current_rank, <span class="va">None</span>]</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>        active_B <span class="op">=</span> <span class="va">self</span>.lora_B[:, :<span class="va">self</span>.current_rank] <span class="op">*</span> <span class="va">self</span>.importance_scores[<span class="va">None</span>, :<span class="va">self</span>.current_rank]</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">@</span> active_A.T <span class="op">@</span> active_B.T</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_rank(<span class="va">self</span>, budget_ratio<span class="op">=</span><span class="fl">0.7</span>):</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Update rank based on importance scores"""</span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> <span class="va">self</span>.importance_scores.<span class="bu">abs</span>()</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>        threshold <span class="op">=</span> torch.quantile(scores, <span class="dv">1</span> <span class="op">-</span> budget_ratio)</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>        new_rank <span class="op">=</span> (scores <span class="op">&gt;=</span> threshold).<span class="bu">sum</span>().item()</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> new_rank <span class="op">!=</span> <span class="va">self</span>.current_rank:</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Rank updated: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>current_rank<span class="sc">}</span><span class="ss"> -&gt; </span><span class="sc">{</span>new_rank<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.current_rank <span class="op">=</span> new_rank</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> new_rank</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstration of AdaLoRA rank adaptation</span></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>adalora_layer <span class="op">=</span> AdaLoRALayer(<span class="dv">768</span>, <span class="dv">768</span>, max_rank<span class="op">=</span><span class="dv">64</span>, init_rank<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"AdaLoRA Rank Adaptation Demo:"</span>)</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Initial rank: </span><span class="sc">{</span>adalora_layer<span class="sc">.</span>current_rank<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate importance score changes</span></span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>adalora_layer.importance_scores.data <span class="op">=</span> torch.rand(<span class="dv">64</span>)  <span class="co"># Random importance scores</span></span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Update rank based on importance</span></span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>new_rank <span class="op">=</span> adalora_layer.update_rank(budget_ratio<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"New rank after adaptation: </span><span class="sc">{</span>new_rank<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>AdaLoRA Rank Adaptation Demo:
Initial rank: 16
Rank updated: 16 -&gt; 32
New rank after adaptation: 32</code></pre>
</div>
</div>
</section>
<section id="dora-weight-decomposed-lora" class="level3">
<h3 class="anchored" data-anchor-id="dora-weight-decomposed-lora">2. DoRA (Weight-Decomposed LoRA)</h3>
<p>Separates magnitude and direction updates:</p>
<div id="dora" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DoRALayer(nn.Module):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_features, out_features, rank<span class="op">=</span><span class="dv">16</span>):</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rank <span class="op">=</span> rank</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Standard LoRA components</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lora_A <span class="op">=</span> nn.Linear(in_features, rank, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lora_B <span class="op">=</span> nn.Linear(rank, out_features, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Magnitude component</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.magnitude <span class="op">=</span> nn.Parameter(torch.ones(out_features))</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize LoRA weights</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        nn.init.kaiming_uniform_(<span class="va">self</span>.lora_A.weight, a<span class="op">=</span>math.sqrt(<span class="dv">5</span>))</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>        nn.init.zeros_(<span class="va">self</span>.lora_B.weight)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, original_weight):</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># LoRA adaptation</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>        lora_result <span class="op">=</span> <span class="va">self</span>.lora_B(<span class="va">self</span>.lora_A(x))</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Direction component (normalized)</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>        adapted_weight <span class="op">=</span> original_weight <span class="op">+</span> lora_result</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>        direction <span class="op">=</span> F.normalize(adapted_weight, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply magnitude scaling</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> direction <span class="op">*</span> <span class="va">self</span>.magnitude.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Compare LoRA vs DoRA</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>original_weight <span class="op">=</span> torch.randn(<span class="dv">32</span>, <span class="dv">768</span>)</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">32</span>, <span class="dv">768</span>)</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Standard LoRA</span></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>lora_layer <span class="op">=</span> LoRALayer(<span class="dv">768</span>, <span class="dv">768</span>, rank<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>lora_output <span class="op">=</span> lora_layer(x)</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a><span class="co"># DoRA</span></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>dora_layer <span class="op">=</span> DoRALayer(<span class="dv">768</span>, <span class="dv">768</span>, rank<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>dora_output <span class="op">=</span> dora_layer(x, original_weight)</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"LoRA vs DoRA Comparison:"</span>)</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"LoRA output shape: </span><span class="sc">{</span>lora_output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"DoRA output shape: </span><span class="sc">{</span>dora_output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"LoRA output norm: </span><span class="sc">{</span>lora_output<span class="sc">.</span>norm()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"DoRA output norm: </span><span class="sc">{</span>dora_output<span class="sc">.</span>norm()<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>LoRA vs DoRA Comparison:
LoRA output shape: torch.Size([32, 768])
DoRA output shape: torch.Size([32, 768])
LoRA output norm: 0.0000
DoRA output norm: 5.6569</code></pre>
</div>
</div>
</section>
<section id="mixture-of-loras-molora" class="level3">
<h3 class="anchored" data-anchor-id="mixture-of-loras-molora">3. Mixture of LoRAs (MoLoRA)</h3>
<p>Multiple LoRA experts for different aspects:</p>
<div id="molora" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MoLoRALayer(nn.Module):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_features, out_features, num_experts<span class="op">=</span><span class="dv">4</span>, rank<span class="op">=</span><span class="dv">16</span>):</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_experts <span class="op">=</span> num_experts</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Multiple LoRA experts</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.experts <span class="op">=</span> nn.ModuleList([</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>            LoRALayer(in_features, out_features, rank)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_experts)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Gating network</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gate <span class="op">=</span> nn.Linear(in_features, num_experts)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute gating weights</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        gate_input <span class="op">=</span> x.mean(dim<span class="op">=</span><span class="dv">1</span>) <span class="cf">if</span> x.dim() <span class="op">&gt;</span> <span class="dv">2</span> <span class="cf">else</span> x</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        gate_weights <span class="op">=</span> F.softmax(<span class="va">self</span>.gate(gate_input), dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Combine expert outputs</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>        expert_outputs <span class="op">=</span> torch.stack([expert(x) <span class="cf">for</span> expert <span class="kw">in</span> <span class="va">self</span>.experts], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Weighted combination</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> gate_weights.dim() <span class="op">==</span> <span class="dv">2</span>:  <span class="co"># Batch of inputs</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>            gate_weights <span class="op">=</span> gate_weights.T.unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> torch.<span class="bu">sum</span>(gate_weights <span class="op">*</span> expert_outputs, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:  <span class="co"># Single input</span></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> torch.<span class="bu">sum</span>(gate_weights[:, <span class="va">None</span>] <span class="op">*</span> expert_outputs, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstration of MoLoRA</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>molora_layer <span class="op">=</span> MoLoRALayer(<span class="dv">768</span>, <span class="dv">768</span>, num_experts<span class="op">=</span><span class="dv">4</span>, rank<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">32</span>, <span class="dv">768</span>)</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> molora_layer(x)</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Mixture of LoRAs (MoLoRA) Demo:"</span>)</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input shape: </span><span class="sc">{</span>x<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Output shape: </span><span class="sc">{</span>output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of experts: </span><span class="sc">{</span>molora_layer<span class="sc">.</span>num_experts<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Show expert utilization</span></span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>    gate_weights <span class="op">=</span> F.softmax(molora_layer.gate(x), dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>    expert_utilization <span class="op">=</span> gate_weights.mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expert utilization:"</span>)</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, util <span class="kw">in</span> <span class="bu">enumerate</span>(expert_utilization):</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Expert </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>util<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Mixture of LoRAs (MoLoRA) Demo:
Input shape: torch.Size([32, 768])
Output shape: torch.Size([32, 768])
Number of experts: 4
Expert utilization:
  Expert 1: 0.252
  Expert 2: 0.271
  Expert 3: 0.234
  Expert 4: 0.242</code></pre>
</div>
</div>
</section>
</section>
<section id="performance-optimization" class="level2">
<h2 class="anchored" data-anchor-id="performance-optimization">Performance Optimization</h2>
<section id="memory-optimization" class="level3">
<h3 class="anchored" data-anchor-id="memory-optimization">Memory Optimization</h3>
<div id="memory-optimization" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MemoryEfficientLoRA:</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> gradient_checkpointing_forward(module, <span class="op">*</span>args):</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Custom gradient checkpointing for LoRA layers"""</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> create_custom_forward(module):</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>            <span class="kw">def</span> custom_forward(<span class="op">*</span>inputs):</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> module(<span class="op">*</span>inputs)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> custom_forward</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.utils.checkpoint.checkpoint(</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>            create_custom_forward(module), <span class="op">*</span>args</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> merge_lora_weights(model):</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Merge LoRA weights into base model for inference"""</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        merged_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, module <span class="kw">in</span> model.named_modules():</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">isinstance</span>(module, LoRALinear):</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Compute merged weight</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>                lora_weight <span class="op">=</span> module.lora.lora_B.weight <span class="op">@</span> module.lora.lora_A.weight</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>                merged_weight <span class="op">=</span> module.original_layer.weight <span class="op">+</span> lora_weight <span class="op">*</span> module.lora.scaling</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Create merged layer</span></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>                merged_layer <span class="op">=</span> nn.Linear(</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>                    module.original_layer.in_features,</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>                    module.original_layer.out_features,</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>                    bias<span class="op">=</span>module.original_layer.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>                merged_layer.weight.data <span class="op">=</span> merged_weight</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> module.original_layer.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>                    merged_layer.bias.data <span class="op">=</span> module.original_layer.bias</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>                merged_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> merged_count</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute_memory_savings(model):</span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Compute memory savings from LoRA"""</span></span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>        total_params <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a>        lora_params <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a>            total_params <span class="op">+=</span> param.numel()</span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="st">'lora'</span> <span class="kw">in</span> name:</span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a>                lora_params <span class="op">+=</span> param.numel()</span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a>        savings_ratio <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> (lora_params <span class="op">/</span> total_params)</span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a>            <span class="st">'total_parameters'</span>: total_params,</span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a>            <span class="st">'lora_parameters'</span>: lora_params,</span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a>            <span class="st">'base_parameters'</span>: total_params <span class="op">-</span> lora_params,</span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a>            <span class="st">'memory_savings'</span>: savings_ratio,</span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a>            <span class="st">'compression_ratio'</span>: total_params <span class="op">/</span> lora_params <span class="cf">if</span> lora_params <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="bu">float</span>(<span class="st">'inf'</span>)</span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstrate memory optimization</span></span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> MemoryEfficientLoRA()</span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a><span class="co"># Example memory analysis (would use real model)</span></span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true" tabindex="-1"></a>example_stats <span class="op">=</span> {</span>
<span id="cb15-65"><a href="#cb15-65" aria-hidden="true" tabindex="-1"></a>    <span class="st">'total_parameters'</span>: <span class="dv">175_000_000</span>,</span>
<span id="cb15-66"><a href="#cb15-66" aria-hidden="true" tabindex="-1"></a>    <span class="st">'lora_parameters'</span>: <span class="dv">1_750_000</span>,</span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true" tabindex="-1"></a>    <span class="st">'base_parameters'</span>: <span class="dv">173_250_000</span>,</span>
<span id="cb15-68"><a href="#cb15-68" aria-hidden="true" tabindex="-1"></a>    <span class="st">'memory_savings'</span>: <span class="fl">0.99</span>,</span>
<span id="cb15-69"><a href="#cb15-69" aria-hidden="true" tabindex="-1"></a>    <span class="st">'compression_ratio'</span>: <span class="dv">100</span></span>
<span id="cb15-70"><a href="#cb15-70" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb15-71"><a href="#cb15-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-72"><a href="#cb15-72" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Memory Optimization Analysis:"</span>)</span>
<span id="cb15-73"><a href="#cb15-73" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total parameters: </span><span class="sc">{</span>example_stats[<span class="st">'total_parameters'</span>]<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb15-74"><a href="#cb15-74" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"LoRA parameters: </span><span class="sc">{</span>example_stats[<span class="st">'lora_parameters'</span>]<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb15-75"><a href="#cb15-75" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Memory savings: </span><span class="sc">{</span>example_stats[<span class="st">'memory_savings'</span>]<span class="sc">:.1%}</span><span class="ss">"</span>)</span>
<span id="cb15-76"><a href="#cb15-76" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Compression ratio: </span><span class="sc">{</span>example_stats[<span class="st">'compression_ratio'</span>]<span class="sc">:.1f}</span><span class="ss">x"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Memory Optimization Analysis:
Total parameters: 175,000,000
LoRA parameters: 1,750,000
Memory savings: 99.0%
Compression ratio: 100.0x</code></pre>
</div>
</div>
</section>
<section id="training-optimizations" class="level3">
<h3 class="anchored" data-anchor-id="training-optimizations">Training Optimizations</h3>
<div id="training-optimization" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> OptimizedLoRATrainer:</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, config):</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.config <span class="op">=</span> config</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Separate parameter groups</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.setup_parameter_groups()</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Mixed precision training</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.scaler <span class="op">=</span> torch.cuda.amp.GradScaler()</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.scaler <span class="op">=</span> <span class="va">None</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> setup_parameter_groups(<span class="va">self</span>):</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Separate LoRA and non-LoRA parameters"""</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>        lora_params <span class="op">=</span> []</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>        other_params <span class="op">=</span> []</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, param <span class="kw">in</span> <span class="va">self</span>.model.named_parameters():</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> param.requires_grad:</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="st">'lora'</span> <span class="kw">in</span> name:</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>                    lora_params.append(param)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>                    other_params.append(param)</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.param_groups <span class="op">=</span> [</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>            {</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>                <span class="st">'params'</span>: lora_params, </span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>                <span class="st">'lr'</span>: <span class="bu">getattr</span>(<span class="va">self</span>.config, <span class="st">'lora_lr'</span>, <span class="fl">1e-4</span>), </span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>                <span class="st">'weight_decay'</span>: <span class="fl">0.01</span>,</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>                <span class="st">'name'</span>: <span class="st">'lora_params'</span></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>            },</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>            {</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>                <span class="st">'params'</span>: other_params, </span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>                <span class="st">'lr'</span>: <span class="bu">getattr</span>(<span class="va">self</span>.config, <span class="st">'base_lr'</span>, <span class="fl">1e-5</span>), </span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>                <span class="st">'weight_decay'</span>: <span class="fl">0.1</span>,</span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>                <span class="st">'name'</span>: <span class="st">'base_params'</span></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Parameter Groups Setup:"</span>)</span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> group <span class="kw">in</span> <span class="va">self</span>.param_groups:</span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>            param_count <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> group[<span class="st">'params'</span>])</span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>group[<span class="st">'name'</span>]<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>param_count<span class="sc">:,}</span><span class="ss"> parameters, lr=</span><span class="sc">{</span>group[<span class="st">'lr'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> training_step(<span class="va">self</span>, batch, optimizer):</span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Optimized training step with mixed precision"""</span></span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.scaler <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Mixed precision training</span></span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.cuda.amp.autocast():</span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a>                outputs <span class="op">=</span> <span class="va">self</span>.model(<span class="op">**</span>batch)</span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> outputs.loss <span class="cf">if</span> <span class="bu">hasattr</span>(outputs, <span class="st">'loss'</span>) <span class="cf">else</span> outputs</span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Scaled backward pass</span></span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.scaler.scale(loss).backward()</span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Gradient clipping for LoRA parameters only</span></span>
<span id="cb17-59"><a href="#cb17-59" aria-hidden="true" tabindex="-1"></a>            lora_params <span class="op">=</span> [p <span class="cf">for</span> group <span class="kw">in</span> <span class="va">self</span>.param_groups </span>
<span id="cb17-60"><a href="#cb17-60" aria-hidden="true" tabindex="-1"></a>                          <span class="cf">for</span> p <span class="kw">in</span> group[<span class="st">'params'</span>] <span class="cf">if</span> group[<span class="st">'name'</span>] <span class="op">==</span> <span class="st">'lora_params'</span>]</span>
<span id="cb17-61"><a href="#cb17-61" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-62"><a href="#cb17-62" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.scaler.unscale_(optimizer)</span>
<span id="cb17-63"><a href="#cb17-63" aria-hidden="true" tabindex="-1"></a>            torch.nn.utils.clip_grad_norm_(lora_params, max_norm<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb17-64"><a href="#cb17-64" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-65"><a href="#cb17-65" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.scaler.step(optimizer)</span>
<span id="cb17-66"><a href="#cb17-66" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.scaler.update()</span>
<span id="cb17-67"><a href="#cb17-67" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb17-68"><a href="#cb17-68" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Regular training</span></span>
<span id="cb17-69"><a href="#cb17-69" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> <span class="va">self</span>.model(<span class="op">**</span>batch)</span>
<span id="cb17-70"><a href="#cb17-70" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> outputs.loss <span class="cf">if</span> <span class="bu">hasattr</span>(outputs, <span class="st">'loss'</span>) <span class="cf">else</span> outputs</span>
<span id="cb17-71"><a href="#cb17-71" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-72"><a href="#cb17-72" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb17-73"><a href="#cb17-73" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-74"><a href="#cb17-74" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Gradient clipping</span></span>
<span id="cb17-75"><a href="#cb17-75" aria-hidden="true" tabindex="-1"></a>            lora_params <span class="op">=</span> [p <span class="cf">for</span> group <span class="kw">in</span> <span class="va">self</span>.param_groups </span>
<span id="cb17-76"><a href="#cb17-76" aria-hidden="true" tabindex="-1"></a>                          <span class="cf">for</span> p <span class="kw">in</span> group[<span class="st">'params'</span>] <span class="cf">if</span> group[<span class="st">'name'</span>] <span class="op">==</span> <span class="st">'lora_params'</span>]</span>
<span id="cb17-77"><a href="#cb17-77" aria-hidden="true" tabindex="-1"></a>            torch.nn.utils.clip_grad_norm_(lora_params, max_norm<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb17-78"><a href="#cb17-78" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-79"><a href="#cb17-79" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb17-80"><a href="#cb17-80" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-81"><a href="#cb17-81" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb17-82"><a href="#cb17-82" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss.item() <span class="cf">if</span> <span class="bu">hasattr</span>(loss, <span class="st">'item'</span>) <span class="cf">else</span> loss</span>
<span id="cb17-83"><a href="#cb17-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-84"><a href="#cb17-84" aria-hidden="true" tabindex="-1"></a><span class="co"># Example configuration</span></span>
<span id="cb17-85"><a href="#cb17-85" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TrainingConfig:</span>
<span id="cb17-86"><a href="#cb17-86" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb17-87"><a href="#cb17-87" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lora_lr <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb17-88"><a href="#cb17-88" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.base_lr <span class="op">=</span> <span class="fl">1e-5</span></span>
<span id="cb17-89"><a href="#cb17-89" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mixed_precision <span class="op">=</span> <span class="va">True</span></span>
<span id="cb17-90"><a href="#cb17-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-91"><a href="#cb17-91" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> TrainingConfig()</span>
<span id="cb17-92"><a href="#cb17-92" aria-hidden="true" tabindex="-1"></a><span class="co"># trainer = OptimizedLoRATrainer(model, config)  # Would use real model</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="use-cases-and-applications" class="level2">
<h2 class="anchored" data-anchor-id="use-cases-and-applications">Use Cases and Applications</h2>
<section id="domain-adaptation" class="level3">
<h3 class="anchored" data-anchor-id="domain-adaptation">1. Domain Adaptation</h3>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-4-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-1" role="tab" aria-controls="tabset-4-1" aria-selected="true">Medical Imaging</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-4-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-2" role="tab" aria-controls="tabset-4-2" aria-selected="false">Satellite Imagery</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-4-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-3" role="tab" aria-controls="tabset-4-3" aria-selected="false">Autonomous Driving</a></li></ul>
<div class="tab-content">
<div id="tabset-4-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-4-1-tab">
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Configuration Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Optimized for medical image analysis</strong></p>
<p><strong>Rank:</strong> 32 | <strong>Alpha:</strong> 32<br>
<strong>Target modules:</strong> q_proj, v_proj, fc1, fc2</p>
</div>
</div>
<div class="tabset-margin-container"></div><div class="panel-tabset" data-group="medical">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">Key Features</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">Technical Details</a></li></ul>
<div class="tab-content" data-group="medical">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="grid">
<section id="higher-rank" class="level4 g-col-4">
<h4 class="anchored" data-anchor-id="higher-rank">Higher Rank</h4>
<p>Complex medical patterns require higher dimensional adaptations for accurate analysis</p>
</section>
<section id="attention-focus" class="level4 g-col-4">
<h4 class="anchored" data-anchor-id="attention-focus">Attention Focus</h4>
<p>Specialized targeting of attention and MLP layers for medical feature detection</p>
</section>
<section id="enhanced-extraction" class="level4 g-col-4">
<h4 class="anchored" data-anchor-id="enhanced-extraction">Enhanced Extraction</h4>
<p>Advanced feature extraction capabilities for diagnostic imaging</p>
</section>
</div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<table class="caption-top table">
<colgroup>
<col style="width: 39%">
<col style="width: 28%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Value</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Rank</td>
<td>32</td>
<td>Handle complex medical pattern recognition</td>
</tr>
<tr class="even">
<td>Alpha</td>
<td>32</td>
<td>Balanced learning rate for medical data</td>
</tr>
<tr class="odd">
<td>Modules</td>
<td>q_proj, v_proj, fc1, fc2</td>
<td>Focus on attention and feed-forward layers</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<div id="tabset-4-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-4-2-tab">
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Configuration Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Adapted for satellite and aerial imagery</strong></p>
<p><strong>Rank:</strong> 16 | <strong>Alpha:</strong> 16<br>
<strong>Target modules:</strong> qkv, proj</p>
</div>
</div>
<div class="tabset-margin-container"></div><div class="panel-tabset" data-group="satellite">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true">Key Features</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false">Technical Details</a></li></ul>
<div class="tab-content" data-group="satellite">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<div class="grid">
<section id="balanced-efficiency" class="level4 g-col-4">
<h4 class="anchored" data-anchor-id="balanced-efficiency">Balanced Efficiency</h4>
<p>Optimized rank for computational efficiency while maintaining accuracy</p>
</section>
<section id="vision-focused" class="level4 g-col-4">
<h4 class="anchored" data-anchor-id="vision-focused">Vision-Focused</h4>
<p>Specialized adaptations for computer vision tasks</p>
</section>
<section id="spatial-modeling" class="level4 g-col-4">
<h4 class="anchored" data-anchor-id="spatial-modeling">Spatial Modeling</h4>
<p>Enhanced spatial relationship understanding for geographic data</p>
</section>
</div>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Parameter</th>
<th>Value</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Rank</td>
<td>16</td>
<td>Balance between performance and efficiency</td>
</tr>
<tr class="even">
<td>Alpha</td>
<td>16</td>
<td>Moderate learning rate for aerial imagery</td>
</tr>
<tr class="odd">
<td>Modules</td>
<td>qkv, proj</td>
<td>Streamlined attention mechanisms</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<div id="tabset-4-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-4-3-tab">
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Configuration Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Designed for autonomous vehicle perception</strong></p>
<p><strong>Rank:</strong> 24 | <strong>Alpha:</strong> 24<br>
<strong>Target modules:</strong> q_proj, k_proj, v_proj, dense</p>
</div>
</div>
<div class="tabset-margin-container"></div><div class="panel-tabset" data-group="driving">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-3-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-1" role="tab" aria-controls="tabset-3-1" aria-selected="true">Key Features</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-2" role="tab" aria-controls="tabset-3-2" aria-selected="false">Technical Details</a></li></ul>
<div class="tab-content" data-group="driving">
<div id="tabset-3-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-3-1-tab">
<div class="grid">
<section id="real-time-performance" class="level4 g-col-4">
<h4 class="anchored" data-anchor-id="real-time-performance">Real-Time Performance</h4>
<p>Optimized for real-time inference requirements in vehicle systems</p>
</section>
<section id="multi-object-detection" class="level4 g-col-4">
<h4 class="anchored" data-anchor-id="multi-object-detection">Multi-Object Detection</h4>
<p>Specialized for detecting and tracking multiple objects simultaneously</p>
</section>
<section id="safety-critical" class="level4 g-col-4">
<h4 class="anchored" data-anchor-id="safety-critical">Safety-Critical</h4>
<p>Designed for safety-critical applications with high reliability standards</p>
</section>
</div>
</div>
<div id="tabset-3-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-2-tab">
<table class="caption-top table">
<colgroup>
<col style="width: 39%">
<col style="width: 28%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Value</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Rank</td>
<td>24</td>
<td>High performance for safety-critical applications</td>
</tr>
<tr class="even">
<td>Alpha</td>
<td>24</td>
<td>Balanced learning for multi-object scenarios</td>
</tr>
<tr class="odd">
<td>Modules</td>
<td>q_proj, k_proj, v_proj, dense</td>
<td>Comprehensive attention and dense layer targeting</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<section id="summary-comparison" class="level4">
<h4 class="anchored" data-anchor-id="summary-comparison">Summary Comparison</h4>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick Reference Table
</div>
</div>
<div class="callout-body-container callout-body">
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 11%">
<col style="width: 12%">
<col style="width: 27%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th>Use Case</th>
<th>Rank</th>
<th>Alpha</th>
<th>Primary Focus</th>
<th>Target Modules</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Medical Imaging</td>
<td>32</td>
<td>32</td>
<td>Complex pattern recognition</td>
<td>q_proj, v_proj, fc1, fc2</td>
</tr>
<tr class="even">
<td>Satellite Imagery</td>
<td>16</td>
<td>16</td>
<td>Efficient spatial analysis</td>
<td>qkv, proj</td>
</tr>
<tr class="odd">
<td>Autonomous Driving</td>
<td>24</td>
<td>24</td>
<td>Real-time multi-object detection</td>
<td>q_proj, k_proj, v_proj, dense</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Configuration Guidelines
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Higher ranks</strong> (24-32) for complex, safety-critical applications</li>
<li><strong>Moderate ranks</strong> (16-20) for balanced efficiency and performance<br>
</li>
<li><strong>Lower ranks</strong> (4-12) for lightweight, fast inference applications</li>
</ul>
</div>
</div>
</section>
</section>
<section id="multi-lingual-vision-language" class="level3">
<h3 class="anchored" data-anchor-id="multi-lingual-vision-language">2. Multi-lingual Vision-Language</h3>
<div id="multilingual-lora" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultilingualLoRA:</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, base_model, languages):</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.base_model <span class="op">=</span> base_model</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.languages <span class="op">=</span> languages</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.language_adapters <span class="op">=</span> {}</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> lang <span class="kw">in</span> languages:</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.language_adapters[lang] <span class="op">=</span> <span class="va">self</span>.create_language_adapter(lang)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> create_language_adapter(<span class="va">self</span>, language):</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Create language-specific LoRA adapter"""</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Language-specific configurations</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>        lang_configs <span class="op">=</span> {</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>            <span class="st">"english"</span>: {<span class="st">"rank"</span>: <span class="dv">16</span>, <span class="st">"alpha"</span>: <span class="dv">16</span>},</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>            <span class="st">"chinese"</span>: {<span class="st">"rank"</span>: <span class="dv">20</span>, <span class="st">"alpha"</span>: <span class="dv">20</span>},  <span class="co"># More complex script</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>            <span class="st">"arabic"</span>: {<span class="st">"rank"</span>: <span class="dv">18</span>, <span class="st">"alpha"</span>: <span class="dv">18</span>},   <span class="co"># RTL language</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>            <span class="st">"hindi"</span>: {<span class="st">"rank"</span>: <span class="dv">22</span>, <span class="st">"alpha"</span>: <span class="dv">22</span>},    <span class="co"># Complex script</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>            <span class="st">"spanish"</span>: {<span class="st">"rank"</span>: <span class="dv">14</span>, <span class="st">"alpha"</span>: <span class="dv">14</span>},  <span class="co"># Similar to English</span></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>        config <span class="op">=</span> lang_configs.get(language, {<span class="st">"rank"</span>: <span class="dv">16</span>, <span class="st">"alpha"</span>: <span class="dv">16</span>})</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> LoRAConfig(</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>            rank<span class="op">=</span>config[<span class="st">"rank"</span>],</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>            alpha<span class="op">=</span>config[<span class="st">"alpha"</span>],</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>            target_modules<span class="op">=</span>[<span class="st">"q_proj"</span>, <span class="st">"k_proj"</span>, <span class="st">"v_proj"</span>],</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>            task_type<span class="op">=</span><span class="ss">f"vlm_</span><span class="sc">{</span>language<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_adapter_stats(<span class="va">self</span>):</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Get statistics about language adapters"""</span></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>        stats <span class="op">=</span> {}</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> lang, adapter <span class="kw">in</span> <span class="va">self</span>.language_adapters.items():</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>            stats[lang] <span class="op">=</span> {</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>                <span class="st">"rank"</span>: adapter.rank,</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>                <span class="st">"alpha"</span>: adapter.alpha,</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>                <span class="st">"parameters"</span>: adapter.rank <span class="op">*</span> <span class="dv">768</span> <span class="op">*</span> <span class="dv">2</span>,  <span class="co"># Approximate</span></span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>                <span class="st">"target_modules"</span>: <span class="bu">len</span>(adapter.target_modules)</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> stats</span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, images, texts, language):</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Forward pass with language-specific adapter"""</span></span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> language <span class="kw">not</span> <span class="kw">in</span> <span class="va">self</span>.language_adapters:</span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Language '</span><span class="sc">{</span>language<span class="sc">}</span><span class="ss">' not supported"</span>)</span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Would activate language-specific adapter</span></span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a>        adapter_config <span class="op">=</span> <span class="va">self</span>.language_adapters[language]</span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Return placeholder for demonstration</span></span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>            <span class="st">"language"</span>: language,</span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a>            <span class="st">"adapter_config"</span>: adapter_config,</span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a>            <span class="st">"message"</span>: <span class="ss">f"Processing with </span><span class="sc">{</span>language<span class="sc">}</span><span class="ss"> adapter"</span></span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstration</span></span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a>languages <span class="op">=</span> [<span class="st">"english"</span>, <span class="st">"chinese"</span>, <span class="st">"arabic"</span>, <span class="st">"hindi"</span>, <span class="st">"spanish"</span>]</span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a>multilingual_model <span class="op">=</span> MultilingualLoRA(<span class="va">None</span>, languages)</span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Multilingual LoRA Configuration:"</span>)</span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">40</span>)</span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a>adapter_stats <span class="op">=</span> multilingual_model.get_adapter_stats()</span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> lang, stats <span class="kw">in</span> adapter_stats.items():</span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>lang<span class="sc">.</span>title()<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Rank: </span><span class="sc">{</span>stats[<span class="st">'rank'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-70"><a href="#cb18-70" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Alpha: </span><span class="sc">{</span>stats[<span class="st">'alpha'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-71"><a href="#cb18-71" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Parameters: ~</span><span class="sc">{</span>stats[<span class="st">'parameters'</span>]<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb18-72"><a href="#cb18-72" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Target modules: </span><span class="sc">{</span>stats[<span class="st">'target_modules'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-73"><a href="#cb18-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-74"><a href="#cb18-74" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb18-75"><a href="#cb18-75" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> multilingual_model.forward(<span class="va">None</span>, <span class="va">None</span>, <span class="st">"chinese"</span>)</span>
<span id="cb18-76"><a href="#cb18-76" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Example usage: </span><span class="sc">{</span>result[<span class="st">'message'</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Multilingual LoRA Configuration:
========================================

English:
  Rank: 16
  Alpha: 16
  Parameters: ~24,576
  Target modules: 3

Chinese:
  Rank: 20
  Alpha: 20
  Parameters: ~30,720
  Target modules: 3

Arabic:
  Rank: 18
  Alpha: 18
  Parameters: ~27,648
  Target modules: 3

Hindi:
  Rank: 22
  Alpha: 22
  Parameters: ~33,792
  Target modules: 3

Spanish:
  Rank: 14
  Alpha: 14
  Parameters: ~21,504
  Target modules: 3

Example usage: Processing with chinese adapter</code></pre>
</div>
</div>
</section>
<section id="few-shot-learning" class="level3">
<h3 class="anchored" data-anchor-id="few-shot-learning">3. Few-Shot Learning</h3>
<div id="few-shot-learning" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FewShotLoRALearner:</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, base_model, config):</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.base_model <span class="op">=</span> base_model</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.config <span class="op">=</span> config</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.task_adapters <span class="op">=</span> {}</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> create_task_adapter(<span class="va">self</span>, task_name, rank<span class="op">=</span><span class="dv">8</span>, alpha<span class="op">=</span><span class="dv">16</span>):</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Create a lightweight adapter for few-shot learning"""</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> LoRAConfig(</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>            rank<span class="op">=</span>rank,</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>            alpha<span class="op">=</span>alpha,</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>            target_modules<span class="op">=</span>[<span class="st">"q_proj"</span>, <span class="st">"v_proj"</span>],  <span class="co"># Minimal modules for efficiency</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>            task_type<span class="op">=</span><span class="ss">f"few_shot_</span><span class="sc">{</span>task_name<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>            learning_rate<span class="op">=</span><span class="fl">1e-3</span>,  <span class="co"># Higher LR for fast adaptation</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>            dropout<span class="op">=</span><span class="fl">0.0</span>  <span class="co"># No dropout for few-shot</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> adapt_to_task(<span class="va">self</span>, task_name, support_examples, num_steps<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Quick adaptation using few examples"""</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Adapting to task: </span><span class="sc">{</span>task_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Support examples: </span><span class="sc">{</span><span class="bu">len</span>(support_examples)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Adaptation steps: </span><span class="sc">{</span>num_steps<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create task-specific adapter</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>        adapter_config <span class="op">=</span> <span class="va">self</span>.create_task_adapter(task_name)</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.task_adapters[task_name] <span class="op">=</span> adapter_config</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Simulate adaptation process</span></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>        adaptation_progress <span class="op">=</span> []</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, num_steps <span class="op">+</span> <span class="dv">1</span>, <span class="dv">20</span>):</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Simulate decreasing loss</span></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> <span class="fl">2.0</span> <span class="op">*</span> np.exp(<span class="op">-</span>step <span class="op">/</span> <span class="dv">50</span>) <span class="op">+</span> <span class="fl">0.1</span></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>            accuracy <span class="op">=</span> <span class="bu">min</span>(<span class="fl">0.95</span>, <span class="fl">0.3</span> <span class="op">+</span> <span class="fl">0.65</span> <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> np.exp(<span class="op">-</span>step <span class="op">/</span> <span class="dv">30</span>)))</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>            adaptation_progress.append({</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>                <span class="st">'step'</span>: step,</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>                <span class="st">'loss'</span>: loss,</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>                <span class="st">'accuracy'</span>: accuracy</span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>            })</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> adaptation_progress</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> evaluate_adaptation(<span class="va">self</span>, task_name, test_examples):</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Evaluate adapted model on test examples"""</span></span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> task_name <span class="kw">not</span> <span class="kw">in</span> <span class="va">self</span>.task_adapters:</span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"No adapter found for task: </span><span class="sc">{</span>task_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Simulate evaluation results</span></span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a>        performance <span class="op">=</span> {</span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>            <span class="st">'accuracy'</span>: <span class="fl">0.87</span>,</span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>            <span class="st">'precision'</span>: <span class="fl">0.89</span>,</span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a>            <span class="st">'recall'</span>: <span class="fl">0.85</span>,</span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>            <span class="st">'f1_score'</span>: <span class="fl">0.87</span>,</span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a>            <span class="st">'test_examples'</span>: <span class="bu">len</span>(test_examples)</span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> performance</span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstration of few-shot learning</span></span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a>few_shot_learner <span class="op">=</span> FewShotLoRALearner(<span class="va">None</span>, <span class="va">None</span>)</span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate different tasks</span></span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a>tasks <span class="op">=</span> {</span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a>    <span class="st">"bird_classification"</span>: <span class="dv">16</span>,  <span class="co"># 16 support examples</span></span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a>    <span class="st">"medical_diagnosis"</span>: <span class="dv">8</span>,     <span class="co"># 8 support examples  </span></span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a>    <span class="st">"product_recognition"</span>: <span class="dv">32</span>   <span class="co"># 32 support examples</span></span>
<span id="cb20-67"><a href="#cb20-67" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-68"><a href="#cb20-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-69"><a href="#cb20-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Few-Shot Learning with LoRA:"</span>)</span>
<span id="cb20-70"><a href="#cb20-70" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">35</span>)</span>
<span id="cb20-71"><a href="#cb20-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-72"><a href="#cb20-72" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> task_name, num_examples <span class="kw">in</span> tasks.items():</span>
<span id="cb20-73"><a href="#cb20-73" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Task: </span><span class="sc">{</span>task_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-74"><a href="#cb20-74" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-75"><a href="#cb20-75" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Adapt to task</span></span>
<span id="cb20-76"><a href="#cb20-76" aria-hidden="true" tabindex="-1"></a>    support_examples <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(num_examples))  <span class="co"># Mock examples</span></span>
<span id="cb20-77"><a href="#cb20-77" aria-hidden="true" tabindex="-1"></a>    progress <span class="op">=</span> few_shot_learner.adapt_to_task(task_name, support_examples)</span>
<span id="cb20-78"><a href="#cb20-78" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-79"><a href="#cb20-79" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Show adaptation progress</span></span>
<span id="cb20-80"><a href="#cb20-80" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Adaptation progress:"</span>)</span>
<span id="cb20-81"><a href="#cb20-81" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> point <span class="kw">in</span> progress[<span class="op">-</span><span class="dv">3</span>:]:  <span class="co"># Show last 3 points</span></span>
<span id="cb20-82"><a href="#cb20-82" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Step </span><span class="sc">{</span>point[<span class="st">'step'</span>]<span class="sc">:3d}</span><span class="ss">: Loss=</span><span class="sc">{</span>point[<span class="st">'loss'</span>]<span class="sc">:.3f}</span><span class="ss">, Acc=</span><span class="sc">{</span>point[<span class="st">'accuracy'</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb20-83"><a href="#cb20-83" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-84"><a href="#cb20-84" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluate</span></span>
<span id="cb20-85"><a href="#cb20-85" aria-hidden="true" tabindex="-1"></a>    test_examples <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">50</span>))  <span class="co"># Mock test set</span></span>
<span id="cb20-86"><a href="#cb20-86" aria-hidden="true" tabindex="-1"></a>    performance <span class="op">=</span> few_shot_learner.evaluate_adaptation(task_name, test_examples)</span>
<span id="cb20-87"><a href="#cb20-87" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Final performance: </span><span class="sc">{</span>performance[<span class="st">'accuracy'</span>]<span class="sc">:.3f}</span><span class="ss"> accuracy"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Few-Shot Learning with LoRA:
===================================

Task: bird_classification
Adapting to task: bird_classification
Support examples: 16
Adaptation steps: 100
Adaptation progress:
  Step  60: Loss=0.702, Acc=0.862
  Step  80: Loss=0.504, Acc=0.905
  Step 100: Loss=0.371, Acc=0.927
Final performance: 0.870 accuracy

Task: medical_diagnosis
Adapting to task: medical_diagnosis
Support examples: 8
Adaptation steps: 100
Adaptation progress:
  Step  60: Loss=0.702, Acc=0.862
  Step  80: Loss=0.504, Acc=0.905
  Step 100: Loss=0.371, Acc=0.927
Final performance: 0.870 accuracy

Task: product_recognition
Adapting to task: product_recognition
Support examples: 32
Adaptation steps: 100
Adaptation progress:
  Step  60: Loss=0.702, Acc=0.862
  Step  80: Loss=0.504, Acc=0.905
  Step 100: Loss=0.371, Acc=0.927
Final performance: 0.870 accuracy</code></pre>
</div>
</div>
</section>
</section>
<section id="best-practices" class="level2">
<h2 class="anchored" data-anchor-id="best-practices">Best Practices</h2>
<section id="hyperparameter-selection" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameter-selection">1. Hyperparameter Selection</h3>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-5-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-1" role="tab" aria-controls="tabset-5-1" aria-selected="true">Simple Classification</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-5-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-2" role="tab" aria-controls="tabset-5-2" aria-selected="false">Medical VQA</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-5-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-3" role="tab" aria-controls="tabset-5-3" aria-selected="false">General Captioning</a></li></ul>
<div class="tab-content">
<div id="tabset-5-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-5-1-tab">
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Recommended Settings
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Rank</strong>: 4</li>
<li><strong>Alpha</strong>: 4</li>
<li><strong>LoRA Learning Rate</strong>: 0.0001</li>
<li><strong>Base Learning Rate</strong>: 1e-05</li>
</ul>
</div>
</div>
<p><strong>Reasoning</strong>: Selected rank 4 for simple task complexity. This configuration provides sufficient adaptation capacity for straightforward classification tasks while maintaining parameter efficiency.</p>
</div>
<div id="tabset-5-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-5-2-tab">
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Recommended Settings
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Rank</strong>: 64</li>
<li><strong>Alpha</strong>: 128</li>
<li><strong>LoRA Learning Rate</strong>: 0.0001</li>
<li><strong>Base Learning Rate</strong>: 1e-05</li>
</ul>
</div>
</div>
<p><strong>Reasoning</strong>: Selected rank 64 for complex task complexity. Medical Visual Question Answering requires higher capacity to handle the intricate relationships between medical imagery and specialized domain knowledge.</p>
</div>
<div id="tabset-5-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-5-3-tab">
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Recommended Settings
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Rank</strong>: 16</li>
<li><strong>Alpha</strong>: 24</li>
<li><strong>LoRA Learning Rate</strong>: 0.0001</li>
<li><strong>Base Learning Rate</strong>: 1e-05</li>
</ul>
</div>
</div>
<p><strong>Reasoning</strong>: Selected rank 16 for balanced task complexity. General captioning strikes a middle ground between simple classification and highly specialized tasks, requiring moderate adaptation capacity.</p>
</div>
</div>
</div>
<section id="summary-table" class="level4">
<h4 class="anchored" data-anchor-id="summary-table">Summary Table</h4>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick Reference Table
</div>
</div>
<div class="callout-body-container callout-body">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Scenario</th>
<th>Rank</th>
<th>Alpha</th>
<th>LoRA LR</th>
<th>Base LR</th>
<th>Task Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Simple Classification</td>
<td>4</td>
<td>4</td>
<td>0.0001</td>
<td>1e-05</td>
<td>Low</td>
</tr>
<tr class="even">
<td>Medical VQA</td>
<td>64</td>
<td>128</td>
<td>0.0001</td>
<td>1e-05</td>
<td>High</td>
</tr>
<tr class="odd">
<td>General Captioning</td>
<td>16</td>
<td>24</td>
<td>0.0001</td>
<td>1e-05</td>
<td>Medium</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
</section>
<section id="module-selection-strategy" class="level3">
<h3 class="anchored" data-anchor-id="module-selection-strategy">2. Module Selection Strategy</h3>
<div id="cell-fig-module-selection" class="cell" data-execution_count="15">
<div class="cell-output cell-output-display">
<div id="fig-module-selection" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-module-selection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-module-selection-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-module-selection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: LoRA Module Selection Impact Analysis
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="training-best-practices" class="level3">
<h3 class="anchored" data-anchor-id="training-best-practices">3. Training Best Practices</h3>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-6-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-6-1" role="tab" aria-controls="tabset-6-1" aria-selected="true">Setup Phase</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-6-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-6-2" role="tab" aria-controls="tabset-6-2" aria-selected="false">Monitoring Phase</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-6-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-6-3" role="tab" aria-controls="tabset-6-3" aria-selected="false">Checkpointing Phase</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-6-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-6-4" role="tab" aria-controls="tabset-6-4" aria-selected="false">Evaluation Phase</a></li></ul>
<div class="tab-content">
<div id="tabset-6-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-6-1-tab">
<ul>
<li>Configure separate learning rates for LoRA and base parameters</li>
<li>Enable mixed precision training</li>
<li>Set up gradient accumulation</li>
<li>Configure gradient clipping</li>
</ul>
</div>
<div id="tabset-6-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-6-2-tab">
<ul>
<li>Track LoRA weight norms</li>
<li>Monitor validation metrics</li>
<li>Check for overfitting signs</li>
<li>Validate rank utilization</li>
</ul>
</div>
<div id="tabset-6-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-6-3-tab">
<ul>
<li>Save model at regular intervals</li>
<li>Keep best performing checkpoint</li>
<li>Save LoRA adapters separately</li>
<li>Document hyperparameters</li>
</ul>
</div>
<div id="tabset-6-4" class="tab-pane" role="tabpanel" aria-labelledby="tabset-6-4-tab">
<ul>
<li>Test on multiple datasets</li>
<li>Measure parameter efficiency</li>
<li>Check inference speed</li>
<li>Validate robustness</li>
</ul>
</div>
</div>
</div>
<section id="configuration-validation" class="level4">
<h4 class="anchored" data-anchor-id="configuration-validation">Configuration Validation</h4>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-7-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-7-1" role="tab" aria-controls="tabset-7-1" aria-selected="true">Good Config</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-7-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-7-2" role="tab" aria-controls="tabset-7-2" aria-selected="false">High Rank</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-7-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-7-3" role="tab" aria-controls="tabset-7-3" aria-selected="false">Low Alpha</a></li></ul>
<div class="tab-content">
<div id="tabset-7-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-7-1-tab">
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Status: ✅ Valid
</div>
</div>
<div class="callout-body-container callout-body">
<p>Configuration is valid and ready to use.</p>
</div>
</div>
</div>
<div id="tabset-7-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-7-2-tab">
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Status: ✅ Valid
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warnings
</div>
</div>
<div class="callout-body-container callout-body">
<p>⚠️ Very high rank may reduce efficiency benefits</p>
</div>
</div>
</div>
<div id="tabset-7-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-7-3-tab">
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Status: ✅ Valid
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warnings
</div>
</div>
<div class="callout-body-container callout-body">
<p>⚠️ Very low alpha may limit adaptation strength</p>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="troubleshooting" class="level2">
<h2 class="anchored" data-anchor-id="troubleshooting">Troubleshooting</h2>
<section id="common-issues-and-solutions" class="level3">
<h3 class="anchored" data-anchor-id="common-issues-and-solutions">Common Issues and Solutions</h3>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-9-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-9-1" role="tab" aria-controls="tabset-9-1" aria-selected="true">Example Diagnosis</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-9-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-9-2" role="tab" aria-controls="tabset-9-2" aria-selected="false">Debugging Checklist</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-9-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-9-3" role="tab" aria-controls="tabset-9-3" aria-selected="false">Debugging Tools</a></li></ul>
<div class="tab-content">
<div id="tabset-9-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-9-1-tab">
<div class="callout callout-style-default callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Training Issue Analysis
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Symptoms Observed:</strong> - Loss spikes during training - Gradient explosion detected<br>
- Poor convergence after many epochs</p>
<p><strong>Diagnosis:</strong> Training Instability<br>
<strong>Confidence Level:</strong> 67%</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Recommended Solutions
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Apply gradient clipping</strong> (max_norm=1.0)</li>
<li><strong>Use learning rate scheduling</strong></li>
<li><strong>Enable gradient accumulation</strong></li>
</ul>
</div>
</div>
</div>
<div id="tabset-9-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-9-2-tab">
<div class="tabset-margin-container"></div><div class="panel-tabset" data-group="checklist">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-8-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-8-1" role="tab" aria-controls="tabset-8-1" aria-selected="true">📊 Data Quality</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-8-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-8-2" role="tab" aria-controls="tabset-8-2" aria-selected="false">🔧 Model Configuration</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-8-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-8-3" role="tab" aria-controls="tabset-8-3" aria-selected="false">📈 Training Metrics</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-8-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-8-4" role="tab" aria-controls="tabset-8-4" aria-selected="false">💾 System Resources</a></li></ul>
<div class="tab-content" data-group="checklist">
<div id="tabset-8-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-8-1-tab">
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-17-contents" aria-controls="callout-17" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Data Validation Steps
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-17" class="callout-17-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox"><strong>Validate input preprocessing</strong></label>
<ul>
<li>Check normalization parameters</li>
<li>Verify tokenization consistency</li>
</ul></li>
<li><label><input type="checkbox"><strong>Check label distribution</strong></label>
<ul>
<li>Examine class balance</li>
<li>Identify potential bias</li>
</ul></li>
<li><label><input type="checkbox"><strong>Verify data augmentation</strong></label>
<ul>
<li>Test augmentation pipeline</li>
<li>Ensure proper randomization</li>
</ul></li>
<li><label><input type="checkbox"><strong>Ensure proper batching</strong></label>
<ul>
<li>Validate batch size settings</li>
<li>Check data loader configuration</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
<div id="tabset-8-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-8-2-tab">
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-18-contents" aria-controls="callout-18" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Configuration Verification
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-18" class="callout-18-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox"><strong>Confirm LoRA target modules</strong></label>
<ul>
<li>Verify layer selection</li>
<li>Check module naming consistency</li>
</ul></li>
<li><label><input type="checkbox"><strong>Check rank and alpha values</strong></label>
<ul>
<li>Validate rank appropriateness</li>
<li>Ensure alpha scaling is correct</li>
</ul></li>
<li><label><input type="checkbox"><strong>Validate learning rates</strong></label>
<ul>
<li>Test different LR values</li>
<li>Check optimizer settings</li>
</ul></li>
<li><label><input type="checkbox"><strong>Ensure proper initialization</strong></label>
<ul>
<li>Verify weight initialization</li>
<li>Check adapter placement</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
<div id="tabset-8-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-8-3-tab">
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-19-contents" aria-controls="callout-19" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Monitoring Guidelines
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-19" class="callout-19-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox"><strong>Track loss curves</strong></label>
<ul>
<li>Monitor training/validation loss</li>
<li>Identify overfitting patterns</li>
</ul></li>
<li><label><input type="checkbox"><strong>Monitor gradient norms</strong></label>
<ul>
<li>Check for gradient explosion</li>
<li>Detect vanishing gradients</li>
</ul></li>
<li><label><input type="checkbox"><strong>Check weight magnitudes</strong></label>
<ul>
<li>Monitor parameter updates</li>
<li>Verify adapter weights</li>
</ul></li>
<li><label><input type="checkbox"><strong>Validate learning rate schedule</strong></label>
<ul>
<li>Confirm schedule implementation</li>
<li>Monitor LR decay patterns</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
<div id="tabset-8-4" class="tab-pane" role="tabpanel" aria-labelledby="tabset-8-4-tab">
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-20-contents" aria-controls="callout-20" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Resource Monitoring
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-20" class="callout-20-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox"><strong>Monitor GPU memory usage</strong></label>
<ul>
<li>Track memory consumption</li>
<li>Optimize memory allocation</li>
</ul></li>
<li><label><input type="checkbox"><strong>Check system RAM</strong></label>
<ul>
<li>Monitor system memory</li>
<li>Identify memory leaks</li>
</ul></li>
<li><label><input type="checkbox"><strong>Verify disk space</strong></label>
<ul>
<li>Check storage availability</li>
<li>Monitor checkpoint sizes</li>
</ul></li>
<li><label><input type="checkbox"><strong>Monitor temperature/throttling</strong></label>
<ul>
<li>Check GPU temperatures</li>
<li>Detect thermal throttling</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="tabset-9-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-9-3-tab">
<section id="lora-debugging-analysis" class="level4">
<h4 class="anchored" data-anchor-id="lora-debugging-analysis">LoRA Debugging Analysis</h4>
<div class="grid">
<div class="g-col-6">
<p><strong>Adapter Information:</strong></p>
<ul>
<li><strong>Name:</strong> medical_vqa_adapter</li>
<li><strong>Health Status:</strong> 🟢 Healthy</li>
</ul>
</div>
<div class="g-col-6">
<p><strong>Rank Utilization Summary:</strong></p>
<ul>
<li><strong>Mean:</strong> 0.537</li>
<li><strong>Std Dev:</strong> 0.184<br>
</li>
<li><strong>Range:</strong> 0.250 - 0.812</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
💡 Recommendation
</div>
</div>
<div class="callout-body-container callout-body">
<p>LoRA configuration appears optimal based on current metrics.</p>
</div>
</div>
</section>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick Summary
</div>
</div>
<div class="callout-body-container callout-body">
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 37%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Issue</th>
<th>Symptoms</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Gradient Explosion</strong></td>
<td>Loss spikes, NaN values</td>
<td>Apply gradient clipping</td>
</tr>
<tr class="even">
<td><strong>Slow Convergence</strong></td>
<td>Plateau in loss</td>
<td>Adjust learning rate</td>
</tr>
<tr class="odd">
<td><strong>Memory Issues</strong></td>
<td>OOM errors</td>
<td>Reduce batch size, use gradient accumulation</td>
</tr>
<tr class="even">
<td><strong>Overfitting</strong></td>
<td>Train/val loss divergence</td>
<td>Add regularization, reduce rank</td>
</tr>
<tr class="odd">
<td><strong>Poor Performance</strong></td>
<td>Low accuracy</td>
<td>Increase rank, check target modules</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="additional-resources" class="level3">
<h3 class="anchored" data-anchor-id="additional-resources">Additional Resources</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-23-contents" aria-controls="callout-23" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Useful Commands
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-23" class="callout-23-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb22"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Monitor GPU usage</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="ex">nvidia-smi</span> <span class="at">-l</span> 1</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Check disk space</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="fu">df</span> <span class="at">-h</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Monitor system resources</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="ex">htop</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="debugging-tools-1" class="level3">
<h3 class="anchored" data-anchor-id="debugging-tools-1">Debugging Tools</h3>
<div id="debugging-tools" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LoRADebugger:</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, adapter_name<span class="op">=</span><span class="st">"default"</span>):</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.adapter_name <span class="op">=</span> adapter_name</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.analysis_cache <span class="op">=</span> {}</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> analyze_lora_weights(<span class="va">self</span>):</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Analyze LoRA weight distributions"""</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'weight_analysis'</span> <span class="kw">in</span> <span class="va">self</span>.analysis_cache:</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.analysis_cache[<span class="st">'weight_analysis'</span>]</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>        stats <span class="op">=</span> {}</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Simulate analysis for demonstration</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>        module_names <span class="op">=</span> [<span class="st">"attention.q_proj"</span>, <span class="st">"attention.k_proj"</span>, <span class="st">"attention.v_proj"</span>, </span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>                       <span class="st">"mlp.fc1"</span>, <span class="st">"mlp.fc2"</span>]</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name <span class="kw">in</span> module_names:</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Simulate weight statistics</span></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>            lora_A_norm <span class="op">=</span> np.random.uniform(<span class="fl">0.1</span>, <span class="fl">2.0</span>)</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>            lora_B_norm <span class="op">=</span> np.random.uniform(<span class="fl">0.1</span>, <span class="fl">2.0</span>)</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>            effective_rank <span class="op">=</span> np.random.randint(<span class="dv">4</span>, <span class="dv">16</span>)</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>            stats[name] <span class="op">=</span> {</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>                <span class="st">"lora_A_norm"</span>: lora_A_norm,</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>                <span class="st">"lora_B_norm"</span>: lora_B_norm,</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>                <span class="st">"effective_rank"</span>: effective_rank,</span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>                <span class="st">"rank_utilization"</span>: effective_rank <span class="op">/</span> <span class="fl">16.0</span></span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.analysis_cache[<span class="st">'weight_analysis'</span>] <span class="op">=</span> stats</span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> stats</span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute_rank_utilization(<span class="va">self</span>, threshold<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Compute rank utilization across modules"""</span></span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a>        weight_stats <span class="op">=</span> <span class="va">self</span>.analyze_lora_weights()</span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>        utilizations <span class="op">=</span> []</span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> module_name, stats <span class="kw">in</span> weight_stats.items():</span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a>            utilizations.append(stats[<span class="st">"rank_utilization"</span>])</span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a>            <span class="st">"mean_utilization"</span>: np.mean(utilizations),</span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a>            <span class="st">"std_utilization"</span>: np.std(utilizations),</span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a>            <span class="st">"min_utilization"</span>: np.<span class="bu">min</span>(utilizations),</span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a>            <span class="st">"max_utilization"</span>: np.<span class="bu">max</span>(utilizations),</span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a>            <span class="st">"per_module"</span>: {name: stats[<span class="st">"rank_utilization"</span>] </span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a>                          <span class="cf">for</span> name, stats <span class="kw">in</span> weight_stats.items()}</span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-51"><a href="#cb23-51" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate_health_report(<span class="va">self</span>):</span>
<span id="cb23-52"><a href="#cb23-52" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Generate comprehensive health report"""</span></span>
<span id="cb23-53"><a href="#cb23-53" aria-hidden="true" tabindex="-1"></a>        weight_analysis <span class="op">=</span> <span class="va">self</span>.analyze_lora_weights()</span>
<span id="cb23-54"><a href="#cb23-54" aria-hidden="true" tabindex="-1"></a>        rank_utilization <span class="op">=</span> <span class="va">self</span>.compute_rank_utilization()</span>
<span id="cb23-55"><a href="#cb23-55" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-56"><a href="#cb23-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Identify potential issues</span></span>
<span id="cb23-57"><a href="#cb23-57" aria-hidden="true" tabindex="-1"></a>        issues <span class="op">=</span> []</span>
<span id="cb23-58"><a href="#cb23-58" aria-hidden="true" tabindex="-1"></a>        warnings <span class="op">=</span> []</span>
<span id="cb23-59"><a href="#cb23-59" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-60"><a href="#cb23-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check for very low rank utilization</span></span>
<span id="cb23-61"><a href="#cb23-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> rank_utilization[<span class="st">"mean_utilization"</span>] <span class="op">&lt;</span> <span class="fl">0.3</span>:</span>
<span id="cb23-62"><a href="#cb23-62" aria-hidden="true" tabindex="-1"></a>            issues.append(<span class="st">"Low average rank utilization - consider reducing rank"</span>)</span>
<span id="cb23-63"><a href="#cb23-63" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-64"><a href="#cb23-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check for very high weight norms</span></span>
<span id="cb23-65"><a href="#cb23-65" aria-hidden="true" tabindex="-1"></a>        high_norm_modules <span class="op">=</span> [name <span class="cf">for</span> name, stats <span class="kw">in</span> weight_analysis.items() </span>
<span id="cb23-66"><a href="#cb23-66" aria-hidden="true" tabindex="-1"></a>                           <span class="cf">if</span> stats[<span class="st">"lora_A_norm"</span>] <span class="op">&gt;</span> <span class="fl">5.0</span> <span class="kw">or</span> stats[<span class="st">"lora_B_norm"</span>] <span class="op">&gt;</span> <span class="fl">5.0</span>]</span>
<span id="cb23-67"><a href="#cb23-67" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> high_norm_modules:</span>
<span id="cb23-68"><a href="#cb23-68" aria-hidden="true" tabindex="-1"></a>            warnings.append(<span class="ss">f"High weight norms in modules: </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(high_norm_modules)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-69"><a href="#cb23-69" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-70"><a href="#cb23-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check for rank imbalance</span></span>
<span id="cb23-71"><a href="#cb23-71" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> rank_utilization[<span class="st">"std_utilization"</span>] <span class="op">&gt;</span> <span class="fl">0.3</span>:</span>
<span id="cb23-72"><a href="#cb23-72" aria-hidden="true" tabindex="-1"></a>            warnings.append(<span class="st">"High variance in rank utilization across modules"</span>)</span>
<span id="cb23-73"><a href="#cb23-73" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-74"><a href="#cb23-74" aria-hidden="true" tabindex="-1"></a>        report <span class="op">=</span> {</span>
<span id="cb23-75"><a href="#cb23-75" aria-hidden="true" tabindex="-1"></a>            <span class="st">"adapter_name"</span>: <span class="va">self</span>.adapter_name,</span>
<span id="cb23-76"><a href="#cb23-76" aria-hidden="true" tabindex="-1"></a>            <span class="st">"weight_analysis"</span>: weight_analysis,</span>
<span id="cb23-77"><a href="#cb23-77" aria-hidden="true" tabindex="-1"></a>            <span class="st">"rank_utilization"</span>: rank_utilization,</span>
<span id="cb23-78"><a href="#cb23-78" aria-hidden="true" tabindex="-1"></a>            <span class="st">"health_status"</span>: <span class="st">"healthy"</span> <span class="cf">if</span> <span class="kw">not</span> issues <span class="cf">else</span> <span class="st">"needs_attention"</span>,</span>
<span id="cb23-79"><a href="#cb23-79" aria-hidden="true" tabindex="-1"></a>            <span class="st">"issues"</span>: issues,</span>
<span id="cb23-80"><a href="#cb23-80" aria-hidden="true" tabindex="-1"></a>            <span class="st">"warnings"</span>: warnings,</span>
<span id="cb23-81"><a href="#cb23-81" aria-hidden="true" tabindex="-1"></a>            <span class="st">"recommendations"</span>: <span class="va">self</span>._generate_recommendations(issues, warnings)</span>
<span id="cb23-82"><a href="#cb23-82" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb23-83"><a href="#cb23-83" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-84"><a href="#cb23-84" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> report</span>
<span id="cb23-85"><a href="#cb23-85" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-86"><a href="#cb23-86" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _generate_recommendations(<span class="va">self</span>, issues, warnings):</span>
<span id="cb23-87"><a href="#cb23-87" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Generate recommendations based on analysis"""</span></span>
<span id="cb23-88"><a href="#cb23-88" aria-hidden="true" tabindex="-1"></a>        recommendations <span class="op">=</span> []</span>
<span id="cb23-89"><a href="#cb23-89" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-90"><a href="#cb23-90" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">any</span>(<span class="st">"rank utilization"</span> <span class="kw">in</span> issue <span class="cf">for</span> issue <span class="kw">in</span> issues):</span>
<span id="cb23-91"><a href="#cb23-91" aria-hidden="true" tabindex="-1"></a>            recommendations.append(<span class="st">"Consider reducing LoRA rank to improve efficiency"</span>)</span>
<span id="cb23-92"><a href="#cb23-92" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-93"><a href="#cb23-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">any</span>(<span class="st">"weight norms"</span> <span class="kw">in</span> warning <span class="cf">for</span> warning <span class="kw">in</span> warnings):</span>
<span id="cb23-94"><a href="#cb23-94" aria-hidden="true" tabindex="-1"></a>            recommendations.append(<span class="st">"Apply stronger weight regularization or gradient clipping"</span>)</span>
<span id="cb23-95"><a href="#cb23-95" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-96"><a href="#cb23-96" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">any</span>(<span class="st">"variance"</span> <span class="kw">in</span> warning <span class="cf">for</span> warning <span class="kw">in</span> warnings):</span>
<span id="cb23-97"><a href="#cb23-97" aria-hidden="true" tabindex="-1"></a>            recommendations.append(<span class="st">"Use different ranks for different module types"</span>)</span>
<span id="cb23-98"><a href="#cb23-98" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-99"><a href="#cb23-99" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> issues <span class="kw">and</span> <span class="kw">not</span> warnings:</span>
<span id="cb23-100"><a href="#cb23-100" aria-hidden="true" tabindex="-1"></a>            recommendations.append(<span class="st">"LoRA configuration appears optimal"</span>)</span>
<span id="cb23-101"><a href="#cb23-101" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-102"><a href="#cb23-102" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> recommendations</span>
<span id="cb23-103"><a href="#cb23-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-104"><a href="#cb23-104" aria-hidden="true" tabindex="-1"></a><span class="co"># Debugging demonstration</span></span>
<span id="cb23-105"><a href="#cb23-105" aria-hidden="true" tabindex="-1"></a>debugger <span class="op">=</span> LoRADebugger(<span class="va">None</span>, <span class="st">"medical_vqa_adapter"</span>)  <span class="co"># Would use real model</span></span>
<span id="cb23-106"><a href="#cb23-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-107"><a href="#cb23-107" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"LoRA Debugging Analysis:"</span>)</span>
<span id="cb23-108"><a href="#cb23-108" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">25</span>)</span>
<span id="cb23-109"><a href="#cb23-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-110"><a href="#cb23-110" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate health report</span></span>
<span id="cb23-111"><a href="#cb23-111" aria-hidden="true" tabindex="-1"></a>health_report <span class="op">=</span> debugger.generate_health_report()</span>
<span id="cb23-112"><a href="#cb23-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-113"><a href="#cb23-113" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Adapter: </span><span class="sc">{</span>health_report[<span class="st">'adapter_name'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-114"><a href="#cb23-114" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Health Status: </span><span class="sc">{</span>health_report[<span class="st">'health_status'</span>]<span class="sc">.</span>title()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-115"><a href="#cb23-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-116"><a href="#cb23-116" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Rank Utilization Summary:"</span>)</span>
<span id="cb23-117"><a href="#cb23-117" aria-hidden="true" tabindex="-1"></a>rank_util <span class="op">=</span> health_report[<span class="st">'rank_utilization'</span>]</span>
<span id="cb23-118"><a href="#cb23-118" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Mean: </span><span class="sc">{</span>rank_util[<span class="st">'mean_utilization'</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb23-119"><a href="#cb23-119" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Std:  </span><span class="sc">{</span>rank_util[<span class="st">'std_utilization'</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb23-120"><a href="#cb23-120" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Range: </span><span class="sc">{</span>rank_util[<span class="st">'min_utilization'</span>]<span class="sc">:.3f}</span><span class="ss"> - </span><span class="sc">{</span>rank_util[<span class="st">'max_utilization'</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb23-121"><a href="#cb23-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-122"><a href="#cb23-122" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> health_report[<span class="st">'issues'</span>]:</span>
<span id="cb23-123"><a href="#cb23-123" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Issues Found:"</span>)</span>
<span id="cb23-124"><a href="#cb23-124" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> issue <span class="kw">in</span> health_report[<span class="st">'issues'</span>]:</span>
<span id="cb23-125"><a href="#cb23-125" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  ❌ </span><span class="sc">{</span>issue<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-126"><a href="#cb23-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-127"><a href="#cb23-127" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> health_report[<span class="st">'warnings'</span>]:</span>
<span id="cb23-128"><a href="#cb23-128" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Warnings:"</span>)</span>
<span id="cb23-129"><a href="#cb23-129" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> warning <span class="kw">in</span> health_report[<span class="st">'warnings'</span>]:</span>
<span id="cb23-130"><a href="#cb23-130" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  ⚠️  </span><span class="sc">{</span>warning<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-131"><a href="#cb23-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-132"><a href="#cb23-132" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Recommendations:"</span>)</span>
<span id="cb23-133"><a href="#cb23-133" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> rec <span class="kw">in</span> health_report[<span class="st">'recommendations'</span>]:</span>
<span id="cb23-134"><a href="#cb23-134" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  💡 </span><span class="sc">{</span>rec<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>LoRA Debugging Analysis:
=========================
Adapter: medical_vqa_adapter
Health Status: Healthy

Rank Utilization Summary:
  Mean: 0.625
  Std:  0.131
  Range: 0.500 - 0.812

Recommendations:
  💡 LoRA configuration appears optimal</code></pre>
</div>
</div>
</section>
</section>
<section id="production-deployment" class="level2">
<h2 class="anchored" data-anchor-id="production-deployment">Production Deployment</h2>
<section id="model-management-system" class="level3">
<h3 class="anchored" data-anchor-id="model-management-system">Model Management System</h3>
<div id="production-deployment" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Dict, Any, Optional, Union</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> contextlib <span class="im">import</span> contextmanager</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LoRAModelManager:</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Production-ready LoRA model management system"""</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, base_model_path: <span class="bu">str</span>, device: <span class="bu">str</span> <span class="op">=</span> <span class="st">"auto"</span>):</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.base_model_path <span class="op">=</span> base_model_path</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.device <span class="op">=</span> <span class="va">self</span>._setup_device(device)</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.base_model <span class="op">=</span> <span class="va">None</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.active_adapters <span class="op">=</span> {}</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.adapter_configs <span class="op">=</span> {}</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Performance monitoring</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.request_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.total_inference_time <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.error_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Setup logging</span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>        logging.basicConfig(level<span class="op">=</span>logging.INFO)</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger <span class="op">=</span> logging.getLogger(<span class="va">__name__</span>)</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"LoRA Model Manager initialized"</span>)</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Device: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>device<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _setup_device(<span class="va">self</span>, device: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Setup compute device"""</span></span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> device <span class="op">==</span> <span class="st">"auto"</span>:</span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> <span class="st">"cuda"</span></span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> <span class="st">"cpu"</span></span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> device</span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> load_adapter(<span class="va">self</span>, adapter_name: <span class="bu">str</span>, adapter_path: <span class="bu">str</span>, config: Optional[Dict] <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Load a LoRA adapter"""</span></span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="ss">f"Loading adapter '</span><span class="sc">{</span>adapter_name<span class="sc">}</span><span class="ss">' from </span><span class="sc">{</span>adapter_path<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>        default_config <span class="op">=</span> {</span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a>            <span class="st">"rank"</span>: <span class="dv">16</span>,</span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a>            <span class="st">"alpha"</span>: <span class="dv">16</span>,</span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a>            <span class="st">"target_modules"</span>: [<span class="st">"q_proj"</span>, <span class="st">"k_proj"</span>, <span class="st">"v_proj"</span>],</span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a>            <span class="st">"task_type"</span>: <span class="st">"multimodal"</span></span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Merge defaults with provided config</span></span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a>        adapter_config <span class="op">=</span> {<span class="op">**</span>default_config, <span class="op">**</span>(config <span class="kw">or</span> {})}</span>
<span id="cb25-50"><a href="#cb25-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-51"><a href="#cb25-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store adapter (in real implementation, would load actual weights)</span></span>
<span id="cb25-52"><a href="#cb25-52" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.active_adapters[adapter_name] <span class="op">=</span> {</span>
<span id="cb25-53"><a href="#cb25-53" aria-hidden="true" tabindex="-1"></a>            <span class="st">"path"</span>: adapter_path,</span>
<span id="cb25-54"><a href="#cb25-54" aria-hidden="true" tabindex="-1"></a>            <span class="st">"loaded_at"</span>: time.time(),</span>
<span id="cb25-55"><a href="#cb25-55" aria-hidden="true" tabindex="-1"></a>            <span class="st">"parameters"</span>: adapter_config[<span class="st">"rank"</span>] <span class="op">*</span> <span class="dv">768</span> <span class="op">*</span> <span class="dv">2</span> <span class="op">*</span> <span class="bu">len</span>(adapter_config[<span class="st">"target_modules"</span>])</span>
<span id="cb25-56"><a href="#cb25-56" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb25-57"><a href="#cb25-57" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.adapter_configs[adapter_name] <span class="op">=</span> adapter_config</span>
<span id="cb25-58"><a href="#cb25-58" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-59"><a href="#cb25-59" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="ss">f"Adapter '</span><span class="sc">{</span>adapter_name<span class="sc">}</span><span class="ss">' loaded successfully"</span>)</span>
<span id="cb25-60"><a href="#cb25-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">True</span></span>
<span id="cb25-61"><a href="#cb25-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-62"><a href="#cb25-62" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-63"><a href="#cb25-63" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> unload_adapter(<span class="va">self</span>, adapter_name: <span class="bu">str</span>):</span>
<span id="cb25-64"><a href="#cb25-64" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Unload a LoRA adapter to free memory"""</span></span>
<span id="cb25-65"><a href="#cb25-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> adapter_name <span class="kw">in</span> <span class="va">self</span>.active_adapters:</span>
<span id="cb25-66"><a href="#cb25-66" aria-hidden="true" tabindex="-1"></a>            <span class="kw">del</span> <span class="va">self</span>.active_adapters[adapter_name]</span>
<span id="cb25-67"><a href="#cb25-67" aria-hidden="true" tabindex="-1"></a>            <span class="kw">del</span> <span class="va">self</span>.adapter_configs[adapter_name]</span>
<span id="cb25-68"><a href="#cb25-68" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.info(<span class="ss">f"Adapter '</span><span class="sc">{</span>adapter_name<span class="sc">}</span><span class="ss">' unloaded"</span>)</span>
<span id="cb25-69"><a href="#cb25-69" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">True</span></span>
<span id="cb25-70"><a href="#cb25-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb25-71"><a href="#cb25-71" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.warning(<span class="ss">f"Adapter '</span><span class="sc">{</span>adapter_name<span class="sc">}</span><span class="ss">' not found"</span>)</span>
<span id="cb25-72"><a href="#cb25-72" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb25-73"><a href="#cb25-73" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-74"><a href="#cb25-74" aria-hidden="true" tabindex="-1"></a>    <span class="at">@contextmanager</span></span>
<span id="cb25-75"><a href="#cb25-75" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> use_adapter(<span class="va">self</span>, adapter_name: <span class="bu">str</span>):</span>
<span id="cb25-76"><a href="#cb25-76" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Context manager for temporarily using an adapter"""</span></span>
<span id="cb25-77"><a href="#cb25-77" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> adapter_name <span class="kw">not</span> <span class="kw">in</span> <span class="va">self</span>.active_adapters:</span>
<span id="cb25-78"><a href="#cb25-78" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Adapter '</span><span class="sc">{</span>adapter_name<span class="sc">}</span><span class="ss">' not loaded"</span>)</span>
<span id="cb25-79"><a href="#cb25-79" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-80"><a href="#cb25-80" aria-hidden="true" tabindex="-1"></a>        <span class="co"># In real implementation, would apply adapter weights</span></span>
<span id="cb25-81"><a href="#cb25-81" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.debug(<span class="ss">f"Applying adapter '</span><span class="sc">{</span>adapter_name<span class="sc">}</span><span class="ss">'"</span>)</span>
<span id="cb25-82"><a href="#cb25-82" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-83"><a href="#cb25-83" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb25-84"><a href="#cb25-84" aria-hidden="true" tabindex="-1"></a>            <span class="cf">yield</span> adapter_name</span>
<span id="cb25-85"><a href="#cb25-85" aria-hidden="true" tabindex="-1"></a>        <span class="cf">finally</span>:</span>
<span id="cb25-86"><a href="#cb25-86" aria-hidden="true" tabindex="-1"></a>            <span class="co"># In real implementation, would restore original weights</span></span>
<span id="cb25-87"><a href="#cb25-87" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.debug(<span class="ss">f"Restored from adapter '</span><span class="sc">{</span>adapter_name<span class="sc">}</span><span class="ss">'"</span>)</span>
<span id="cb25-88"><a href="#cb25-88" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-89"><a href="#cb25-89" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> inference(<span class="va">self</span>, inputs: Dict[<span class="bu">str</span>, Any], adapter_name: Optional[<span class="bu">str</span>] <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, Any]:</span>
<span id="cb25-90"><a href="#cb25-90" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Perform inference with optional adapter"""</span></span>
<span id="cb25-91"><a href="#cb25-91" aria-hidden="true" tabindex="-1"></a>        start_time <span class="op">=</span> time.time()</span>
<span id="cb25-92"><a href="#cb25-92" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-93"><a href="#cb25-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb25-94"><a href="#cb25-94" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> adapter_name:</span>
<span id="cb25-95"><a href="#cb25-95" aria-hidden="true" tabindex="-1"></a>                <span class="cf">with</span> <span class="va">self</span>.use_adapter(adapter_name):</span>
<span id="cb25-96"><a href="#cb25-96" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Simulate inference with adapter</span></span>
<span id="cb25-97"><a href="#cb25-97" aria-hidden="true" tabindex="-1"></a>                    time.sleep(<span class="fl">0.01</span>)  <span class="co"># Simulate processing time</span></span>
<span id="cb25-98"><a href="#cb25-98" aria-hidden="true" tabindex="-1"></a>                    outputs <span class="op">=</span> {<span class="st">"prediction"</span>: <span class="st">"sample_output"</span>, <span class="st">"confidence"</span>: <span class="fl">0.95</span>}</span>
<span id="cb25-99"><a href="#cb25-99" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb25-100"><a href="#cb25-100" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Simulate base model inference</span></span>
<span id="cb25-101"><a href="#cb25-101" aria-hidden="true" tabindex="-1"></a>                time.sleep(<span class="fl">0.008</span>)  <span class="co"># Slightly faster without adapter</span></span>
<span id="cb25-102"><a href="#cb25-102" aria-hidden="true" tabindex="-1"></a>                outputs <span class="op">=</span> {<span class="st">"prediction"</span>: <span class="st">"base_output"</span>, <span class="st">"confidence"</span>: <span class="fl">0.85</span>}</span>
<span id="cb25-103"><a href="#cb25-103" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb25-104"><a href="#cb25-104" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update performance metrics</span></span>
<span id="cb25-105"><a href="#cb25-105" aria-hidden="true" tabindex="-1"></a>            inference_time <span class="op">=</span> time.time() <span class="op">-</span> start_time</span>
<span id="cb25-106"><a href="#cb25-106" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.request_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb25-107"><a href="#cb25-107" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.total_inference_time <span class="op">+=</span> inference_time</span>
<span id="cb25-108"><a href="#cb25-108" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb25-109"><a href="#cb25-109" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> {</span>
<span id="cb25-110"><a href="#cb25-110" aria-hidden="true" tabindex="-1"></a>                <span class="st">'outputs'</span>: outputs,</span>
<span id="cb25-111"><a href="#cb25-111" aria-hidden="true" tabindex="-1"></a>                <span class="st">'inference_time'</span>: inference_time,</span>
<span id="cb25-112"><a href="#cb25-112" aria-hidden="true" tabindex="-1"></a>                <span class="st">'adapter_used'</span>: adapter_name,</span>
<span id="cb25-113"><a href="#cb25-113" aria-hidden="true" tabindex="-1"></a>                <span class="st">'request_id'</span>: <span class="va">self</span>.request_count</span>
<span id="cb25-114"><a href="#cb25-114" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb25-115"><a href="#cb25-115" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb25-116"><a href="#cb25-116" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb25-117"><a href="#cb25-117" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.error_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb25-118"><a href="#cb25-118" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.error(<span class="ss">f"Inference failed: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-119"><a href="#cb25-119" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span></span>
<span id="cb25-120"><a href="#cb25-120" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-121"><a href="#cb25-121" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_performance_stats(<span class="va">self</span>) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, <span class="bu">float</span>]:</span>
<span id="cb25-122"><a href="#cb25-122" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Get performance statistics"""</span></span>
<span id="cb25-123"><a href="#cb25-123" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.request_count <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb25-124"><a href="#cb25-124" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> {<span class="st">'requests'</span>: <span class="dv">0</span>, <span class="st">'avg_time'</span>: <span class="dv">0</span>, <span class="st">'total_time'</span>: <span class="dv">0</span>, <span class="st">'error_rate'</span>: <span class="dv">0</span>}</span>
<span id="cb25-125"><a href="#cb25-125" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-126"><a href="#cb25-126" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb25-127"><a href="#cb25-127" aria-hidden="true" tabindex="-1"></a>            <span class="st">'requests'</span>: <span class="va">self</span>.request_count,</span>
<span id="cb25-128"><a href="#cb25-128" aria-hidden="true" tabindex="-1"></a>            <span class="st">'avg_time'</span>: <span class="va">self</span>.total_inference_time <span class="op">/</span> <span class="va">self</span>.request_count,</span>
<span id="cb25-129"><a href="#cb25-129" aria-hidden="true" tabindex="-1"></a>            <span class="st">'total_time'</span>: <span class="va">self</span>.total_inference_time,</span>
<span id="cb25-130"><a href="#cb25-130" aria-hidden="true" tabindex="-1"></a>            <span class="st">'requests_per_second'</span>: <span class="va">self</span>.request_count <span class="op">/</span> <span class="va">self</span>.total_inference_time <span class="cf">if</span> <span class="va">self</span>.total_inference_time <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span>,</span>
<span id="cb25-131"><a href="#cb25-131" aria-hidden="true" tabindex="-1"></a>            <span class="st">'error_rate'</span>: <span class="va">self</span>.error_count <span class="op">/</span> <span class="va">self</span>.request_count,</span>
<span id="cb25-132"><a href="#cb25-132" aria-hidden="true" tabindex="-1"></a>            <span class="st">'error_count'</span>: <span class="va">self</span>.error_count</span>
<span id="cb25-133"><a href="#cb25-133" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb25-134"><a href="#cb25-134" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-135"><a href="#cb25-135" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> health_check(<span class="va">self</span>) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, Any]:</span>
<span id="cb25-136"><a href="#cb25-136" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Perform system health check"""</span></span>
<span id="cb25-137"><a href="#cb25-137" aria-hidden="true" tabindex="-1"></a>        health_status <span class="op">=</span> {</span>
<span id="cb25-138"><a href="#cb25-138" aria-hidden="true" tabindex="-1"></a>            <span class="st">'status'</span>: <span class="st">'healthy'</span>,</span>
<span id="cb25-139"><a href="#cb25-139" aria-hidden="true" tabindex="-1"></a>            <span class="st">'active_adapters'</span>: <span class="bu">list</span>(<span class="va">self</span>.active_adapters.keys()),</span>
<span id="cb25-140"><a href="#cb25-140" aria-hidden="true" tabindex="-1"></a>            <span class="st">'device'</span>: <span class="bu">str</span>(<span class="va">self</span>.device),</span>
<span id="cb25-141"><a href="#cb25-141" aria-hidden="true" tabindex="-1"></a>            <span class="st">'performance'</span>: <span class="va">self</span>.get_performance_stats(),</span>
<span id="cb25-142"><a href="#cb25-142" aria-hidden="true" tabindex="-1"></a>            <span class="st">'memory_usage'</span>: <span class="va">self</span>._get_memory_usage()</span>
<span id="cb25-143"><a href="#cb25-143" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb25-144"><a href="#cb25-144" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-145"><a href="#cb25-145" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check for issues</span></span>
<span id="cb25-146"><a href="#cb25-146" aria-hidden="true" tabindex="-1"></a>        perf_stats <span class="op">=</span> health_status[<span class="st">'performance'</span>]</span>
<span id="cb25-147"><a href="#cb25-147" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> perf_stats[<span class="st">'error_rate'</span>] <span class="op">&gt;</span> <span class="fl">0.05</span>:  <span class="co"># 5% error threshold</span></span>
<span id="cb25-148"><a href="#cb25-148" aria-hidden="true" tabindex="-1"></a>            health_status[<span class="st">'status'</span>] <span class="op">=</span> <span class="st">'degraded'</span></span>
<span id="cb25-149"><a href="#cb25-149" aria-hidden="true" tabindex="-1"></a>            health_status[<span class="st">'issues'</span>] <span class="op">=</span> [<span class="st">'High error rate detected'</span>]</span>
<span id="cb25-150"><a href="#cb25-150" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-151"><a href="#cb25-151" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> perf_stats[<span class="st">'avg_time'</span>] <span class="op">&gt;</span> <span class="fl">1.0</span>:  <span class="co"># 1 second threshold</span></span>
<span id="cb25-152"><a href="#cb25-152" aria-hidden="true" tabindex="-1"></a>            health_status[<span class="st">'status'</span>] <span class="op">=</span> <span class="st">'degraded'</span></span>
<span id="cb25-153"><a href="#cb25-153" aria-hidden="true" tabindex="-1"></a>            health_status.setdefault(<span class="st">'issues'</span>, []).append(<span class="st">'High latency detected'</span>)</span>
<span id="cb25-154"><a href="#cb25-154" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-155"><a href="#cb25-155" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> health_status</span>
<span id="cb25-156"><a href="#cb25-156" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-157"><a href="#cb25-157" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _get_memory_usage(<span class="va">self</span>):</span>
<span id="cb25-158"><a href="#cb25-158" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Get memory usage statistics"""</span></span>
<span id="cb25-159"><a href="#cb25-159" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Simulate memory usage</span></span>
<span id="cb25-160"><a href="#cb25-160" aria-hidden="true" tabindex="-1"></a>        total_adapters <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.active_adapters)</span>
<span id="cb25-161"><a href="#cb25-161" aria-hidden="true" tabindex="-1"></a>        estimated_memory <span class="op">=</span> total_adapters <span class="op">*</span> <span class="fl">0.1</span>  <span class="co"># GB per adapter</span></span>
<span id="cb25-162"><a href="#cb25-162" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-163"><a href="#cb25-163" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb25-164"><a href="#cb25-164" aria-hidden="true" tabindex="-1"></a>            <span class="st">'estimated_adapter_memory_gb'</span>: estimated_memory,</span>
<span id="cb25-165"><a href="#cb25-165" aria-hidden="true" tabindex="-1"></a>            <span class="st">'active_adapters'</span>: total_adapters</span>
<span id="cb25-166"><a href="#cb25-166" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb25-167"><a href="#cb25-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-168"><a href="#cb25-168" aria-hidden="true" tabindex="-1"></a><span class="co"># Production deployment demonstration</span></span>
<span id="cb25-169"><a href="#cb25-169" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Production LoRA Deployment Demo:"</span>)</span>
<span id="cb25-170"><a href="#cb25-170" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">35</span>)</span>
<span id="cb25-171"><a href="#cb25-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-172"><a href="#cb25-172" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize model manager</span></span>
<span id="cb25-173"><a href="#cb25-173" aria-hidden="true" tabindex="-1"></a>manager <span class="op">=</span> LoRAModelManager(<span class="st">"path/to/base/model"</span>, device<span class="op">=</span><span class="st">"cuda"</span>)</span>
<span id="cb25-174"><a href="#cb25-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-175"><a href="#cb25-175" aria-hidden="true" tabindex="-1"></a><span class="co"># Load multiple adapters</span></span>
<span id="cb25-176"><a href="#cb25-176" aria-hidden="true" tabindex="-1"></a>adapters_to_load <span class="op">=</span> [</span>
<span id="cb25-177"><a href="#cb25-177" aria-hidden="true" tabindex="-1"></a>    {<span class="st">"name"</span>: <span class="st">"medical_adapter"</span>, <span class="st">"path"</span>: <span class="st">"adapters/medical"</span>, <span class="st">"config"</span>: {<span class="st">"rank"</span>: <span class="dv">32</span>, <span class="st">"task"</span>: <span class="st">"medical_vqa"</span>}},</span>
<span id="cb25-178"><a href="#cb25-178" aria-hidden="true" tabindex="-1"></a>    {<span class="st">"name"</span>: <span class="st">"general_adapter"</span>, <span class="st">"path"</span>: <span class="st">"adapters/general"</span>, <span class="st">"config"</span>: {<span class="st">"rank"</span>: <span class="dv">16</span>, <span class="st">"task"</span>: <span class="st">"general_vqa"</span>}},</span>
<span id="cb25-179"><a href="#cb25-179" aria-hidden="true" tabindex="-1"></a>    {<span class="st">"name"</span>: <span class="st">"multilingual_adapter"</span>, <span class="st">"path"</span>: <span class="st">"adapters/multilingual"</span>, <span class="st">"config"</span>: {<span class="st">"rank"</span>: <span class="dv">24</span>, <span class="st">"task"</span>: <span class="st">"multilingual"</span>}}</span>
<span id="cb25-180"><a href="#cb25-180" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb25-181"><a href="#cb25-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-182"><a href="#cb25-182" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> adapter <span class="kw">in</span> adapters_to_load:</span>
<span id="cb25-183"><a href="#cb25-183" aria-hidden="true" tabindex="-1"></a>    manager.load_adapter(adapter[<span class="st">"name"</span>], adapter[<span class="st">"path"</span>], adapter[<span class="st">"config"</span>])</span>
<span id="cb25-184"><a href="#cb25-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-185"><a href="#cb25-185" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Loaded </span><span class="sc">{</span><span class="bu">len</span>(manager.active_adapters)<span class="sc">}</span><span class="ss"> adapters"</span>)</span>
<span id="cb25-186"><a href="#cb25-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-187"><a href="#cb25-187" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate inference requests</span></span>
<span id="cb25-188"><a href="#cb25-188" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Simulating inference requests..."</span>)</span>
<span id="cb25-189"><a href="#cb25-189" aria-hidden="true" tabindex="-1"></a>test_inputs <span class="op">=</span> {<span class="st">"image"</span>: <span class="st">"test_image.jpg"</span>, <span class="st">"text"</span>: <span class="st">"What is in this image?"</span>}</span>
<span id="cb25-190"><a href="#cb25-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-191"><a href="#cb25-191" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb25-192"><a href="#cb25-192" aria-hidden="true" tabindex="-1"></a>    adapter <span class="op">=</span> [<span class="st">"medical_adapter"</span>, <span class="st">"general_adapter"</span>, <span class="va">None</span>][i <span class="op">%</span> <span class="dv">3</span>]</span>
<span id="cb25-193"><a href="#cb25-193" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> manager.inference(test_inputs, adapter)</span>
<span id="cb25-194"><a href="#cb25-194" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Request </span><span class="sc">{</span>result[<span class="st">'request_id'</span>]<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>result[<span class="st">'inference_time'</span>]<span class="sc">:.3f}</span><span class="ss">s (</span><span class="sc">{</span><span class="st">'with '</span> <span class="op">+</span> result[<span class="st">'adapter_used'</span>] <span class="cf">if</span> result[<span class="st">'adapter_used'</span>] <span class="cf">else</span> <span class="st">'base model'</span><span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb25-195"><a href="#cb25-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-196"><a href="#cb25-196" aria-hidden="true" tabindex="-1"></a><span class="co"># Check system health</span></span>
<span id="cb25-197"><a href="#cb25-197" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">System Health Check:"</span>)</span>
<span id="cb25-198"><a href="#cb25-198" aria-hidden="true" tabindex="-1"></a>health <span class="op">=</span> manager.health_check()</span>
<span id="cb25-199"><a href="#cb25-199" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Status: </span><span class="sc">{</span>health[<span class="st">'status'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-200"><a href="#cb25-200" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Active adapters: </span><span class="sc">{</span><span class="bu">len</span>(health[<span class="st">'active_adapters'</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-201"><a href="#cb25-201" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Average latency: </span><span class="sc">{</span>health[<span class="st">'performance'</span>][<span class="st">'avg_time'</span>]<span class="sc">:.3f}</span><span class="ss">s"</span>)</span>
<span id="cb25-202"><a href="#cb25-202" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Error rate: </span><span class="sc">{</span>health[<span class="st">'performance'</span>][<span class="st">'error_rate'</span>]<span class="sc">:.1%}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>INFO:__main__:Loading adapter 'medical_adapter' from adapters/medical
INFO:__main__:Adapter 'medical_adapter' loaded successfully
INFO:__main__:Loading adapter 'general_adapter' from adapters/general
INFO:__main__:Adapter 'general_adapter' loaded successfully
INFO:__main__:Loading adapter 'multilingual_adapter' from adapters/multilingual
INFO:__main__:Adapter 'multilingual_adapter' loaded successfully</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Production LoRA Deployment Demo:
===================================
LoRA Model Manager initialized
Device: cuda

Loaded 3 adapters

Simulating inference requests...
Request 1: 0.013s (with medical_adapter)
Request 2: 0.013s (with general_adapter)
Request 3: 0.010s (base model)
Request 4: 0.013s (with medical_adapter)
Request 5: 0.010s (with general_adapter)

System Health Check:
Status: healthy
Active adapters: 3
Average latency: 0.012s
Error rate: 0.0%</code></pre>
</div>
</div>
</section>
<section id="api-server-implementation" class="level3">
<h3 class="anchored" data-anchor-id="api-server-implementation">API Server Implementation</h3>
<div id="api-server" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LoRAAPIServer:</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""FastAPI-style server for LoRA model serving"""</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model_manager: LoRAModelManager):</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model_manager <span class="op">=</span> model_manager</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.request_history <span class="op">=</span> []</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"LoRA API Server initialized"</span>)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Available endpoints:"</span>)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  POST /inference - Perform inference"</span>)</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  POST /load_adapter - Load new adapter"</span>)</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  DELETE /adapter/</span><span class="sc">{name}</span><span class="st"> - Unload adapter"</span>)</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  GET /health - Health check"</span>)</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  GET /adapters - List adapters"</span>)</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> inference_endpoint(<span class="va">self</span>, request_data: Dict[<span class="bu">str</span>, Any]) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, Any]:</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Handle inference requests"""</span></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>            inputs <span class="op">=</span> request_data.get(<span class="st">"inputs"</span>, {})</span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>            adapter_name <span class="op">=</span> request_data.get(<span class="st">"adapter_name"</span>)</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>            parameters <span class="op">=</span> request_data.get(<span class="st">"parameters"</span>, {})</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Perform inference</span></span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>            result <span class="op">=</span> <span class="va">self</span>.model_manager.inference(inputs, adapter_name)</span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Log request</span></span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.request_history.append({</span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>                <span class="st">"timestamp"</span>: time.time(),</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>                <span class="st">"adapter"</span>: adapter_name,</span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>                <span class="st">"latency"</span>: result[<span class="st">"inference_time"</span>],</span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a>                <span class="st">"status"</span>: <span class="st">"success"</span></span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a>            })</span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> {</span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a>                <span class="st">"status"</span>: <span class="st">"success"</span>,</span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a>                <span class="st">"outputs"</span>: result[<span class="st">"outputs"</span>],</span>
<span id="cb28-37"><a href="#cb28-37" aria-hidden="true" tabindex="-1"></a>                <span class="st">"inference_time"</span>: result[<span class="st">"inference_time"</span>],</span>
<span id="cb28-38"><a href="#cb28-38" aria-hidden="true" tabindex="-1"></a>                <span class="st">"adapter_used"</span>: result[<span class="st">"adapter_used"</span>],</span>
<span id="cb28-39"><a href="#cb28-39" aria-hidden="true" tabindex="-1"></a>                <span class="st">"request_id"</span>: result[<span class="st">"request_id"</span>]</span>
<span id="cb28-40"><a href="#cb28-40" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb28-41"><a href="#cb28-41" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb28-42"><a href="#cb28-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb28-43"><a href="#cb28-43" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Log error</span></span>
<span id="cb28-44"><a href="#cb28-44" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.request_history.append({</span>
<span id="cb28-45"><a href="#cb28-45" aria-hidden="true" tabindex="-1"></a>                <span class="st">"timestamp"</span>: time.time(),</span>
<span id="cb28-46"><a href="#cb28-46" aria-hidden="true" tabindex="-1"></a>                <span class="st">"adapter"</span>: request_data.get(<span class="st">"adapter_name"</span>),</span>
<span id="cb28-47"><a href="#cb28-47" aria-hidden="true" tabindex="-1"></a>                <span class="st">"status"</span>: <span class="st">"error"</span>,</span>
<span id="cb28-48"><a href="#cb28-48" aria-hidden="true" tabindex="-1"></a>                <span class="st">"error"</span>: <span class="bu">str</span>(e)</span>
<span id="cb28-49"><a href="#cb28-49" aria-hidden="true" tabindex="-1"></a>            })</span>
<span id="cb28-50"><a href="#cb28-50" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb28-51"><a href="#cb28-51" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> {</span>
<span id="cb28-52"><a href="#cb28-52" aria-hidden="true" tabindex="-1"></a>                <span class="st">"status"</span>: <span class="st">"error"</span>,</span>
<span id="cb28-53"><a href="#cb28-53" aria-hidden="true" tabindex="-1"></a>                <span class="st">"error"</span>: <span class="bu">str</span>(e),</span>
<span id="cb28-54"><a href="#cb28-54" aria-hidden="true" tabindex="-1"></a>                <span class="st">"request_id"</span>: <span class="va">None</span></span>
<span id="cb28-55"><a href="#cb28-55" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb28-56"><a href="#cb28-56" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-57"><a href="#cb28-57" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> load_adapter_endpoint(<span class="va">self</span>, request_data: Dict[<span class="bu">str</span>, Any]) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, Any]:</span>
<span id="cb28-58"><a href="#cb28-58" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Handle adapter loading requests"""</span></span>
<span id="cb28-59"><a href="#cb28-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb28-60"><a href="#cb28-60" aria-hidden="true" tabindex="-1"></a>            adapter_name <span class="op">=</span> request_data[<span class="st">"adapter_name"</span>]</span>
<span id="cb28-61"><a href="#cb28-61" aria-hidden="true" tabindex="-1"></a>            adapter_path <span class="op">=</span> request_data[<span class="st">"adapter_path"</span>]</span>
<span id="cb28-62"><a href="#cb28-62" aria-hidden="true" tabindex="-1"></a>            config <span class="op">=</span> request_data.get(<span class="st">"config"</span>)</span>
<span id="cb28-63"><a href="#cb28-63" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb28-64"><a href="#cb28-64" aria-hidden="true" tabindex="-1"></a>            success <span class="op">=</span> <span class="va">self</span>.model_manager.load_adapter(adapter_name, adapter_path, config)</span>
<span id="cb28-65"><a href="#cb28-65" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb28-66"><a href="#cb28-66" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> success:</span>
<span id="cb28-67"><a href="#cb28-67" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> {</span>
<span id="cb28-68"><a href="#cb28-68" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"status"</span>: <span class="st">"success"</span>,</span>
<span id="cb28-69"><a href="#cb28-69" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"message"</span>: <span class="ss">f"Adapter '</span><span class="sc">{</span>adapter_name<span class="sc">}</span><span class="ss">' loaded successfully"</span></span>
<span id="cb28-70"><a href="#cb28-70" aria-hidden="true" tabindex="-1"></a>                }</span>
<span id="cb28-71"><a href="#cb28-71" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb28-72"><a href="#cb28-72" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> {</span>
<span id="cb28-73"><a href="#cb28-73" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"status"</span>: <span class="st">"error"</span>,</span>
<span id="cb28-74"><a href="#cb28-74" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"message"</span>: <span class="ss">f"Failed to load adapter '</span><span class="sc">{</span>adapter_name<span class="sc">}</span><span class="ss">'"</span></span>
<span id="cb28-75"><a href="#cb28-75" aria-hidden="true" tabindex="-1"></a>                }</span>
<span id="cb28-76"><a href="#cb28-76" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb28-77"><a href="#cb28-77" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb28-78"><a href="#cb28-78" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> {</span>
<span id="cb28-79"><a href="#cb28-79" aria-hidden="true" tabindex="-1"></a>                <span class="st">"status"</span>: <span class="st">"error"</span>,</span>
<span id="cb28-80"><a href="#cb28-80" aria-hidden="true" tabindex="-1"></a>                <span class="st">"message"</span>: <span class="bu">str</span>(e)</span>
<span id="cb28-81"><a href="#cb28-81" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb28-82"><a href="#cb28-82" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-83"><a href="#cb28-83" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> unload_adapter_endpoint(<span class="va">self</span>, adapter_name: <span class="bu">str</span>) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, Any]:</span>
<span id="cb28-84"><a href="#cb28-84" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Handle adapter unloading requests"""</span></span>
<span id="cb28-85"><a href="#cb28-85" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb28-86"><a href="#cb28-86" aria-hidden="true" tabindex="-1"></a>            success <span class="op">=</span> <span class="va">self</span>.model_manager.unload_adapter(adapter_name)</span>
<span id="cb28-87"><a href="#cb28-87" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb28-88"><a href="#cb28-88" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> success:</span>
<span id="cb28-89"><a href="#cb28-89" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> {</span>
<span id="cb28-90"><a href="#cb28-90" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"status"</span>: <span class="st">"success"</span>, </span>
<span id="cb28-91"><a href="#cb28-91" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"message"</span>: <span class="ss">f"Adapter '</span><span class="sc">{</span>adapter_name<span class="sc">}</span><span class="ss">' unloaded successfully"</span></span>
<span id="cb28-92"><a href="#cb28-92" aria-hidden="true" tabindex="-1"></a>                }</span>
<span id="cb28-93"><a href="#cb28-93" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb28-94"><a href="#cb28-94" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> {</span>
<span id="cb28-95"><a href="#cb28-95" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"status"</span>: <span class="st">"error"</span>,</span>
<span id="cb28-96"><a href="#cb28-96" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"message"</span>: <span class="ss">f"Adapter '</span><span class="sc">{</span>adapter_name<span class="sc">}</span><span class="ss">' not found"</span></span>
<span id="cb28-97"><a href="#cb28-97" aria-hidden="true" tabindex="-1"></a>                }</span>
<span id="cb28-98"><a href="#cb28-98" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb28-99"><a href="#cb28-99" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb28-100"><a href="#cb28-100" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> {</span>
<span id="cb28-101"><a href="#cb28-101" aria-hidden="true" tabindex="-1"></a>                <span class="st">"status"</span>: <span class="st">"error"</span>,</span>
<span id="cb28-102"><a href="#cb28-102" aria-hidden="true" tabindex="-1"></a>                <span class="st">"message"</span>: <span class="bu">str</span>(e)</span>
<span id="cb28-103"><a href="#cb28-103" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb28-104"><a href="#cb28-104" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-105"><a href="#cb28-105" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> health_endpoint(<span class="va">self</span>) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, Any]:</span>
<span id="cb28-106"><a href="#cb28-106" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Handle health check requests"""</span></span>
<span id="cb28-107"><a href="#cb28-107" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.model_manager.health_check()</span>
<span id="cb28-108"><a href="#cb28-108" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-109"><a href="#cb28-109" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> list_adapters_endpoint(<span class="va">self</span>) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, Any]:</span>
<span id="cb28-110"><a href="#cb28-110" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Handle adapter listing requests"""</span></span>
<span id="cb28-111"><a href="#cb28-111" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb28-112"><a href="#cb28-112" aria-hidden="true" tabindex="-1"></a>            <span class="st">"active_adapters"</span>: <span class="bu">list</span>(<span class="va">self</span>.model_manager.active_adapters.keys()),</span>
<span id="cb28-113"><a href="#cb28-113" aria-hidden="true" tabindex="-1"></a>            <span class="st">"adapter_configs"</span>: <span class="va">self</span>.model_manager.adapter_configs,</span>
<span id="cb28-114"><a href="#cb28-114" aria-hidden="true" tabindex="-1"></a>            <span class="st">"total_adapters"</span>: <span class="bu">len</span>(<span class="va">self</span>.model_manager.active_adapters)</span>
<span id="cb28-115"><a href="#cb28-115" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb28-116"><a href="#cb28-116" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-117"><a href="#cb28-117" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_metrics_endpoint(<span class="va">self</span>) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, Any]:</span>
<span id="cb28-118"><a href="#cb28-118" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Get detailed metrics"""</span></span>
<span id="cb28-119"><a href="#cb28-119" aria-hidden="true" tabindex="-1"></a>        recent_requests <span class="op">=</span> [req <span class="cf">for</span> req <span class="kw">in</span> <span class="va">self</span>.request_history </span>
<span id="cb28-120"><a href="#cb28-120" aria-hidden="true" tabindex="-1"></a>                          <span class="cf">if</span> time.time() <span class="op">-</span> req[<span class="st">"timestamp"</span>] <span class="op">&lt;</span> <span class="dv">3600</span>]  <span class="co"># Last hour</span></span>
<span id="cb28-121"><a href="#cb28-121" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-122"><a href="#cb28-122" aria-hidden="true" tabindex="-1"></a>        success_requests <span class="op">=</span> [req <span class="cf">for</span> req <span class="kw">in</span> recent_requests <span class="cf">if</span> req[<span class="st">"status"</span>] <span class="op">==</span> <span class="st">"success"</span>]</span>
<span id="cb28-123"><a href="#cb28-123" aria-hidden="true" tabindex="-1"></a>        error_requests <span class="op">=</span> [req <span class="cf">for</span> req <span class="kw">in</span> recent_requests <span class="cf">if</span> req[<span class="st">"status"</span>] <span class="op">==</span> <span class="st">"error"</span>]</span>
<span id="cb28-124"><a href="#cb28-124" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-125"><a href="#cb28-125" aria-hidden="true" tabindex="-1"></a>        metrics <span class="op">=</span> {</span>
<span id="cb28-126"><a href="#cb28-126" aria-hidden="true" tabindex="-1"></a>            <span class="st">"total_requests_last_hour"</span>: <span class="bu">len</span>(recent_requests),</span>
<span id="cb28-127"><a href="#cb28-127" aria-hidden="true" tabindex="-1"></a>            <span class="st">"successful_requests"</span>: <span class="bu">len</span>(success_requests),</span>
<span id="cb28-128"><a href="#cb28-128" aria-hidden="true" tabindex="-1"></a>            <span class="st">"failed_requests"</span>: <span class="bu">len</span>(error_requests),</span>
<span id="cb28-129"><a href="#cb28-129" aria-hidden="true" tabindex="-1"></a>            <span class="st">"success_rate"</span>: <span class="bu">len</span>(success_requests) <span class="op">/</span> <span class="bu">len</span>(recent_requests) <span class="cf">if</span> recent_requests <span class="cf">else</span> <span class="dv">0</span>,</span>
<span id="cb28-130"><a href="#cb28-130" aria-hidden="true" tabindex="-1"></a>            <span class="st">"average_latency"</span>: np.mean([req[<span class="st">"latency"</span>] <span class="cf">for</span> req <span class="kw">in</span> success_requests]) <span class="cf">if</span> success_requests <span class="cf">else</span> <span class="dv">0</span>,</span>
<span id="cb28-131"><a href="#cb28-131" aria-hidden="true" tabindex="-1"></a>            <span class="st">"adapter_usage"</span>: {}</span>
<span id="cb28-132"><a href="#cb28-132" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb28-133"><a href="#cb28-133" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-134"><a href="#cb28-134" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Adapter usage statistics</span></span>
<span id="cb28-135"><a href="#cb28-135" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> req <span class="kw">in</span> success_requests:</span>
<span id="cb28-136"><a href="#cb28-136" aria-hidden="true" tabindex="-1"></a>            adapter <span class="op">=</span> req.get(<span class="st">"adapter"</span>, <span class="st">"base_model"</span>)</span>
<span id="cb28-137"><a href="#cb28-137" aria-hidden="true" tabindex="-1"></a>            metrics[<span class="st">"adapter_usage"</span>][adapter] <span class="op">=</span> metrics[<span class="st">"adapter_usage"</span>].get(adapter, <span class="dv">0</span>) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb28-138"><a href="#cb28-138" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-139"><a href="#cb28-139" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> metrics</span>
<span id="cb28-140"><a href="#cb28-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-141"><a href="#cb28-141" aria-hidden="true" tabindex="-1"></a><span class="co"># API server demonstration</span></span>
<span id="cb28-142"><a href="#cb28-142" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">API Server Demo:"</span>)</span>
<span id="cb28-143"><a href="#cb28-143" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">20</span>)</span>
<span id="cb28-144"><a href="#cb28-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-145"><a href="#cb28-145" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize API server</span></span>
<span id="cb28-146"><a href="#cb28-146" aria-hidden="true" tabindex="-1"></a>api_server <span class="op">=</span> LoRAAPIServer(manager)</span>
<span id="cb28-147"><a href="#cb28-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-148"><a href="#cb28-148" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate API requests</span></span>
<span id="cb28-149"><a href="#cb28-149" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Simulating API requests..."</span>)</span>
<span id="cb28-150"><a href="#cb28-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-151"><a href="#cb28-151" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Inference request</span></span>
<span id="cb28-152"><a href="#cb28-152" aria-hidden="true" tabindex="-1"></a>inference_request <span class="op">=</span> {</span>
<span id="cb28-153"><a href="#cb28-153" aria-hidden="true" tabindex="-1"></a>    <span class="st">"inputs"</span>: {<span class="st">"image"</span>: <span class="st">"test.jpg"</span>, <span class="st">"text"</span>: <span class="st">"Describe this image"</span>},</span>
<span id="cb28-154"><a href="#cb28-154" aria-hidden="true" tabindex="-1"></a>    <span class="st">"adapter_name"</span>: <span class="st">"medical_adapter"</span></span>
<span id="cb28-155"><a href="#cb28-155" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb28-156"><a href="#cb28-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-157"><a href="#cb28-157" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> api_server.inference_endpoint(inference_request)</span>
<span id="cb28-158"><a href="#cb28-158" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Inference response: </span><span class="sc">{</span>response[<span class="st">'status'</span>]<span class="sc">}</span><span class="ss"> (took </span><span class="sc">{</span>response<span class="sc">.</span>get(<span class="st">'inference_time'</span>, <span class="dv">0</span>)<span class="sc">:.3f}</span><span class="ss">s)"</span>)</span>
<span id="cb28-159"><a href="#cb28-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-160"><a href="#cb28-160" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Load new adapter</span></span>
<span id="cb28-161"><a href="#cb28-161" aria-hidden="true" tabindex="-1"></a>load_request <span class="op">=</span> {</span>
<span id="cb28-162"><a href="#cb28-162" aria-hidden="true" tabindex="-1"></a>    <span class="st">"adapter_name"</span>: <span class="st">"custom_adapter"</span>,</span>
<span id="cb28-163"><a href="#cb28-163" aria-hidden="true" tabindex="-1"></a>    <span class="st">"adapter_path"</span>: <span class="st">"adapters/custom"</span>,</span>
<span id="cb28-164"><a href="#cb28-164" aria-hidden="true" tabindex="-1"></a>    <span class="st">"config"</span>: {<span class="st">"rank"</span>: <span class="dv">20</span>, <span class="st">"alpha"</span>: <span class="dv">20</span>}</span>
<span id="cb28-165"><a href="#cb28-165" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb28-166"><a href="#cb28-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-167"><a href="#cb28-167" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> api_server.load_adapter_endpoint(load_request)</span>
<span id="cb28-168"><a href="#cb28-168" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Load adapter response: </span><span class="sc">{</span>response[<span class="st">'status'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb28-169"><a href="#cb28-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-170"><a href="#cb28-170" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Health check</span></span>
<span id="cb28-171"><a href="#cb28-171" aria-hidden="true" tabindex="-1"></a>health_response <span class="op">=</span> api_server.health_endpoint()</span>
<span id="cb28-172"><a href="#cb28-172" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Health status: </span><span class="sc">{</span>health_response[<span class="st">'status'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb28-173"><a href="#cb28-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-174"><a href="#cb28-174" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. List adapters</span></span>
<span id="cb28-175"><a href="#cb28-175" aria-hidden="true" tabindex="-1"></a>adapters_response <span class="op">=</span> api_server.list_adapters_endpoint()</span>
<span id="cb28-176"><a href="#cb28-176" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Active adapters: </span><span class="sc">{</span>adapters_response[<span class="st">'total_adapters'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb28-177"><a href="#cb28-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-178"><a href="#cb28-178" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Get metrics</span></span>
<span id="cb28-179"><a href="#cb28-179" aria-hidden="true" tabindex="-1"></a>metrics_response <span class="op">=</span> api_server.get_metrics_endpoint()</span>
<span id="cb28-180"><a href="#cb28-180" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Success rate: </span><span class="sc">{</span>metrics_response[<span class="st">'success_rate'</span>]<span class="sc">:.1%}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
API Server Demo:
====================
LoRA API Server initialized
Available endpoints:
  POST /inference - Perform inference
  POST /load_adapter - Load new adapter
  DELETE /adapter/{name} - Unload adapter
  GET /health - Health check
  GET /adapters - List adapters

Simulating API requests...</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>INFO:__main__:Loading adapter 'custom_adapter' from adapters/custom
INFO:__main__:Adapter 'custom_adapter' loaded successfully</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Inference response: success (took 0.013s)
Load adapter response: success
Health status: healthy
Active adapters: 4
Success rate: 100.0%</code></pre>
</div>
</div>
</section>
</section>
<section id="monitoring-and-observability" class="level2">
<h2 class="anchored" data-anchor-id="monitoring-and-observability">Monitoring and Observability</h2>
<section id="performance-monitoring" class="level3">
<h3 class="anchored" data-anchor-id="performance-monitoring">Performance Monitoring</h3>
<div id="monitoring-system" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict, deque</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LoRAMonitor:</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Comprehensive monitoring for LoRA-adapted VLMs"""</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, adapter_name: <span class="bu">str</span> <span class="op">=</span> <span class="st">"default"</span>, window_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1000</span>):</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.adapter_name <span class="op">=</span> adapter_name</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.window_size <span class="op">=</span> window_size</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Metrics storage</span></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.metrics <span class="op">=</span> {</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>            <span class="st">'inference_times'</span>: deque(maxlen<span class="op">=</span>window_size),</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>            <span class="st">'memory_usage'</span>: deque(maxlen<span class="op">=</span>window_size),</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>            <span class="st">'accuracy_scores'</span>: deque(maxlen<span class="op">=</span>window_size),</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>            <span class="st">'request_counts'</span>: defaultdict(<span class="bu">int</span>),</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>            <span class="st">'error_counts'</span>: defaultdict(<span class="bu">int</span>),</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>            <span class="st">'timestamps'</span>: deque(maxlen<span class="op">=</span>window_size)</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># LoRA-specific metrics</span></span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lora_metrics <span class="op">=</span> {</span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>            <span class="st">'weight_norms'</span>: {},</span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>            <span class="st">'rank_utilization'</span>: {},</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>            <span class="st">'adaptation_strength'</span>: {}</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Performance thresholds</span></span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.thresholds <span class="op">=</span> {</span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a>            <span class="st">'max_inference_time'</span>: <span class="fl">2.0</span>,  <span class="co"># seconds</span></span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a>            <span class="st">'max_memory_usage'</span>: <span class="fl">4.0</span>,    <span class="co"># GB</span></span>
<span id="cb32-34"><a href="#cb32-34" aria-hidden="true" tabindex="-1"></a>            <span class="st">'min_accuracy'</span>: <span class="fl">0.8</span>,        <span class="co"># minimum acceptable accuracy</span></span>
<span id="cb32-35"><a href="#cb32-35" aria-hidden="true" tabindex="-1"></a>            <span class="st">'max_error_rate'</span>: <span class="fl">0.02</span>      <span class="co"># maximum error rate</span></span>
<span id="cb32-36"><a href="#cb32-36" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb32-37"><a href="#cb32-37" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-38"><a href="#cb32-38" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"LoRA Monitor initialized for adapter: </span><span class="sc">{</span>adapter_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-39"><a href="#cb32-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-40"><a href="#cb32-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> log_inference(<span class="va">self</span>, inference_time: <span class="bu">float</span>, memory_usage: <span class="bu">float</span>, </span>
<span id="cb32-41"><a href="#cb32-41" aria-hidden="true" tabindex="-1"></a>                     accuracy: Optional[<span class="bu">float</span>] <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb32-42"><a href="#cb32-42" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Log inference metrics"""</span></span>
<span id="cb32-43"><a href="#cb32-43" aria-hidden="true" tabindex="-1"></a>        current_time <span class="op">=</span> time.time()</span>
<span id="cb32-44"><a href="#cb32-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-45"><a href="#cb32-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.metrics[<span class="st">'inference_times'</span>].append(inference_time)</span>
<span id="cb32-46"><a href="#cb32-46" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.metrics[<span class="st">'memory_usage'</span>].append(memory_usage)</span>
<span id="cb32-47"><a href="#cb32-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.metrics[<span class="st">'timestamps'</span>].append(current_time)</span>
<span id="cb32-48"><a href="#cb32-48" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-49"><a href="#cb32-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> accuracy <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb32-50"><a href="#cb32-50" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.metrics[<span class="st">'accuracy_scores'</span>].append(accuracy)</span>
<span id="cb32-51"><a href="#cb32-51" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-52"><a href="#cb32-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check thresholds and alert if necessary</span></span>
<span id="cb32-53"><a href="#cb32-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.check_thresholds(inference_time, memory_usage, accuracy)</span>
<span id="cb32-54"><a href="#cb32-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-55"><a href="#cb32-55" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> check_thresholds(<span class="va">self</span>, inference_time: <span class="bu">float</span>, memory_usage: <span class="bu">float</span>, </span>
<span id="cb32-56"><a href="#cb32-56" aria-hidden="true" tabindex="-1"></a>                        accuracy: Optional[<span class="bu">float</span>] <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb32-57"><a href="#cb32-57" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Check if metrics exceed defined thresholds"""</span></span>
<span id="cb32-58"><a href="#cb32-58" aria-hidden="true" tabindex="-1"></a>        alerts <span class="op">=</span> []</span>
<span id="cb32-59"><a href="#cb32-59" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-60"><a href="#cb32-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> inference_time <span class="op">&gt;</span> <span class="va">self</span>.thresholds[<span class="st">'max_inference_time'</span>]:</span>
<span id="cb32-61"><a href="#cb32-61" aria-hidden="true" tabindex="-1"></a>            alerts.append(<span class="ss">f"HIGH_LATENCY: </span><span class="sc">{</span>inference_time<span class="sc">:.3f}</span><span class="ss">s &gt; </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>thresholds[<span class="st">'max_inference_time'</span>]<span class="sc">}</span><span class="ss">s"</span>)</span>
<span id="cb32-62"><a href="#cb32-62" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-63"><a href="#cb32-63" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> memory_usage <span class="op">&gt;</span> <span class="va">self</span>.thresholds[<span class="st">'max_memory_usage'</span>]:</span>
<span id="cb32-64"><a href="#cb32-64" aria-hidden="true" tabindex="-1"></a>            alerts.append(<span class="ss">f"HIGH_MEMORY: </span><span class="sc">{</span>memory_usage<span class="sc">:.2f}</span><span class="ss">GB &gt; </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>thresholds[<span class="st">'max_memory_usage'</span>]<span class="sc">}</span><span class="ss">GB"</span>)</span>
<span id="cb32-65"><a href="#cb32-65" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-66"><a href="#cb32-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> accuracy <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> accuracy <span class="op">&lt;</span> <span class="va">self</span>.thresholds[<span class="st">'min_accuracy'</span>]:</span>
<span id="cb32-67"><a href="#cb32-67" aria-hidden="true" tabindex="-1"></a>            alerts.append(<span class="ss">f"LOW_ACCURACY: </span><span class="sc">{</span>accuracy<span class="sc">:.3f}</span><span class="ss"> &lt; </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>thresholds[<span class="st">'min_accuracy'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-68"><a href="#cb32-68" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-69"><a href="#cb32-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> alert <span class="kw">in</span> alerts:</span>
<span id="cb32-70"><a href="#cb32-70" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"🚨 ALERT [</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>adapter_name<span class="sc">}</span><span class="ss">]: </span><span class="sc">{</span>alert<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-71"><a href="#cb32-71" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-72"><a href="#cb32-72" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute_performance_stats(<span class="va">self</span>) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, Any]:</span>
<span id="cb32-73"><a href="#cb32-73" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Compute performance statistics from collected metrics"""</span></span>
<span id="cb32-74"><a href="#cb32-74" aria-hidden="true" tabindex="-1"></a>        stats <span class="op">=</span> {}</span>
<span id="cb32-75"><a href="#cb32-75" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-76"><a href="#cb32-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Inference time statistics</span></span>
<span id="cb32-77"><a href="#cb32-77" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.metrics[<span class="st">'inference_times'</span>]:</span>
<span id="cb32-78"><a href="#cb32-78" aria-hidden="true" tabindex="-1"></a>            times <span class="op">=</span> <span class="bu">list</span>(<span class="va">self</span>.metrics[<span class="st">'inference_times'</span>])</span>
<span id="cb32-79"><a href="#cb32-79" aria-hidden="true" tabindex="-1"></a>            stats[<span class="st">'inference_time'</span>] <span class="op">=</span> {</span>
<span id="cb32-80"><a href="#cb32-80" aria-hidden="true" tabindex="-1"></a>                <span class="st">'mean'</span>: np.mean(times),</span>
<span id="cb32-81"><a href="#cb32-81" aria-hidden="true" tabindex="-1"></a>                <span class="st">'std'</span>: np.std(times),</span>
<span id="cb32-82"><a href="#cb32-82" aria-hidden="true" tabindex="-1"></a>                <span class="st">'p50'</span>: np.percentile(times, <span class="dv">50</span>),</span>
<span id="cb32-83"><a href="#cb32-83" aria-hidden="true" tabindex="-1"></a>                <span class="st">'p95'</span>: np.percentile(times, <span class="dv">95</span>),</span>
<span id="cb32-84"><a href="#cb32-84" aria-hidden="true" tabindex="-1"></a>                <span class="st">'p99'</span>: np.percentile(times, <span class="dv">99</span>),</span>
<span id="cb32-85"><a href="#cb32-85" aria-hidden="true" tabindex="-1"></a>                <span class="st">'min'</span>: np.<span class="bu">min</span>(times),</span>
<span id="cb32-86"><a href="#cb32-86" aria-hidden="true" tabindex="-1"></a>                <span class="st">'max'</span>: np.<span class="bu">max</span>(times)</span>
<span id="cb32-87"><a href="#cb32-87" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb32-88"><a href="#cb32-88" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-89"><a href="#cb32-89" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Memory usage statistics</span></span>
<span id="cb32-90"><a href="#cb32-90" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.metrics[<span class="st">'memory_usage'</span>]:</span>
<span id="cb32-91"><a href="#cb32-91" aria-hidden="true" tabindex="-1"></a>            memory <span class="op">=</span> <span class="bu">list</span>(<span class="va">self</span>.metrics[<span class="st">'memory_usage'</span>])</span>
<span id="cb32-92"><a href="#cb32-92" aria-hidden="true" tabindex="-1"></a>            stats[<span class="st">'memory_usage'</span>] <span class="op">=</span> {</span>
<span id="cb32-93"><a href="#cb32-93" aria-hidden="true" tabindex="-1"></a>                <span class="st">'mean'</span>: np.mean(memory),</span>
<span id="cb32-94"><a href="#cb32-94" aria-hidden="true" tabindex="-1"></a>                <span class="st">'max'</span>: np.<span class="bu">max</span>(memory),</span>
<span id="cb32-95"><a href="#cb32-95" aria-hidden="true" tabindex="-1"></a>                <span class="st">'min'</span>: np.<span class="bu">min</span>(memory),</span>
<span id="cb32-96"><a href="#cb32-96" aria-hidden="true" tabindex="-1"></a>                <span class="st">'current'</span>: memory[<span class="op">-</span><span class="dv">1</span>] <span class="cf">if</span> memory <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb32-97"><a href="#cb32-97" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb32-98"><a href="#cb32-98" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-99"><a href="#cb32-99" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Accuracy statistics</span></span>
<span id="cb32-100"><a href="#cb32-100" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.metrics[<span class="st">'accuracy_scores'</span>]:</span>
<span id="cb32-101"><a href="#cb32-101" aria-hidden="true" tabindex="-1"></a>            accuracy <span class="op">=</span> <span class="bu">list</span>(<span class="va">self</span>.metrics[<span class="st">'accuracy_scores'</span>])</span>
<span id="cb32-102"><a href="#cb32-102" aria-hidden="true" tabindex="-1"></a>            stats[<span class="st">'accuracy'</span>] <span class="op">=</span> {</span>
<span id="cb32-103"><a href="#cb32-103" aria-hidden="true" tabindex="-1"></a>                <span class="st">'mean'</span>: np.mean(accuracy),</span>
<span id="cb32-104"><a href="#cb32-104" aria-hidden="true" tabindex="-1"></a>                <span class="st">'std'</span>: np.std(accuracy),</span>
<span id="cb32-105"><a href="#cb32-105" aria-hidden="true" tabindex="-1"></a>                <span class="st">'min'</span>: np.<span class="bu">min</span>(accuracy),</span>
<span id="cb32-106"><a href="#cb32-106" aria-hidden="true" tabindex="-1"></a>                <span class="st">'max'</span>: np.<span class="bu">max</span>(accuracy),</span>
<span id="cb32-107"><a href="#cb32-107" aria-hidden="true" tabindex="-1"></a>                <span class="st">'recent'</span>: np.mean(accuracy[<span class="op">-</span><span class="dv">10</span>:]) <span class="cf">if</span> <span class="bu">len</span>(accuracy) <span class="op">&gt;=</span> <span class="dv">10</span> <span class="cf">else</span> np.mean(accuracy)</span>
<span id="cb32-108"><a href="#cb32-108" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb32-109"><a href="#cb32-109" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-110"><a href="#cb32-110" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Throughput calculation</span></span>
<span id="cb32-111"><a href="#cb32-111" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.metrics[<span class="st">'timestamps'</span>]) <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb32-112"><a href="#cb32-112" aria-hidden="true" tabindex="-1"></a>            time_span <span class="op">=</span> <span class="va">self</span>.metrics[<span class="st">'timestamps'</span>][<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> <span class="va">self</span>.metrics[<span class="st">'timestamps'</span>][<span class="dv">0</span>]</span>
<span id="cb32-113"><a href="#cb32-113" aria-hidden="true" tabindex="-1"></a>            stats[<span class="st">'throughput'</span>] <span class="op">=</span> {</span>
<span id="cb32-114"><a href="#cb32-114" aria-hidden="true" tabindex="-1"></a>                <span class="st">'requests_per_second'</span>: <span class="bu">len</span>(<span class="va">self</span>.metrics[<span class="st">'timestamps'</span>]) <span class="op">/</span> time_span <span class="cf">if</span> time_span <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span>,</span>
<span id="cb32-115"><a href="#cb32-115" aria-hidden="true" tabindex="-1"></a>                <span class="st">'time_span_minutes'</span>: time_span <span class="op">/</span> <span class="dv">60</span></span>
<span id="cb32-116"><a href="#cb32-116" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb32-117"><a href="#cb32-117" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-118"><a href="#cb32-118" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> stats</span>
<span id="cb32-119"><a href="#cb32-119" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-120"><a href="#cb32-120" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> analyze_trends(<span class="va">self</span>, window_minutes: <span class="bu">int</span> <span class="op">=</span> <span class="dv">30</span>) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, Any]:</span>
<span id="cb32-121"><a href="#cb32-121" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Analyze performance trends over time"""</span></span>
<span id="cb32-122"><a href="#cb32-122" aria-hidden="true" tabindex="-1"></a>        current_time <span class="op">=</span> time.time()</span>
<span id="cb32-123"><a href="#cb32-123" aria-hidden="true" tabindex="-1"></a>        cutoff_time <span class="op">=</span> current_time <span class="op">-</span> (window_minutes <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb32-124"><a href="#cb32-124" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-125"><a href="#cb32-125" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Filter recent metrics</span></span>
<span id="cb32-126"><a href="#cb32-126" aria-hidden="true" tabindex="-1"></a>        recent_indices <span class="op">=</span> [i <span class="cf">for</span> i, t <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.metrics[<span class="st">'timestamps'</span>]) </span>
<span id="cb32-127"><a href="#cb32-127" aria-hidden="true" tabindex="-1"></a>                         <span class="cf">if</span> t <span class="op">&gt;=</span> cutoff_time]</span>
<span id="cb32-128"><a href="#cb32-128" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-129"><a href="#cb32-129" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(recent_indices) <span class="op">&lt;</span> <span class="dv">2</span>:</span>
<span id="cb32-130"><a href="#cb32-130" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> {<span class="st">"error"</span>: <span class="st">"Insufficient data for trend analysis"</span>}</span>
<span id="cb32-131"><a href="#cb32-131" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-132"><a href="#cb32-132" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract recent data</span></span>
<span id="cb32-133"><a href="#cb32-133" aria-hidden="true" tabindex="-1"></a>        recent_times <span class="op">=</span> [<span class="va">self</span>.metrics[<span class="st">'inference_times'</span>][i] <span class="cf">for</span> i <span class="kw">in</span> recent_indices]</span>
<span id="cb32-134"><a href="#cb32-134" aria-hidden="true" tabindex="-1"></a>        recent_memory <span class="op">=</span> [<span class="va">self</span>.metrics[<span class="st">'memory_usage'</span>][i] <span class="cf">for</span> i <span class="kw">in</span> recent_indices]</span>
<span id="cb32-135"><a href="#cb32-135" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-136"><a href="#cb32-136" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate trends (simple linear regression slope)</span></span>
<span id="cb32-137"><a href="#cb32-137" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> np.arange(<span class="bu">len</span>(recent_times))</span>
<span id="cb32-138"><a href="#cb32-138" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-139"><a href="#cb32-139" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Inference time trend</span></span>
<span id="cb32-140"><a href="#cb32-140" aria-hidden="true" tabindex="-1"></a>        time_slope <span class="op">=</span> np.polyfit(x, recent_times, <span class="dv">1</span>)[<span class="dv">0</span>] <span class="cf">if</span> <span class="bu">len</span>(recent_times) <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb32-141"><a href="#cb32-141" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-142"><a href="#cb32-142" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Memory usage trend  </span></span>
<span id="cb32-143"><a href="#cb32-143" aria-hidden="true" tabindex="-1"></a>        memory_slope <span class="op">=</span> np.polyfit(x, recent_memory, <span class="dv">1</span>)[<span class="dv">0</span>] <span class="cf">if</span> <span class="bu">len</span>(recent_memory) <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb32-144"><a href="#cb32-144" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-145"><a href="#cb32-145" aria-hidden="true" tabindex="-1"></a>        trends <span class="op">=</span> {</span>
<span id="cb32-146"><a href="#cb32-146" aria-hidden="true" tabindex="-1"></a>            <span class="st">'window_minutes'</span>: window_minutes,</span>
<span id="cb32-147"><a href="#cb32-147" aria-hidden="true" tabindex="-1"></a>            <span class="st">'data_points'</span>: <span class="bu">len</span>(recent_indices),</span>
<span id="cb32-148"><a href="#cb32-148" aria-hidden="true" tabindex="-1"></a>            <span class="st">'inference_time_trend'</span>: {</span>
<span id="cb32-149"><a href="#cb32-149" aria-hidden="true" tabindex="-1"></a>                <span class="st">'slope'</span>: time_slope,</span>
<span id="cb32-150"><a href="#cb32-150" aria-hidden="true" tabindex="-1"></a>                <span class="st">'direction'</span>: <span class="st">'increasing'</span> <span class="cf">if</span> time_slope <span class="op">&gt;</span> <span class="fl">0.001</span> <span class="cf">else</span> <span class="st">'decreasing'</span> <span class="cf">if</span> time_slope <span class="op">&lt;</span> <span class="op">-</span><span class="fl">0.001</span> <span class="cf">else</span> <span class="st">'stable'</span>,</span>
<span id="cb32-151"><a href="#cb32-151" aria-hidden="true" tabindex="-1"></a>                <span class="st">'severity'</span>: <span class="st">'high'</span> <span class="cf">if</span> <span class="bu">abs</span>(time_slope) <span class="op">&gt;</span> <span class="fl">0.01</span> <span class="cf">else</span> <span class="st">'medium'</span> <span class="cf">if</span> <span class="bu">abs</span>(time_slope) <span class="op">&gt;</span> <span class="fl">0.005</span> <span class="cf">else</span> <span class="st">'low'</span></span>
<span id="cb32-152"><a href="#cb32-152" aria-hidden="true" tabindex="-1"></a>            },</span>
<span id="cb32-153"><a href="#cb32-153" aria-hidden="true" tabindex="-1"></a>            <span class="st">'memory_usage_trend'</span>: {</span>
<span id="cb32-154"><a href="#cb32-154" aria-hidden="true" tabindex="-1"></a>                <span class="st">'slope'</span>: memory_slope,</span>
<span id="cb32-155"><a href="#cb32-155" aria-hidden="true" tabindex="-1"></a>                <span class="st">'direction'</span>: <span class="st">'increasing'</span> <span class="cf">if</span> memory_slope <span class="op">&gt;</span> <span class="fl">0.01</span> <span class="cf">else</span> <span class="st">'decreasing'</span> <span class="cf">if</span> memory_slope <span class="op">&lt;</span> <span class="op">-</span><span class="fl">0.01</span> <span class="cf">else</span> <span class="st">'stable'</span>,</span>
<span id="cb32-156"><a href="#cb32-156" aria-hidden="true" tabindex="-1"></a>                <span class="st">'severity'</span>: <span class="st">'high'</span> <span class="cf">if</span> <span class="bu">abs</span>(memory_slope) <span class="op">&gt;</span> <span class="fl">0.1</span> <span class="cf">else</span> <span class="st">'medium'</span> <span class="cf">if</span> <span class="bu">abs</span>(memory_slope) <span class="op">&gt;</span> <span class="fl">0.05</span> <span class="cf">else</span> <span class="st">'low'</span></span>
<span id="cb32-157"><a href="#cb32-157" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb32-158"><a href="#cb32-158" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb32-159"><a href="#cb32-159" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-160"><a href="#cb32-160" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> trends</span>
<span id="cb32-161"><a href="#cb32-161" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-162"><a href="#cb32-162" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate_monitoring_report(<span class="va">self</span>) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, Any]:</span>
<span id="cb32-163"><a href="#cb32-163" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Generate comprehensive monitoring report"""</span></span>
<span id="cb32-164"><a href="#cb32-164" aria-hidden="true" tabindex="-1"></a>        report <span class="op">=</span> {</span>
<span id="cb32-165"><a href="#cb32-165" aria-hidden="true" tabindex="-1"></a>            <span class="st">'adapter_name'</span>: <span class="va">self</span>.adapter_name,</span>
<span id="cb32-166"><a href="#cb32-166" aria-hidden="true" tabindex="-1"></a>            <span class="st">'report_timestamp'</span>: time.time(),</span>
<span id="cb32-167"><a href="#cb32-167" aria-hidden="true" tabindex="-1"></a>            <span class="st">'performance_stats'</span>: <span class="va">self</span>.compute_performance_stats(),</span>
<span id="cb32-168"><a href="#cb32-168" aria-hidden="true" tabindex="-1"></a>            <span class="st">'trends'</span>: <span class="va">self</span>.analyze_trends(),</span>
<span id="cb32-169"><a href="#cb32-169" aria-hidden="true" tabindex="-1"></a>            <span class="st">'thresholds'</span>: <span class="va">self</span>.thresholds,</span>
<span id="cb32-170"><a href="#cb32-170" aria-hidden="true" tabindex="-1"></a>            <span class="st">'health_status'</span>: <span class="va">self</span>._compute_health_status()</span>
<span id="cb32-171"><a href="#cb32-171" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb32-172"><a href="#cb32-172" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-173"><a href="#cb32-173" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> report</span>
<span id="cb32-174"><a href="#cb32-174" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-175"><a href="#cb32-175" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _compute_health_status(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb32-176"><a href="#cb32-176" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Compute overall health status"""</span></span>
<span id="cb32-177"><a href="#cb32-177" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.metrics[<span class="st">'inference_times'</span>]:</span>
<span id="cb32-178"><a href="#cb32-178" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="st">'unknown'</span></span>
<span id="cb32-179"><a href="#cb32-179" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-180"><a href="#cb32-180" aria-hidden="true" tabindex="-1"></a>        recent_times <span class="op">=</span> <span class="bu">list</span>(<span class="va">self</span>.metrics[<span class="st">'inference_times'</span>])[<span class="op">-</span><span class="dv">10</span>:]</span>
<span id="cb32-181"><a href="#cb32-181" aria-hidden="true" tabindex="-1"></a>        recent_memory <span class="op">=</span> <span class="bu">list</span>(<span class="va">self</span>.metrics[<span class="st">'memory_usage'</span>])[<span class="op">-</span><span class="dv">10</span>:]</span>
<span id="cb32-182"><a href="#cb32-182" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-183"><a href="#cb32-183" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check for threshold violations</span></span>
<span id="cb32-184"><a href="#cb32-184" aria-hidden="true" tabindex="-1"></a>        high_latency <span class="op">=</span> <span class="bu">any</span>(t <span class="op">&gt;</span> <span class="va">self</span>.thresholds[<span class="st">'max_inference_time'</span>] <span class="cf">for</span> t <span class="kw">in</span> recent_times)</span>
<span id="cb32-185"><a href="#cb32-185" aria-hidden="true" tabindex="-1"></a>        high_memory <span class="op">=</span> <span class="bu">any</span>(m <span class="op">&gt;</span> <span class="va">self</span>.thresholds[<span class="st">'max_memory_usage'</span>] <span class="cf">for</span> m <span class="kw">in</span> recent_memory)</span>
<span id="cb32-186"><a href="#cb32-186" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-187"><a href="#cb32-187" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> high_latency <span class="kw">or</span> high_memory:</span>
<span id="cb32-188"><a href="#cb32-188" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="st">'degraded'</span></span>
<span id="cb32-189"><a href="#cb32-189" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-190"><a href="#cb32-190" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check for accuracy issues</span></span>
<span id="cb32-191"><a href="#cb32-191" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.metrics[<span class="st">'accuracy_scores'</span>]:</span>
<span id="cb32-192"><a href="#cb32-192" aria-hidden="true" tabindex="-1"></a>            recent_accuracy <span class="op">=</span> <span class="bu">list</span>(<span class="va">self</span>.metrics[<span class="st">'accuracy_scores'</span>])[<span class="op">-</span><span class="dv">10</span>:]</span>
<span id="cb32-193"><a href="#cb32-193" aria-hidden="true" tabindex="-1"></a>            low_accuracy <span class="op">=</span> <span class="bu">any</span>(a <span class="op">&lt;</span> <span class="va">self</span>.thresholds[<span class="st">'min_accuracy'</span>] <span class="cf">for</span> a <span class="kw">in</span> recent_accuracy)</span>
<span id="cb32-194"><a href="#cb32-194" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> low_accuracy:</span>
<span id="cb32-195"><a href="#cb32-195" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> <span class="st">'degraded'</span></span>
<span id="cb32-196"><a href="#cb32-196" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-197"><a href="#cb32-197" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="st">'healthy'</span></span>
<span id="cb32-198"><a href="#cb32-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-199"><a href="#cb32-199" aria-hidden="true" tabindex="-1"></a><span class="co"># Monitoring demonstration</span></span>
<span id="cb32-200"><a href="#cb32-200" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"LoRA Monitoring System Demo:"</span>)</span>
<span id="cb32-201"><a href="#cb32-201" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">30</span>)</span>
<span id="cb32-202"><a href="#cb32-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-203"><a href="#cb32-203" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize monitor</span></span>
<span id="cb32-204"><a href="#cb32-204" aria-hidden="true" tabindex="-1"></a>monitor <span class="op">=</span> LoRAMonitor(<span class="va">None</span>, <span class="st">"production_adapter"</span>)</span>
<span id="cb32-205"><a href="#cb32-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-206"><a href="#cb32-206" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate monitoring data</span></span>
<span id="cb32-207"><a href="#cb32-207" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Simulating monitoring data..."</span>)</span>
<span id="cb32-208"><a href="#cb32-208" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)  <span class="co"># For reproducible results</span></span>
<span id="cb32-209"><a href="#cb32-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-210"><a href="#cb32-210" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">50</span>):</span>
<span id="cb32-211"><a href="#cb32-211" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simulate varying performance</span></span>
<span id="cb32-212"><a href="#cb32-212" aria-hidden="true" tabindex="-1"></a>    base_latency <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb32-213"><a href="#cb32-213" aria-hidden="true" tabindex="-1"></a>    latency_noise <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.02</span>)</span>
<span id="cb32-214"><a href="#cb32-214" aria-hidden="true" tabindex="-1"></a>    memory_base <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb32-215"><a href="#cb32-215" aria-hidden="true" tabindex="-1"></a>    memory_noise <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.1</span>)</span>
<span id="cb32-216"><a href="#cb32-216" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-217"><a href="#cb32-217" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add some performance degradation over time</span></span>
<span id="cb32-218"><a href="#cb32-218" aria-hidden="true" tabindex="-1"></a>    degradation_factor <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> (i <span class="op">/</span> <span class="dv">1000</span>)</span>
<span id="cb32-219"><a href="#cb32-219" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-220"><a href="#cb32-220" aria-hidden="true" tabindex="-1"></a>    inference_time <span class="op">=</span> base_latency <span class="op">*</span> degradation_factor <span class="op">+</span> latency_noise</span>
<span id="cb32-221"><a href="#cb32-221" aria-hidden="true" tabindex="-1"></a>    memory_usage <span class="op">=</span> memory_base <span class="op">+</span> memory_noise</span>
<span id="cb32-222"><a href="#cb32-222" aria-hidden="true" tabindex="-1"></a>    accuracy <span class="op">=</span> <span class="fl">0.92</span> <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.03</span>)</span>
<span id="cb32-223"><a href="#cb32-223" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-224"><a href="#cb32-224" aria-hidden="true" tabindex="-1"></a>    monitor.log_inference(inference_time, memory_usage, accuracy)</span>
<span id="cb32-225"><a href="#cb32-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-226"><a href="#cb32-226" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate performance report</span></span>
<span id="cb32-227"><a href="#cb32-227" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Generating performance report..."</span>)</span>
<span id="cb32-228"><a href="#cb32-228" aria-hidden="true" tabindex="-1"></a>report <span class="op">=</span> monitor.generate_monitoring_report()</span>
<span id="cb32-229"><a href="#cb32-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-230"><a href="#cb32-230" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Health Status: </span><span class="sc">{</span>report[<span class="st">'health_status'</span>]<span class="sc">.</span>upper()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-231"><a href="#cb32-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-232"><a href="#cb32-232" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="st">'performance_stats'</span> <span class="kw">in</span> report:</span>
<span id="cb32-233"><a href="#cb32-233" aria-hidden="true" tabindex="-1"></a>    perf <span class="op">=</span> report[<span class="st">'performance_stats'</span>]</span>
<span id="cb32-234"><a href="#cb32-234" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-235"><a href="#cb32-235" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">'inference_time'</span> <span class="kw">in</span> perf:</span>
<span id="cb32-236"><a href="#cb32-236" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Inference Time - Mean: </span><span class="sc">{</span>perf[<span class="st">'inference_time'</span>][<span class="st">'mean'</span>]<span class="sc">:.3f}</span><span class="ss">s, P95: </span><span class="sc">{</span>perf[<span class="st">'inference_time'</span>][<span class="st">'p95'</span>]<span class="sc">:.3f}</span><span class="ss">s"</span>)</span>
<span id="cb32-237"><a href="#cb32-237" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-238"><a href="#cb32-238" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">'memory_usage'</span> <span class="kw">in</span> perf:</span>
<span id="cb32-239"><a href="#cb32-239" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Memory Usage - Mean: </span><span class="sc">{</span>perf[<span class="st">'memory_usage'</span>][<span class="st">'mean'</span>]<span class="sc">:.2f}</span><span class="ss">GB, Max: </span><span class="sc">{</span>perf[<span class="st">'memory_usage'</span>][<span class="st">'max'</span>]<span class="sc">:.2f}</span><span class="ss">GB"</span>)</span>
<span id="cb32-240"><a href="#cb32-240" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-241"><a href="#cb32-241" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">'accuracy'</span> <span class="kw">in</span> perf:</span>
<span id="cb32-242"><a href="#cb32-242" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Accuracy - Mean: </span><span class="sc">{</span>perf[<span class="st">'accuracy'</span>][<span class="st">'mean'</span>]<span class="sc">:.3f}</span><span class="ss">, Recent: </span><span class="sc">{</span>perf[<span class="st">'accuracy'</span>][<span class="st">'recent'</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb32-243"><a href="#cb32-243" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-244"><a href="#cb32-244" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">'throughput'</span> <span class="kw">in</span> perf:</span>
<span id="cb32-245"><a href="#cb32-245" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Throughput: </span><span class="sc">{</span>perf[<span class="st">'throughput'</span>][<span class="st">'requests_per_second'</span>]<span class="sc">:.1f}</span><span class="ss"> req/s"</span>)</span>
<span id="cb32-246"><a href="#cb32-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-247"><a href="#cb32-247" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="st">'trends'</span> <span class="kw">in</span> report <span class="kw">and</span> <span class="st">'error'</span> <span class="kw">not</span> <span class="kw">in</span> report[<span class="st">'trends'</span>]:</span>
<span id="cb32-248"><a href="#cb32-248" aria-hidden="true" tabindex="-1"></a>    trends <span class="op">=</span> report[<span class="st">'trends'</span>]</span>
<span id="cb32-249"><a href="#cb32-249" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Trend Analysis (</span><span class="sc">{</span>trends[<span class="st">'window_minutes'</span>]<span class="sc">}</span><span class="ss"> min window):"</span>)</span>
<span id="cb32-250"><a href="#cb32-250" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Latency trend: </span><span class="sc">{</span>trends[<span class="st">'inference_time_trend'</span>][<span class="st">'direction'</span>]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>trends[<span class="st">'inference_time_trend'</span>][<span class="st">'severity'</span>]<span class="sc">}</span><span class="ss"> severity)"</span>)</span>
<span id="cb32-251"><a href="#cb32-251" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Memory trend: </span><span class="sc">{</span>trends[<span class="st">'memory_usage_trend'</span>][<span class="st">'direction'</span>]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>trends[<span class="st">'memory_usage_trend'</span>][<span class="st">'severity'</span>]<span class="sc">}</span><span class="ss"> severity)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>LoRA Monitoring System Demo:
==============================
LoRA Monitor initialized for adapter: production_adapter

Simulating monitoring data...

Generating performance report...
Health Status: HEALTHY
Inference Time - Mean: 0.102s, P95: 0.131s
Memory Usage - Mean: 1.99GB, Max: 2.19GB
Accuracy - Mean: 0.917, Recent: 0.926
Throughput: 562239.1 req/s

Trend Analysis (30 min window):
Latency trend: stable (low severity)
Memory trend: stable (low severity)</code></pre>
</div>
</div>
</section>
<section id="visualization-and-dashboards" class="level3">
<h3 class="anchored" data-anchor-id="visualization-and-dashboards">Visualization and Dashboards</h3>
<div id="cell-fig-monitoring-dashboard" class="cell" data-execution_count="20">
<div class="cell-output cell-output-display">
<div id="fig-monitoring-dashboard" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-monitoring-dashboard-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-monitoring-dashboard-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-monitoring-dashboard-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: LoRA Monitoring Dashboard
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="future-directions" class="level2">
<h2 class="anchored" data-anchor-id="future-directions">Future Directions</h2>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-10-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-10-1" role="tab" aria-controls="tabset-10-1" aria-selected="true">Emerging Techniques</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-10-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-10-2" role="tab" aria-controls="tabset-10-2" aria-selected="false">Research Roadmap</a></li></ul>
<div class="tab-content">
<div id="tabset-10-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-10-1-tab">
<section id="dynamic-lora" class="level3">
<h3 class="anchored" data-anchor-id="dynamic-lora">Dynamic LoRA</h3>
<ul>
<li><strong>Description</strong>: Adaptive rank and module selection during training</li>
<li><strong>Potential Impact</strong>: 30-50% efficiency improvement</li>
<li><strong>Maturity</strong>: Research phase</li>
<li><strong>Status</strong>: 🔬 Active Research</li>
</ul>
</section>
<section id="hierarchical-lora" class="level3">
<h3 class="anchored" data-anchor-id="hierarchical-lora">Hierarchical LoRA</h3>
<ul>
<li><strong>Description</strong>: Multi-level adaptation for different abstraction levels</li>
<li><strong>Potential Impact</strong>: Better transfer learning</li>
<li><strong>Maturity</strong>: Early development</li>
<li><strong>Status</strong>: 🌱 Early Development</li>
</ul>
</section>
<section id="conditional-lora" class="level3">
<h3 class="anchored" data-anchor-id="conditional-lora">Conditional LoRA</h3>
<ul>
<li><strong>Description</strong>: Task-conditional parameter generation</li>
<li><strong>Potential Impact</strong>: Unlimited task adaptation</li>
<li><strong>Maturity</strong>: Conceptual</li>
<li><strong>Status</strong>: 💡 Conceptual</li>
</ul>
</section>
<section id="federated-lora" class="level3">
<h3 class="anchored" data-anchor-id="federated-lora">Federated LoRA</h3>
<ul>
<li><strong>Description</strong>: Distributed learning with privacy preservation</li>
<li><strong>Potential Impact</strong>: Privacy-safe collaboration</li>
<li><strong>Maturity</strong>: Active research</li>
<li><strong>Status</strong>: 🔬 Active Research</li>
</ul>
</section>
<section id="neural-architecture-lora" class="level3">
<h3 class="anchored" data-anchor-id="neural-architecture-lora">Neural Architecture LoRA</h3>
<ul>
<li><strong>Description</strong>: Architecture search for optimal LoRA configurations</li>
<li><strong>Potential Impact</strong>: Optimal configurations automatically</li>
<li><strong>Maturity</strong>: Research phase</li>
<li><strong>Status</strong>: 🔬 Research Phase</li>
</ul>
</section>
</div>
<div id="tabset-10-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-10-2-tab">
<section id="short-term-6-12-months" class="level3">
<h3 class="anchored" data-anchor-id="short-term-6-12-months">Short Term (6-12 months)</h3>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Focus Areas
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Improved rank selection algorithms</li>
<li>Better initialization strategies</li>
<li>Enhanced debugging tools</li>
<li>Standardized evaluation protocols</li>
</ul>
</div>
</div>
<p><strong>Expected Outcomes:</strong></p>
<ul>
<li>More stable training</li>
<li>Better out-of-box performance</li>
<li>Easier troubleshooting</li>
</ul>
</section>
<section id="medium-term-1-2-years" class="level3">
<h3 class="anchored" data-anchor-id="medium-term-1-2-years">Medium Term (1-2 years)</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Focus Areas
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Dynamic and adaptive LoRA</li>
<li>Multi-modal LoRA extensions</li>
<li>Automated hyperparameter optimization</li>
<li>Large-scale deployment frameworks</li>
</ul>
</div>
</div>
<p><strong>Expected Outcomes:</strong></p>
<ul>
<li>Self-optimizing systems</li>
<li>Audio-visual-text models</li>
<li>Production-ready pipelines</li>
</ul>
</section>
<section id="long-term-2-5-years" class="level3">
<h3 class="anchored" data-anchor-id="long-term-2-5-years">Long Term (2-5 years)</h3>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Focus Areas
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Theoretical understanding of adaptation</li>
<li>Novel mathematical frameworks</li>
<li>Integration with emerging architectures</li>
<li>Quantum-inspired adaptations</li>
</ul>
</div>
</div>
<p><strong>Expected Outcomes:</strong></p>
<ul>
<li>Principled design guidelines</li>
<li>Next-generation efficiency</li>
<li>Revolutionary capabilities</li>
</ul>
</section>
</div>
</div>
</div>
<section id="impact-analysis" class="level3">
<h3 class="anchored" data-anchor-id="impact-analysis">Impact Analysis</h3>
<section id="dynamic-lora-case-study" class="level4">
<h4 class="anchored" data-anchor-id="dynamic-lora-case-study">Dynamic LoRA Case Study</h4>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Predicted Impact Analysis
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Technique</strong>: Dynamic LoRA<br>
<strong>Description</strong>: Adaptive rank and module selection during training</p>
</div>
</div>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Metric</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Efficiency Gain</strong></td>
<td>1.8x</td>
</tr>
<tr class="even">
<td><strong>Performance Improvement</strong></td>
<td>+3.0%</td>
</tr>
<tr class="odd">
<td><strong>Adoption Timeline</strong></td>
<td>6 months</td>
</tr>
<tr class="even">
<td><strong>Implementation Complexity</strong></td>
<td>Medium</td>
</tr>
<tr class="odd">
<td><strong>Research Interest Score</strong></td>
<td>0.94/1.00</td>
</tr>
</tbody>
</table>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">gantt
    title LoRA Research Timeline
    dateFormat  YYYY-MM
    section Short Term
    Rank Selection     :active, st1, 2024-08, 6M
    Initialization     :active, st2, 2024-08, 6M
    Debugging Tools    :st3, after st1, 4M
    section Medium Term
    Dynamic LoRA       :mt1, 2025-02, 12M
    Multi-modal        :mt2, 2025-06, 18M
    Auto-optimization  :mt3, after mt1, 12M
    section Long Term
    Theory Framework   :lt1, 2026-01, 24M
    Next-gen Arch      :lt2, 2026-06, 30M
    Quantum Inspired   :lt3, 2027-01, 36M
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="summary" class="level4">
<h4 class="anchored" data-anchor-id="summary">Summary</h4>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Takeaways
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><strong>Dynamic LoRA</strong> shows the most immediate promise with 1.8x efficiency gains</li>
<li><strong>Short-term focus</strong> should be on stability and usability improvements</li>
<li><strong>Long-term vision</strong> includes theoretical breakthroughs and quantum adaptations</li>
<li><strong>Timeline</strong> spans from 6 months to 5 years for full roadmap completion</li>
</ol>
</div>
</div>
</section>
</section>
<section id="research-opportunities" class="level3">
<h3 class="anchored" data-anchor-id="research-opportunities">Research Opportunities</h3>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Research Domains
</div>
</div>
<div class="callout-body-container callout-body">
<p>Three primary areas have been identified for immediate investigation:</p>
</div>
</div>
<div class="quarto-layout-panel" data-layout-ncol="3">
<div class="quarto-layout-row">
<div class="card quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<p><strong>Theoretical Analysis</strong></p>
<ul>
<li>Better understanding of LoRA’s approximation capabilities</li>
<li>4 key research questions identified</li>
<li>Focus on mathematical foundations</li>
</ul>
</div>
<div class="card quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<p><strong>Architecture Specific</strong></p>
<ul>
<li>Optimized LoRA for different VLM architectures</li>
<li>4 key research questions identified</li>
<li>Vision-language model specialization</li>
</ul>
</div>
<div class="card quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<p><strong>Efficiency Optimization</strong></p>
<ul>
<li>Hardware-aware LoRA optimization</li>
<li>4 key research questions identified</li>
<li>Performance and resource utilization</li>
</ul>
</div>
</div>
</div>
</section>
<section id="detailed-proposals" class="level3">
<h3 class="anchored" data-anchor-id="detailed-proposals">Detailed Proposals</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-30-contents" aria-controls="callout-30" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Research Proposal Details
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-30" class="callout-30-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p><strong>Area:</strong> Theoretical Analysis<br>
<strong>Priority:</strong> HIGH<br>
<strong>Description:</strong> Better understanding of LoRA’s approximation capabilities</p>
<section id="proposal-1-theoretical-limits-investigation" class="level4">
<h4 class="anchored" data-anchor-id="proposal-1-theoretical-limits-investigation">Proposal 1: Theoretical Limits Investigation</h4>
<ul>
<li><strong>Objective:</strong> What is the theoretical limit of low-rank approximation?</li>
<li><strong>Methodology:</strong> Matrix perturbation theory</li>
<li><strong>Timeline:</strong> 12-18 months</li>
<li><strong>Expected Outcomes:</strong>
<ul>
<li>Mathematical bounds on approximation quality</li>
<li>Guidelines for rank selection</li>
<li>Theoretical framework for optimization</li>
</ul></li>
</ul>
</section>
</div>
</div>
</div>
<section id="research-questions-framework" class="level4">
<h4 class="anchored" data-anchor-id="research-questions-framework">Research Questions Framework</h4>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-11-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-11-1" role="tab" aria-controls="tabset-11-1" aria-selected="true">Theoretical</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-11-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-11-2" role="tab" aria-controls="tabset-11-2" aria-selected="false">Architectural</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-11-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-11-3" role="tab" aria-controls="tabset-11-3" aria-selected="false">Efficiency</a></li></ul>
<div class="tab-content">
<div id="tabset-11-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-11-1-tab">
<ol type="1">
<li>What are the fundamental limits of low-rank approximation in neural networks?</li>
<li>How does rank selection impact convergence and generalization?</li>
<li>Can we establish theoretical guarantees for LoRA performance?</li>
<li>What is the relationship between rank and model capacity?</li>
</ol>
</div>
<div id="tabset-11-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-11-2-tab">
<ol type="1">
<li>How can LoRA be optimized for transformer architectures?</li>
<li>What are the best practices for multi-modal model adaptation?</li>
<li>How does LoRA performance vary across different layer types?</li>
<li>Can we develop architecture-specific rank selection strategies?</li>
</ol>
</div>
<div id="tabset-11-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-11-3-tab">
<ol type="1">
<li>What are the optimal hardware configurations for LoRA training?</li>
<li>How can we minimize memory overhead during adaptation?</li>
<li>What parallelization strategies work best for LoRA?</li>
<li>Can we develop real-time adaptation capabilities?</li>
</ol>
</div>
</div>
</div>
</section>
</section>
<section id="impact-assessment" class="level3">
<h3 class="anchored" data-anchor-id="impact-assessment">Impact Assessment</h3>
<div id="335b60a3" class="cell" data-execution_count="21">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-22-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="impact-scores-summary" class="level3">
<h3 class="anchored" data-anchor-id="impact-scores-summary">Impact Scores Summary</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 19%">
<col style="width: 22%">
<col style="width: 21%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th>Research Area</th>
<th>Overall Impact</th>
<th>Scientific Impact</th>
<th>Practical Impact</th>
<th>Recommendation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Multimodal Extensions</strong></td>
<td>0.75</td>
<td>0.79</td>
<td>0.79</td>
<td>MEDIUM PRIORITY</td>
</tr>
<tr class="even">
<td><strong>Continual Learning</strong></td>
<td>0.72</td>
<td>0.86</td>
<td>0.72</td>
<td>MEDIUM PRIORITY</td>
</tr>
<tr class="odd">
<td><strong>Architecture Specific</strong></td>
<td>0.65</td>
<td>0.84</td>
<td>0.66</td>
<td>MEDIUM PRIORITY</td>
</tr>
<tr class="even">
<td><strong>Theoretical Analysis</strong></td>
<td>0.64</td>
<td>0.75</td>
<td>0.53</td>
<td>MEDIUM PRIORITY</td>
</tr>
<tr class="odd">
<td><strong>Efficiency Optimization</strong></td>
<td>0.63</td>
<td>0.72</td>
<td>0.80</td>
<td>MEDIUM PRIORITY</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="summary-of-key-points" class="level2">
<h2 class="anchored" data-anchor-id="summary-of-key-points">Summary of Key Points</h2>
<ol type="1">
<li><strong>Conservative Hyperparameter Initialization</strong></li>
</ol>
<ul>
<li>Start with conservative hyperparameters (rank=16, alpha=16)</li>
<li>Gradually increase complexity based on validation performance</li>
<li>Avoid overfitting with aggressive initial configurations</li>
</ul>
<ol start="2" type="1">
<li><strong>Strategic Module Selection</strong></li>
</ol>
<ul>
<li>Focus on high-impact modules (attention layers, cross-modal fusion)</li>
<li>Prioritize modules that maximize efficiency gains</li>
<li>Consider computational cost vs.&nbsp;performance trade-offs</li>
</ul>
<ol start="3" type="1">
<li><strong>Comprehensive Monitoring</strong></li>
</ol>
<ul>
<li>Monitor both performance and efficiency metrics throughout development</li>
<li>Track convergence patterns and training stability</li>
<li>Implement early stopping based on validation metrics</li>
</ul>
<ol start="4" type="1">
<li><strong>Debugging and Analysis Tools</strong></li>
</ol>
<ul>
<li>Use appropriate debugging tools to understand adapter behavior</li>
<li>Analyze attention patterns and feature representations</li>
<li>Implement gradient flow monitoring for training diagnostics</li>
</ul>
<ol start="5" type="1">
<li><strong>Progressive Training Strategies</strong></li>
</ol>
<ul>
<li>Implement progressive training strategies for stable convergence</li>
<li>Use curriculum learning approaches when appropriate</li>
<li>Consider staged training with increasing complexity</li>
</ul>
<ol start="6" type="1">
<li><strong>Memory Optimization</strong></li>
</ol>
<ul>
<li>Apply memory optimization techniques for large-scale deployment</li>
<li>Implement gradient checkpointing and mixed precision training</li>
<li>Optimize batch sizes and sequence lengths</li>
</ul>
<ol start="7" type="1">
<li><strong>Production Monitoring</strong></li>
</ol>
<ul>
<li>Establish comprehensive monitoring for production systems</li>
<li>Track model performance drift and adaptation effectiveness</li>
<li>Implement automated alerts for performance degradation</li>
</ul>
<ol start="8" type="1">
<li><strong>Continuous Learning</strong></li>
</ol>
<ul>
<li>Stay updated with emerging techniques and research developments</li>
<li>Regularly evaluate new LoRA variants and improvements</li>
<li>Participate in community discussions and knowledge sharing</li>
</ul>
<ol start="9" type="1">
<li><strong>Task-Specific Optimization</strong></li>
</ol>
<ul>
<li>Consider task-specific configurations for optimal performance</li>
<li>Adapt hyperparameters based on domain requirements</li>
<li>Fine-tune approaches for different VLM applications</li>
</ul>
<ol start="10" type="1">
<li><strong>Robust Troubleshooting</strong></li>
</ol>
<ul>
<li>Implement robust troubleshooting procedures for common issues</li>
<li>Maintain comprehensive error handling and recovery mechanisms</li>
<li>Document solutions for recurring problems</li>
</ul>
</section>
<section id="implementation-checklist" class="level2">
<h2 class="anchored" data-anchor-id="implementation-checklist">Implementation Checklist</h2>
<ul class="task-list">
<li><label><input type="checkbox">Initialize with conservative hyperparameters</label></li>
<li><label><input type="checkbox">Identify and target high-impact modules</label></li>
<li><label><input type="checkbox">Set up comprehensive monitoring systems</label></li>
<li><label><input type="checkbox">Configure debugging and analysis tools</label></li>
<li><label><input type="checkbox">Implement progressive training pipeline</label></li>
<li><label><input type="checkbox">Apply memory optimization techniques</label></li>
<li><label><input type="checkbox">Establish production monitoring</label></li>
<li><label><input type="checkbox">Create update and maintenance procedures</label></li>
<li><label><input type="checkbox">Customize for specific task requirements</label></li>
<li><label><input type="checkbox">Prepare troubleshooting documentation</label></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pro Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Remember that successful LoRA implementation is an iterative process. Start simple, monitor carefully, and gradually optimize based on empirical results rather than theoretical assumptions.</p>
</div>
</div>
</section>
<section id="future-outlook" class="level2">
<h2 class="anchored" data-anchor-id="future-outlook">Future Outlook</h2>
<p>As the field continues to evolve, LoRA and its variants will likely become even more sophisticated, enabling more efficient and capable multimodal AI systems. The techniques and principles outlined in this guide provide a solid foundation for leveraging these advances in your own Vision-Language Model applications.</p>
</section>
<section id="resources-for-further-learning" class="level2">
<h2 class="anchored" data-anchor-id="resources-for-further-learning">Resources for Further Learning</h2>
<ul>
<li><strong>Hugging Face PEFT</strong>: Parameter-Efficient Fine-Tuning library</li>
<li><strong>LoRA Paper</strong>: “LoRA: Low-Rank Adaptation of Large Language Models” (Hu et al., 2021)</li>
<li><strong>CLIP Paper</strong>: “Learning Transferable Visual Representations from Natural Language Supervision” (Radford et al., 2021)</li>
<li><strong>LLaVA Paper</strong>: “Visual Instruction Tuning” (Liu et al., 2023)</li>
<li><strong>AdaLoRA Paper</strong>: “Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning” (Zhang et al., 2023)</li>
</ul>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ol type="1">
<li><p>Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., … &amp; Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. <em>arXiv preprint arXiv:2106.09685</em>.</p></li>
<li><p>Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … &amp; Sutskever, I. (2021). Learning Transferable Visual Representations from Natural Language Supervision. <em>International Conference on Machine Learning</em>.</p></li>
<li><p>Li, J., Li, D., Xiong, C., &amp; Hoi, S. (2022). BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. <em>International Conference on Machine Learning</em>.</p></li>
<li><p>Liu, H., Li, C., Wu, Q., &amp; Lee, Y. J. (2023). Visual Instruction Tuning. <em>arXiv preprint arXiv:2304.08485</em>.</p></li>
<li><p>Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen, W., &amp; Zhao, T. (2023). AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning. <em>International Conference on Learning Representations</em>.</p></li>
</ol>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/theja-vanka\.github\.io\/blogs\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>© 2025 Krishnatheja Vanka</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>