<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Krishnatheja Vanka">
<meta name="dcterms.date" content="2025-07-02">

<title>The Mathematics Behind Kolmogorov-Arnold Networks – Krishnatheja Vanka’s Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-62c28fcd870f55f61984f019219cbd7c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Krishnatheja Vanka’s Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/theja-vanka"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-right">
      <h1 class="title">The Mathematics Behind Kolmogorov-Arnold Networks</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">research</div>
                <div class="quarto-category">advanced</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta column-page-right">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Krishnatheja Vanka </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 2, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-mathematics-behind-kolmogorov-arnold-networks" id="toc-the-mathematics-behind-kolmogorov-arnold-networks" class="nav-link active" data-scroll-target="#the-mathematics-behind-kolmogorov-arnold-networks">The Mathematics Behind Kolmogorov-Arnold Networks</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#the-kolmogorov-arnold-representation-theorem" id="toc-the-kolmogorov-arnold-representation-theorem" class="nav-link" data-scroll-target="#the-kolmogorov-arnold-representation-theorem">The Kolmogorov-Arnold Representation Theorem</a>
  <ul class="collapse">
  <li><a href="#historical-context-and-statement" id="toc-historical-context-and-statement" class="nav-link" data-scroll-target="#historical-context-and-statement">Historical Context and Statement</a></li>
  <li><a href="#mathematical-significance" id="toc-mathematical-significance" class="nav-link" data-scroll-target="#mathematical-significance">Mathematical Significance</a></li>
  </ul></li>
  <li><a href="#from-classical-theory-to-neural-networks" id="toc-from-classical-theory-to-neural-networks" class="nav-link" data-scroll-target="#from-classical-theory-to-neural-networks">From Classical Theory to Neural Networks</a>
  <ul class="collapse">
  <li><a href="#traditional-neural-networks-vs.-kans" id="toc-traditional-neural-networks-vs.-kans" class="nav-link" data-scroll-target="#traditional-neural-networks-vs.-kans">Traditional Neural Networks vs.&nbsp;KANs</a></li>
  <li><a href="#the-fundamental-difference" id="toc-the-fundamental-difference" class="nav-link" data-scroll-target="#the-fundamental-difference">The Fundamental Difference</a></li>
  </ul></li>
  <li><a href="#mathematical-foundations-of-kan-architecture" id="toc-mathematical-foundations-of-kan-architecture" class="nav-link" data-scroll-target="#mathematical-foundations-of-kan-architecture">Mathematical Foundations of KAN Architecture</a>
  <ul class="collapse">
  <li><a href="#function-parametrization" id="toc-function-parametrization" class="nav-link" data-scroll-target="#function-parametrization">Function Parametrization</a></li>
  <li><a href="#layer-wise-composition" id="toc-layer-wise-composition" class="nav-link" data-scroll-target="#layer-wise-composition">Layer-wise Composition</a></li>
  <li><a href="#residual-connections" id="toc-residual-connections" class="nav-link" data-scroll-target="#residual-connections">Residual Connections</a></li>
  </ul></li>
  <li><a href="#optimization-and-training" id="toc-optimization-and-training" class="nav-link" data-scroll-target="#optimization-and-training">Optimization and Training</a>
  <ul class="collapse">
  <li><a href="#loss-function" id="toc-loss-function" class="nav-link" data-scroll-target="#loss-function">Loss Function</a></li>
  <li><a href="#sparsity-regularization" id="toc-sparsity-regularization" class="nav-link" data-scroll-target="#sparsity-regularization">Sparsity Regularization</a></li>
  <li><a href="#smoothness-regularization" id="toc-smoothness-regularization" class="nav-link" data-scroll-target="#smoothness-regularization">Smoothness Regularization</a></li>
  </ul></li>
  <li><a href="#theoretical-properties" id="toc-theoretical-properties" class="nav-link" data-scroll-target="#theoretical-properties">Theoretical Properties</a>
  <ul class="collapse">
  <li><a href="#universal-approximation" id="toc-universal-approximation" class="nav-link" data-scroll-target="#universal-approximation">Universal Approximation</a></li>
  <li><a href="#expressivity-analysis" id="toc-expressivity-analysis" class="nav-link" data-scroll-target="#expressivity-analysis">Expressivity Analysis</a></li>
  <li><a href="#approximation-rates" id="toc-approximation-rates" class="nav-link" data-scroll-target="#approximation-rates">Approximation Rates</a></li>
  </ul></li>
  <li><a href="#computational-complexity" id="toc-computational-complexity" class="nav-link" data-scroll-target="#computational-complexity">Computational Complexity</a>
  <ul class="collapse">
  <li><a href="#forward-pass-complexity" id="toc-forward-pass-complexity" class="nav-link" data-scroll-target="#forward-pass-complexity">Forward Pass Complexity</a></li>
  <li><a href="#backward-pass-complexity" id="toc-backward-pass-complexity" class="nav-link" data-scroll-target="#backward-pass-complexity">Backward Pass Complexity</a></li>
  </ul></li>
  <li><a href="#interpretability-and-symbolic-regression" id="toc-interpretability-and-symbolic-regression" class="nav-link" data-scroll-target="#interpretability-and-symbolic-regression">Interpretability and Symbolic Regression</a>
  <ul class="collapse">
  <li><a href="#automatic-symbolification" id="toc-automatic-symbolification" class="nav-link" data-scroll-target="#automatic-symbolification">Automatic Symbolification</a></li>
  <li><a href="#mathematical-insights" id="toc-mathematical-insights" class="nav-link" data-scroll-target="#mathematical-insights">Mathematical Insights</a></li>
  </ul></li>
  <li><a href="#advanced-mathematical-concepts" id="toc-advanced-mathematical-concepts" class="nav-link" data-scroll-target="#advanced-mathematical-concepts">Advanced Mathematical Concepts</a>
  <ul class="collapse">
  <li><a href="#measure-theory-perspectives" id="toc-measure-theory-perspectives" class="nav-link" data-scroll-target="#measure-theory-perspectives">Measure Theory Perspectives</a></li>
  <li><a href="#functional-analysis" id="toc-functional-analysis" class="nav-link" data-scroll-target="#functional-analysis">Functional Analysis</a></li>
  <li><a href="#information-theory" id="toc-information-theory" class="nav-link" data-scroll-target="#information-theory">Information Theory</a></li>
  </ul></li>
  <li><a href="#limitations-and-extensions" id="toc-limitations-and-extensions" class="nav-link" data-scroll-target="#limitations-and-extensions">Limitations and Extensions</a>
  <ul class="collapse">
  <li><a href="#theoretical-limitations" id="toc-theoretical-limitations" class="nav-link" data-scroll-target="#theoretical-limitations">Theoretical Limitations</a></li>
  <li><a href="#recent-extensions" id="toc-recent-extensions" class="nav-link" data-scroll-target="#recent-extensions">Recent Extensions</a></li>
  </ul></li>
  <li><a href="#future-directions" id="toc-future-directions" class="nav-link" data-scroll-target="#future-directions">Future Directions</a>
  <ul class="collapse">
  <li><a href="#theoretical-developments" id="toc-theoretical-developments" class="nav-link" data-scroll-target="#theoretical-developments">Theoretical Developments</a></li>
  <li><a href="#practical-innovations" id="toc-practical-innovations" class="nav-link" data-scroll-target="#practical-innovations">Practical Innovations</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-right" id="quarto-document-content">





<section id="the-mathematics-behind-kolmogorov-arnold-networks" class="level1">
<h1>The Mathematics Behind Kolmogorov-Arnold Networks</h1>
<p><img src="kan.png" class="img-fluid"></p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Kolmogorov-Arnold Networks (KANs) represent a paradigm shift in neural network architecture, moving away from the traditional linear combinations of fixed activation functions toward networks that learn the activation functions themselves. This revolutionary approach is grounded in the profound mathematical insights of Andrey Kolmogorov and Vladimir Arnold, whose representation theorem provides the theoretical foundation for these networks.</p>
</section>
<section id="the-kolmogorov-arnold-representation-theorem" class="level2">
<h2 class="anchored" data-anchor-id="the-kolmogorov-arnold-representation-theorem">The Kolmogorov-Arnold Representation Theorem</h2>
<section id="historical-context-and-statement" class="level3">
<h3 class="anchored" data-anchor-id="historical-context-and-statement">Historical Context and Statement</h3>
<p>In 1957, Andrey Kolmogorov and his student Vladimir Arnold proved a remarkable theorem that fundamentally changed our understanding of multivariate function representation. The theorem states:</p>
<p><strong>Kolmogorov-Arnold Theorem</strong>: Every continuous multivariate function defined on a bounded domain can be represented as a composition and superposition of continuous functions of a single variable.</p>
<p>Formally, for any continuous function <span class="math inline">\(f: [0,1]^n \to \mathbb{R}\)</span>, there exist continuous functions <span class="math inline">\(\phi_{q,p}: [0,1] \to \mathbb{R}\)</span> and <span class="math inline">\(\Phi_q: \mathbb{R} \to \mathbb{R}\)</span> such that:</p>
<p><span class="math display">\[
f(x_1, x_2, \ldots, x_n) = \sum_{q=0}^{2n} \Phi_q\left(\sum_{p=1}^{n} \phi_{q,p}(x_p)\right)
\]</span></p>
<p>where the inner functions <span class="math inline">\(\phi_{q,p}\)</span> are independent of <span class="math inline">\(f\)</span> and depend only on the dimension <span class="math inline">\(n\)</span>.</p>
</section>
<section id="mathematical-significance" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-significance">Mathematical Significance</h3>
<p>This theorem is remarkable because it demonstrates that the curse of dimensionality can be overcome through clever composition of univariate functions. The key insights are:</p>
<ol type="1">
<li><strong>Universality</strong>: The inner functions <span class="math inline">\(\phi_{q,p}\)</span> are universal and independent of the target function <span class="math inline">\(f\)</span></li>
<li><strong>Compositionality</strong>: Complex multivariate functions can be decomposed into simpler univariate components</li>
<li><strong>Finite Width</strong>: Only <span class="math inline">\(2n+1\)</span> terms are needed in the outer sum</li>
</ol>
</section>
</section>
<section id="from-classical-theory-to-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="from-classical-theory-to-neural-networks">From Classical Theory to Neural Networks</h2>
<section id="traditional-neural-networks-vs.-kans" class="level3">
<h3 class="anchored" data-anchor-id="traditional-neural-networks-vs.-kans">Traditional Neural Networks vs.&nbsp;KANs</h3>
<p>Traditional multilayer perceptrons (MLPs) implement the universal approximation theorem through:</p>
<p><span class="math display">\[
f(x) = \sum_{i=1}^{m} w_i \sigma\left(\sum_{j=1}^{n} w_{ij} x_j + b_i\right)
\]</span></p>
<p>where <span class="math inline">\(\sigma\)</span> is a fixed activation function (e.g., ReLU, sigmoid, tanh).</p>
<p>KANs, inspired by the Kolmogorov-Arnold theorem, instead use:</p>
<p><span class="math display">\[
f(x) = \sum_{q=0}^{2n} \Phi_q\left(\sum_{p=1}^{n} \phi_{q,p}(x_p)\right)
\]</span></p>
<p>where both <span class="math inline">\(\phi_{q,p}\)</span> and <span class="math inline">\(\Phi_q\)</span> are learnable functions.</p>
</section>
<section id="the-fundamental-difference" class="level3">
<h3 class="anchored" data-anchor-id="the-fundamental-difference">The Fundamental Difference</h3>
<p>The crucial difference lies in <strong>where</strong> the nonlinearity is applied: - <strong>MLPs</strong>: Apply fixed nonlinear activations to linear combinations of inputs - <strong>KANs</strong>: Learn the nonlinear functions themselves, applied to individual variables</p>
</section>
</section>
<section id="mathematical-foundations-of-kan-architecture" class="level2">
<h2 class="anchored" data-anchor-id="mathematical-foundations-of-kan-architecture">Mathematical Foundations of KAN Architecture</h2>
<section id="function-parametrization" class="level3">
<h3 class="anchored" data-anchor-id="function-parametrization">Function Parametrization</h3>
<p>In practical implementations, the learnable functions <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\Phi\)</span> are typically parametrized using:</p>
<section id="b-splines" class="level4">
<h4 class="anchored" data-anchor-id="b-splines">B-Splines</h4>
<p>B-splines provide a flexible and numerically stable way to represent univariate functions:</p>
<p><span class="math display">\[\phi(x) = \sum_{i=0}^{G} c_i B_i^k(x)\]</span></p>
<p>where: - <span class="math inline">\(B_i^k(x)\)</span> are B-spline basis functions of degree <span class="math inline">\(k\)</span> - <span class="math inline">\(c_i\)</span> are learnable coefficients - <span class="math inline">\(G\)</span> is the number of control points</p>
</section>
<section id="advantages-of-b-splines" class="level4">
<h4 class="anchored" data-anchor-id="advantages-of-b-splines">Advantages of B-Splines:</h4>
<ul>
<li><strong>Local Support</strong>: Changes in coefficients affect only local regions</li>
<li><strong>Smoothness</strong>: Degree <span class="math inline">\(k\)</span> splines are <span class="math inline">\(C^{k-1}\)</span> continuous</li>
<li><strong>Numerical Stability</strong>: Well-conditioned basis functions</li>
<li><strong>Interpretability</strong>: Control points provide intuitive understanding</li>
</ul>
</section>
</section>
<section id="layer-wise-composition" class="level3">
<h3 class="anchored" data-anchor-id="layer-wise-composition">Layer-wise Composition</h3>
<p>A practical KAN extends the basic representation through multiple layers:</p>
<p><span class="math display">\[\text{KAN}(x) = \text{KAN}_L \circ \text{KAN}_{L-1} \circ \cdots \circ \text{KAN}_1(x)\]</span></p>
<p>where each layer <span class="math inline">\(\text{KAN}_\ell\)</span> transforms inputs through learnable univariate functions:</p>
<p><span class="math display">\[\text{KAN}_\ell(x^{(\ell-1)}) = \left(\sum_{j=1}^{n_{\ell-1}} \phi_{\ell,i,j}(x^{(\ell-1)}_j)\right)_{i=1}^{n_\ell}\]</span></p>
</section>
<section id="residual-connections" class="level3">
<h3 class="anchored" data-anchor-id="residual-connections">Residual Connections</h3>
<p>To enhance expressivity and training stability, KANs often include residual connections:</p>
<p><span class="math display">\[\phi_{\ell,i,j}(x) = w_{\ell,i,j} \cdot \text{spline}_{\ell,i,j}(x) + b_{\ell,i,j} \cdot x\]</span></p>
<p>where: - <span class="math inline">\(\text{spline}_{\ell,i,j}(x)\)</span> is the B-spline component - <span class="math inline">\(w_{\ell,i,j}\)</span> and <span class="math inline">\(b_{\ell,i,j}\)</span> are learnable parameters - The linear term <span class="math inline">\(b_{\ell,i,j} \cdot x\)</span> provides a residual connection</p>
</section>
</section>
<section id="optimization-and-training" class="level2">
<h2 class="anchored" data-anchor-id="optimization-and-training">Optimization and Training</h2>
<section id="loss-function" class="level3">
<h3 class="anchored" data-anchor-id="loss-function">Loss Function</h3>
<p>The training objective for KANs typically includes both accuracy and regularization terms:</p>
<p><span class="math display">\[\mathcal{L} = \mathcal{L}_{\text{data}} + \lambda_1 \mathcal{L}_{\text{sparse}} + \lambda_2 \mathcal{L}_{\text{smooth}}\]</span></p>
<p>where: - <span class="math inline">\(\mathcal{L}_{\text{data}}\)</span> is the standard prediction loss (MSE, cross-entropy, etc.) - <span class="math inline">\(\mathcal{L}_{\text{sparse}}\)</span> encourages sparsity in the network - <span class="math inline">\(\mathcal{L}_{\text{smooth}}\)</span> promotes smooth activation functions</p>
</section>
<section id="sparsity-regularization" class="level3">
<h3 class="anchored" data-anchor-id="sparsity-regularization">Sparsity Regularization</h3>
<p>To encourage interpretable networks, KANs use sparsity regularization:</p>
<p><span class="math display">\[
\mathcal{L}_{\text{sparse}} = \sum_{\ell,i,j} |w_{\ell,i,j}| + |b_{\ell,i,j}|
\]</span></p>
<p>This L1 penalty encourages many connections to become exactly zero, leading to sparse, interpretable networks.</p>
</section>
<section id="smoothness-regularization" class="level3">
<h3 class="anchored" data-anchor-id="smoothness-regularization">Smoothness Regularization</h3>
<p>To prevent overfitting and ensure smooth activation functions:</p>
<p><span class="math display">\[
\mathcal{L}_{\text{smooth}} = \sum_{\ell,i,j} \int \left(\frac{d^2}{dx^2} \phi_{\ell,i,j}(x)\right)^2 dx
\]</span></p>
<p>This penalizes high curvature in the learned functions, promoting smooth and generalizable representations.</p>
</section>
</section>
<section id="theoretical-properties" class="level2">
<h2 class="anchored" data-anchor-id="theoretical-properties">Theoretical Properties</h2>
<section id="universal-approximation" class="level3">
<h3 class="anchored" data-anchor-id="universal-approximation">Universal Approximation</h3>
<p>KANs inherit universal approximation properties from the Kolmogorov-Arnold theorem:</p>
<p><strong>Theorem</strong>: Given sufficient width and depth, KANs can approximate any continuous function on a compact domain to arbitrary accuracy.</p>
<p><strong>Proof Sketch</strong>: The constructive proof of the Kolmogorov-Arnold theorem shows that any continuous function can be represented in the KAN form. The B-spline parametrization provides the flexibility to approximate the required univariate functions.</p>
</section>
<section id="expressivity-analysis" class="level3">
<h3 class="anchored" data-anchor-id="expressivity-analysis">Expressivity Analysis</h3>
<p>The expressivity of KANs can be analyzed through several lenses:</p>
<section id="parameter-efficiency" class="level4">
<h4 class="anchored" data-anchor-id="parameter-efficiency">Parameter Efficiency</h4>
<p>For a function of <span class="math inline">\(n\)</span> variables requiring <span class="math inline">\(m\)</span> parameters in an MLP, a KAN might achieve similar approximation quality with fewer parameters due to its compositional structure.</p>
</section>
<section id="sample-complexity" class="level4">
<h4 class="anchored" data-anchor-id="sample-complexity">Sample Complexity</h4>
<p>The sample complexity of KANs is related to the intrinsic dimensionality of the target function rather than the ambient dimensionality, potentially providing advantages for high-dimensional problems with low-dimensional structure.</p>
</section>
</section>
<section id="approximation-rates" class="level3">
<h3 class="anchored" data-anchor-id="approximation-rates">Approximation Rates</h3>
<p>Under smoothness assumptions on the target function, KANs can achieve superior approximation rates:</p>
<p><strong>Theorem</strong>: For target functions with bounded mixed derivatives, KANs achieve approximation error <span class="math inline">\(O(n^{-r/d})\)</span> where <span class="math inline">\(r\)</span> is the smoothness parameter and <span class="math inline">\(d\)</span> is the intrinsic dimension.</p>
</section>
</section>
<section id="computational-complexity" class="level2">
<h2 class="anchored" data-anchor-id="computational-complexity">Computational Complexity</h2>
<section id="forward-pass-complexity" class="level3">
<h3 class="anchored" data-anchor-id="forward-pass-complexity">Forward Pass Complexity</h3>
<p>For a KAN with <span class="math inline">\(L\)</span> layers and width <span class="math inline">\(n\)</span>: - <strong>Time Complexity</strong>: <span class="math inline">\(O(L \cdot n^2 \cdot G)\)</span> where <span class="math inline">\(G\)</span> is the number of B-spline coefficients - <strong>Space Complexity</strong>: <span class="math inline">\(O(L \cdot n^2 \cdot G)\)</span> for parameter storage</p>
</section>
<section id="backward-pass-complexity" class="level3">
<h3 class="anchored" data-anchor-id="backward-pass-complexity">Backward Pass Complexity</h3>
<p>The gradient computation involves: - Gradients with respect to B-spline coefficients - Gradients with respect to residual connection parameters - Chain rule application through the compositional structure</p>
<p>The overall complexity remains <span class="math inline">\(O(L \cdot n^2 \cdot G)\)</span> for both forward and backward passes.</p>
</section>
</section>
<section id="interpretability-and-symbolic-regression" class="level2">
<h2 class="anchored" data-anchor-id="interpretability-and-symbolic-regression">Interpretability and Symbolic Regression</h2>
<section id="automatic-symbolification" class="level3">
<h3 class="anchored" data-anchor-id="automatic-symbolification">Automatic Symbolification</h3>
<p>One of the most remarkable features of KANs is their ability to discover symbolic representations:</p>
<section id="pruning-process" class="level4">
<h4 class="anchored" data-anchor-id="pruning-process">Pruning Process</h4>
<ol type="1">
<li><strong>Training</strong>: Train the full KAN with sparsity regularization</li>
<li><strong>Pruning</strong>: Remove connections with small weights</li>
<li><strong>Symbolification</strong>: Replace smooth functions with symbolic equivalents</li>
</ol>
</section>
<section id="symbol-discovery" class="level4">
<h4 class="anchored" data-anchor-id="symbol-discovery">Symbol Discovery</h4>
<p>KANs can automatically discover that learned functions correspond to elementary functions: - Polynomials: <span class="math inline">\(x^n\)</span> - Exponentials: <span class="math inline">\(e^x\)</span> - Trigonometric: <span class="math inline">\(\sin(x)\)</span>, <span class="math inline">\(\cos(x)\)</span> - Logarithmic: <span class="math inline">\(\log(x)\)</span></p>
</section>
</section>
<section id="mathematical-insights" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-insights">Mathematical Insights</h3>
<p>The learned functions often reveal mathematical structure:</p>
<p><span class="math display">\[
f(x_1, x_2) = \sin(x_1) + x_2^2
\]</span></p>
<p>might be discovered as:</p>
<p><span class="math display">\[
\text{KAN}(x_1, x_2) = \Phi_1(\phi_{1,1}(x_1)) + \Phi_2(\phi_{2,2}(x_2))
\]</span></p>
<p>where <span class="math inline">\(\phi_{1,1} \approx \sin\)</span> and <span class="math inline">\(\phi_{2,2} \approx x^2\)</span>.</p>
</section>
</section>
<section id="advanced-mathematical-concepts" class="level2">
<h2 class="anchored" data-anchor-id="advanced-mathematical-concepts">Advanced Mathematical Concepts</h2>
<section id="measure-theory-perspectives" class="level3">
<h3 class="anchored" data-anchor-id="measure-theory-perspectives">Measure Theory Perspectives</h3>
<p>From a measure-theoretic viewpoint, the Kolmogorov-Arnold theorem can be understood as a statement about the existence of certain measurable functions that achieve the required representation.</p>
</section>
<section id="functional-analysis" class="level3">
<h3 class="anchored" data-anchor-id="functional-analysis">Functional Analysis</h3>
<p>The space of functions representable by KANs forms a dense subset of <span class="math inline">\(C([0,1]^n)\)</span> under the uniform norm, providing a functional analytic foundation for their approximation capabilities.</p>
</section>
<section id="information-theory" class="level3">
<h3 class="anchored" data-anchor-id="information-theory">Information Theory</h3>
<p>The representational efficiency of KANs can be analyzed through the lens of information theory, where the learned functions encode essential information about the target function’s structure.</p>
</section>
</section>
<section id="limitations-and-extensions" class="level2">
<h2 class="anchored" data-anchor-id="limitations-and-extensions">Limitations and Extensions</h2>
<section id="theoretical-limitations" class="level3">
<h3 class="anchored" data-anchor-id="theoretical-limitations">Theoretical Limitations</h3>
<ol type="1">
<li><strong>Constructive vs.&nbsp;Practical</strong>: The original Kolmogorov-Arnold theorem is non-constructive; practical KANs use approximations</li>
<li><strong>Smoothness Requirements</strong>: The theorem applies to continuous functions; practical considerations require differentiability</li>
<li><strong>Domain Restrictions</strong>: The theorem is stated for bounded domains; extensions to unbounded domains require careful treatment</li>
</ol>
</section>
<section id="recent-extensions" class="level3">
<h3 class="anchored" data-anchor-id="recent-extensions">Recent Extensions</h3>
<section id="multidimensional-kans" class="level4">
<h4 class="anchored" data-anchor-id="multidimensional-kans">Multidimensional KANs</h4>
<p>Extensions to handle tensor-valued inputs and outputs:</p>
<p><span class="math display">\[
\text{Tensor-KAN}: \mathbb{R}^{n_1 \times n_2 \times \cdots} \to \mathbb{R}^{m_1 \times m_2 \times \cdots}
\]</span></p>
</section>
<section id="convolutional-kans" class="level4">
<h4 class="anchored" data-anchor-id="convolutional-kans">Convolutional KANs</h4>
<p>Incorporating spatial structure through learnable convolution-like operations:</p>
<p><span class="math display">\[
\text{Conv-KAN}(x) = \sum_{i,j} \phi_{i,j}(x * k_{i,j})
\]</span></p>
<p>where <span class="math inline">\(k_{i,j}\)</span> are learnable kernels and <span class="math inline">\(\phi_{i,j}\)</span> are learnable activation functions.</p>
</section>
</section>
</section>
<section id="future-directions" class="level2">
<h2 class="anchored" data-anchor-id="future-directions">Future Directions</h2>
<section id="theoretical-developments" class="level3">
<h3 class="anchored" data-anchor-id="theoretical-developments">Theoretical Developments</h3>
<ol type="1">
<li><strong>Approximation Theory</strong>: Tighter bounds on approximation rates</li>
<li><strong>Optimization Theory</strong>: Convergence guarantees for KAN training</li>
<li><strong>Generalization Theory</strong>: Sample complexity bounds for KANs</li>
</ol>
</section>
<section id="practical-innovations" class="level3">
<h3 class="anchored" data-anchor-id="practical-innovations">Practical Innovations</h3>
<ol type="1">
<li><strong>Efficient Implementations</strong>: GPU-optimized B-spline evaluations</li>
<li><strong>Architecture Search</strong>: Automated design of KAN topologies</li>
<li><strong>Hybrid Models</strong>: Combinations of KANs with other architectures</li>
</ol>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Kolmogorov-Arnold Networks represent a fundamental shift in neural network design, moving from fixed activation functions to learnable univariate functions. The mathematical foundations, rooted in the profound insights of Kolmogorov and Arnold, provide both theoretical guarantees and practical advantages. The ability to automatically discover symbolic representations while maintaining universal approximation capabilities makes KANs a powerful tool for both machine learning and mathematical discovery.</p>
<p>The interplay between classical approximation theory and modern deep learning exemplified by KANs suggests that there are still fundamental insights to be gained by revisiting classical mathematical results through the lens of contemporary computational capabilities. As we continue to develop and refine these networks, we can expect them to play an increasingly important role in both theoretical understanding and practical applications of neural computation.</p>
<p>The mathematical elegance of KANs lies not just in their theoretical foundations, but in their ability to bridge the gap between approximation theory and interpretable machine learning, offering a path toward more transparent and mathematically principled artificial intelligence systems.</p>



</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>© 2025 Krishnatheja Vanka</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>