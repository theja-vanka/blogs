<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Krishnatheja Vanka">
<meta name="dcterms.date" content="2025-07-15">

<title>GShard: Scaling Giant Neural Networks with Conditional Computation – Krishnatheja Vanka’s Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-2fef5ea3f8957b3e4ecc936fc74692ca.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-62c28fcd870f55f61984f019219cbd7c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-5a86c4bd0c1f9981a70f893fdae069f2.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-62c28fcd870f55f61984f019219cbd7c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles/styles.css">
</head>

<body class="floating nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Krishnatheja Vanka’s Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/theja-vanka"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-right">
      <h1 class="title">GShard: Scaling Giant Neural Networks with Conditional Computation</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">research</div>
                <div class="quarto-category">advanced</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta column-page-right">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Krishnatheja Vanka </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 15, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#gshard-scaling-giant-neural-networks-with-conditional-computation" id="toc-gshard-scaling-giant-neural-networks-with-conditional-computation" class="nav-link active" data-scroll-target="#gshard-scaling-giant-neural-networks-with-conditional-computation">GShard: Scaling Giant Neural Networks with Conditional Computation</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#background-and-motivation" id="toc-background-and-motivation" class="nav-link" data-scroll-target="#background-and-motivation">Background and Motivation</a>
  <ul class="collapse">
  <li><a href="#the-scaling-challenge" id="toc-the-scaling-challenge" class="nav-link" data-scroll-target="#the-scaling-challenge">The Scaling Challenge</a></li>
  <li><a href="#conditional-computation-as-a-solution" id="toc-conditional-computation-as-a-solution" class="nav-link" data-scroll-target="#conditional-computation-as-a-solution">Conditional Computation as a Solution</a></li>
  </ul></li>
  <li><a href="#gshard-architecture" id="toc-gshard-architecture" class="nav-link" data-scroll-target="#gshard-architecture">GShard Architecture</a>
  <ul class="collapse">
  <li><a href="#core-components" id="toc-core-components" class="nav-link" data-scroll-target="#core-components">Core Components</a></li>
  <li><a href="#mixture-of-experts-implementation" id="toc-mixture-of-experts-implementation" class="nav-link" data-scroll-target="#mixture-of-experts-implementation">Mixture-of-Experts Implementation</a></li>
  <li><a href="#parallelization-strategy" id="toc-parallelization-strategy" class="nav-link" data-scroll-target="#parallelization-strategy">Parallelization Strategy</a></li>
  </ul></li>
  <li><a href="#training-methodology" id="toc-training-methodology" class="nav-link" data-scroll-target="#training-methodology">Training Methodology</a>
  <ul class="collapse">
  <li><a href="#distributed-training-challenges" id="toc-distributed-training-challenges" class="nav-link" data-scroll-target="#distributed-training-challenges">Distributed Training Challenges</a></li>
  <li><a href="#optimization-techniques" id="toc-optimization-techniques" class="nav-link" data-scroll-target="#optimization-techniques">Optimization Techniques</a></li>
  </ul></li>
  <li><a href="#performance-analysis" id="toc-performance-analysis" class="nav-link" data-scroll-target="#performance-analysis">Performance Analysis</a>
  <ul class="collapse">
  <li><a href="#computational-efficiency" id="toc-computational-efficiency" class="nav-link" data-scroll-target="#computational-efficiency">Computational Efficiency</a></li>
  <li><a href="#quality-and-capability" id="toc-quality-and-capability" class="nav-link" data-scroll-target="#quality-and-capability">Quality and Capability</a></li>
  </ul></li>
  <li><a href="#implementation-details" id="toc-implementation-details" class="nav-link" data-scroll-target="#implementation-details">Implementation Details</a>
  <ul class="collapse">
  <li><a href="#technical-architecture" id="toc-technical-architecture" class="nav-link" data-scroll-target="#technical-architecture">Technical Architecture</a></li>
  <li><a href="#hyperparameter-considerations" id="toc-hyperparameter-considerations" class="nav-link" data-scroll-target="#hyperparameter-considerations">Hyperparameter Considerations</a></li>
  </ul></li>
  <li><a href="#applications-and-use-cases" id="toc-applications-and-use-cases" class="nav-link" data-scroll-target="#applications-and-use-cases">Applications and Use Cases</a>
  <ul class="collapse">
  <li><a href="#machine-translation" id="toc-machine-translation" class="nav-link" data-scroll-target="#machine-translation">Machine Translation</a></li>
  <li><a href="#language-modeling" id="toc-language-modeling" class="nav-link" data-scroll-target="#language-modeling">Language Modeling</a></li>
  </ul></li>
  <li><a href="#comparison-with-other-approaches" id="toc-comparison-with-other-approaches" class="nav-link" data-scroll-target="#comparison-with-other-approaches">Comparison with Other Approaches</a>
  <ul class="collapse">
  <li><a href="#dense-models" id="toc-dense-models" class="nav-link" data-scroll-target="#dense-models">Dense Models</a></li>
  <li><a href="#other-sparse-approaches" id="toc-other-sparse-approaches" class="nav-link" data-scroll-target="#other-sparse-approaches">Other Sparse Approaches</a></li>
  </ul></li>
  <li><a href="#limitations-and-challenges" id="toc-limitations-and-challenges" class="nav-link" data-scroll-target="#limitations-and-challenges">Limitations and Challenges</a>
  <ul class="collapse">
  <li><a href="#technical-limitations" id="toc-technical-limitations" class="nav-link" data-scroll-target="#technical-limitations">Technical Limitations</a></li>
  <li><a href="#scaling-challenges" id="toc-scaling-challenges" class="nav-link" data-scroll-target="#scaling-challenges">Scaling Challenges</a></li>
  </ul></li>
  <li><a href="#future-directions" id="toc-future-directions" class="nav-link" data-scroll-target="#future-directions">Future Directions</a>
  <ul class="collapse">
  <li><a href="#architectural-innovations" id="toc-architectural-innovations" class="nav-link" data-scroll-target="#architectural-innovations">Architectural Innovations</a></li>
  <li><a href="#optimization-improvements" id="toc-optimization-improvements" class="nav-link" data-scroll-target="#optimization-improvements">Optimization Improvements</a></li>
  <li><a href="#applications-and-domains" id="toc-applications-and-domains" class="nav-link" data-scroll-target="#applications-and-domains">Applications and Domains</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-right" id="quarto-document-content">






<section id="gshard-scaling-giant-neural-networks-with-conditional-computation" class="level1">
<h1>GShard: Scaling Giant Neural Networks with Conditional Computation</h1>
<p><img src="gshard.png" class="img-fluid"></p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>GShard represents a pivotal advancement in neural network scaling, introduced by Google Research in 2020. This innovative approach addresses one of the most pressing challenges in deep learning: how to scale neural networks to unprecedented sizes while maintaining computational efficiency. By leveraging sparsely-gated mixture-of-experts (MoE) and sophisticated parallelization strategies, GShard enables the training of models with trillions of parameters using conditional computation.</p>
<p>The significance of GShard extends beyond mere parameter scaling. It fundamentally changes how we think about model capacity, computational efficiency, and distributed training. Rather than activating all parameters for every input, GShard selectively activates only a subset of experts, allowing for massive models that remain computationally tractable during inference and training.</p>
</section>
<section id="background-and-motivation" class="level2">
<h2 class="anchored" data-anchor-id="background-and-motivation">Background and Motivation</h2>
<section id="the-scaling-challenge" class="level3">
<h3 class="anchored" data-anchor-id="the-scaling-challenge">The Scaling Challenge</h3>
<p>Traditional neural network scaling follows a straightforward principle: more parameters generally lead to better performance. However, this approach faces significant limitations as model sizes grow exponentially. Dense models require all parameters to be activated for every input, creating computational bottlenecks that become increasingly prohibitive as models scale to hundreds of billions or trillions of parameters.</p>
<p>The computational cost of training and inference scales linearly with model size in dense architectures. For a transformer model with N parameters, each forward pass requires O(N) operations, regardless of the input complexity. This relationship creates unsustainable resource requirements as models grow larger.</p>
</section>
<section id="conditional-computation-as-a-solution" class="level3">
<h3 class="anchored" data-anchor-id="conditional-computation-as-a-solution">Conditional Computation as a Solution</h3>
<p>Conditional computation offers an elegant solution to this scaling challenge. Instead of activating all parameters for every input, conditional computation selectively activates only relevant portions of the network. This approach allows for models with massive parameter counts while maintaining reasonable computational costs.</p>
<p>The mixture-of-experts paradigm serves as the foundation for GShard’s conditional computation approach. By decomposing the model into specialized expert networks and learning to route inputs to appropriate experts, GShard achieves sub-linear scaling of computational cost with respect to model size.</p>
</section>
</section>
<section id="gshard-architecture" class="level2">
<h2 class="anchored" data-anchor-id="gshard-architecture">GShard Architecture</h2>
<section id="core-components" class="level3">
<h3 class="anchored" data-anchor-id="core-components">Core Components</h3>
<p>GShard’s architecture centers around several key innovations that work together to enable efficient scaling:</p>
<p><strong>Sparsely-Gated Mixture-of-Experts (MoE)</strong>: The fundamental building block of GShard replaces dense feed-forward layers in transformer architectures with MoE layers. Each MoE layer consists of multiple expert networks and a gating network that determines which experts to activate for each input.</p>
<p><strong>Expert Networks</strong>: Individual expert networks are typically simple feed-forward networks, similar to the feed-forward layers in standard transformers. The key difference lies in their selective activation rather than their architecture. Each expert specializes in processing certain types of inputs, though this specialization emerges naturally during training rather than being explicitly programmed.</p>
<p><strong>Gating Network</strong>: The gating network serves as the routing mechanism, determining which experts should process each input token. This network learns to make routing decisions based on the input representation, typically selecting only a small subset of available experts for each token.</p>
</section>
<section id="mixture-of-experts-implementation" class="level3">
<h3 class="anchored" data-anchor-id="mixture-of-experts-implementation">Mixture-of-Experts Implementation</h3>
<p>The MoE layer in GShard operates through a sophisticated gating mechanism that balances computational efficiency with model expressiveness. For each input token, the gating network computes a probability distribution over all available experts. Rather than using all experts, GShard selects only the top-k experts (typically k=2) for each token, significantly reducing computational requirements.</p>
<p>The gating function can be expressed mathematically as:</p>
<p><span class="math display">\[G(x) = \text{Softmax}(x \cdot W_g)\]</span></p>
<p>Where <span class="math inline">\(x\)</span> represents the input token embedding and <span class="math inline">\(W_g\)</span> represents the learned gating weights. The top-k selection mechanism ensures that only the most relevant experts are activated, while the softmax normalization maintains proper probability distributions.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Load Balancing
</div>
</div>
<div class="callout-body-container callout-body">
<p>One critical challenge in MoE architectures is ensuring balanced load distribution across experts. Without proper load balancing, some experts may receive disproportionately more training examples, leading to underutilization of model capacity. GShard addresses this through auxiliary loss functions that encourage balanced expert utilization.</p>
</div>
</div>
<p><strong>Expert Capacity</strong>: To prevent memory overflow and ensure predictable computational costs, GShard implements expert capacity limits. Each expert can process a maximum number of tokens per batch, with overflow tokens either dropped or routed to alternative experts.</p>
</section>
<section id="parallelization-strategy" class="level3">
<h3 class="anchored" data-anchor-id="parallelization-strategy">Parallelization Strategy</h3>
<p>GShard’s parallelization approach represents a significant departure from traditional data parallelism. The system employs a hybrid strategy that combines expert parallelism with data parallelism to efficiently distribute computation across multiple devices.</p>
<p><strong>Expert Parallelism</strong>: Different experts are placed on different devices, allowing for parallel processing of different expert computations. This approach scales naturally with the number of experts and available devices.</p>
<p><strong>Data Parallelism</strong>: Within each expert, traditional data parallelism is employed to process multiple examples simultaneously. This hybrid approach maximizes hardware utilization while maintaining efficient communication patterns.</p>
<p><strong>Communication Optimization</strong>: The routing of tokens to experts requires careful communication optimization. GShard implements efficient all-to-all communication patterns that minimize the overhead of token routing across devices.</p>
</section>
</section>
<section id="training-methodology" class="level2">
<h2 class="anchored" data-anchor-id="training-methodology">Training Methodology</h2>
<section id="distributed-training-challenges" class="level3">
<h3 class="anchored" data-anchor-id="distributed-training-challenges">Distributed Training Challenges</h3>
<p>Training GShard models presents unique challenges compared to traditional dense models. The sparse activation patterns create irregular communication requirements, and the load balancing constraints require careful optimization to prevent training instabilities.</p>
<p><strong>Gradient Synchronization</strong>: Unlike dense models where gradients can be synchronized using standard all-reduce operations, GShard requires more sophisticated gradient synchronization strategies. Only the experts that were activated during the forward pass need gradient updates, creating sparse gradient patterns that require efficient handling.</p>
<p><strong>Load Balancing During Training</strong>: Maintaining balanced expert utilization during training is crucial for model performance. GShard employs auxiliary loss functions that penalize imbalanced expert usage, encouraging the gating network to distribute load evenly across all experts.</p>
<p><strong>Stability Considerations</strong>: The discrete routing decisions in MoE architectures can create training instabilities. GShard addresses these challenges through careful initialization strategies, gradient clipping, and regularization techniques that promote stable training dynamics.</p>
</section>
<section id="optimization-techniques" class="level3">
<h3 class="anchored" data-anchor-id="optimization-techniques">Optimization Techniques</h3>
<p>GShard incorporates several optimization techniques specifically designed for MoE architectures:</p>
<p><strong>Auxiliary Loss Functions</strong>: These loss functions encourage balanced expert utilization and prevent the collapse of expert diversity. The auxiliary loss is typically added to the main task loss with a small weighting factor.</p>
<p><strong>Expert Dropout</strong>: During training, GShard sometimes randomly drops entire experts to prevent over-reliance on specific experts and improve model robustness. This technique is analogous to traditional dropout but operates at the expert level.</p>
<p><strong>Capacity Factor Tuning</strong>: The capacity factor determines how many tokens each expert can process. Tuning this parameter involves balancing computational efficiency with model expressiveness, as higher capacity factors allow more flexible routing but increase computational costs.</p>
</section>
</section>
<section id="performance-analysis" class="level2">
<h2 class="anchored" data-anchor-id="performance-analysis">Performance Analysis</h2>
<section id="computational-efficiency" class="level3">
<h3 class="anchored" data-anchor-id="computational-efficiency">Computational Efficiency</h3>
<p>GShard’s primary advantage lies in its computational efficiency compared to dense models of equivalent parameter count. By activating only a subset of experts for each input, GShard achieves sub-linear scaling of computational cost with respect to model size.</p>
<p><strong>FLOPs Analysis</strong>: For a GShard model with E experts and top-k routing, the computational cost per token is approximately k/E times that of a dense model with equivalent total parameters. This represents a significant efficiency gain, especially as E increases.</p>
<p><strong>Memory Efficiency</strong>: While GShard models have large parameter counts, the memory requirements during inference are determined by the number of activated experts rather than the total parameter count. This allows for efficient deployment of very large models.</p>
<p><strong>Scaling Behavior</strong>: Empirical results demonstrate that GShard models can achieve better performance than dense models while using less computational resources. This scaling behavior enables the training of models that would be computationally prohibitive in dense architectures.</p>
</section>
<section id="quality-and-capability" class="level3">
<h3 class="anchored" data-anchor-id="quality-and-capability">Quality and Capability</h3>
<p>GShard has demonstrated impressive performance across various natural language processing tasks, particularly in machine translation and language modeling. The model’s ability to scale to trillions of parameters while maintaining computational efficiency has enabled breakthrough results in several domains.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Performance Metrics
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Translation Quality</strong>: GShard models have achieved state-of-the-art results on numerous machine translation benchmarks</li>
<li><strong>Language Modeling</strong>: Improved perplexity scores compared to dense models with equivalent computational budgets</li>
<li><strong>Generalization</strong>: Better generalization through expert specialization</li>
</ul>
</div>
</div>
<p><strong>Translation Quality</strong>: GShard models have achieved state-of-the-art results on numerous machine translation benchmarks, demonstrating that the MoE approach can effectively scale model capacity without sacrificing translation quality.</p>
<p><strong>Language Modeling</strong>: In language modeling tasks, GShard models have shown improved perplexity scores compared to dense models with equivalent computational budgets, indicating more efficient use of model capacity.</p>
<p><strong>Generalization</strong>: The sparse activation patterns in GShard models appear to promote better generalization, as different experts can specialize in different aspects of the input distribution.</p>
</section>
</section>
<section id="implementation-details" class="level2">
<h2 class="anchored" data-anchor-id="implementation-details">Implementation Details</h2>
<section id="technical-architecture" class="level3">
<h3 class="anchored" data-anchor-id="technical-architecture">Technical Architecture</h3>
<p>GShard’s implementation requires careful consideration of several technical aspects:</p>
<p><strong>Framework Integration</strong>: GShard builds upon the Mesh-TensorFlow framework, which provides the necessary infrastructure for efficient distributed training of MoE models. The framework handles the complex communication patterns required for expert routing and gradient synchronization.</p>
<p><strong>Device Placement</strong>: The placement of experts across devices requires careful planning to minimize communication overhead while maximizing computational efficiency. GShard employs sophisticated placement strategies that consider both computational load and communication patterns.</p>
<p><strong>Memory Management</strong>: Managing memory efficiently across experts requires careful attention to buffer sizes, expert capacities, and gradient accumulation strategies. GShard implements dynamic memory management techniques that adapt to varying load distributions.</p>
</section>
<section id="hyperparameter-considerations" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameter-considerations">Hyperparameter Considerations</h3>
<p>Training GShard models requires careful tuning of several hyperparameters specific to MoE architectures:</p>
<div id="tbl-hyperparameters" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-hyperparameters-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Key hyperparameters for GShard training
</figcaption>
<div aria-describedby="tbl-hyperparameters-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 38%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Typical Range</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Number of Experts</td>
<td>8-2048</td>
<td>Affects model capacity and computational efficiency</td>
</tr>
<tr class="even">
<td>Capacity Factor</td>
<td>1.0-2.0</td>
<td>Determines tokens per expert</td>
</tr>
<tr class="odd">
<td>Auxiliary Loss Weight</td>
<td>0.01-0.1</td>
<td>Balances task performance and expert utilization</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><strong>Number of Experts</strong>: The number of experts represents a fundamental design choice that affects both model capacity and computational efficiency. More experts provide greater capacity but require more sophisticated load balancing.</p>
<p><strong>Capacity Factor</strong>: This parameter determines how many tokens each expert can process and directly impacts both computational cost and model expressiveness. Typical values range from 1.0 to 2.0, with higher values allowing more flexible routing.</p>
<p><strong>Auxiliary Loss Weight</strong>: The weighting of auxiliary loss functions affects the balance between task performance and expert utilization. This parameter requires careful tuning to achieve optimal results.</p>
</section>
</section>
<section id="applications-and-use-cases" class="level2">
<h2 class="anchored" data-anchor-id="applications-and-use-cases">Applications and Use Cases</h2>
<section id="machine-translation" class="level3">
<h3 class="anchored" data-anchor-id="machine-translation">Machine Translation</h3>
<p>GShard has demonstrated particular success in machine translation applications, where the model’s ability to scale to massive parameter counts has enabled breakthrough performance on challenging translation tasks.</p>
<p><strong>Multilingual Translation</strong>: GShard’s expert architecture naturally lends itself to multilingual translation, where different experts can specialize in different language pairs or linguistic phenomena. This specialization enables more efficient processing of diverse linguistic inputs.</p>
<p><strong>Low-Resource Languages</strong>: The increased model capacity provided by GShard has proven particularly beneficial for low-resource language translation, where the additional parameters can compensate for limited training data.</p>
<p><strong>Domain Adaptation</strong>: Different experts can specialize in different domains, allowing GShard models to handle diverse translation contexts more effectively than dense models.</p>
</section>
<section id="language-modeling" class="level3">
<h3 class="anchored" data-anchor-id="language-modeling">Language Modeling</h3>
<p>GShard has also shown impressive results in language modeling tasks, where the model’s ability to scale efficiently has enabled training of extremely large language models.</p>
<p><strong>Text Generation</strong>: The sparse activation patterns in GShard models appear to promote more diverse and coherent text generation, as different experts can specialize in different aspects of language generation.</p>
<p><strong>Few-Shot Learning</strong>: The increased model capacity provided by GShard has improved few-shot learning performance, enabling better adaptation to new tasks with minimal examples.</p>
<p><strong>Reasoning Tasks</strong>: GShard models have demonstrated improved performance on reasoning tasks that require complex logical operations, suggesting that the expert specialization enables more sophisticated reasoning capabilities.</p>
</section>
</section>
<section id="comparison-with-other-approaches" class="level2">
<h2 class="anchored" data-anchor-id="comparison-with-other-approaches">Comparison with Other Approaches</h2>
<section id="dense-models" class="level3">
<h3 class="anchored" data-anchor-id="dense-models">Dense Models</h3>
<p>Compared to traditional dense models, GShard offers several key advantages:</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">Advantages</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">Disadvantages</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<ul>
<li><strong>Computational Efficiency</strong>: Better performance per FLOP than dense models</li>
<li><strong>Scalability</strong>: Sub-linear scaling of computational cost with model size</li>
<li><strong>Specialization</strong>: Natural expert specialization improves performance on diverse tasks</li>
</ul>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<ul>
<li><strong>Simplicity</strong>: Dense models are conceptually simpler and easier to implement</li>
<li><strong>Hardware Optimization</strong>: Existing optimizations are designed for dense computations</li>
<li><strong>Predictable Performance</strong>: Dense models have more predictable requirements</li>
</ul>
</div>
</div>
</div>
</section>
<section id="other-sparse-approaches" class="level3">
<h3 class="anchored" data-anchor-id="other-sparse-approaches">Other Sparse Approaches</h3>
<p>GShard represents one approach to sparse neural networks, but several alternative methods exist:</p>
<p><strong>Magnitude-Based Pruning</strong>: Traditional pruning approaches remove weights based on magnitude, but these methods typically don’t achieve the same level of sparsity as GShard while maintaining performance.</p>
<p><strong>Structured Sparsity</strong>: Other approaches enforce structured sparsity patterns that are more hardware-friendly but may be less flexible than GShard’s learned sparsity.</p>
<p><strong>Dynamic Sparsity</strong>: Some approaches learn to dynamically adjust sparsity patterns during training, offering different trade-offs between flexibility and efficiency.</p>
</section>
</section>
<section id="limitations-and-challenges" class="level2">
<h2 class="anchored" data-anchor-id="limitations-and-challenges">Limitations and Challenges</h2>
<section id="technical-limitations" class="level3">
<h3 class="anchored" data-anchor-id="technical-limitations">Technical Limitations</h3>
<p>Despite its advantages, GShard faces several technical limitations:</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Limitations
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Communication Overhead</strong>: All-to-all communication can become a bottleneck</li>
<li><strong>Load Balancing Complexity</strong>: Requires sophisticated auxiliary loss functions</li>
<li><strong>Hardware Utilization</strong>: Irregular computation patterns may lead to suboptimal hardware use</li>
<li><strong>Debugging Complexity</strong>: Sparse activation patterns make analysis challenging</li>
</ul>
</div>
</div>
</section>
<section id="scaling-challenges" class="level3">
<h3 class="anchored" data-anchor-id="scaling-challenges">Scaling Challenges</h3>
<p>As GShard models scale to larger sizes, several challenges emerge:</p>
<p><strong>Expert Utilization</strong>: Ensuring efficient utilization of all experts becomes increasingly difficult as the number of experts grows.</p>
<p><strong>Communication Scaling</strong>: The communication requirements for expert routing may not scale favorably with very large numbers of experts.</p>
<p><strong>Memory Constraints</strong>: While GShard is more memory-efficient than dense models, very large models still face memory limitations, especially during training.</p>
</section>
</section>
<section id="future-directions" class="level2">
<h2 class="anchored" data-anchor-id="future-directions">Future Directions</h2>
<section id="architectural-innovations" class="level3">
<h3 class="anchored" data-anchor-id="architectural-innovations">Architectural Innovations</h3>
<p>Several promising directions for future development include:</p>
<p><strong>Hierarchical Experts</strong>: Organizing experts in hierarchical structures could improve routing efficiency and enable more sophisticated specialization patterns.</p>
<p><strong>Dynamic Expert Creation</strong>: Allowing the model to dynamically create new experts during training could improve adaptability to new tasks and domains.</p>
<p><strong>Cross-Layer Expert Sharing</strong>: Sharing experts across different layers could reduce parameter counts while maintaining model expressiveness.</p>
</section>
<section id="optimization-improvements" class="level3">
<h3 class="anchored" data-anchor-id="optimization-improvements">Optimization Improvements</h3>
<p>Future work could focus on improving the optimization of GShard models:</p>
<p><strong>Better Load Balancing</strong>: Developing more sophisticated load balancing techniques could improve expert utilization and model performance.</p>
<p><strong>Adaptive Routing</strong>: Learning to adaptively adjust routing strategies based on input characteristics could improve efficiency.</p>
<p><strong>Hardware-Aware Design</strong>: Designing MoE architectures that are more compatible with existing hardware could improve practical deployment.</p>
</section>
<section id="applications-and-domains" class="level3">
<h3 class="anchored" data-anchor-id="applications-and-domains">Applications and Domains</h3>
<p>GShard’s approach could be extended to new domains and applications:</p>
<p><strong>Computer Vision</strong>: Adapting MoE architectures for computer vision tasks could enable more efficient processing of visual data.</p>
<p><strong>Multimodal Learning</strong>: Combining GShard with multimodal architectures could enable more efficient processing of diverse input types.</p>
<p><strong>Reinforcement Learning</strong>: Applying MoE principles to reinforcement learning could enable more efficient learning in complex environments.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>GShard represents a significant breakthrough in neural network scaling, demonstrating that it’s possible to train models with trillions of parameters while maintaining computational efficiency. The combination of sparsely-gated mixture-of-experts with sophisticated parallelization strategies has opened new possibilities for model scaling that were previously computationally prohibitive.</p>
<p>The success of GShard has fundamental implications for the future of deep learning. It suggests that the path to more capable AI systems may lie not just in scaling model size, but in developing more efficient architectures that can leverage massive parameter counts through conditional computation.</p>
<p>While GShard faces certain limitations and challenges, its core innovations have established a new paradigm for neural network architecture design. The principles underlying GShard—sparse activation, expert specialization, and efficient parallelization—are likely to influence future developments in large-scale machine learning.</p>
<p>As the field continues to evolve, GShard’s contributions to our understanding of scalable neural architectures will undoubtedly continue to shape the development of increasingly capable and efficient AI systems. The model’s demonstration that trillion-parameter models can be both practical and effective has fundamentally changed our perspective on what’s possible in neural network scaling.</p>
<p>The ongoing research building upon GShard’s foundations promises to unlock even greater capabilities in artificial intelligence, potentially leading to systems that can process and understand information at unprecedented scales while remaining computationally efficient. This balance between scale and efficiency represents a crucial step toward more practical and deployable AI systems that can benefit a broader range of applications and users.</p>



</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/theja-vanka\.github\.io\/blogs\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>© 2025 Krishnatheja Vanka</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>