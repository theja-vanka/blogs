<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Krishnatheja Vanka">
<meta name="dcterms.date" content="2025-05-24">

<title>Vision Transformers (ViT): A Simple Guide – Krishnatheja Vanka’s Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-7cf12f9d5c5caf5e13008aedb6606350.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Krishnatheja Vanka’s Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/theja-vanka"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Vision Transformers (ViT): A Simple Guide</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">research</div>
                <div class="quarto-category">beginner</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Krishnatheja Vanka </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 24, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="vision-transformers-vit-a-simple-guide" class="level1">
<h1>Vision Transformers (ViT): A Simple Guide</h1>
<p><img src="vit.png" class="img-fluid"></p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Vision Transformers (ViTs) represent a paradigm shift in computer vision, adapting the transformer architecture that revolutionized natural language processing for image classification and other visual tasks. Instead of relying on convolutional neural networks (CNNs), ViTs treat images as sequences of patches, applying the self-attention mechanism to understand spatial relationships and visual features.</p>
</section>
<section id="background-from-cnns-to-transformers" class="level2">
<h2 class="anchored" data-anchor-id="background-from-cnns-to-transformers">Background: From CNNs to Transformers</h2>
<p>Traditional computer vision relied heavily on Convolutional Neural Networks (CNNs), which process images through layers of convolutions that detect local features like edges, textures, and patterns. While effective, CNNs have limitations in capturing long-range dependencies across an image due to their local receptive fields.</p>
<p>Transformers, originally designed for language tasks, excel at modeling long-range dependencies through self-attention mechanisms. The key insight behind Vision Transformers was asking: “What if we could apply this powerful attention mechanism to images?”</p>
</section>
<section id="core-concept-images-as-sequences" class="level2">
<h2 class="anchored" data-anchor-id="core-concept-images-as-sequences">Core Concept: Images as Sequences</h2>
<p>The fundamental innovation of ViTs lies in treating images as sequences of patches rather than pixel grids. Here’s how this transformation works:</p>
<section id="image-patch-embedding" class="level3">
<h3 class="anchored" data-anchor-id="image-patch-embedding">Image Patch Embedding</h3>
<ol type="1">
<li><strong>Patch Division</strong>: An input image (typically 224×224 pixels) is divided into fixed-size patches (commonly 16×16 pixels), resulting in a sequence of patches</li>
<li><strong>Linear Projection</strong>: Each patch is flattened into a vector and linearly projected to create patch embeddings</li>
<li><strong>Position Encoding</strong>: Since transformers don’t inherently understand spatial relationships, positional encodings are added to maintain spatial information</li>
<li><strong>Classification Token</strong>: A special learnable [CLS] token is prepended to the sequence, similar to BERT’s approach</li>
</ol>
</section>
<section id="mathematical-formulation" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-formulation">Mathematical Formulation</h3>
<p>For an image of size H×W×C divided into patches of size P×P:</p>
<ul>
<li>Number of patches: N = (H×W)/P²</li>
<li>Each patch becomes a vector of size P²×C</li>
<li>After linear projection: embedding dimension D</li>
</ul>
</section>
</section>
<section id="architecture-components" class="level2">
<h2 class="anchored" data-anchor-id="architecture-components">Architecture Components</h2>
<section id="patch-embedding-layer" class="level3">
<h3 class="anchored" data-anchor-id="patch-embedding-layer">1. Patch Embedding Layer</h3>
<p>The patch embedding layer converts image patches into token embeddings that the transformer can process. This involves:</p>
<ul>
<li>Reshaping patches into vectors</li>
<li>Linear transformation to desired embedding dimension</li>
<li>Adding positional encodings</li>
</ul>
</section>
<section id="transformer-encoder" class="level3">
<h3 class="anchored" data-anchor-id="transformer-encoder">2. Transformer Encoder</h3>
<p>The core of ViT consists of standard transformer encoder blocks, each containing:</p>
<ul>
<li><strong>Multi-Head Self-Attention (MSA)</strong>: Allows patches to attend to all other patches</li>
<li><strong>Layer Normalization</strong>: Applied before both attention and MLP layers</li>
<li><strong>Multi-Layer Perceptron (MLP)</strong>: Two-layer feedforward network with GELU activation</li>
<li><strong>Residual Connections</strong>: Skip connections around both attention and MLP blocks</li>
</ul>
</section>
<section id="classification-head" class="level3">
<h3 class="anchored" data-anchor-id="classification-head">3. Classification Head</h3>
<p>The final component extracts the [CLS] token’s representation and passes it through:</p>
<ul>
<li>Layer normalization</li>
<li>Linear classifier to produce class predictions</li>
</ul>
</section>
</section>
<section id="self-attention-in-vision" class="level2">
<h2 class="anchored" data-anchor-id="self-attention-in-vision">Self-Attention in Vision</h2>
<p>The self-attention mechanism in ViTs operates differently from CNNs:</p>
<section id="attention-maps" class="level3">
<h3 class="anchored" data-anchor-id="attention-maps">Attention Maps</h3>
<ul>
<li>Each patch can attend to every other patch in the image</li>
<li>Attention weights reveal which parts of the image are most relevant for classification</li>
<li>This enables modeling of long-range spatial dependencies</li>
</ul>
</section>
<section id="global-context" class="level3">
<h3 class="anchored" data-anchor-id="global-context">Global Context</h3>
<p>Unlike CNNs that build up receptive fields gradually, ViTs have global receptive fields from the first layer, allowing immediate access to information across the entire image.</p>
</section>
</section>
<section id="training-considerations" class="level2">
<h2 class="anchored" data-anchor-id="training-considerations">Training Considerations</h2>
<section id="data-requirements" class="level3">
<h3 class="anchored" data-anchor-id="data-requirements">Data Requirements</h3>
<p>Vision Transformers typically require large amounts of training data to perform well:</p>
<ul>
<li><strong>Pre-training</strong>: Often trained on large datasets like ImageNet-21k or JFT-300M</li>
<li><strong>Fine-tuning</strong>: Then adapted to specific tasks with smaller datasets</li>
<li><strong>Data Efficiency</strong>: ViTs can be less data-efficient than CNNs when training from scratch</li>
</ul>
</section>
<section id="optimization-challenges" class="level3">
<h3 class="anchored" data-anchor-id="optimization-challenges">Optimization Challenges</h3>
<ul>
<li><strong>Initialization</strong>: Careful weight initialization is crucial</li>
<li><strong>Learning Rate</strong>: Often requires different learning rates for different components</li>
<li><strong>Regularization</strong>: Techniques like dropout and weight decay are important</li>
<li><strong>Warmup</strong>: Learning rate warmup is commonly used</li>
</ul>
</section>
</section>
<section id="variants-and-improvements" class="level2">
<h2 class="anchored" data-anchor-id="variants-and-improvements">Variants and Improvements</h2>
<section id="vit-variants" class="level3">
<h3 class="anchored" data-anchor-id="vit-variants">ViT Variants</h3>
<ul>
<li><strong>ViT-B/16, ViT-L/16, ViT-H/14</strong>: Different model sizes with varying patch sizes</li>
<li><strong>DeiT (Data-efficient ViT)</strong>: Improved training strategies for smaller datasets</li>
<li><strong>Swin Transformer</strong>: Hierarchical vision transformer with shifted windows</li>
<li><strong>CaiT</strong>: Class-Attention in Image Transformers with separate class attention</li>
</ul>
</section>
<section id="architectural-improvements" class="level3">
<h3 class="anchored" data-anchor-id="architectural-improvements">Architectural Improvements</h3>
<ul>
<li><strong>Hierarchical Processing</strong>: Multi-scale feature extraction</li>
<li><strong>Local Attention</strong>: Restricting attention to local neighborhoods</li>
<li><strong>Hybrid Models</strong>: Combining CNN features with transformer processing</li>
</ul>
</section>
</section>
<section id="advantages-of-vision-transformers" class="level2">
<h2 class="anchored" data-anchor-id="advantages-of-vision-transformers">Advantages of Vision Transformers</h2>
<section id="strengths" class="level3">
<h3 class="anchored" data-anchor-id="strengths">Strengths</h3>
<ul>
<li><strong>Long-range Dependencies</strong>: Natural ability to model global relationships</li>
<li><strong>Interpretability</strong>: Attention maps provide insights into model decisions</li>
<li><strong>Scalability</strong>: Performance improves with larger models and datasets</li>
<li><strong>Transfer Learning</strong>: Excellent pre-trained representations</li>
<li><strong>Architectural Simplicity</strong>: Unified architecture for various vision tasks</li>
</ul>
</section>
<section id="performance-benefits" class="level3">
<h3 class="anchored" data-anchor-id="performance-benefits">Performance Benefits</h3>
<ul>
<li>State-of-the-art results on image classification</li>
<li>Strong performance on object detection and segmentation when adapted</li>
<li>Excellent transfer learning capabilities across domains</li>
</ul>
</section>
</section>
<section id="limitations-and-challenges" class="level2">
<h2 class="anchored" data-anchor-id="limitations-and-challenges">Limitations and Challenges</h2>
<section id="current-limitations" class="level3">
<h3 class="anchored" data-anchor-id="current-limitations">Current Limitations</h3>
<ul>
<li><strong>Data Hunger</strong>: Requires large datasets for optimal performance</li>
<li><strong>Computational Cost</strong>: High memory and compute requirements</li>
<li><strong>Inductive Bias</strong>: Lacks CNN’s built-in spatial inductive biases</li>
<li><strong>Small Dataset Performance</strong>: Can underperform CNNs on limited data</li>
</ul>
</section>
<section id="ongoing-research-areas" class="level3">
<h3 class="anchored" data-anchor-id="ongoing-research-areas">Ongoing Research Areas</h3>
<ul>
<li>Improving data efficiency</li>
<li>Reducing computational requirements</li>
<li>Better integration of spatial inductive biases</li>
<li>Hybrid CNN-Transformer architectures</li>
</ul>
</section>
</section>
<section id="applications-beyond-classification" class="level2">
<h2 class="anchored" data-anchor-id="applications-beyond-classification">Applications Beyond Classification</h2>
<section id="computer-vision-tasks" class="level3">
<h3 class="anchored" data-anchor-id="computer-vision-tasks">Computer Vision Tasks</h3>
<ul>
<li><strong>Object Detection</strong>: DETR (Detection Transformer) applies transformers to detection</li>
<li><strong>Semantic Segmentation</strong>: Segmentation transformers for pixel-level predictions</li>
<li><strong>Image Generation</strong>: Vision transformers in generative models</li>
<li><strong>Video Analysis</strong>: Extending to temporal sequences</li>
</ul>
</section>
<section id="multimodal-applications" class="level3">
<h3 class="anchored" data-anchor-id="multimodal-applications">Multimodal Applications</h3>
<ul>
<li><strong>Vision-Language Models</strong>: CLIP and similar models combining vision and text</li>
<li><strong>Visual Question Answering</strong>: Integrating visual and textual understanding</li>
<li><strong>Image Captioning</strong>: Generating descriptions from visual content</li>
</ul>
</section>
</section>
<section id="implementation-considerations" class="level2">
<h2 class="anchored" data-anchor-id="implementation-considerations">Implementation Considerations</h2>
<section id="model-selection" class="level3">
<h3 class="anchored" data-anchor-id="model-selection">Model Selection</h3>
<p>Choose ViT variants based on:</p>
<ul>
<li>Available computational resources</li>
<li>Dataset size and characteristics</li>
<li>Required inference speed</li>
<li>Target accuracy requirements</li>
</ul>
</section>
<section id="training-strategy" class="level3">
<h3 class="anchored" data-anchor-id="training-strategy">Training Strategy</h3>
<ul>
<li>Use pre-trained models when possible</li>
<li>Apply appropriate data augmentation</li>
<li>Consider knowledge distillation for smaller models</li>
<li>Monitor for overfitting, especially on smaller datasets</li>
</ul>
</section>
<section id="optimization-tips" class="level3">
<h3 class="anchored" data-anchor-id="optimization-tips">Optimization Tips</h3>
<ul>
<li>Use mixed precision training to reduce memory usage</li>
<li>Implement gradient checkpointing for large models</li>
<li>Consider model parallelism for very large architectures</li>
<li>Apply appropriate regularization techniques</li>
</ul>
</section>
</section>
<section id="future-directions" class="level2">
<h2 class="anchored" data-anchor-id="future-directions">Future Directions</h2>
<section id="research-trends" class="level3">
<h3 class="anchored" data-anchor-id="research-trends">Research Trends</h3>
<ul>
<li><strong>Efficiency Improvements</strong>: Making ViTs more computationally efficient</li>
<li><strong>Architecture Search</strong>: Automated design of vision transformer architectures</li>
<li><strong>Self-Supervised Learning</strong>: Reducing dependence on labeled data</li>
<li><strong>Unified Architectures</strong>: Single models handling multiple vision tasks</li>
</ul>
</section>
<section id="emerging-applications" class="level3">
<h3 class="anchored" data-anchor-id="emerging-applications">Emerging Applications</h3>
<ul>
<li>Real-time vision applications</li>
<li>Mobile and edge deployment</li>
<li>Scientific imaging and medical applications</li>
<li>Autonomous systems and robotics</li>
</ul>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Vision Transformers represent a fundamental shift in computer vision, demonstrating that the transformer architecture’s success in NLP can extend to visual tasks. While they present challenges in terms of data requirements and computational cost, their ability to model long-range dependencies and achieve state-of-the-art performance makes them a crucial tool in modern computer vision.</p>
<p>The field continues to evolve rapidly, with ongoing research addressing current limitations while exploring new applications. As the technology matures, we can expect ViTs to become increasingly practical for a wider range of real-world applications, potentially reshaping how we approach visual understanding tasks.</p>
<p>Understanding Vision Transformers is essential for anyone working in modern computer vision, as they represent not just a new model architecture, but a new way of thinking about how machines can understand and process visual information.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/theja-vanka\.github\.io\/blogs\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>